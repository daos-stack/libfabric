diff --git a/Makefile.am b/Makefile.am
index 3074e625b..fff7c2c01 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -66,6 +66,7 @@ common_srcs =				\
 	prov/util/src/util_ns.c		\
 	prov/util/src/util_shm.c	\
 	prov/util/src/util_mem_monitor.c\
+	prov/util/src/util_mem_hooks.c	\
 	prov/util/src/util_mr_cache.c
 
 
@@ -210,8 +211,10 @@ real_man_pages = \
         man/man1/fi_pingpong.1 \
         man/man1/fi_strerror.1 \
         man/man3/fi_av.3 \
+        man/man3/fi_av_set.3 \
         man/man3/fi_cm.3 \
         man/man3/fi_cntr.3 \
+        man/man3/fi_collective.3 \
         man/man3/fi_control.3 \
         man/man3/fi_cq.3 \
         man/man3/fi_domain.3 \
@@ -236,6 +239,9 @@ real_man_pages = \
 dummy_man_pages = \
         man/man3/fi_accept.3 \
         man/man3/fi_alias.3 \
+        man/man3/fi_allgather.3 \
+        man/man3/fi_allreduce.3 \
+        man/man3/fi_alltoall.3 \
         man/man3/fi_atomic.3 \
         man/man3/fi_atomic_valid.3 \
         man/man3/fi_atomicmsg.3 \
@@ -246,7 +252,14 @@ dummy_man_pages = \
         man/man3/fi_av_lookup.3 \
         man/man3/fi_av_open.3 \
         man/man3/fi_av_remove.3 \
+        man/man3/fi_av_set_diff.3 \
+        man/man3/fi_av_set_insert.3 \
+        man/man3/fi_av_set_intersect.3 \
+        man/man3/fi_av_set_remove.3 \
+        man/man3/fi_av_set_union.3 \
         man/man3/fi_av_straddr.3 \
+        man/man3/fi_barrier.3 \
+        man/man3/fi_broadcast.3 \
         man/man3/fi_cancel.3 \
         man/man3/fi_close.3 \
         man/man3/fi_cntr_add.3 \
@@ -293,6 +306,7 @@ dummy_man_pages = \
         man/man3/fi_inject_write.3 \
         man/man3/fi_inject_writedata.3 \
         man/man3/fi_join.3 \
+        man/man3/fi_join_collective.3 \
         man/man3/fi_leave.3 \
         man/man3/fi_listen.3 \
         man/man3/fi_mr_bind.3 \
@@ -308,12 +322,15 @@ dummy_man_pages = \
         man/man3/fi_poll_add.3 \
         man/man3/fi_poll_del.3 \
         man/man3/fi_poll_open.3 \
+        man/man3/fi_query_atomic.3 \
+        man/man3/fi_query_collective.3 \
         man/man3/fi_read.3 \
         man/man3/fi_readmsg.3 \
         man/man3/fi_readv.3 \
         man/man3/fi_recv.3 \
         man/man3/fi_recvmsg.3 \
         man/man3/fi_recvv.3 \
+        man/man3/fi_reduce_scatter.3 \
         man/man3/fi_reject.3 \
         man/man3/fi_rx_addr.3 \
         man/man3/fi_rx_size_left.3 \
@@ -389,6 +406,7 @@ include prov/tcp/Makefile.include
 include prov/rstream/Makefile.include
 include prov/hook/Makefile.include
 include prov/hook/perf/Makefile.include
+include prov/hook/hook_debug/Makefile.include
 
 man_MANS = $(real_man_pages) $(prov_install_man_pages) $(dummy_man_pages)
 
diff --git a/configure.ac b/configure.ac
index aa0de0d2d..8d19361e2 100644
--- a/configure.ac
+++ b/configure.ac
@@ -6,7 +6,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ([2.60])
-AC_INIT([libfabric], [1.8.0], [ofiwg@lists.openfabrics.org])
+AC_INIT([libfabric], [1.9.0a1], [ofiwg@lists.openfabrics.org])
 AC_CONFIG_SRCDIR([src/fabric.c])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
@@ -451,6 +451,9 @@ AS_IF([test $have_uffd -eq 1],
 AC_DEFINE_UNQUOTED([HAVE_UFFD_UNMAP], [$have_uffd],
 	[Define to 1 if platform supports userfault fd unmap])
 
+dnl Check support to intercept syscalls
+AC_CHECK_HEADERS_ONCE(elf.h sys/auxv.h)
+
 dnl Provider-specific checks
 FI_PROVIDER_INIT
 FI_PROVIDER_SETUP([psm])
@@ -472,6 +475,7 @@ FI_PROVIDER_SETUP([bgq])
 FI_PROVIDER_SETUP([shm])
 FI_PROVIDER_SETUP([rstream])
 FI_PROVIDER_SETUP([perf])
+FI_PROVIDER_SETUP([hook_debug])
 FI_PROVIDER_FINI
 dnl Configure the .pc file
 FI_PROVIDER_SETUP_PC
diff --git a/contrib/cray/Jenkinsfile.verbs b/contrib/cray/Jenkinsfile.verbs
index 1d6d9d462..7ace8f1c8 100644
--- a/contrib/cray/Jenkinsfile.verbs
+++ b/contrib/cray/Jenkinsfile.verbs
@@ -3,6 +3,12 @@
 
 @Library(['CrayNetworkCI@master', 'dst-shared@master']) _
 
+if (!isBuildable()) {
+    echo "build request is not valid, skipping build"
+    currentBuild.result = 'SUCCESS'
+    return
+}
+
 pipeline {
     options {
         // Generic build options
@@ -269,37 +275,49 @@ pipeline {
             }
             post {
                 always {
-                    step ([$class: 'XUnitBuilder',
+                    step ([$class: 'XUnitPublisher',
                        thresholds: [
                             [$class: 'FailedThreshold', unstableThreshold: '0']],
                             tools: [[$class: 'JUnitType', pattern: "smoketests.xml"]]])
-                    step ([$class: 'XUnitBuilder',
+                    step ([$class: 'XUnitPublisher',
                        thresholds: [
                             [$class: 'FailedThreshold', unstableThreshold: '0']],
                             tools: [[$class: 'JUnitType', pattern: "*-rc.xml"]]])
-                    step ([$class: 'XUnitBuilder',
+                    step ([$class: 'XUnitPublisher',
                        thresholds: [
                             [$class: 'FailedThreshold', unstableThreshold: '0']],
                             tools: [[$class: 'JUnitType', pattern: "*-xrc.xml"]]])
-                    step ([$class: 'XUnitBuilder',
+                    step ([$class: 'XUnitPublisher',
                        thresholds: [
                             [$class: 'FailedThreshold', unstableThreshold: '0']],
                             tools: [[$class: 'JUnitType', pattern: "sft_test_results/RC/sft_*_test_results.xml"]]])
-                    step ([$class: 'XUnitBuilder',
+                    step ([$class: 'XUnitPublisher',
                        thresholds: [
                             [$class: 'FailedThreshold', unstableThreshold: '0']],
                             tools: [[$class: 'JUnitType', pattern: "sft_test_results/XRC/sft_*_test_results.xml"]]])
-                    step ([$class: 'XUnitBuilder',
+                    step ([$class: 'XUnitPublisher',
                        thresholds: [
                             [$class: 'FailedThreshold', unstableThreshold: '0']],
                             tools: [[$class: 'JUnitType', pattern: "mpi.xml"]]])
                 }
+                cleanup {
+                    echo "*** Test: Post: Cleanup: env.BRANCH_NAME: ${BRANCH_NAME} ***"
+                    script {
+                        if ( isInternalBuild() ) {
+                            echo "*** Test: Post: Cleanup: isInternalBuild: TRUE ***"
+                        } else {
+                            echo "*** Test: Post: Cleanup: isInternalBuild: FALSE ***"
+                        }
+                    }
+                    echo "*** Test: Post: Cleanup: currentBuild.currentResult: ${currentBuild.currentResult} ***"
+                }
             }
         }
         stage("Install Libfabric Build") {
             when {
                 allOf {
-                    expression { currentBuild.result == 'SUCCESS' } ;
+                    expression { currentBuild.currentResult == 'SUCCESS' } ;
+                    expression { isInternalBuild() } ;
                     anyOf {
                         expression { env.BRANCH_NAME == 'master' } ;
                         buildingTag() ;
@@ -319,7 +337,8 @@ pipeline {
         stage("Deploy") {
             when {
                 allOf {
-                    expression { currentBuild.result == 'SUCCESS' } ;
+                    expression { currentBuild.currentResult == 'SUCCESS' } ;
+                    expression { isInternalBuild() } ;
                     anyOf {
                         expression { env.BRANCH_NAME == 'master' } ;
                         buildingTag()
@@ -356,65 +375,10 @@ pipeline {
                         }
                     }
                 }
-                stage("Create RPMs") {
-                    steps {
-                        sh 'make dist-bzip2'
-                        sh '''$WORKSPACE/contrib/buildrpm/buildrpmLibfabric.sh \
-                                -i verbs \
-                                -i sockets \
-                                -smv \
-                                -r '--define "_prefix /opt/cray/libfabric/$version"' \
-                                -r '--define "modulefile_path /opt/cray/modulefiles"' \
-                                $(ls libfabric-*.tar.bz2)'''
-                    }
-                    post {
-                        success {
-                            stash name: 'rpms', includes: 'rpmbuild/RPMS/**/*'
-                            stash name: 'sources',  includes: 'rpmbuild/SOURCES/*'
-                        }
-                    }
-                }
-            }
-        }
-        stage('Publish') {
-            when {
-                allOf {
-                    expression { currentBuild.result == 'SUCCESS' } ;
-                    expression { return isRelease("${env.GIT_BRANCH}") }
-                }
-            }
-            agent {
-                node {
-                    label 'utility_pod'
-                }
-            }
-            steps {
-                container('utility') {
-                    sh 'tar -cvzf /tmp/libfabric-source.tar.gz --exclude "*.log" .'
-
-                    // publishes the source RPM to DST's Artifactory instance
-                    transfer(artifactName: '/tmp/libfabric-source.tar.gz')
-
-                    // Sends event to message bus to notify other builds
-                    publishEvents(["os-networking-libfabric-verbs-publish"])
-                }
             }
         }
     }
     post {
-        success {
-            script {
-                try {
-                    unstash 'rpms'
-                    unstash 'sources'
-                    archiveArtifacts 'rpmbuild/SOURCES/*'
-                    archiveArtifacts 'rpmbuild/RPMS/**/*'
-                }
-                catch (Exception e) {
-                    echo 'No rpms to archive'
-                }
-            }
-        }
         changed {
             script {
                 // send email when the state of the pipeline changes
diff --git a/fabtests/Makefile.am b/fabtests/Makefile.am
index bd7832a4b..f8670cdaa 100644
--- a/fabtests/Makefile.am
+++ b/fabtests/Makefile.am
@@ -40,6 +40,7 @@ bin_PROGRAMS = \
 	functional/fi_resmgmt_test \
 	functional/fi_rdm_atomic \
 	functional/fi_multi_recv \
+	functional/fi_bw \
 	benchmarks/fi_msg_pingpong \
 	benchmarks/fi_msg_bw \
 	benchmarks/fi_rma_bw \
@@ -56,7 +57,8 @@ bin_PROGRAMS = \
 	unit/fi_dom_test \
 	unit/fi_getinfo_test \
 	unit/fi_resource_freeing \
-	ubertest/fi_ubertest
+	ubertest/fi_ubertest	\
+	multinode/fi_multinode
 
 dist_bin_SCRIPTS = \
 	scripts/runfabtests.sh \
@@ -99,6 +101,7 @@ nobase_dist_config_DATA = \
 	test_configs/efa/efa.exclude
 
 noinst_LTLIBRARIES = libfabtests.la
+
 libfabtests_la_SOURCES = \
 	common/shared.c \
 	common/jsmn.c \
@@ -237,6 +240,10 @@ functional_fi_multi_recv_SOURCES = \
 	functional/multi_recv.c
 functional_fi_multi_recv_LDADD = libfabtests.la
 
+functional_fi_bw_SOURCES = \
+	functional/bw.c
+functional_fi_bw_LDADD = libfabtests.la
+
 benchmarks_fi_msg_pingpong_SOURCES = \
 	benchmarks/msg_pingpong.c \
 	$(benchmarks_srcs)
@@ -332,6 +339,19 @@ ubertest_fi_ubertest_SOURCES = \
 	ubertest/test_ctrl.c
 ubertest_fi_ubertest_LDADD = libfabtests.la
 
+multinode_fi_multinode_SOURCES = \
+	multinode/src/harness.c \
+	multinode/src/pattern/full_mesh.c \
+	multinode/include/pattern.h \
+	multinode/src/core.c \
+	multinode/include/core.h
+
+multinode_fi_multinode_LDADD = 	libfabtests.la
+
+multinode_fi_multinode_CFLAGS = \
+	$(AM_CFLAGS) \
+	-I$(srcdir)/multinode/include
+
 real_man_pages = \
 	 man/man7/fabtests.7
 
@@ -380,6 +400,7 @@ dummy_man_pages = \
 	man/man1/fi_getinfo_test.1 \
 	man/man1/fi_mr_test.1 \
 	man/man1/fi_resource_freeing.1 \
+	man/man1/fi_bw.1 \
 	man/man1/fi_ubertest.1
 
 nroff:
diff --git a/fabtests/Makefile.win b/fabtests/Makefile.win
index 1e59d3c56..9be5046ff 100644
--- a/fabtests/Makefile.win
+++ b/fabtests/Makefile.win
@@ -60,7 +60,7 @@ benchmarks: $(outdir)\msg_pingpong.exe $(outdir)\rdm_cntr_pingpong.exe \
 functional: $(outdir)\cq_data.exe $(outdir)\dgram.exe $(outdir)\dgram_waitset.exe $(outdir)\msg.exe \
 	$(outdir)\msg_epoll.exe $(outdir)\msg_sockets.exe \
 	$(outdir)\poll.exe $(outdir)\rdm.exe $(outdir)\rdm_rma_simple.exe $(outdir)\rdm_rma_trigger.exe \
-	$(outdir)\rdm_tagged_peek.exe $(outdir)\scalable_ep.exe $(outdir)\inj_complete.exe
+	$(outdir)\rdm_tagged_peek.exe $(outdir)\scalable_ep.exe $(outdir)\inj_complete.exe $(outdir)\bw.exe
 
 unit: $(outdir)\av_test.exe $(outdir)\dom_test.exe $(outdir)\eq_test.exe
 
@@ -105,6 +105,8 @@ $(outdir)\scalable_ep.exe: {functional}scalable_ep.c $(basedeps)
 
 $(outdir)\inj_complete.exe: {functional}inj_complete.c $(basedeps)
 
+$(outdir)\bw.exe: {functional}bw.c $(basedeps)
+
 $(outdir)\av_test.exe: {unit}av_test.c $(basedeps) {unit}common.c
 
 $(outdir)\dom_test.exe: {unit}dom_test.c $(basedeps) {unit}common.c
diff --git a/fabtests/benchmarks/benchmark_shared.c b/fabtests/benchmarks/benchmark_shared.c
index ba98bc6a3..26c37dc0a 100644
--- a/fabtests/benchmarks/benchmark_shared.c
+++ b/fabtests/benchmarks/benchmark_shared.c
@@ -70,16 +70,6 @@ void ft_benchmark_usage(void)
 			"# of iterations > window size");
 }
 
-int ft_bw_init(void)
-{
-	if (opts.window_size > 0) {
-		tx_ctx_arr = calloc(opts.window_size, sizeof(struct fi_context));
-		if (!tx_ctx_arr)
-			return -FI_ENOMEM;
-	}
-	return 0;
-}
-
 int pingpong(void)
 {
 	int ret, i;
@@ -177,7 +167,7 @@ int bandwidth(void)
 				ret = ft_inject(ep, remote_fi_addr, opts.transfer_size);
 			else
 				ret = ft_post_tx(ep, remote_fi_addr, opts.transfer_size,
-						 NO_CQ_DATA, &tx_ctx_arr[j]);
+						 NO_CQ_DATA, &tx_ctx_arr[j].context);
 			if (ret)
 				return ret;
 
@@ -196,7 +186,7 @@ int bandwidth(void)
 			if (i == opts.warmup_iterations)
 				ft_start();
 
-			ret = ft_post_rx(ep, opts.transfer_size, &tx_ctx_arr[j]);
+			ret = ft_post_rx(ep, opts.transfer_size, &rx_ctx_arr[j].context);
 			if (ret)
 				return ret;
 
@@ -260,13 +250,13 @@ int bandwidth_rma(enum ft_rma_opcodes rma_op, struct fi_rma_iov *remote)
 						opts.transfer_size, remote);
 			} else {
 				ret = ft_post_rma(rma_op, ep, opts.transfer_size,
-						remote,	&tx_ctx_arr[j]);
+						remote,	&tx_ctx_arr[j].context);
 			}
 			break;
 		case FT_RMA_WRITEDATA:
 			if (!opts.dst_addr) {
 				if (fi->rx_attr->mode & FI_RX_CQ_DATA)
-					ret = ft_post_rx(ep, 0, &tx_ctx_arr[j]);
+					ret = ft_post_rx(ep, 0, &rx_ctx_arr[j].context);
 				else
 					/* Just increment the seq # instead of
 					 * posting recv so that we wait for
@@ -284,13 +274,13 @@ int bandwidth_rma(enum ft_rma_opcodes rma_op, struct fi_rma_iov *remote)
 					ret = ft_post_rma(FT_RMA_WRITEDATA,
 							ep,
 							opts.transfer_size,
-							remote,	&tx_ctx_arr[j]);
+							remote,	&tx_ctx_arr[j].context);
 				}
 			}
 			break;
 		case FT_RMA_READ:
 			ret = ft_post_rma(FT_RMA_READ, ep, opts.transfer_size,
-					remote,	&tx_ctx_arr[j]);
+					remote,	&tx_ctx_arr[j].context);
 			break;
 		default:
 			FT_ERR("Unknown RMA op type\n");
diff --git a/fabtests/benchmarks/benchmark_shared.h b/fabtests/benchmarks/benchmark_shared.h
index 9bc31201f..ce1927b60 100644
--- a/fabtests/benchmarks/benchmark_shared.h
+++ b/fabtests/benchmarks/benchmark_shared.h
@@ -45,7 +45,6 @@ extern "C" {
 
 void ft_parse_benchmark_opts(int op, char *optarg);
 void ft_benchmark_usage(void);
-int ft_bw_init(void);
 int pingpong(void);
 int bandwidth(void);
 int bandwidth_rma(enum ft_rma_opcodes op, struct fi_rma_iov *remote);
diff --git a/fabtests/benchmarks/msg_bw.c b/fabtests/benchmarks/msg_bw.c
index 4183b4005..f273d6a1f 100644
--- a/fabtests/benchmarks/msg_bw.c
+++ b/fabtests/benchmarks/msg_bw.c
@@ -51,10 +51,6 @@ static int run(void)
 		return ret;
 	}
 
-	ret = ft_bw_init();
-	if (ret)
-		return ret;
-
 	if (!(opts.options & FT_OPT_SIZE)) {
 		for (i = 0; i < TEST_CNT; i++) {
 			if (!ft_use_size(i, opts.sizes_enabled))
diff --git a/fabtests/benchmarks/rdm_tagged_bw.c b/fabtests/benchmarks/rdm_tagged_bw.c
index 11fbac7e9..5252d4484 100644
--- a/fabtests/benchmarks/rdm_tagged_bw.c
+++ b/fabtests/benchmarks/rdm_tagged_bw.c
@@ -44,10 +44,6 @@ static int run(void)
 	if (ret)
 		return ret;
 
-	ret = ft_bw_init();
-	if (ret)
-		return ret;
-
 	if (!(opts.options & FT_OPT_SIZE)) {
 		for (i = 0; i < TEST_CNT; i++) {
 			if (!ft_use_size(i, opts.sizes_enabled))
diff --git a/fabtests/benchmarks/rma_bw.c b/fabtests/benchmarks/rma_bw.c
index 7a4d32a7e..e4351c89b 100644
--- a/fabtests/benchmarks/rma_bw.c
+++ b/fabtests/benchmarks/rma_bw.c
@@ -54,10 +54,6 @@ static int run(void)
 	if (ret)
 		return ret;
 
-	ret = ft_bw_init();
-	if (ret)
-		return ret;
-
 	ret = ft_exchange_keys(&remote);
 	if (ret)
 		return ret;
diff --git a/fabtests/common/shared.c b/fabtests/common/shared.c
index da5470f6d..44304ab99 100644
--- a/fabtests/common/shared.c
+++ b/fabtests/common/shared.c
@@ -66,7 +66,7 @@ struct fid_mc *mc;
 
 struct fid_mr no_mr;
 struct fi_context tx_ctx, rx_ctx;
-struct fi_context *tx_ctx_arr = NULL, *rx_ctx_arr = NULL;
+struct ft_context *tx_ctx_arr = NULL, *rx_ctx_arr = NULL;
 uint64_t remote_cq_data = 0;
 
 uint64_t tx_seq, rx_seq, tx_cq_cntr, rx_cq_cntr;
@@ -78,7 +78,8 @@ int ft_socket_pair[2];
 
 fi_addr_t remote_fi_addr = FI_ADDR_UNSPEC;
 char *buf, *tx_buf, *rx_buf;
-size_t buf_size, tx_size, rx_size;
+char **tx_mr_bufs = NULL, **rx_mr_bufs = NULL;
+size_t buf_size, tx_size, rx_size, tx_mr_size, rx_mr_size;
 int rx_fd = -1, tx_fd = -1;
 char default_port[8] = "9228";
 static char default_oob_port[8] = "3000";
@@ -349,6 +350,61 @@ void ft_free_bit_combo(uint64_t *combo)
 	free(combo);
 }
 
+static int ft_alloc_ctx_array(struct ft_context **mr_array, char ***mr_bufs,
+			      char *default_buf, size_t mr_size,
+			      uint64_t start_key)
+{
+	int i, ret;
+	uint64_t access = ft_info_to_mr_access(fi);
+	struct ft_context *context;
+
+	*mr_array = calloc(opts.window_size, sizeof(**mr_array));
+	if (!*mr_array)
+		return -FI_ENOMEM;
+
+	if (opts.options & FT_OPT_ALLOC_MULT_MR) {
+		*mr_bufs = calloc(opts.window_size, sizeof(**mr_bufs));
+		if (!mr_bufs)
+			return -FI_ENOMEM;
+	}
+
+	for (i = 0; i < opts.window_size; i++) {
+		context = &(*mr_array)[i];
+		if (!(opts.options & FT_OPT_ALLOC_MULT_MR)) {
+			context->buf = default_buf;
+			continue;
+		}
+		(*mr_bufs)[i] = calloc(1, mr_size);
+		context->buf = (*mr_bufs)[i];
+    		if (((fi->domain_attr->mr_mode & FI_MR_LOCAL) ||
+		     (fi->caps & (FI_RMA | FI_ATOMIC)))) {
+			ret = fi_mr_reg(domain, context->buf,
+					mr_size, access, 0,
+					start_key + i, 0,
+					&context->mr, NULL);
+			if (ret)
+				return ret;
+
+			context->desc = fi_mr_desc(context->mr);
+		} else {
+			context->mr =  NULL;
+			context->desc = NULL;
+		}
+	}
+
+	return 0;
+}
+
+static void ft_set_tx_rx_sizes(size_t *set_tx, size_t *set_rx)
+{
+	*set_tx = opts.options & FT_OPT_SIZE ?
+		  opts.transfer_size : test_size[TEST_CNT - 1].size;
+	if (*set_tx > fi->ep_attr->max_msg_size)
+		*set_tx = fi->ep_attr->max_msg_size;
+	*set_rx = *set_tx + ft_rx_prefix_size();
+	*set_tx += ft_tx_prefix_size();
+}
+
 /*
  * Include FI_MSG_PREFIX space in the allocated buffer, and ensure that the
  * buffer is large enough for a control message used to exchange addressing
@@ -362,13 +418,17 @@ static int ft_alloc_msgs(void)
 	if (ft_check_opts(FT_OPT_SKIP_MSG_ALLOC))
 		return 0;
 
-	tx_size = opts.options & FT_OPT_SIZE ?
-		  opts.transfer_size : test_size[TEST_CNT - 1].size;
-	if (tx_size > fi->ep_attr->max_msg_size)
-		tx_size = fi->ep_attr->max_msg_size;
-	rx_size = tx_size + ft_rx_prefix_size();
-	tx_size += ft_tx_prefix_size();
-	buf_size = MAX(tx_size, FT_MAX_CTRL_MSG) + MAX(rx_size, FT_MAX_CTRL_MSG);
+	if (opts.options & FT_OPT_ALLOC_MULT_MR) {
+		ft_set_tx_rx_sizes(&tx_mr_size, &rx_mr_size);
+		rx_size = FT_MAX_CTRL_MSG + ft_rx_prefix_size();
+		tx_size = FT_MAX_CTRL_MSG + ft_tx_prefix_size();
+		buf_size = rx_size + tx_size;
+	} else {
+		ft_set_tx_rx_sizes(&tx_size, &rx_size);
+		tx_mr_size = 0;
+		rx_mr_size = 0;		
+		buf_size = MAX(tx_size, FT_MAX_CTRL_MSG) + MAX(rx_size, FT_MAX_CTRL_MSG);
+	}
 
 	if (opts.options & FT_OPT_ALIGN) {
 		alignment = sysconf(_SC_PAGESIZE);
@@ -417,6 +477,16 @@ static int ft_alloc_msgs(void)
 		mr = &no_mr;
 	}
 
+	ret = ft_alloc_ctx_array(&tx_ctx_arr, &tx_mr_bufs, tx_buf,
+				 tx_mr_size, FT_TX_MR_KEY);
+	if (ret)
+		return -FI_ENOMEM;
+
+	ret = ft_alloc_ctx_array(&rx_ctx_arr, &rx_mr_bufs, rx_buf,
+				 rx_mr_size, FT_RX_MR_KEY);
+	if (ret)
+		return -FI_ENOMEM;
+
 	return 0;
 }
 
@@ -1350,6 +1420,19 @@ int ft_exchange_keys(struct fi_rma_iov *peer_iov)
 	return ret;
 }
 
+static void ft_cleanup_mr_array(struct ft_context *ctx_arr, char **mr_bufs)
+{
+	int i;
+
+	if (!mr_bufs)
+		return;
+
+	for (i = 0; i < opts.window_size; i++) {
+		FT_CLOSE_FID(ctx_arr[i].mr);
+		free(mr_bufs[i]);
+	}
+}
+
 static void ft_close_fids(void)
 {
 	if (mr != &no_mr)
@@ -1376,17 +1459,20 @@ static void ft_close_fids(void)
 
 void ft_free_res(void)
 {
-	ft_close_fids();
+	ft_cleanup_mr_array(tx_ctx_arr, tx_mr_bufs);
+	ft_cleanup_mr_array(rx_ctx_arr, rx_mr_bufs);
 
 	free(tx_ctx_arr);
 	free(rx_ctx_arr);
 	tx_ctx_arr = NULL;
 	rx_ctx_arr = NULL;
 
+	ft_close_fids();
+
 	if (buf) {
 		free(buf);
 		buf = rx_buf = tx_buf = NULL;
-		buf_size = rx_size = tx_size = 0;
+		buf_size = rx_size = tx_size = tx_mr_size = rx_mr_size = 0;
 	}
 	if (fi_pep) {
 		fi_freeinfo(fi_pep);
@@ -1623,7 +1709,7 @@ static int ft_progress(struct fid_cq *cq, uint64_t total, uint64_t *cq_cntr)
 	} while (0)
 
 ssize_t ft_post_tx_buf(struct fid_ep *ep, fi_addr_t fi_addr, size_t size,
-		       uint64_t data, struct fi_context* ctx,
+		       uint64_t data, void *ctx,
 		       void *op_buf, void *op_mr_desc, uint64_t op_tag)
 {
 	size += ft_tx_prefix_size();
@@ -1654,13 +1740,13 @@ ssize_t ft_post_tx_buf(struct fid_ep *ep, fi_addr_t fi_addr, size_t size,
 }
 
 ssize_t ft_post_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size,
-		   uint64_t data, struct fi_context* ctx)
+		   uint64_t data, void *ctx)
 {
 	return ft_post_tx_buf(ep, fi_addr, size, data,
 			      ctx, tx_buf, mr_desc, ft_tag);
 }
 
-ssize_t ft_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size, struct fi_context *ctx)
+ssize_t ft_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size, void *ctx)
 {
 	ssize_t ret;
 
@@ -1885,7 +1971,7 @@ int check_compare_atomic_op(struct fid_ep *endpoint, enum fi_op op,
 	return check_atomic_attr(op, datatype, FI_COMPARE_ATOMIC);
 }
 
-ssize_t ft_post_rx_buf(struct fid_ep *ep, size_t size, struct fi_context* ctx,
+ssize_t ft_post_rx_buf(struct fid_ep *ep, size_t size, void *ctx,
 		       void *op_buf, void *op_mr_desc, uint64_t op_tag)
 {
 	size = MAX(size, FT_MAX_CTRL_MSG) + ft_rx_prefix_size();
@@ -1901,7 +1987,7 @@ ssize_t ft_post_rx_buf(struct fid_ep *ep, size_t size, struct fi_context* ctx,
 	return 0;
 }
 
-ssize_t ft_post_rx(struct fid_ep *ep, size_t size, struct fi_context* ctx)
+ssize_t ft_post_rx(struct fid_ep *ep, size_t size, void *ctx)
 {
 	return ft_post_rx_buf(ep, size, ctx, rx_buf, mr_desc, ft_tag);
 }
@@ -2165,7 +2251,7 @@ int ft_get_tx_comp(uint64_t total)
 }
 
 int ft_sendmsg(struct fid_ep *ep, fi_addr_t fi_addr,
-		size_t size, struct fi_context *ctx, int flags)
+		size_t size, void *ctx, int flags)
 {
 	int ret;
 	struct fi_msg msg;
@@ -2209,7 +2295,7 @@ int ft_sendmsg(struct fid_ep *ep, fi_addr_t fi_addr,
 }
 
 int ft_recvmsg(struct fid_ep *ep, fi_addr_t fi_addr,
-	       size_t size, struct fi_context *ctx, int flags)
+	       size_t size, void *ctx, int flags)
 {
 	int ret;
 	struct fi_msg msg;
@@ -2618,6 +2704,7 @@ void ft_usage(char *name, char *desc)
 	FT_PRINT_OPTS_USAGE("", "fi_unexpected_msg");
 	FT_PRINT_OPTS_USAGE("", "fi_resmgmt_test");
 	FT_PRINT_OPTS_USAGE("", "fi_inj_complete");
+	FT_PRINT_OPTS_USAGE("", "fi_bw");
 	FT_PRINT_OPTS_USAGE("-M <mode>", "Disable mode bit from test");
 	FT_PRINT_OPTS_USAGE("", "mr_local");
 	FT_PRINT_OPTS_USAGE("-a <address vector name>", "name of address vector");
diff --git a/fabtests/configure.ac b/fabtests/configure.ac
index c42a57bec..bd5a1fde7 100644
--- a/fabtests/configure.ac
+++ b/fabtests/configure.ac
@@ -5,7 +5,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ(2.57)
-AC_INIT([fabtests], [1.8.0], [ofiwg@lists.openfabrics.org])
+AC_INIT([fabtests], [1.9.0a1], [ofiwg@lists.openfabrics.org])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
 AC_CONFIG_HEADERS(config.h)
@@ -92,7 +92,7 @@ AM_CONDITIONAL(HAVE_CLOCK_GETTIME, [test $have_clock_gettime -eq 1])
 AC_ARG_WITH([libfabric],
             AC_HELP_STRING([--with-libfabric], [Use non-default libfabric location - default NO]),
             [AS_IF([test -d $withval/lib64], [fab_libdir="lib64"], [fab_libdir="lib"])
-             CPPFLAGS="-I $withval/include $CPPFLAGS"
+             CPPFLAGS="-I$withval/include $CPPFLAGS"
              LDFLAGS="-L$withval/$fab_libdir $LDFLAGS"],
             [])
 
@@ -122,4 +122,5 @@ AC_DEFINE_UNQUOTED([HAVE_EPOLL], [$have_epoll],
 		   [Defined to 1 if Linux epoll is available])
 
 AC_CONFIG_FILES([Makefile fabtests.spec])
+
 AC_OUTPUT
diff --git a/fabtests/fabtests.vcxproj b/fabtests/fabtests.vcxproj
index 96374f465..d6f06bf49 100644
--- a/fabtests/fabtests.vcxproj
+++ b/fabtests/fabtests.vcxproj
@@ -135,6 +135,7 @@
     <ClCompile Include="functional\rdm_netdir.c" />
     <ClCompile Include="functional\scalable_ep.c" />
     <ClCompile Include="functional\inj_complete.c" />
+    <ClCompile Include="functional\bw.c" />
     <ClCompile Include="unit\av_test.c" />
     <ClCompile Include="unit\cntr_test.c" />
     <ClCompile Include="unit\common.c" />
diff --git a/fabtests/functional/bw.c b/fabtests/functional/bw.c
new file mode 100644
index 000000000..4da65402b
--- /dev/null
+++ b/fabtests/functional/bw.c
@@ -0,0 +1,249 @@
+/*
+ * Copyright (c) 2019 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under the BSD license
+ * below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <getopt.h>
+#include <unistd.h>
+
+#include <shared.h>
+
+int sleep_time = 0;
+
+static ssize_t post_one_tx(struct ft_context *msg)
+{
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
+		ft_fill_buf(msg->buf + ft_tx_prefix_size(),
+			    opts.transfer_size);
+
+	return ft_post_tx_buf(ep, remote_fi_addr, opts.transfer_size,
+			      NO_CQ_DATA, &msg->context, msg->buf,
+			      msg->desc, 0);
+}
+
+static ssize_t wait_check_rx_bufs(void)
+{
+	ssize_t ret;
+	int i;
+
+	ret = ft_get_rx_comp(rx_seq);
+	if (ret)
+		return ret;
+
+	if (!ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
+		return 0;
+
+	for (i = 0; i < opts.window_size; i++) {
+		ret = ft_check_buf((char *) rx_ctx_arr[i].buf +
+				   ft_rx_prefix_size(), opts.transfer_size);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int post_rx_sync(void)
+{
+	int ret;
+
+	ret = ft_post_rx(ep, rx_size, &rx_ctx);
+	if (ret)
+		return ret;
+
+	if (opts.dst_addr) {
+		ret = ft_tx(ep, remote_fi_addr, 1, &tx_ctx);
+		if (ret)
+			return ret;
+
+		ret = ft_get_rx_comp(rx_seq);
+	} else {
+		ret = ft_get_rx_comp(rx_seq);
+		if (ret)
+			return ret;
+
+		ret = ft_tx(ep, remote_fi_addr, 1, &tx_ctx);
+	}
+
+	return ret;
+}
+
+static int run_loop(void)
+{
+	int ret, i;
+
+	/* Receive side delay is used in order to let the sender
+ 	   get ahead of the receiver and post multiple sends
+	   before the receiver begins processing them. */
+	if (!opts.dst_addr)
+		sleep(sleep_time);
+
+	ft_start();
+	if (opts.dst_addr) {
+		for (i = 0; i < opts.window_size; i++) {
+			ret = post_one_tx(&tx_ctx_arr[i]);
+			if (ret)
+				return ret;
+		}
+
+		ret = ft_get_tx_comp(tx_seq);
+		if (ret)
+			return ret;
+	} else {
+		for (i = 0; i < opts.window_size; i++) {
+			ret = ft_post_rx_buf(ep, opts.transfer_size,
+					     &rx_ctx_arr[i].context,
+					     rx_ctx_arr[i].buf,
+					     rx_ctx_arr[i].desc, 0);
+			if (ret)
+				return ret;
+		}
+
+		ret = wait_check_rx_bufs();
+		if (ret)
+			return ret;
+	}
+	ft_stop();
+
+	if (opts.options & FT_OPT_OOB_SYNC)
+		ret = ft_sync();
+	else
+		ret = post_rx_sync();
+	if (ret)
+		return ret;
+
+	if (opts.machr)
+		show_perf_mr(opts.transfer_size, opts.window_size, &start, &end, 1,
+				opts.argc, opts.argv);
+	else
+		show_perf(NULL, opts.transfer_size, opts.window_size, &start, &end, 1);
+
+	return ret;
+}
+
+static int run(void)
+{
+	int ret, i;
+
+	ret = hints->ep_attr->type == FI_EP_MSG ?
+		ft_init_fabric_cm() : ft_init_fabric();
+	if (ret)
+		return ret;
+	
+	ret = ft_tx(ep, remote_fi_addr, 1, &tx_ctx);
+	if (ret)
+		return ret;
+
+	ret = ft_get_tx_comp(tx_seq);
+	if (ret)
+		return ret;
+
+	ret = ft_get_rx_comp(rx_seq);
+	if (ret)
+		return ret;
+
+	if (!(opts.options & FT_OPT_SIZE)) {
+		for (i = 0; i < TEST_CNT; i++) {
+			if (!ft_use_size(i, opts.sizes_enabled))
+				continue;
+			opts.transfer_size = test_size[i].size;
+			ret = run_loop();
+			if (ret)
+				goto out;
+		}
+	} else {
+		ret = run_loop();
+		if (ret)
+			goto out;
+	}
+
+out:
+	return ret;
+}
+
+int main(int argc, char **argv)
+{
+	int op, ret;
+
+	opts = INIT_OPTS;
+
+	hints = fi_allocinfo();
+	if (!hints)
+		return EXIT_FAILURE;
+
+	hints->ep_attr->type = FI_EP_RDM;
+
+	while ((op = getopt(argc, argv, "W:vT:h" CS_OPTS ADDR_OPTS INFO_OPTS)) != -1) {
+		switch (op) {
+		default:
+			ft_parse_addr_opts(op, optarg, &opts);
+			ft_parseinfo(op, optarg, hints, &opts);
+			ft_parsecsopts(op, optarg, &opts);
+			break;
+		case 'W':
+			opts.window_size = atoi(optarg);
+			break;
+		case 'v':
+			opts.options |= FT_OPT_VERIFY_DATA;
+			break;
+		case 'T':
+			sleep_time = atoi(optarg);
+			break;
+		case '?':
+		case 'h':
+			ft_usage(argv[0], "A bandwidth test with data verification.");
+			return EXIT_FAILURE;
+		}
+	}
+
+	if (optind < argc)
+		opts.dst_addr = argv[optind];
+
+	hints->caps = FI_MSG;
+	hints->mode = FI_CONTEXT;
+	hints->domain_attr->mr_mode = opts.mr_mode;
+
+	opts.options |= FT_OPT_ALLOC_MULT_MR;
+
+	if (hints->ep_attr->type == FI_EP_DGRAM) {
+		fprintf(stderr, "This test does not support DGRAM endpoints\n");
+		return -FI_EINVAL;
+	}
+
+	if (opts.options & FT_OPT_VERIFY_DATA) {
+		hints->tx_attr->msg_order |= FI_ORDER_SAS;
+		hints->rx_attr->msg_order |= FI_ORDER_SAS;
+	}
+
+	ret = run();
+
+	ft_free_res();
+
+	return ft_exit_code(ret);
+}
diff --git a/fabtests/functional/rdm_tagged_peek.c b/fabtests/functional/rdm_tagged_peek.c
index 4001d927e..655b91918 100644
--- a/fabtests/functional/rdm_tagged_peek.c
+++ b/fabtests/functional/rdm_tagged_peek.c
@@ -107,9 +107,6 @@ static int run(void)
 	if (ret)
 		return ret;
 
-	if (!(tx_ctx_arr = calloc(5, sizeof *tx_ctx_arr)))
-		return -FI_ENOMEM;
-
 	if (opts.dst_addr) {
 		printf("Searching for a bad msg\n");
 		ret = tag_queue_op(0xbad, 0, FI_PEEK);
@@ -193,7 +190,7 @@ static int run(void)
 		 * maintained by common code */
 		ret = fi_tsend(ep, tx_buf, 1, mr_desc,
 				remote_fi_addr, 0xabc,
-				&tx_ctx_arr[0]);
+				&tx_ctx_arr[0].context);
 		if (ret)
 			return ret;
 		ret = wait_for_send_comp(1);
@@ -204,7 +201,7 @@ static int run(void)
 		for(i = 0; i < 5; i++) {
 			ret = fi_tsend(ep, tx_buf, tx_size, mr_desc,
 				       remote_fi_addr, 0x900d+i,
-				       &tx_ctx_arr[i]);
+				       &tx_ctx_arr[i].context);
 			if (ret)
 				return ret;
 		}
@@ -230,6 +227,7 @@ int main(int argc, char **argv)
 	opts = INIT_OPTS;
 	opts.options |= FT_OPT_SIZE;
 	opts.comp_method = FT_COMP_SREAD;
+	opts.window_size = 5;
 
 	hints = fi_allocinfo();
 	if (!hints) {
diff --git a/fabtests/functional/shared_ctx.c b/fabtests/functional/shared_ctx.c
index 1bcec3b70..7aa04e791 100644
--- a/fabtests/functional/shared_ctx.c
+++ b/fabtests/functional/shared_ctx.c
@@ -196,20 +196,14 @@ static int run_test()
 {
 	int ret, i;
 
-	if (!(tx_ctx_arr = calloc(ep_cnt, sizeof *tx_ctx_arr)))
-		return -FI_ENOMEM;
-
-	if (!(rx_ctx_arr = calloc(ep_cnt, sizeof *rx_ctx_arr)))
-		return -FI_ENOMEM;
-
 	/* Post recvs */
 	for (i = 0; i < ep_cnt; i++) {
 		if (rx_shared_ctx) {
 			fprintf(stdout, "Posting recv #%d for shared rx ctx\n", i);
-			ret = ft_post_rx(srx_ctx, rx_size, &rx_ctx_arr[i]);
+			ret = ft_post_rx(srx_ctx, rx_size, &rx_ctx_arr[i].context);
 		 } else {
 			fprintf(stdout, "Posting recv for endpoint #%d\n", i);
-			ret = ft_post_rx(ep_array[i], rx_size, &rx_ctx_arr[i]);
+			ret = ft_post_rx(ep_array[i], rx_size, &rx_ctx_arr[i].context);
 		 }
 		if (ret)
 			return ret;
@@ -222,7 +216,7 @@ static int run_test()
 				fprintf(stdout, "Posting send #%d to shared tx ctx\n", i);
 			else
 				fprintf(stdout, "Posting send to endpoint #%d\n", i);
-			ret = ft_tx(ep_array[i], addr_array[i], tx_size, &tx_ctx_arr[i]);
+			ret = ft_tx(ep_array[i], addr_array[i], tx_size, &tx_ctx_arr[i].context);
 			if (ret)
 				return ret;
 		}
@@ -240,7 +234,7 @@ static int run_test()
 				fprintf(stdout, "Posting send #%d to shared tx ctx\n", i);
 			else
 				fprintf(stdout, "Posting send to endpoint #%d\n", i);
-			ret = ft_tx(ep_array[i], addr_array[i], tx_size, &tx_ctx_arr[i]);
+			ret = ft_tx(ep_array[i], addr_array[i], tx_size, &tx_ctx_arr[i].context);
 			if (ret)
 				return ret;
 		}
diff --git a/fabtests/functional/unexpected_msg.c b/fabtests/functional/unexpected_msg.c
index 594e95fde..006ba4bae 100644
--- a/fabtests/functional/unexpected_msg.c
+++ b/fabtests/functional/unexpected_msg.c
@@ -138,7 +138,7 @@ static int run_test_loop(void)
 
 			ret = ft_post_tx_buf(ep, remote_fi_addr,
 					     opts.transfer_size,
-					     op_data, &tx_ctx_arr[j],
+					     op_data, &tx_ctx_arr[j].context,
 					     op_buf, mr_desc, op_tag);
 			if (ret) {
 				printf("ERROR send_msg returned %d\n", ret);
@@ -153,7 +153,7 @@ static int run_test_loop(void)
 		for (j = 0; j < concurrent_msgs; j++) {
 			op_buf = get_rx_buf(j);
 			ret = ft_post_rx_buf(ep, opts.transfer_size,
-					     &rx_ctx_arr[j], op_buf,
+					     &rx_ctx_arr[j].context, op_buf,
 					     mr_desc, op_tag);
 			if (ret) {
 				printf("ERROR recv_msg returned %d\n", ret);
diff --git a/fabtests/include/shared.h b/fabtests/include/shared.h
index 342490dd0..a66428e67 100644
--- a/fabtests/include/shared.h
+++ b/fabtests/include/shared.h
@@ -118,6 +118,7 @@ enum {
 	FT_OPT_SKIP_MSG_ALLOC	= 1 << 12,
 	FT_OPT_SKIP_REG_MR	= 1 << 13,
 	FT_OPT_OOB_ADDR_EXCH	= 1 << 14,
+	FT_OPT_ALLOC_MULT_MR	= 1 << 15,
 	FT_OPT_OOB_CTRL		= FT_OPT_OOB_SYNC | FT_OPT_OOB_ADDR_EXCH,
 };
 
@@ -135,6 +136,19 @@ enum ft_atomic_opcodes {
 	FT_ATOMIC_COMPARE,
 };
 
+enum op_state {
+	OP_DONE = 0,
+	OP_PENDING
+};
+
+struct ft_context {
+	char *buf;
+	void *desc;
+	enum op_state state;
+	struct fid_mr *mr;
+	struct fi_context2 context;
+};
+
 struct ft_opts {
 	int iterations;
 	int warmup_iterations;
@@ -180,12 +194,13 @@ extern struct fid_mc *mc;
 
 extern fi_addr_t remote_fi_addr;
 extern char *buf, *tx_buf, *rx_buf;
-extern size_t buf_size, tx_size, rx_size;
+extern struct ft_context *tx_ctx_arr, *rx_ctx_arr;
+extern char **tx_mr_bufs, **rx_mr_bufs;
+extern size_t buf_size, tx_size, rx_size, tx_mr_size, rx_mr_size;
 extern int tx_fd, rx_fd;
 extern int timeout;
 
 extern struct fi_context tx_ctx, rx_ctx;
-extern struct fi_context *tx_ctx_arr, *rx_ctx_arr;
 extern uint64_t remote_cq_data;
 
 extern uint64_t tx_seq, rx_seq, tx_cq_cntr, rx_cq_cntr;
@@ -256,6 +271,8 @@ extern char default_port[8];
 #define FT_STR_LEN 32
 #define FT_MAX_CTRL_MSG 64
 #define FT_MR_KEY 0xC0DE
+#define FT_TX_MR_KEY (FT_MR_KEY + 1)
+#define FT_RX_MR_KEY 0xFFFF
 #define FT_MSG_MR_ACCESS (FI_SEND | FI_RECV)
 #define FT_RMA_MR_ACCESS (FI_READ | FI_WRITE | FI_REMOTE_READ | FI_REMOTE_WRITE)
 
@@ -409,16 +426,16 @@ int ft_finalize_ep(struct fid_ep *ep);
 
 size_t ft_rx_prefix_size(void);
 size_t ft_tx_prefix_size(void);
-ssize_t ft_post_rx(struct fid_ep *ep, size_t size, struct fi_context* ctx);
-ssize_t ft_post_rx_buf(struct fid_ep *ep, size_t size, struct fi_context* ctx,
+ssize_t ft_post_rx(struct fid_ep *ep, size_t size, void *ctx);
+ssize_t ft_post_rx_buf(struct fid_ep *ep, size_t size, void *ctx,
 		       void *op_buf, void *op_mr_desc, uint64_t op_tag);
 ssize_t ft_post_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size,
-		uint64_t data, struct fi_context* ctx);
+		uint64_t data, void *ctx);
 ssize_t ft_post_tx_buf(struct fid_ep *ep, fi_addr_t fi_addr, size_t size,
-		       uint64_t data, struct fi_context* ctx,
+		       uint64_t data, void *ctx,
 		       void *op_buf, void *op_mr_desc, uint64_t op_tag);
 ssize_t ft_rx(struct fid_ep *ep, size_t size);
-ssize_t ft_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size, struct fi_context *ctx);
+ssize_t ft_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size, void *ctx);
 ssize_t ft_inject(struct fid_ep *ep, fi_addr_t fi_addr, size_t size);
 ssize_t ft_post_rma(enum ft_rma_opcodes op, struct fid_ep *ep, size_t size,
 		struct fi_rma_iov *remote, void *context);
@@ -444,9 +461,9 @@ int ft_cq_readerr(struct fid_cq *cq);
 int ft_get_rx_comp(uint64_t total);
 int ft_get_tx_comp(uint64_t total);
 int ft_recvmsg(struct fid_ep *ep, fi_addr_t fi_addr,
-		size_t size, struct fi_context *ctx, int flags);
+		size_t size, void *ctx, int flags);
 int ft_sendmsg(struct fid_ep *ep, fi_addr_t fi_addr,
-		size_t size, struct fi_context *ctx, int flags);
+		size_t size, void *ctx, int flags);
 int ft_cq_read_verify(struct fid_cq *cq, void *op_context);
 
 void eq_readerr(struct fid_eq *eq, const char *eq_str);
diff --git a/fabtests/man/fabtests.7.md b/fabtests/man/fabtests.7.md
index 1d89a4385..da2a8c9c6 100644
--- a/fabtests/man/fabtests.7.md
+++ b/fabtests/man/fabtests.7.md
@@ -142,6 +142,11 @@ features of libfabric.
   buffer tries to remain the same.  This test is used to validate the
   correct behavior of memory registration caches.
 
+*fi_bw*
+: Performs a one-sided bandwidth test with an option for data verification.
+  A sleep time on the receiving side can be enabled in order to allow
+  the sender to get ahead of the receiver.
+
 # Benchmarks
 
 The client and the server exchange messages in either a ping-pong manner,
diff --git a/fabtests/man/man1/fi_bw.1 b/fabtests/man/man1/fi_bw.1
new file mode 100644
index 000000000..3f6ccf96f
--- /dev/null
+++ b/fabtests/man/man1/fi_bw.1
@@ -0,0 +1 @@
+.so man7/fabtests.7
diff --git a/fabtests/man/man7/fabtests.7 b/fabtests/man/man7/fabtests.7
index 641add08d..26c9682cd 100644
--- a/fabtests/man/man7/fabtests.7
+++ b/fabtests/man/man7/fabtests.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fabtests" "7" "2019\-05\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fabtests" "7" "2019\-07\-12" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -201,6 +201,14 @@ This test is used to validate the correct behavior of memory
 registration caches.
 .RS
 .RE
+.TP
+.B \f[I]fi_bw\f[]
+Performs a one\-sided bandwidth test with an option for data
+verification.
+A sleep time on the receiving side can be enabled in order to allow the
+sender to get ahead of the receiver.
+.RS
+.RE
 .SH Benchmarks
 .PP
 The client and the server exchange messages in either a ping\-pong
diff --git a/fabtests/multinode/include/core.h b/fabtests/multinode/include/core.h
new file mode 100644
index 000000000..526bf1adc
--- /dev/null
+++ b/fabtests/multinode/include/core.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright (c) 2017-2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#pragma once
+
+#include <stdlib.h>
+#include <stdbool.h>
+
+#include <rdma/fabric.h>
+#include <rdma/fi_trigger.h>
+#include <sys/uio.h>
+#include <sys/socket.h>
+
+#include "pattern.h"
+
+#define PM_DEFAULT_OOB_PORT (8228)
+
+struct pm_job_info {
+	size_t		my_rank;
+	size_t		num_ranks;
+	int		sock;
+	int		*clients; //only valid for server
+	struct sockaddr_storage oob_server_addr;
+	void		*names;
+	size_t		name_len;
+	fi_addr_t	*fi_addrs;
+};
+
+
+struct multinode_xfer_state {
+	int 			iteration;
+	size_t			recvs_posted;
+	size_t			sends_posted;
+
+	size_t			tx_window;
+	size_t			rx_window;
+
+	/* pattern iterator state */
+	int			cur_source;
+	int			cur_target;
+
+	bool			all_recvs_posted;
+	bool			all_sends_posted;
+	bool			all_completions_done;
+
+	uint64_t		tx_flags;
+	uint64_t		rx_flags;
+};
+
+extern struct pm_job_info pm_job;
+int multinode_run_tests(int argc, char **argv);
+int pm_allgather(void *my_item, void *items, int item_size);
+void pm_barrier();
diff --git a/fabtests/multinode/include/pattern.h b/fabtests/multinode/include/pattern.h
new file mode 100644
index 000000000..0a1121c6a
--- /dev/null
+++ b/fabtests/multinode/include/pattern.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright (c) 2017-2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#pragma once
+
+#include <stdlib.h>
+#include <stdbool.h>
+#include <errno.h>
+
+#include <rdma/fabric.h>
+
+/* Initial value for iterator position. */
+#define PATTERN_NO_CURRENT (-1)
+
+struct pattern_ops {
+	char *name;
+	int (*next_source)(int *cur);
+	int (*next_target) (int *cur);
+};
+
+extern struct pattern_ops full_mesh_ops;
diff --git a/fabtests/multinode/src/core.c b/fabtests/multinode/src/core.c
new file mode 100644
index 000000000..558330ff2
--- /dev/null
+++ b/fabtests/multinode/src/core.c
@@ -0,0 +1,298 @@
+/*
+ * Copyright (c) 2017-2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHWARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. const NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER const AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS const THE
+ * SOFTWARE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <getopt.h>
+#include <limits.h>
+#include <stdarg.h>
+
+#include <rdma/fi_errno.h>
+#include <rdma/fi_domain.h>
+#include <rdma/fabric.h>
+#include <rdma/fi_endpoint.h>
+#include <rdma/fi_cm.h>
+#include <rdma/fi_trigger.h>
+
+#include <core.h>
+#include <pattern.h>
+#include <shared.h>
+#include <sys/socket.h>
+#include <netinet/in.h>
+#include <arpa/inet.h>
+#include <assert.h>
+
+struct pattern_ops *pattern;
+struct multinode_xfer_state state;
+
+static int multinode_setup_fabric(int argc, char **argv)
+{
+	char my_name[FT_MAX_CTRL_MSG];
+	size_t len;
+	int ret;
+
+	hints->ep_attr->type = FI_EP_RDM;
+	hints->caps = FI_MSG;
+	hints->mode = FI_CONTEXT;
+	hints->domain_attr->mr_mode = opts.mr_mode;
+
+	tx_seq = 0;
+	rx_seq = 0;
+	tx_cq_cntr = 0;
+	rx_cq_cntr = 0;
+
+	ret = ft_getinfo(hints, &fi);
+	if (ret)
+		return ret;
+
+	ret = ft_open_fabric_res();
+	if (ret)
+		return ret;
+
+	opts.av_size = pm_job.num_ranks;
+	ret = ft_alloc_active_res(fi);
+	if (ret)
+		return ret;
+
+	ret = ft_enable_ep(ep, eq, av, txcq, rxcq, txcntr, rxcntr);
+	if (ret)
+		return ret;
+
+	len = FT_MAX_CTRL_MSG;
+	ret = fi_getname(&ep->fid, (void *) my_name, &len);
+	if (ret) {
+		FT_PRINTERR("error determining local endpoint name\n", ret);
+		goto err;
+	}
+
+	pm_job.name_len = len;
+	pm_job.names = malloc(len * pm_job.num_ranks);
+	if (!pm_job.names) {
+		FT_ERR("error allocating memory for address exchange\n");
+		ret = -FI_ENOMEM;
+		goto err;
+	}
+
+	ret = pm_allgather(my_name, pm_job.names, pm_job.name_len);
+	if (ret) {
+		FT_PRINTERR("error exchanging addresses\n", ret);
+		goto err;
+	}
+
+	pm_job.fi_addrs = calloc(pm_job.num_ranks, sizeof(*pm_job.fi_addrs));
+	if (!pm_job.fi_addrs) {
+		FT_ERR("error allocating memory for av fi addrs\n");
+		ret = -FI_ENOMEM;
+		goto err;
+	}
+
+	ret = fi_av_insert(av, pm_job.names, pm_job.num_ranks,
+			   pm_job.fi_addrs, 0, NULL);
+	if (ret != pm_job.num_ranks) {
+		FT_ERR("unable to insert all addresses into AV table\n");
+		ret = -1;
+		goto err;
+	}
+	return 0;
+err:
+	ft_free_res();
+	return ft_exit_code(ret);
+}
+
+static int multinode_post_rx()
+{
+	int ret, offset;
+
+	/* post receives */
+	while (!state.all_recvs_posted) {
+
+		if (state.rx_window == 0)
+			break;
+
+		ret = pattern->next_source(&state.cur_source);
+		if (ret == -ENODATA) {
+			state.all_recvs_posted = true;
+			break;
+		} else if (ret < 0) {
+			return ret;
+		}
+
+		offset = state.recvs_posted % opts.window_size ;
+		assert(rx_ctx_arr[offset].state == OP_DONE);
+
+		ret = ft_post_rx_buf(ep, opts.transfer_size,
+				     &rx_ctx_arr[offset],
+				     rx_ctx_arr[offset].buf,
+				     rx_ctx_arr[offset].desc, 0);
+		if (ret)
+			return ret;
+
+		rx_ctx_arr[offset].state = OP_PENDING;
+		state.recvs_posted++;
+		state.rx_window--;
+	};
+	return 0;
+}
+
+static int multinode_post_tx()
+{
+	int ret, offset;
+	fi_addr_t dest;
+
+	while (!state.all_sends_posted) {
+
+		if (state.tx_window == 0)
+			break;
+
+		ret = pattern->next_target(&state.cur_target);
+		if (ret == -ENODATA) {
+			state.all_sends_posted = true;
+			break;
+		} else if (ret < 0) {
+			return ret;
+		}
+
+		offset = state.sends_posted % opts.window_size;
+		assert(tx_ctx_arr[offset].state == OP_DONE);
+
+		tx_ctx_arr[offset].buf[0] = offset;
+		dest = pm_job.fi_addrs[state.cur_target];
+		ret = ft_post_tx_buf(ep, dest, opts.transfer_size,
+				     NO_CQ_DATA,
+				     &tx_ctx_arr[offset],
+				     tx_ctx_arr[offset].buf,
+				     tx_ctx_arr[offset].desc, 0);
+		if (ret)
+			return ret;
+
+		tx_ctx_arr[offset].state = OP_PENDING;
+		state.sends_posted++;
+		state.tx_window--;
+	}
+	return 0;
+}
+
+static int multinode_wait_for_comp()
+{
+	int ret, i;
+
+	ret = ft_get_tx_comp(tx_seq);
+	if (ret)
+		return ret;
+
+	ret = ft_get_rx_comp(rx_seq);
+	if (ret)
+		return ret;
+
+	for (i = 0; i < opts.window_size; i++) {
+		rx_ctx_arr[i].state = OP_DONE;
+		tx_ctx_arr[i].state = OP_DONE;
+	}
+
+	state.rx_window = opts.window_size;
+	state.tx_window = opts.window_size;
+
+	if (state.all_recvs_posted && state.all_sends_posted)
+		state.all_completions_done = true;
+
+	return 0;
+}
+
+static inline void multinode_init_state()
+{
+	state.cur_source = PATTERN_NO_CURRENT;
+	state.cur_target = PATTERN_NO_CURRENT;
+
+	state.all_completions_done = false;
+	state.all_recvs_posted = false;
+	state.all_sends_posted = false;
+
+	state.rx_window = opts.window_size;
+	state.tx_window = opts.window_size;
+}
+
+static int multinode_run_test()
+{
+	int ret;
+	int iter;
+
+	for (iter = 0; iter < opts.iterations; iter++) {
+
+		multinode_init_state();
+
+		while (!state.all_completions_done ||
+				!state.all_recvs_posted ||
+				!state.all_sends_posted) {
+			ret = multinode_post_rx();
+			if (ret)
+				return ret;
+
+			ret = multinode_post_tx();
+			if (ret)
+				return ret;
+
+			ret = multinode_wait_for_comp();
+			if (ret)
+				return ret;
+		}
+	}
+	pm_barrier();
+	return 0;
+}
+
+static void pm_job_free_res()
+{
+	if (pm_job.names)
+		free(pm_job.names);
+
+	if (pm_job.fi_addrs)
+	free(pm_job.fi_addrs);
+}
+
+int multinode_run_tests(int argc, char **argv)
+{
+	int ret = FI_SUCCESS;
+
+	ret = multinode_setup_fabric(argc, argv);
+	if (ret)
+		return ret;
+
+	pattern = &full_mesh_ops;
+
+	ret = multinode_run_test();
+
+	pm_job_free_res();
+	ft_free_res();
+	return ft_exit_code(ret);
+}
diff --git a/fabtests/multinode/src/harness.c b/fabtests/multinode/src/harness.c
new file mode 100644
index 000000000..1b3ca9ff6
--- /dev/null
+++ b/fabtests/multinode/src/harness.c
@@ -0,0 +1,300 @@
+/*
+ * Copyright (c) 2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <stdio.h>
+#include <errno.h>
+#include <shared.h>
+#include <unistd.h>
+#include <string.h>
+#include <errno.h>
+#include <sys/types.h>
+#include <sys/socket.h>
+#include <netinet/in.h>
+#include <arpa/inet.h>
+#include <netdb.h>
+
+#include <core.h>
+struct pm_job_info pm_job;
+
+static inline ssize_t socket_send(int sock, void *buf, size_t len, int flags)
+{
+	ssize_t ret;
+	size_t m = 0;
+	uint8_t *ptr = (uint8_t *) buf;
+
+	do {
+		ret = send(sock, (void *) &ptr[m], len-m, flags);
+		if (ret < 0)
+			return ret;
+
+		m += ret;
+	} while (m != len);
+
+	return len;
+}
+
+static inline int socket_recv(int sock, void *buf, size_t len, int flags)
+{
+	ssize_t ret;
+	size_t m = 0;
+	uint8_t *ptr = (uint8_t *) buf;
+
+	do {
+		ret = recv(sock, (void *) &ptr[m], len-m, flags);
+		if (ret <= 0)
+			return -1;
+
+		m += ret;
+	} while (m < len);
+
+	return len;
+}
+
+int pm_allgather(void *my_item, void *items, int item_size)
+
+{
+	int i, ret;
+	uint8_t *offset;
+
+	/* client */
+	if (!pm_job.clients) {
+		ret = socket_send(pm_job.sock, my_item, item_size, 0);
+		if (ret < 0)
+			return errno == EPIPE ? -FI_ENOTCONN : -errno;
+
+		ret = socket_recv(pm_job.sock, items,
+				  pm_job.num_ranks*item_size, 0);
+		if (ret <= 0)
+			return (ret)? -errno : -FI_ENOTCONN;
+
+		return 0;
+	}
+
+	/* server */
+	memcpy(items, my_item, item_size);
+
+	for (i = 0; i < pm_job.num_ranks-1; i++) {
+		offset = (uint8_t *)items + item_size * (i+1);
+
+		ret = socket_recv(pm_job.clients[i], (void *)offset,
+				  item_size, 0);
+		if (ret <= 0)
+			return ret;
+	}
+
+	for (i = 0; i < pm_job.num_ranks-1; i++) {
+		ret = socket_send(pm_job.clients[i], items,
+				  pm_job.num_ranks*item_size, 0);
+		if (ret < 0)
+			return ret;
+	}
+	return 0;
+}
+
+void pm_barrier()
+{
+	char ch;
+	char chs[pm_job.num_ranks];
+
+	pm_allgather(&ch, chs, 1);
+}
+
+static int server_connect()
+{
+	int new_sock;
+	int ret, i;
+
+	ret = listen(pm_job.sock, pm_job.num_ranks);
+	if (ret)
+		return ret;
+
+	pm_job.clients = calloc(pm_job.num_ranks, sizeof(int));
+	if (!pm_job.clients)
+		return -FI_ENOMEM;
+
+	for (i = 0; i < pm_job.num_ranks-1; i++){
+		new_sock = accept(pm_job.sock, NULL, NULL);
+		if (new_sock < 0) {
+			FT_ERR("error during server init\n");
+			goto err;
+		}
+		pm_job.clients[i] = new_sock;
+		FT_DEBUG("connection established\n");
+	}
+	close(pm_job.sock);
+	return 0;
+err:
+	while (i--) {
+		close(pm_job.clients[i]);
+	}
+	free(pm_job.clients);
+	return new_sock;
+}
+
+static int pm_conn_setup()
+{
+	int sock,  ret;
+	int optval = 1;
+
+	sock = socket(AF_INET, SOCK_STREAM, 0);
+	if (sock < 0)
+		return -1;
+
+	pm_job.sock = sock;
+
+	ret = setsockopt(sock, SOL_SOCKET, SO_REUSEADDR, (char *) &optval,
+			 sizeof(optval));
+	if (ret) {
+		FT_ERR("error setting socket options\n");
+		return ret;
+	}
+
+	ret = bind(sock, (struct sockaddr *)&pm_job.oob_server_addr,
+		   sizeof(pm_job.oob_server_addr));
+	if (ret == 0) {
+		ret = server_connect();
+	} else {
+		opts.dst_addr = opts.src_addr;
+		opts.dst_port = opts.src_port;
+		opts.src_addr = NULL;
+		opts.src_port = 0;
+		ret = connect(pm_job.sock, (struct sockaddr *)&pm_job.oob_server_addr,
+			      sizeof(pm_job.oob_server_addr));
+	}
+	if (ret) {
+		FT_ERR("OOB conn failed - %s\n", strerror(errno));
+		return ret;
+	}
+
+	return 0;
+}
+
+static void pm_finalize()
+{
+	int i;
+
+	if (!pm_job.clients) {
+		close(pm_job.sock);
+		return;
+	}
+
+	for (i = 0; i < pm_job.num_ranks-1; i++) {
+		close(pm_job.clients[i]);
+	}
+	free(pm_job.clients);
+}
+
+int pm_get_oob_server_addr()
+{
+	struct addrinfo *res;
+	struct sockaddr_in *in;
+	struct sockaddr_in6 *in6;
+        int ret;
+
+        ret = getaddrinfo(opts.src_addr, NULL, NULL, &res);
+        if (ret) {
+		FT_ERR( "getaddrinfo failed\n");
+                return ret;
+        }
+
+	memcpy(&pm_job.oob_server_addr, res->ai_addr, res->ai_addrlen);
+
+	switch (pm_job.oob_server_addr.ss_family) {
+	case AF_INET:
+		in = (struct sockaddr_in *) &pm_job.oob_server_addr;
+		in->sin_port = PM_DEFAULT_OOB_PORT;
+		break;
+	case AF_INET6:
+		in6 = (struct sockaddr_in6 *) &pm_job.oob_server_addr;
+		in6->sin6_port = PM_DEFAULT_OOB_PORT;
+		break;
+	default:
+		FT_ERR( "Unsupported Address family\n");
+		ret = -1;
+		break;
+	}
+
+	freeaddrinfo(res);
+        return ret;
+}
+
+int main(int argc, char **argv)
+{
+	extern char *optarg;
+	int c, ret;
+
+	opts = INIT_OPTS;
+	opts.options |= (FT_OPT_SIZE | FT_OPT_ALLOC_MULT_MR);
+
+	pm_job.clients = NULL;
+
+	hints = fi_allocinfo();
+	if (!hints)
+		return EXIT_FAILURE;
+
+	while ((c = getopt(argc, argv, "n:h" ADDR_OPTS INFO_OPTS)) != -1) {
+		switch (c) {
+		default:
+			ft_parse_addr_opts(c, optarg, &opts);
+			ft_parseinfo(c, optarg, hints, &opts);
+			break;
+		case '?':
+		case 'n':
+			pm_job.num_ranks = atoi(optarg);
+			break;
+		case 'h':
+			ft_usage(argv[0], "A simple multinode test");
+			return EXIT_FAILURE;
+		}
+	}
+
+	ret = pm_get_oob_server_addr();
+	if (ret)
+		goto err1;
+
+	ret = pm_conn_setup();
+	if (ret)
+		goto err1;
+
+	FT_DEBUG("OOB job setup done\n");
+
+	ret = multinode_run_tests(argc, argv);
+	if (ret) {
+		FT_ERR( "Tests failed\n");
+		goto err2;
+	}
+	FT_DEBUG("Tests Passed\n");
+err2:
+	pm_finalize();
+err1:
+	return ret;
+}
diff --git a/fabtests/multinode/src/pattern/full_mesh.c b/fabtests/multinode/src/pattern/full_mesh.c
new file mode 100644
index 000000000..ead6ab614
--- /dev/null
+++ b/fabtests/multinode/src/pattern/full_mesh.c
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2017-2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <pattern.h>
+#include <core.h>
+
+static int pattern_next(int *cur)
+{
+	int next = *cur + 1;
+
+	if (next >= pm_job.num_ranks)
+		return -ENODATA;
+
+	*cur = next;
+	return 0;
+}
+
+
+struct pattern_ops full_mesh_ops = {
+	.name = "full_mesh",
+	.next_source = pattern_next,
+	.next_target = pattern_next,
+};
diff --git a/fabtests/scripts/runfabtests.cmd b/fabtests/scripts/runfabtests.cmd
index 8d31b7704..eb74c5215 100644
--- a/fabtests/scripts/runfabtests.cmd
+++ b/fabtests/scripts/runfabtests.cmd
@@ -27,6 +27,8 @@ set functional_tests=^
 	"rdm_rma_simple"^
 	"rdm_rma_trigger"^
 	"rdm_tagged_peek"^
+	"bw -e rdm -v -T 1"^
+	"bw -e msg -v -T 1"^
 	"scalable_ep"
 rem	"msg_epoll"
 
diff --git a/fabtests/scripts/runfabtests.sh b/fabtests/scripts/runfabtests.sh
index aba23be6f..030b321cc 100755
--- a/fabtests/scripts/runfabtests.sh
+++ b/fabtests/scripts/runfabtests.sh
@@ -126,6 +126,8 @@ functional_tests=(
 	"fi_inj_complete -e msg -SR"
 	"fi_inj_complete -e rdm -SR"
 	"fi_inj_complete -e dgram -SR"
+	"fi_bw -e rdm -v -T 1"
+	"fi_bw -e msg -v -T 1"
 )
 
 short_tests=(
diff --git a/include/ofi_hook.h b/include/ofi_hook.h
index abf827d6a..b13bca869 100644
--- a/include/ofi_hook.h
+++ b/include/ofi_hook.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018 Intel Corporation. All rights reserved.
+ * Copyright (c) 2018-2019 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -44,16 +44,21 @@
 #include <rdma/fi_rma.h>
 #include <rdma/fi_tagged.h>
 
+#include <ofi.h>
 #include <rdma/providers/fi_prov.h>
 
+/* This field needs to be updated whenever new FI class is added in fabric.h */
+#define HOOK_FI_CLASS_MAX (FI_CLASS_NIC + 1)
 
 /*
  * Hooks are installed from top down.
  * Values must start at 0 and increment by one.
  */
+// TODO figure out how to remove this now that we have ini/fini calls
 enum ofi_hook_class {
 	HOOK_NOOP,
 	HOOK_PERF,
+	HOOK_DEBUG,
 	MAX_HOOKS
 };
 
@@ -67,18 +72,78 @@ extern struct fi_ops hook_fid_ops;
 struct fid *hook_to_hfid(const struct fid *fid);
 struct fid_wait *hook_to_hwait(const struct fid_wait *wait);
 
+/*
+ * TODO
+ * comment from GitHub PR #5052:
+ * "another option would be to store the ini/fini calls in a separate structure
+ * that we reference from struct fi_prov_context. We could even extend the
+ * definition of fi_prov_context with a union that is accessed based on the
+ * prov_type. That might work better if we want to support external hooks,
+ * without the external hook provider needing to implement everything"
+ */
+struct hook_prov_ctx {
+	struct fi_provider	prov;
+	/*
+	 * Hooking providers can override ini/fini calls of a specific fid class
+	 * to override any initializations that the common code may have done.
+	 * For example, this allows overriding any specific op and not having to
+	 * hook into every resource creation call until the point where the op
+	 * can be overridden. Refer to hook_perf for an example.
+	 *
+	 * Note: if a hooking provider overrides any of the resource creation calls
+	 * (e.g. fi_endpoint) directly, then these ini/fini calls won't be
+	 * invoked. */
+	int 			(*ini_fid[HOOK_FI_CLASS_MAX])(struct fid *fid);
+	int 			(*fini_fid[HOOK_FI_CLASS_MAX])(struct fid *fid);
+};
+
+/*
+ * TODO
+ * comment from GitHub PR #5052:
+ * "We could set all ini/fini calls to a no-op as part of hook initialization
+ * to avoid this check"
+ */
+static inline int hook_ini_fid(struct hook_prov_ctx *prov_ctx, struct fid *fid)
+{
+	return (prov_ctx->ini_fid[fid->fclass] ?
+		prov_ctx->ini_fid[fid->fclass](fid) : 0);
+}
+
+static inline int hook_fini_fid(struct hook_prov_ctx *prov_ctx, struct fid *fid)
+{
+	return (prov_ctx->fini_fid[fid->fclass] ?
+		prov_ctx->fini_fid[fid->fclass](fid) : 0);
+}
 
 struct hook_fabric {
 	struct fid_fabric	fabric;
 	struct fid_fabric	*hfabric;
 	enum ofi_hook_class	hclass;
-	struct fi_provider	*prov;
+	struct fi_provider	*hprov;
+	struct hook_prov_ctx	*prov_ctx;
 };
 
 void hook_fabric_init(struct hook_fabric *fabric, enum ofi_hook_class hclass,
 		      struct fid_fabric *hfabric, struct fi_provider *hprov,
-		      struct fi_ops *f_ops);
+		      struct fi_ops *f_ops, struct hook_prov_ctx *prov_ctx);
+
+struct hook_fabric *hook_to_fabric(const struct fid *fid);
+
+static inline struct hook_prov_ctx *hook_to_prov_ctx(const struct fid *fid)
+{
+	return (hook_to_fabric(fid))->prov_ctx;
+}
+
+static inline struct fi_provider *
+hook_fabric_to_hprov(const struct hook_fabric *fabric)
+{
+	return fabric->hprov;
+}
 
+static inline struct fi_provider *hook_to_hprov(const struct fid *fid)
+{
+	return hook_fabric_to_hprov(hook_to_fabric(fid));
+}
 
 struct hook_domain {
 	struct fid_domain domain;
@@ -120,6 +185,11 @@ struct hook_poll {
 int hook_poll_open(struct fid_domain *domain, struct fi_poll_attr *attr,
 		   struct fid_poll **pollset);
 
+/*
+ * EQ
+ */
+
+extern struct fi_ops_eq hook_eq_ops;
 
 struct hook_eq {
 	struct fid_eq eq;
@@ -127,23 +197,34 @@ struct hook_eq {
 	struct hook_fabric *fabric;
 };
 
+ssize_t hook_eq_read(struct fid_eq *eq, uint32_t *event,
+			    void *buf, size_t len, uint64_t flags);
+ssize_t hook_eq_sread(struct fid_eq *eq, uint32_t *event,
+			     void *buf, size_t len, int timeout, uint64_t flags);
+int hook_eq_init(struct fid_fabric *fabric, struct fi_eq_attr *attr,
+		 struct fid_eq **eq, void *context, struct hook_eq *myeq);
 int hook_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
 		 struct fid_eq **eq, void *context);
 
+/*
+ * CQ
+ */
 
 struct hook_cq {
 	struct fid_cq cq;
 	struct fid_cq *hcq;
 	struct hook_domain *domain;
+	void *context;
 };
 
+int hook_cq_init(struct fid_domain *domain, struct fi_cq_attr *attr,
+		 struct fid_cq **cq, void *context, struct hook_cq *mycq);
 int hook_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 		 struct fid_cq **cq, void *context);
 const char *
 hook_cq_strerror(struct fid_cq *cq, int prov_errno,
 		 const void *err_data, char *buf, size_t len);
 
-
 struct hook_cntr {
 	struct fid_cntr cntr;
 	struct fid_cntr *hcntr;
@@ -158,8 +239,11 @@ struct hook_ep {
 	struct fid_ep ep;
 	struct fid_ep *hep;
 	struct hook_domain *domain;
+	void *context;
 };
 
+int hook_endpoint_init(struct fid_domain *domain, struct fi_info *info,
+		       struct fid_ep **ep, void *context, struct hook_ep *myep);
 int hook_endpoint(struct fid_domain *domain, struct fi_info *info,
 		  struct fid_ep **ep, void *context);
 int hook_scalable_ep(struct fid_domain *domain, struct fi_info *info,
@@ -168,6 +252,15 @@ int hook_srx_ctx(struct fid_domain *domain,
 		 struct fi_rx_attr *attr, struct fid_ep **rx_ep,
 		 void *context);
 
+int hook_query_atomic(struct fid_domain *domain, enum fi_datatype datatype,
+		  enum fi_op op, struct fi_atomic_attr *attr, uint64_t flags);
+
+extern struct fi_ops hook_fabric_fid_ops;
+extern struct fi_ops_fabric hook_fabric_ops;
+extern struct fi_ops_domain hook_domain_ops;
+extern struct fi_ops_cq hook_cq_ops;
+extern struct fi_ops_cntr hook_cntr_ops;
+
 extern struct fi_ops_cm hook_cm_ops;
 extern struct fi_ops_msg hook_msg_ops;
 extern struct fi_ops_rma hook_rma_ops;
diff --git a/include/ofi_mem.h b/include/ofi_mem.h
index 51a5c6ff1..9ffc4ac66 100644
--- a/include/ofi_mem.h
+++ b/include/ofi_mem.h
@@ -414,7 +414,6 @@ static inline void ofi_ibuf_free(void *buf)
 
 static inline size_t ofi_buf_index(void *buf)
 {
-	assert(ofi_buf_region(buf)->use_cnt);
 	return ofi_buf_hdr(buf)->index;
 }
 
diff --git a/include/ofi_mr.h b/include/ofi_mr.h
index 341fb4061..e4edca14e 100644
--- a/include/ofi_mr.h
+++ b/include/ofi_mr.h
@@ -46,6 +46,10 @@
 #include <ofi_list.h>
 #include <ofi_tree.h>
 
+struct ofi_mr_info {
+	struct iovec iov;
+};
+
 
 #define OFI_MR_BASIC_MAP (FI_MR_ALLOCATED | FI_MR_PROV_KEY | FI_MR_VIRT_ADDR)
 
@@ -120,6 +124,8 @@ int ofi_monitor_subscribe(struct ofi_mem_monitor *monitor,
 void ofi_monitor_unsubscribe(struct ofi_mem_monitor *monitor,
 			     const void *addr, size_t len);
 
+extern struct ofi_mem_monitor *default_monitor;
+
 /*
  * Userfault fd memory monitor
  */
@@ -134,6 +140,19 @@ void ofi_uffd_cleanup(void);
 
 extern struct ofi_mem_monitor *uffd_monitor;
 
+/*
+ * Memory intercept call memory monitor
+ */
+struct ofi_memhooks {
+	struct ofi_mem_monitor          monitor;
+	struct dlist_entry		intercept_list;
+};
+
+int ofi_memhooks_init(void);
+void ofi_memhooks_cleanup(void);
+
+extern struct ofi_mem_monitor *memhooks_monitor;
+
 
 /*
  * Used to store registered memory regions into a lookup map.  This
@@ -179,7 +198,7 @@ int ofi_mr_close(struct fid *fid);
 int ofi_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 		   uint64_t flags, struct fid_mr **mr_fid);
 int ofi_mr_regv(struct fid *fid, const struct iovec *iov,
-	        size_t count, uint64_t access, uint64_t offset,
+		size_t count, uint64_t access, uint64_t offset,
 		uint64_t requested_key, uint64_t flags,
 		struct fid_mr **mr_fid, void *context);
 int ofi_mr_reg(struct fid *fid, const void *buf, size_t len,
@@ -196,12 +215,13 @@ struct ofi_mr_cache_params {
 	size_t				max_cnt;
 	size_t				max_size;
 	int				merge_regions;
+	char *				monitor;
 };
 
 extern struct ofi_mr_cache_params	cache_params;
 
 struct ofi_mr_entry {
-	struct iovec			iov;
+	struct ofi_mr_info		info;
 	unsigned int			cached:1;
 	unsigned int			subscribed:1;
 	int				use_cnt;
@@ -220,14 +240,14 @@ struct ofi_mr_storage {
 	void				*storage;
 
 	struct ofi_mr_entry *		(*find)(struct ofi_mr_storage *storage,
-						const struct iovec *key);
+						const struct ofi_mr_info *key);
 	struct ofi_mr_entry *		(*overlap)(struct ofi_mr_storage *storage,
 						const struct iovec *key);
 	int				(*insert)(struct ofi_mr_storage *storage,
-						  struct iovec *key,
-						  struct ofi_mr_entry *entry);
+						struct ofi_mr_info *key,
+						struct ofi_mr_entry *entry);
 	int				(*erase)(struct ofi_mr_storage *storage,
-						 struct ofi_mr_entry *entry);
+						struct ofi_mr_entry *entry);
 	void				(*destroy)(struct ofi_mr_storage *storage);
 };
 
diff --git a/include/ofi_prov.h b/include/ofi_prov.h
index 4081ebeb8..2fa887b4b 100644
--- a/include/ofi_prov.h
+++ b/include/ofi_prov.h
@@ -240,15 +240,23 @@ RSTREAM_INI ;
 #endif
 
 #if(HAVE_PERF)
-#  define PERF_HOOK_INI INI_SIG(fi_perf_hook_ini)
-#  define PERF_HOOK_INIT fi_perf_hook_ini()
-PERF_HOOK_INI ;
+#  define HOOK_PERF_INI INI_SIG(fi_hook_perf_ini)
+#  define HOOK_PERF_INIT fi_hook_perf_ini()
+HOOK_PERF_INI ;
 #else
-#  define PERF_HOOK_INIT NULL
+#  define HOOK_PERF_INIT NULL
 #endif
 
-#  define NOOP_HOOK_INI INI_SIG(fi_noop_hook_ini)
-#  define NOOP_HOOK_INIT fi_noop_hook_ini()
-NOOP_HOOK_INI ;
+#if(HAVE_HOOK_DEBUG)
+#  define HOOK_DEBUG_INI INI_SIG(fi_debug_hook_ini)
+#  define HOOK_DEBUG_INIT fi_debug_hook_ini()
+HOOK_DEBUG_INI ;
+#else
+#  define HOOK_DEBUG_INIT NULL
+#endif
+
+#  define HOOK_NOOP_INI INI_SIG(fi_hook_noop_ini)
+#  define HOOK_NOOP_INIT fi_hook_noop_ini()
+HOOK_NOOP_INI ;
 
 #endif /* _OFI_PROV_H_ */
diff --git a/include/ofi_tree.h b/include/ofi_tree.h
index 81b17c014..5415c6682 100644
--- a/include/ofi_tree.h
+++ b/include/ofi_tree.h
@@ -84,7 +84,8 @@ void ofi_rbmap_cleanup(struct ofi_rbmap *map);
 struct ofi_rbnode *ofi_rbmap_find(struct ofi_rbmap *map, void *key);
 struct ofi_rbnode *ofi_rbmap_search(struct ofi_rbmap *map, void *key,
 		int (*compare)(struct ofi_rbmap *map, void *key, void *data));
-int ofi_rbmap_insert(struct ofi_rbmap *map, void *key, void *data);
+int ofi_rbmap_insert(struct ofi_rbmap *map, void *key, void *data,
+		struct ofi_rbnode **node);
 void ofi_rbmap_delete(struct ofi_rbmap *map, struct ofi_rbnode *node);
 int ofi_rbmap_empty(struct ofi_rbmap *map);
 
diff --git a/include/ofi_util.h b/include/ofi_util.h
index 77177af32..9fb544d48 100644
--- a/include/ofi_util.h
+++ b/include/ofi_util.h
@@ -72,9 +72,14 @@
 extern "C" {
 #endif
 
+/* EQ / CQ flags
+ * ERROR: The added entry was the result of an error completion
+ * OVERFLOW: The CQ has overflowed, and events have been lost
+ */
 #define UTIL_FLAG_ERROR		(1ULL << 60)
 #define UTIL_FLAG_OVERFLOW	(1ULL << 61)
 
+/* Indicates that an EP has been bound to a counter */
 #define OFI_CNTR_ENABLED	(1ULL << 61)
 
 #define OFI_Q_STRERROR(prov, level, subsys, q, q_str, entry, q_strerror)	\
diff --git a/include/rdma/fabric.h b/include/rdma/fabric.h
index 40728d262..d905ef526 100644
--- a/include/rdma/fabric.h
+++ b/include/rdma/fabric.h
@@ -135,6 +135,7 @@ typedef struct fid *fid_t;
 #define FI_ATOMIC		(1ULL << 4)
 #define FI_ATOMICS		FI_ATOMIC
 #define FI_MULTICAST		(1ULL << 5)
+#define FI_COLLECTIVE		(1ULL << 6)
 
 #define FI_READ			(1ULL << 8)
 #define FI_WRITE		(1ULL << 9)
diff --git a/include/rdma/fi_atomic.h b/include/rdma/fi_atomic.h
index cc8b1e520..a7dc068b7 100644
--- a/include/rdma/fi_atomic.h
+++ b/include/rdma/fi_atomic.h
@@ -44,6 +44,7 @@ extern "C" {
 
 
 /* Atomic flags */
+#define FI_SCATTER		(1ULL << 57)
 #define FI_FETCH_ATOMIC		(1ULL << 58)
 #define FI_COMPARE_ATOMIC	(1ULL << 59)
 
diff --git a/include/rdma/fi_collective.h b/include/rdma/fi_collective.h
new file mode 100644
index 000000000..67eff6a28
--- /dev/null
+++ b/include/rdma/fi_collective.h
@@ -0,0 +1,234 @@
+/*
+ * Copyright (c) 2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef FI_COLLECTIVE_H
+#define FI_COLLECTIVE_H
+
+#include <rdma/fi_atomic.h>
+#include <rdma/fi_domain.h>
+#include <rdma/fi_cm.h>
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+struct fi_ops_av_set {
+	size_t	size;
+	int	(*set_union)(struct fid_av_set *dst,
+			const struct fid_av_set *src);
+	int	(*intersect)(struct fid_av_set *dst,
+			const struct fid_av_set *src);
+	int	(*diff)(struct fid_av_set *dst, const struct fid_av_set *src);
+	int	(*insert)(struct fid_av_set *set, fi_addr_t addr);
+	int	(*remove)(struct fid_av_set *set, fi_addr_t addr);
+};
+
+struct fid_av_set {
+	struct fid		fid;
+	struct fi_ops_av_set	*ops;
+};
+
+
+struct fi_collective_attr {
+	struct fi_atomic_attr	datatype_attr;
+	size_t			max_members;
+	uint64_t		mode;
+};
+
+struct fi_collective_addr {
+	const struct fid_av_set	*set;
+	fi_addr_t		coll_addr;
+};
+
+struct fi_msg_collective {
+	const struct fi_ioc	*msg_iov;
+	void			**desc;
+	size_t			iov_count;
+	fi_addr_t		coll_addr;
+	enum fi_datatype	datatype;
+	enum fi_op		op;
+	void			*context;
+};
+
+struct fi_ops_collective {
+	size_t	size;
+	ssize_t	(*barrier)(struct fid_ep *ep, fi_addr_t coll_addr,
+			void *context);
+	ssize_t	(*writeread)(struct fid_ep *ep,
+			const void *buf, size_t count, void *desc,
+			void *result, void *result_desc, fi_addr_t coll_addr,
+			enum fi_datatype datatype, enum fi_op op,
+			uint64_t flags, void *context);
+	ssize_t	(*writereadmsg)(struct fid_ep *ep,
+			const struct fi_msg_collective *msg,
+			struct fi_ioc *resultv, void **result_desc,
+			size_t result_count, uint64_t flags);
+};
+
+
+#ifdef FABRIC_DIRECT
+#include <rdma/fi_direct_collective.h>
+#endif /* FABRIC_DIRECT */
+
+#ifndef FABRIC_DIRECT_COLLECTIVE
+
+static inline int
+fi_av_set(struct fid_av *av, struct fi_av_set_attr *attr,
+	  struct fid_av_set **av_set, void * context)
+{
+	return FI_CHECK_OP(av->ops, struct fi_ops_av, av_set) ?
+		av->ops->av_set(av, attr, av_set, context) : -FI_ENOSYS;
+}
+
+static inline int
+fi_av_set_union(struct fid_av_set *dst, const struct fid_av_set *src)
+{
+	return dst->ops->set_union(dst, src);
+}
+
+static inline int
+fi_av_set_intersect(struct fid_av_set *dst, const struct fid_av_set *src)
+{
+	return dst->ops->intersect(dst, src);
+}
+
+static inline int
+fi_av_set_diff(struct fid_av_set *dst, const struct fid_av_set *src)
+{
+	return dst->ops->diff(dst, src);
+}
+
+static inline int
+fi_av_set_insert(struct fid_av_set *set, fi_addr_t addr)
+{
+	return set->ops->insert(set, addr);
+}
+
+static inline int
+fi_av_set_remove(struct fid_av_set *set, fi_addr_t addr)
+{
+	return set->ops->remove(set, addr);
+}
+
+static inline int
+fi_join_collective(struct fid_ep *ep, fi_addr_t coll_addr,
+		   const struct fid_av_set *set,
+		   uint64_t flags, struct fid_mc **mc, void *context)
+{
+	struct fi_collective_addr addr;
+
+	addr.set = set;
+	addr.join_addr = coll_addr;
+	return fi_join(ep, &addr, flags | FI_COLLECTIVE, mc, context);
+}
+
+static inline ssize_t
+fi_barrier(struct fid_ep *ep, fi_addr_t coll_addr, void *context)
+{
+	return ep->collective->barrier(ep, coll_addr, context);
+}
+
+static inline ssize_t
+fi_broadcast(struct fid_ep *ep, void *buf, size_t count, void *desc,
+	     fi_addr_t coll_addr, enum fi_datatype datatype,
+	     enum fi_op op, uint64_t flags, void *context)
+{
+	if (flags & FI_SEND) {
+		return ep->collective->writeread(ep, buf, count, desc,
+			NULL, NULL, coll_addr, datatype, op, flags, context);
+	} else {
+		return ep->collective->writeread(ep, NULL, count, NULL,
+			buf, desc, coll_addr, datatype, op, flags, context);
+	}
+}
+
+static inline ssize_t
+fi_allreduce(struct fid_ep *ep, const void *buf, size_t count, void *desc,
+	     void *result, void *result_desc, fi_addr_t coll_addr,
+	     enum fi_datatype datatype, enum fi_op op,
+	     uint64_t flags, void *context)
+{
+	return ep->collective->writeread(ep, buf, count, desc,
+		result, result_desc, coll_addr, datatype, op, flags, context);
+}
+
+static inline ssize_t
+fi_reduce_scatter(struct fid_ep *ep, const void *buf, size_t count, void *desc,
+		  void *result, void *result_desc,
+		  fi_addr_t coll_addr, enum fi_datatype datatype, enum fi_op op,
+		  uint64_t flags, void *context)
+{
+	return ep->collective->writeread(ep, buf, count, desc,
+		result, result_desc, coll_addr, datatype, op,
+		flags | FI_SCATTER, context);
+}
+
+static inline ssize_t
+fi_alltoall(struct fid_ep *ep, const void *buf, size_t count, void *desc,
+	    void *result, void *result_desc,
+	    fi_addr_t coll_addr, enum fi_datatype datatype,
+	    uint64_t flags, void *context)
+{
+	return ep->collective->writeread(ep, buf, count, desc,
+		result, result_desc, coll_addr, datatype, FI_ALLTOALL,
+		flags, context);
+}
+
+static inline ssize_t
+fi_allgather(struct fid_ep *ep, const void *buf, size_t count, void *desc,
+	     void *result, void *result_desc,
+	     fi_addr_t coll_addr, enum fi_datatype datatype,
+	     uint64_t flags, void *context)
+{
+	return ep->collective->writeread(ep, buf, count, desc,
+		result, result_desc, coll_addr, datatype, FI_ALLGATHER,
+		flags, context);
+}
+
+static inline int
+fi_query_collective(struct fid_domain *domain,
+		    enum fi_datatype datatype, enum fi_op op,
+		    struct fi_collective_attr *attr, uint64_t flags)
+{
+	return fi_query_atomic(domain, datatype, op, &attr->datatype_attr,
+			       flags | FI_COLLECTIVE);
+}
+
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* FI_COLLECTIVE_H */
diff --git a/include/rdma/fi_domain.h b/include/rdma/fi_domain.h
index 138465e89..44638f930 100644
--- a/include/rdma/fi_domain.h
+++ b/include/rdma/fi_domain.h
@@ -33,6 +33,7 @@
 #ifndef FI_DOMAIN_H
 #define FI_DOMAIN_H
 
+#include <string.h>
 #include <rdma/fabric.h>
 #include <rdma/fi_eq.h>
 
@@ -49,6 +50,7 @@ extern "C" {
 
 #define FI_SYMMETRIC		(1ULL << 59)
 #define FI_SYNC_ERR		(1ULL << 58)
+#define FI_UNIVERSE		(1ULL << 57)
 
 
 struct fi_av_attr {
@@ -61,6 +63,18 @@ struct fi_av_attr {
 	uint64_t		flags;
 };
 
+struct fi_av_set_attr {
+	size_t			count;
+	fi_addr_t		start_addr;
+	fi_addr_t		end_addr;
+	uint64_t		stride;
+	size_t			comm_key_size;
+	uint8_t			*comm_key;
+	uint64_t		flags;
+};
+
+struct fid_av_set;
+
 struct fi_ops_av {
 	size_t	size;
 	int	(*insert)(struct fid_av *av, const void *addr, size_t count,
@@ -77,6 +91,8 @@ struct fi_ops_av {
 			size_t *addrlen);
 	const char * (*straddr)(struct fid_av *av, const void *addr,
 			char *buf, size_t *len);
+	int	(*av_set)(struct fid_av *av, struct fi_av_set_attr *attr,
+			struct fid_av_set **av_set, void *context);
 };
 
 struct fid_av {
@@ -119,6 +135,8 @@ struct fi_mr_modify {
 
 #ifndef FABRIC_DIRECT_ATOMIC_DEF
 
+#define FI_COLLECTIVE_OFFSET 256
+
 enum fi_datatype {
 	FI_INT8,
 	FI_UINT8,
@@ -134,7 +152,11 @@ enum fi_datatype {
 	FI_DOUBLE_COMPLEX,
 	FI_LONG_DOUBLE,
 	FI_LONG_DOUBLE_COMPLEX,
-	FI_DATATYPE_LAST
+	/* End of point to point atomic datatypes */
+	FI_DATATYPE_LAST,
+
+	/* Collective datatypes */
+	FI_VOID = FI_COLLECTIVE_OFFSET,
 };
 
 enum fi_op {
@@ -157,7 +179,14 @@ enum fi_op {
 	FI_CSWAP_GE,
 	FI_CSWAP_GT,
 	FI_MSWAP,
-	FI_ATOMIC_OP_LAST
+	/* End of point to point atomic ops */
+	FI_ATOMIC_OP_LAST,
+
+	/* Collective only ops */
+	FI_BARRIER = FI_COLLECTIVE_OFFSET,
+	FI_BROADCAST,
+	FI_ALLTOALL,
+	FI_ALLGATHER,
 };
 
 #endif
@@ -341,7 +370,8 @@ static inline int
 fi_mr_refresh(struct fid_mr *mr, const struct iovec *iov, size_t count,
 	      uint64_t flags)
 {
-	struct fi_mr_modify modify = { 0 };
+	struct fi_mr_modify modify;
+	memset(&modify, 0, sizeof(modify));
 	modify.flags = flags;
 	modify.attr.mr_iov = iov;
 	modify.attr.iov_count = count;
diff --git a/include/rdma/fi_endpoint.h b/include/rdma/fi_endpoint.h
index 6989840ac..7f7a4c814 100644
--- a/include/rdma/fi_endpoint.h
+++ b/include/rdma/fi_endpoint.h
@@ -111,6 +111,7 @@ struct fi_ops_cm;
 struct fi_ops_rma;
 struct fi_ops_tagged;
 struct fi_ops_atomic;
+struct fi_ops_collective;
 
 /*
  * Calls which modify the properties of a endpoint (control, setopt, bind, ...)
@@ -129,6 +130,7 @@ struct fid_ep {
 	struct fi_ops_rma	*rma;
 	struct fi_ops_tagged	*tagged;
 	struct fi_ops_atomic	*atomic;
+	struct fi_ops_collective *collective;
 };
 
 struct fid_pep {
diff --git a/include/windows/config.h b/include/windows/config.h
index e4d01d989..0e3031974 100644
--- a/include/windows/config.h
+++ b/include/windows/config.h
@@ -162,7 +162,7 @@
 #define PACKAGE_NAME "libfabric"
 
 /* Define to the full name and version of this package. */
-#define PACKAGE_STRING "libfabric 1.8.0"
+#define PACKAGE_STRING "libfabric 1.9.0a1"
 
 /* Define to the one symbol short name of this package. */
 #define PACKAGE_TARNAME "libfabric"
@@ -171,7 +171,7 @@
 #define PACKAGE_URL ""
 
 /* Define to the version of this package. */
-#define PACKAGE_VERSION "1.8.0"
+#define PACKAGE_VERSION "1.9.0a1"
 
 /* Define to 1 if pthread_spin_init is available. */
 /* #undef PT_LOCK_SPIN */
diff --git a/include/windows/ifaddrs.h b/include/windows/ifaddrs.h
index 42d537e3c..40d84dea2 100644
--- a/include/windows/ifaddrs.h
+++ b/include/windows/ifaddrs.h
@@ -24,9 +24,11 @@ struct ifaddrs {
 	struct sockaddr *ifa_addr;    /* Address of interface */
 	struct sockaddr *ifa_netmask; /* Netmask of interface */
 
-	struct sockaddr_in in_addr;
-	struct sockaddr_in in_netmask;
+	struct sockaddr_storage in_addrs;
+	struct sockaddr_storage in_netmasks;
+
 	char		   ad_name[16];
+	size_t		   speed;
 };
 
 int getifaddrs(struct ifaddrs **ifap);
diff --git a/include/windows/osd.h b/include/windows/osd.h
index 345b3506b..5c2ea78cb 100644
--- a/include/windows/osd.h
+++ b/include/windows/osd.h
@@ -882,10 +882,7 @@ static inline int ofi_is_loopback_addr(struct sockaddr *addr) {
 		((struct sockaddr_in6 *)addr)->sin6_addr.u.Word[7] == ntohs(1));
 }
 
-static inline size_t ofi_ifaddr_get_speed(struct ifaddrs *ifa)
-{
-	return 0;
-}
+size_t ofi_ifaddr_get_speed(struct ifaddrs *ifa);
 
 /* complex operations implementation */
 
diff --git a/libfabric.vcxproj b/libfabric.vcxproj
index 5a084bd69..7336144cc 100644
--- a/libfabric.vcxproj
+++ b/libfabric.vcxproj
@@ -557,6 +557,7 @@
     <ClCompile Include="prov\util\src\util_poll.c" />
     <ClCompile Include="prov\util\src\util_wait.c" />
     <ClCompile Include="prov\util\src\util_mem_monitor.c" />
+    <ClCompile Include="prov\util\src\util_mem_hooks.c" />
     <ClCompile Include="prov\util\src\util_mr_cache.c" />
     <ClCompile Include="src\common.c" />
     <ClCompile Include="src\enosys.c">
diff --git a/libfabric.vcxproj.filters b/libfabric.vcxproj.filters
index 67a99fca6..e89b39ad3 100644
--- a/libfabric.vcxproj.filters
+++ b/libfabric.vcxproj.filters
@@ -180,6 +180,9 @@
     <ClCompile Include="prov\util\src\util_mem_monitor.c">
       <Filter>Source Files\prov\util</Filter>
     </ClCompile>
+    <ClCompile Include="prov\util\src\util_mem_hooks.c">
+      <Filter>Source Files\prov\util</Filter>
+    </ClCompile>
     <ClCompile Include="prov\util\src\util_mr_cache.c">
       <Filter>Source Files\prov\util</Filter>
     </ClCompile>
diff --git a/man/fi_atomic.3.md b/man/fi_atomic.3.md
index 7fb0d7438..37f9daaea 100644
--- a/man/fi_atomic.3.md
+++ b/man/fi_atomic.3.md
@@ -516,7 +516,9 @@ struct fi_atomic_attr {
 ```
 
 The count attribute field is as defined for the atomic valid calls.  The
-size field indicates the size in bytes of the atomic datatype.
+size field indicates the size in bytes of the atomic datatype.  The
+size field is useful for datatypes that may differ in sizes based on the
+platform or compiler, such FI_LONG_DOUBLE.
 
 ## Completions
 
diff --git a/man/fi_av.3.md b/man/fi_av.3.md
index a4cd5634a..2ae93afc7 100644
--- a/man/fi_av.3.md
+++ b/man/fi_av.3.md
@@ -461,9 +461,6 @@ fabric errno on error.
 Fabric errno values are defined in
 `rdma/fi_errno.h`.
 
-# ERRORS
-
-
 # SEE ALSO
 
 [`fi_getinfo`(3)](fi_getinfo.3.html),
diff --git a/man/fi_av_set.3.md b/man/fi_av_set.3.md
new file mode 100644
index 000000000..f8004b47f
--- /dev/null
+++ b/man/fi_av_set.3.md
@@ -0,0 +1,156 @@
+---
+layout: page
+title: fi_av_set(3)
+tagline: Libfabric Programmer's Manual
+---
+{% include JB/setup %}
+
+# NAME
+
+fi_av_set \- Address vector set operations
+
+fi_av_open / fi_close
+: Open or close an address vector
+
+
+# SYNOPSIS
+
+```c
+#include <rdma/fi_av_set.h>
+
+int fi_av_open(struct fid_domain *domain, struct fi_av_attr *attr,
+    struct fid_av **av, void *context);
+
+int fi_close(struct fid *av_set);
+```
+
+# ARGUMENTS
+
+*av*
+: Address vector
+
+*attr*
+: Address vector set attributes
+
+*context*
+: User specified context associated with the address vector set
+
+*flags*
+: Additional flags to apply to the operation.
+
+# DESCRIPTION
+
+An address vector set (AV set) represents an ordered subset of addresses of an
+address vector.  AV sets are used to identify the participants in a collective
+operation.  Endpoints use the fi_join_collective() operation to associate
+itself with an AV set.  The join collective operation provides an fi_addr that
+is used when communicating with a collective group.
+
+The creation and manipulation of an AV set is a local operation.  No fabric
+traffic is exchanged between peers.  As a result, each peer is responsible
+for creating matching AV sets as part of their collective membership definition.
+See [`fi_collective`(3)](fi_collective.3.html) for a discussion of membership
+models.
+
+## fi_av_set
+
+The fi_av_set call creates a new AV set.  The initial properties of the AV
+set are specified through the struct fi_av_set_attr parameter.  This
+structure is defined below, and allows including a subset of addresses in the
+AV set as part of AV set creation.  Addresses may be added or removed from an
+AV set using the AV set interfaces defined below.
+
+## fi_av_set_attr
+
+{% highlight c %}
+struct fi_av_set_attr {
+	size_t count;
+	fi_addr_t start_addr;
+	fi_addr_t end_addr;
+	uint64_t stride;
+	size_t comm_key_size;
+	uint8_t *comm_key;
+	uint64_t flags;
+};
+{% endhighlight %}
+
+*count*
+: Indicates the expected the number of members that will be a part of
+  the AV set.  The provider uses this to optimize resource allocations.
+
+*start_addr / end_addr*
+: The starting and ending addresses, inclusive, to
+  include as part of the AV set.  The use of start and end address require
+  that the associated AV have been created as type FI_AV_TABLE.  Valid
+  addresses in the AV which fall within the specified range and which meet other
+  requirements (such as stride) will be added as initial members to the AV set.
+  The start_addr and end_addr must be set to FI_ADDR_NOTAVAIL if creating an
+  empty AV set, a communication key is being provided, or the AV is of
+  type FI_AV_MAP.
+
+*stride*
+: The number of entries between successive addresses included in the
+  AV set.  The AV set will include all addresses from start_addr + stride x i,
+  for increasing, non-negative, integer values of i, up to end_addr.  A stride
+  of 1 indicates that all addresses between start_addr and end_addr should be
+  added to the AV set.  Stride should be set to 0 unless the start_addr and
+  end_addr fields are valid.
+
+*comm_key_size*
+: The length of the communication key in bytes.  This
+  field should be 0 if a communication key is not available.
+
+*comm_key*
+: If supported by the fabric, this represents a key
+  associated with the AV set.  The communication key is used by applications
+  that directly manage collective membership through a fabric management agent
+  or resource manager.  The key is used to convey that results of the
+  membership setup to the underlying provider.  The use and format of a
+  communication key is fabric provider specific.
+
+*flags*
+: If the flag FI_UNIVERSE is set, then the AV set will be created
+  containing all addresses stored in the AV.
+
+## fi_av_set_union
+
+The AV set union call adds all addresses in the source AV set that are not
+in the destination AV set to the destination AV set.  Where ordering matters,
+the newly inserted addresses are placed at the end of the AV set.
+
+## fi_av_set_intersect
+
+The AV set intersect call remove all addresses from the destination AV set that
+are not also members of the source AV set.  The order of the addresses in the
+destination AV set is unchanged.
+
+## fi_av_set_diff
+
+The AV set difference call removes all address from the destination AV set
+that are also members of the source AV set.  The order of the addresses in the
+destination AV set is unchanged.
+
+## fi_av_set_insert
+
+The AV set insert call appends the specified address to the end of the AV set.
+
+## fi_av_set_remove
+
+The AV set remove call removes the specified address from the given AV set.
+The order of the remaining addresses in the AV set is unchanged.
+
+# NOTES
+
+Developers who are familiar with MPI will find that AV sets are similar to
+MPI groups, and may act as a direct mapping in some, but not all, situations.
+
+# RETURN VALUES
+
+Returns 0 on success. On error, a negative value corresponding to fabric
+errno is returned. Fabric errno values are defined in
+`rdma/fi_errno.h`.
+
+# SEE ALSO
+
+[`fi_av`(3)](fi_av.3.html),
+[`fi_collective`(3)](fi_collective.3.html)
diff --git a/man/fi_collective.3.md b/man/fi_collective.3.md
new file mode 100644
index 000000000..e0a0246f1
--- /dev/null
+++ b/man/fi_collective.3.md
@@ -0,0 +1,458 @@
+---
+layout: page
+title: fi_collective(3)
+tagline: Libfabric Programmer's Manual
+---
+{% include JB/setup %}
+
+# NAME
+
+fi_join_collective
+: Operation where a subset of peers join a new collective group.
+
+fi_barrier
+: Collective operation that does not complete until all peers have entered
+  the barrier call.
+
+fi_broadcast
+: A single sender transmits data to all receiver peers.
+
+fi_allreduce
+: Collective operation where all peers broadcast an atomic operation to all
+  other peers.
+
+fi_reduce_scatter
+: Collective call where data is collected from all peers and merged (reduced).
+  The results of the reduction is distributed back to the peers, with each
+  peer receiving a slice of the results.
+
+fi_alltoall
+: Each peer distributes a slice of its local data to all peers.
+
+fi_allgather
+: Each peer sends a complete copy of its local data to all peers.
+
+fi_query_collective
+: Returns information about which collective operations are supported by a
+  provider, and limitations on the collective.
+
+# SYNOPSIS
+
+```c
+#include <rdma/fi_collective.h>
+
+int fi_join_collective(struct fid_ep *ep, fi_addr_t coll_addr,
+	const struct fid_av_set *set,
+	uint64_t flags, struct fid_mc **mc, void *context);
+
+ssize_t fi_barrier(struct fid_ep *ep, fi_addr_t coll_addr,
+	void *context);
+
+ssize_t fi_broadcast(struct fid_ep *ep, void *buf, size_t count, void *desc,
+	fi_addr_t coll_addr, enum fi_datatype datatype, enum fi_op op,
+	uint64_t flags, void *context);
+
+ssize_t fi_allreduce(struct fid_ep *ep, const void *buf, size_t count,
+	void *desc, void *result, void *result_desc,
+	fi_addr_t coll_addr, enum fi_datatype datatype, enum fi_op op,
+	uint64_t flags, void *context);
+
+ssize_t fi_reduce_scatter(struct fid_ep *ep, const void *buf, size_t count,
+	void *desc, void *result, void *result_desc,
+	fi_addr_t coll_addr, enum fi_datatype datatype, enum fi_op op,
+	uint64_t flags, void *context);
+
+ssize_t fi_alltoall(struct fid_ep *ep, const void *buf, size_t count,
+	void *desc, void *result, void *result_desc,
+	fi_addr_t coll_addr, enum fi_datatype datatype,
+	uint64_t flags, void *context);
+
+ssize_t fi_allgather(struct fid_ep *ep, const void *buf, size_t count,
+	void *desc, void *result, void *result_desc,
+	fi_addr_t coll_addr, enum fi_datatype datatype,
+	uint64_t flags, void *context);
+
+int fi_query_collective(struct fid_domain *domain,
+	enum fi_datatype datatype, enum fi_op op,
+	struct fi_collective_attr *attr, uint64_t flags);
+```
+
+# ARGUMENTS
+
+*ep*
+: Fabric endpoint on which to initiate collective operation.
+
+*set*
+: Address vector set defining the collective membership.
+
+*mc*
+: Multicast group associated with the collective.
+
+*buf*
+: Local data buffer that specifies first operand of collective operation
+
+*datatype*
+: Datatype associated with atomic operands
+
+*op*
+: Atomic operation to perform
+
+*result*
+: Local data buffer to store the result of the collective operation.
+
+*desc / result_desc*
+: Data descriptor associated with the local data buffer
+  and local result buffer, respectively.
+
+*coll_addr*
+: Address referring to the collective group of endpoints.
+
+*flags*
+: Additional flags to apply for the atomic operation
+
+*context*
+: User specified pointer to associate with the operation.  This parameter is
+  ignored if the operation will not generate a successful completion, unless
+  an op flag specifies the context parameter be used for required input.
+
+# DESCRIPTION
+
+In general collective operations can be thought of as coordinated atomic
+operations between a set of peer endpoints.  Readers should refer to the
+[`fi_atomic`(3)](fi_atomic.3.html) man page for details on the
+atomic operations and datatypes defined by libfabric.
+
+A collective operation is a group communication exchange.  It involves
+multiple peers exchanging data with other peers participating in the
+collective call.  Collective operations require close coordination by all
+participating members.  All participants must invoke the same collective call
+before any single member can complete its operation locally.  As a result,
+collective calls can strain the fabric, as well as local and remote data
+buffers.
+
+Libfabric collective interfaces target fabrics that support offloading
+portions of the collective communication into network switches, NICs, and
+other devices.  However, no implementation requirement is placed on the
+provider.
+
+The first step in using a collective call is identifying the peer endpoints
+that will participate.  Collective membership follows one of two models, both
+supported by libfabric.  In the first model, the application manages the
+membership.  This usually means that the application is performing a
+collective operation itself using point to point communication to identify
+the members who will participate.  Additionally, the application may be
+interacting with a fabric resource manager to reserve network resources
+needed to execute collective operations.  In this model, the application will
+inform libfabric that the membership has already been established.
+
+A separate model moves the membership management under libfabric and directly
+into the provider.  In this model, the application must identify which peer
+addresses will be members.  That information is conveyed to the libfabric
+provider, which is then responsible for coordinating the creation of the
+collective group.  In the provider managed model, the provider will usually
+perform the necessary collective operation to establish the communication
+group and interact with any fabric management agents.
+
+In both models, the collective membership is communicated to the provider by
+creating and configuring an address vector set (AV set).  An AV set
+represents an ordered subset of addresses in an address vector (AV).
+Details on creating and configuring an AV set are available in
+[`fi_av_set`(3)](fi_av_set.3.html).
+
+Once an AV set has been programmed with the collective membership
+information, an endpoint is joined to the set.  This uses the fi_join_collective
+operation and operates asynchronously.  This differs from how an endpoint is
+associated synchronously with an AV using the fi_ep_bind() call.  Upon
+completion of the fi_join_collective operation, an fi_addr is provided that
+is used as the target address when invoking a collective operation.
+
+For developer convenience, a set of collective APIs are defined.  However,
+these are inline wrappers around the atomic interfaces.  Collective APIs
+differ from message and RMA interfaces in that the format of the data is
+known to the provider, and the collective may perform an operation on that
+data.  This aligns collective operations closely with the atomic interfaces.
+
+## Join Collective (fi_join_collective)
+
+This call attaches an endpoint to a collective membership group.  Libfabric
+treats collective members as a multicast group, and the fi_join_collective
+call attaches the endpoint to that multicast group.  By default, the endpoint
+will join the group based on the data transfer capabilities of the endpoint.
+For example, if the endpoint has been configured to both send and receive data,
+then the endpoint will be able to initiate and receive transfers to and from
+the collective.  The input flags may be used to restrict access to the
+collective group, subject to endpoint capability limitations.
+
+Join collective operations complete asynchronously, and may involve fabric
+transfers, dependent on the provider implementation.  An endpoint must be bound
+to an event queue prior to calling fi_join_collective.  The result of the join
+operation will be reported to the EQ as an FI_JOIN_COMPLETE event.  Applications
+cannot issue collective transfers until receiving notification that the join
+operation has completed.  Note that an endpoint may begin receiving
+messages from the collective group as soon as the join completes, which can
+occur prior to the FI_JOIN_COMPLETE event being generated.
+
+The join collective operation is itself a collective operation.  All
+participating peers must call fi_join_collective before any individual peer
+will report that the join has completed.  Application managed collective
+memberships are an exception.  With application managed memberships, the
+fi_join_collective call may be completed locally without fabric communication.
+For provider managed memberships, the join collective call requires as
+input a coll_addr that refers to an existing collective group.  The
+fi_join_collective call will create a new collective subgroup.  If there is
+no existing collective group (e.g. this is the first group being created),
+or if application managed memberships are used, coll_addr should be set to
+FI_ADDR_UNAVAIL.  For provider managed memberships, this will result in
+using all entries in the associated AV as the base.
+
+Applications must call fi_close on the collective group to disconnect the
+endpoint from the group.  After a join operation has completed, the
+fi_mc_addr call may be used to retrieve the address associated with the
+multicast group.  See [`fi_cm`(3)](fi_cm.3.html) for additional details on
+fi_mc_addr().
+
+## Barrier (fi_barrier)
+
+The fi_barrier operation provides a mechanism to synchronize peers.  Barrier
+does not result in any data being transferred at the application level.  A
+barrier does not complete locally until all peers have invoked the barrier
+call.  This signifies to the local application that work by peers that
+completed prior to them calling barrier has finished.
+
+## Broadcast (fi_broadcast)
+
+fi_broadcast transfers an array of data from a single sender to all other
+members of the collective group.  The sender of the broadcast data must
+specify the FI_SEND flag, while receivers use the FI_RECV flag.  The input
+buf parameter is treated as either the transmit buffer, if FI_SEND is set, or
+the receive buffer, if FI_RECV is set.  Either the FI_SEND or FI_RECV flag
+must be set.  The broadcast operation acts as an atomic write or read to a
+data array.  As a result, the format of the data in buf is specified through
+the datatype parameter.  Any non-void datatype may be broadcast.
+
+The following diagram shows an example of broadcast being used to transfer an
+array of integers to a group of peers.
+
+```
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+ |____^    ^
+ |_________|
+ broadcast
+```
+
+## All Reduce (fi_allreduce)
+
+fi_allreduce can be described as all peers providing input into an atomic
+operation, with the result copied back to each peer.  Conceptually, this can
+be viewed as each peer issuing a multicast atomic operation to all other
+peers, fetching the results, and combining them.  The combining of the
+results is referred to as the reduction.  The fi_allreduce() operation takes
+as input an array of data and the specified atomic operation to perform.  The
+results of the reduction are written into the result buffer.
+
+Any non-void datatype may be specified.  Valid atomic operations are listed
+below in the fi_query_collective call.  The following diagram shows an
+example of an all reduce operation involving summing an array of integers
+between three peers.
+
+```
+ [1]  [1]  [1]
+ [5]  [5]  [5]
+ [9]  [9]  [9]
+   \   |   /
+      sum
+   /   |   \
+ [3]  [3]  [3]
+[15] [15] [15]
+[27] [27] [27]
+  All Reduce
+```
+
+## All to All (fi_alltoall)
+
+The fi_alltoall collective involves distributing (or scattering) different
+portions of an array of data to peers.  It is best explained using an
+example.  Here three peers perform an all to all collective to exchange
+different entries in an integer array.
+
+```
+[1]   [2]   [3]
+[5]   [6]   [7]
+[9]  [10]  [11]
+   \   |   /
+   All to all
+   /   |   \
+[1]   [5]   [9]
+[5]   [6]   [7]
+[9]  [10]  [11]
+```
+
+All to all operations may be performed on any non-void datatype.  However,
+all to all does not perform an operation on the data itself, so no operation
+is specified.
+
+## Reduce-Scatter (fi_reduce_scatter)
+
+The fi_reduce_scatter collective is similar to an fi_allreduce operation,
+followed by all to all.  With reduce scatter, all peers provide input into an
+atomic operation, similar to all reduce.  However, rather than the full result
+being copied to each peer, each participant receives only a slice of the result.
+
+This is shown by the following example:
+
+```
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+  \   |   /
+     sum (reduce)
+      |
+     [3]
+    [15]
+    [27]
+      |
+   scatter
+  /   |   \
+[3] [15] [27]
+```
+
+The reduce scatter call supports the same datatype and atomic operation as
+fi_allreduce.
+
+## All Gather (fi_allgather)
+
+Conceptually, all gather can be viewed as the opposite of the scatter
+component from reduce-scatter.  All gather collects data from all peers into
+a single array, then copies that array back to each peer.
+
+```
+[1]  [5]  [9]
+  \   |   /
+ All gather
+  /   |   \
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+```
+
+All gather may be performed on any non-void datatype.  However, all gather
+does not perform an operation on the data itself, so no operation is
+specified.
+
+## Query Collective Attributes (fi_query_collective)
+
+The fi_query_collective call reports which collective operations are
+supported by the underlying provider, for suitably configured endpoints.
+Collective operations needed by an application that are not supported
+by the provider must be implemented by the application.  The query
+call checks whether a provider supports a specific collective operation
+for a given datatype and operation, if applicable.
+
+The datatype and operation of the collective are provided as input
+into fi_query_collective.  For operations that do not exchange
+application data, such as fi_barrier, the datatype should be set to
+FI_VOID.  The op parameter may reference one of these atomic opcodes:
+FI_MIN, FI_MAX, FI_SUM, FI_PROD, FI_LOR, FI_LAND, FI_BOR, FI_BAND,
+FI_LXOR, FI_BXOR, or a collective operation: FI_BARRIER, FI_BROADCAST,
+FI_ALLTOALL, FI_ALLGATHER.  The use of an atomic opcode will indicate
+if the provider supports the fi_allreduce() call for the given
+operation and datatype, unless the FI_SCATTER flag has been specified.  If
+FI_SCATTER has been set, query will return if the provider supports the
+fi_reduce_scatter() call for the given operation and datatype.
+Specifying a collective operation for the op parameter queries support
+for the corresponding collective.
+
+On success, fi_query_collective will provide information about
+the supported limits through the struct fi_collective_attr parameter.
+
+{% highlight c %}
+struct fi_collective_attr {
+	struct fi_atomic_attr datatype_attr;
+	size_t max_members;
+	uint64_t mode;
+};
+{% endhighlight %}
+
+For a description of struct fi_atomic_attr, see
+[`fi_atomic`(3)](fi_atomic.3.html).
+
+*datatype_attr.count*
+: The maximum number of elements that may be used with the collective.
+
+*datatype.size*
+: The size of the datatype as supported by the provider.  Applications
+  should validate the size of datatypes that differ based on the platform,
+  such as FI_LONG_DOUBLE.
+
+*max_members*
+: The maximum number of peers that may participate in a collective
+  operation.
+
+*mode*
+: This field is reserved and should be 0.
+
+If a collective operation is supported, the query call will return 0,
+along with attributes on the limits for using that collective operation
+through the provider.
+
+## Completions
+
+Collective operations map to underlying fi_atomic operations.  For a
+discussion of atomic completion semantics, see
+[`fi_atomic`(3)](fi_atomic.3.html).  The completion, ordering, and
+atomicity of collective operations match those defined for point to
+point atomic operations.
+
+# FLAGS
+
+The following flags are defined for the specified operations.
+
+*FI_SEND*
+: Applies to fi_broadcast() operations.  This indicates that the caller
+  is the transmitter of the broadcast data.  There should only be a single
+  transmitter for each broadcast collective operation.
+
+*FI_RECV*
+: Applies to fi_broadcast() operation.  This indicates that the caller
+  is the receiver of broadcase data.
+
+*FI_SCATTER*
+: Applies to fi_query_collective.  When set, requests attribute information
+  on the reduce-scatter collective operation.
+
+# RETURN VALUE
+
+Returns 0 on success. On error, a negative value corresponding to fabric
+errno is returned. Fabric errno values are defined in
+`rdma/fi_errno.h`.
+
+# ERRORS
+
+*-FI_EAGAIN*
+: See [`fi_msg`(3)](fi_msg.3.html) for a detailed description of handling
+  FI_EAGAIN.
+
+*-FI_EOPNOTSUPP*
+: The requested atomic operation is not supported on this endpoint.
+
+*-FI_EMSGSIZE*
+: The number of collective operations in a single request exceeds that
+  supported by the underlying provider.
+
+# NOTES
+
+Collective operations map to atomic operations.  As such, they follow
+most of the conventions and restrictions as peer to peer atomic operations.
+This includes data atomicity, data alignment, and message ordering
+semantics.  See [`fi_atomic`(3)](fi_atomic.3.html) for additional
+information on the datatypes and operations defined for atomic and
+collective operations.
+
+# SEE ALSO
+
+[`fi_getinfo`(3)](fi_getinfo.3.html),
+[`fi_av`(3)](fi_av.3.html),
+[`fi_atomic`(3)](fi_atomic.3.html),
+[`fi_cm`(3)](fi_cm.3.html)
diff --git a/man/fi_hook.7.md b/man/fi_hook.7.md
index 8ab726128..75571a49b 100644
--- a/man/fi_hook.7.md
+++ b/man/fi_hook.7.md
@@ -28,7 +28,7 @@ usually identified by 'hook' appearing in the provider name.
 
 Known hooking providers include the following:
 
-*ofi_perf_hook*
+*ofi_hook_perf*
 : This hooks 'fast path' data operation calls.  Performance data is
   captured on call entrance and exit, in order to provide an average of
   how long each call takes to complete.  See the PERFORMANCE HOOKS section
diff --git a/man/fi_verbs.7.md b/man/fi_verbs.7.md
index 1bf8da428..4c239bea4 100644
--- a/man/fi_verbs.7.md
+++ b/man/fi_verbs.7.md
@@ -178,10 +178,6 @@ The verbs provider checks for the following environment variables.
 *FI_VERBS_MIN_RNR_TIMER*
 : Set min_rnr_timer QP attribute (0 - 31) (default: 12)
 
-*FI_VERBS_USE_ODP*
-: Enable On-Demand-Paging (ODP) experimental feature. The feature is supported only
-  on Mellanox OFED (default: 0)
-
 *FI_VERBS_CQREAD_BUNCH_SIZE*
 : The number of entries to be read from the verbs completion queue at a time (default: 8).
 
diff --git a/man/man3/fi_allgather.3 b/man/man3/fi_allgather.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_allgather.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_allreduce.3 b/man/man3/fi_allreduce.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_allreduce.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_alltoall.3 b/man/man3/fi_alltoall.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_alltoall.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_atomic.3 b/man/man3/fi_atomic.3
index c70ea8fea..36ed67a91 100644
--- a/man/man3/fi_atomic.3
+++ b/man/man3/fi_atomic.3
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_atomic" "3" "2019\-02\-27" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_atomic" "3" "2019\-07\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -665,6 +665,8 @@ struct\ fi_atomic_attr\ {
 .PP
 The count attribute field is as defined for the atomic valid calls.
 The size field indicates the size in bytes of the atomic datatype.
+The size field is useful for datatypes that may differ in sizes based on
+the platform or compiler, such FI_LONG_DOUBLE.
 .SS Completions
 .PP
 Completed atomic operations are reported to the initiator of the request
diff --git a/man/man3/fi_av.3 b/man/man3/fi_av.3
index 565ed4591..34cfbe939 100644
--- a/man/man3/fi_av.3
+++ b/man/man3/fi_av.3
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_av" "3" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_av" "3" "2019\-07\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -550,7 +550,6 @@ FI_ADDR_NOTAVAIL.
 All other calls return 0 on success, or a negative value corresponding
 to fabric errno on error.
 Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
-.SH ERRORS
 .SH SEE ALSO
 .PP
 \f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
diff --git a/man/man3/fi_av_set.3 b/man/man3/fi_av_set.3
new file mode 100644
index 000000000..8ba0dcb5a
--- /dev/null
+++ b/man/man3/fi_av_set.3
@@ -0,0 +1,181 @@
+.\" Automatically generated by Pandoc 1.19.2.4
+.\"
+.TH "fi_av_set" "3" "2019\-07\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.hy
+.SH NAME
+.PP
+fi_av_set \- Address vector set operations
+.TP
+.B fi_av_open / fi_close
+Open or close an address vector
+.RS
+.RE
+.SH SYNOPSIS
+.IP
+.nf
+\f[C]
+#include\ <rdma/fi_av_set.h>
+
+int\ fi_av_open(struct\ fid_domain\ *domain,\ struct\ fi_av_attr\ *attr,
+\ \ \ \ struct\ fid_av\ **av,\ void\ *context);
+
+int\ fi_close(struct\ fid\ *av_set);
+\f[]
+.fi
+.SH ARGUMENTS
+.TP
+.B \f[I]av\f[]
+Address vector
+.RS
+.RE
+.TP
+.B \f[I]attr\f[]
+Address vector set attributes
+.RS
+.RE
+.TP
+.B \f[I]context\f[]
+User specified context associated with the address vector set
+.RS
+.RE
+.TP
+.B \f[I]flags\f[]
+Additional flags to apply to the operation.
+.RS
+.RE
+.SH DESCRIPTION
+.PP
+An address vector set (AV set) represents an ordered subset of addresses
+of an address vector.
+AV sets are used to identify the participants in a collective operation.
+Endpoints use the fi_join_collective() operation to associate itself
+with an AV set.
+The join collective operation provides an fi_addr that is used when
+communicating with a collective group.
+.PP
+The creation and manipulation of an AV set is a local operation.
+No fabric traffic is exchanged between peers.
+As a result, each peer is responsible for creating matching AV sets as
+part of their collective membership definition.
+See \f[C]fi_collective\f[](3) for a discussion of membership models.
+.SS fi_av_set
+.PP
+The fi_av_set call creates a new AV set.
+The initial properties of the AV set are specified through the struct
+fi_av_set_attr parameter.
+This structure is defined below, and allows including a subset of
+addresses in the AV set as part of AV set creation.
+Addresses may be added or removed from an AV set using the AV set
+interfaces defined below.
+.SS fi_av_set_attr
+.IP
+.nf
+\f[C]
+struct\ fi_av_set_attr\ {
+\ \ \ \ size_t\ count;
+\ \ \ \ fi_addr_t\ start_addr;
+\ \ \ \ fi_addr_t\ end_addr;
+\ \ \ \ uint64_t\ stride;
+\ \ \ \ size_t\ comm_key_size;
+\ \ \ \ uint8_t\ *comm_key;
+\ \ \ \ uint64_t\ flags;
+};
+\f[]
+.fi
+.TP
+.B \f[I]count\f[]
+Indicates the expected the number of members that will be a part of the
+AV set.
+The provider uses this to optimize resource allocations.
+.RS
+.RE
+.TP
+.B \f[I]start_addr / end_addr\f[]
+The starting and ending addresses, inclusive, to include as part of the
+AV set.
+The use of start and end address require that the associated AV have
+been created as type FI_AV_TABLE.
+Valid addresses in the AV which fall within the specified range and
+which meet other requirements (such as stride) will be added as initial
+members to the AV set.
+The start_addr and end_addr must be set to FI_ADDR_NOTAVAIL if creating
+an empty AV set, a communication key is being provided, or the AV is of
+type FI_AV_MAP.
+.RS
+.RE
+.TP
+.B \f[I]stride\f[]
+The number of entries between successive addresses included in the AV
+set.
+The AV set will include all addresses from start_addr + stride x i, for
+increasing, non\-negative, integer values of i, up to end_addr.
+A stride of 1 indicates that all addresses between start_addr and
+end_addr should be added to the AV set.
+Stride should be set to 0 unless the start_addr and end_addr fields are
+valid.
+.RS
+.RE
+.TP
+.B \f[I]comm_key_size\f[]
+The length of the communication key in bytes.
+This field should be 0 if a communication key is not available.
+.RS
+.RE
+.TP
+.B \f[I]comm_key\f[]
+If supported by the fabric, this represents a key associated with the AV
+set.
+The communication key is used by applications that directly manage
+collective membership through a fabric management agent or resource
+manager.
+The key is used to convey that results of the membership setup to the
+underlying provider.
+The use and format of a communication key is fabric provider specific.
+.RS
+.RE
+.TP
+.B \f[I]flags\f[]
+If the flag FI_UNIVERSE is set, then the AV set will be created
+containing all addresses stored in the AV.
+.RS
+.RE
+.SS fi_av_set_union
+.PP
+The AV set union call adds all addresses in the source AV set that are
+not in the destination AV set to the destination AV set.
+Where ordering matters, the newly inserted addresses are placed at the
+end of the AV set.
+.SS fi_av_set_intersect
+.PP
+The AV set intersect call remove all addresses from the destination AV
+set that are not also members of the source AV set.
+The order of the addresses in the destination AV set is unchanged.
+.SS fi_av_set_diff
+.PP
+The AV set difference call removes all address from the destination AV
+set that are also members of the source AV set.
+The order of the addresses in the destination AV set is unchanged.
+.SS fi_av_set_insert
+.PP
+The AV set insert call appends the specified address to the end of the
+AV set.
+.SS fi_av_set_remove
+.PP
+The AV set remove call removes the specified address from the given AV
+set.
+The order of the remaining addresses in the AV set is unchanged.
+.SH NOTES
+.PP
+Developers who are familiar with MPI will find that AV sets are similar
+to MPI groups, and may act as a direct mapping in some, but not all,
+situations.
+.SH RETURN VALUES
+.PP
+Returns 0 on success.
+On error, a negative value corresponding to fabric errno is returned.
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+.SH SEE ALSO
+.PP
+\f[C]fi_av\f[](3), \f[C]fi_collective\f[](3)
+.SH AUTHORS
+OpenFabrics.
diff --git a/man/man3/fi_av_set_diff.3 b/man/man3/fi_av_set_diff.3
new file mode 100644
index 000000000..c68505184
--- /dev/null
+++ b/man/man3/fi_av_set_diff.3
@@ -0,0 +1 @@
+.so man3/fi_av_set.3
diff --git a/man/man3/fi_av_set_insert.3 b/man/man3/fi_av_set_insert.3
new file mode 100644
index 000000000..c68505184
--- /dev/null
+++ b/man/man3/fi_av_set_insert.3
@@ -0,0 +1 @@
+.so man3/fi_av_set.3
diff --git a/man/man3/fi_av_set_intersect.3 b/man/man3/fi_av_set_intersect.3
new file mode 100644
index 000000000..c68505184
--- /dev/null
+++ b/man/man3/fi_av_set_intersect.3
@@ -0,0 +1 @@
+.so man3/fi_av_set.3
diff --git a/man/man3/fi_av_set_remove.3 b/man/man3/fi_av_set_remove.3
new file mode 100644
index 000000000..c68505184
--- /dev/null
+++ b/man/man3/fi_av_set_remove.3
@@ -0,0 +1 @@
+.so man3/fi_av_set.3
diff --git a/man/man3/fi_av_set_union.3 b/man/man3/fi_av_set_union.3
new file mode 100644
index 000000000..c68505184
--- /dev/null
+++ b/man/man3/fi_av_set_union.3
@@ -0,0 +1 @@
+.so man3/fi_av_set.3
diff --git a/man/man3/fi_barrier.3 b/man/man3/fi_barrier.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_barrier.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_broadcast.3 b/man/man3/fi_broadcast.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_broadcast.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_collective.3 b/man/man3/fi_collective.3
new file mode 100644
index 000000000..ba4bfb1e2
--- /dev/null
+++ b/man/man3/fi_collective.3
@@ -0,0 +1,545 @@
+.\" Automatically generated by Pandoc 1.19.2.4
+.\"
+.TH "fi_collective" "3" "2019\-07\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.hy
+.SH NAME
+.TP
+.B fi_join_collective
+Operation where a subset of peers join a new collective group.
+.RS
+.RE
+.TP
+.B fi_barrier
+Collective operation that does not complete until all peers have entered
+the barrier call.
+.RS
+.RE
+.TP
+.B fi_broadcast
+A single sender transmits data to all receiver peers.
+.RS
+.RE
+.TP
+.B fi_allreduce
+Collective operation where all peers broadcast an atomic operation to
+all other peers.
+.RS
+.RE
+.TP
+.B fi_reduce_scatter
+Collective call where data is collected from all peers and merged
+(reduced).
+The results of the reduction is distributed back to the peers, with each
+peer receiving a slice of the results.
+.RS
+.RE
+.TP
+.B fi_alltoall
+Each peer distributes a slice of its local data to all peers.
+.RS
+.RE
+.TP
+.B fi_allgather
+Each peer sends a complete copy of its local data to all peers.
+.RS
+.RE
+.TP
+.B fi_query_collective
+Returns information about which collective operations are supported by a
+provider, and limitations on the collective.
+.RS
+.RE
+.SH SYNOPSIS
+.IP
+.nf
+\f[C]
+#include\ <rdma/fi_collective.h>
+
+int\ fi_join_collective(struct\ fid_ep\ *ep,\ fi_addr_t\ coll_addr,
+\ \ \ \ const\ struct\ fid_av_set\ *set,
+\ \ \ \ uint64_t\ flags,\ struct\ fid_mc\ **mc,\ void\ *context);
+
+ssize_t\ fi_barrier(struct\ fid_ep\ *ep,\ fi_addr_t\ coll_addr,
+\ \ \ \ void\ *context);
+
+ssize_t\ fi_broadcast(struct\ fid_ep\ *ep,\ void\ *buf,\ size_t\ count,\ void\ *desc,
+\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
+\ \ \ \ uint64_t\ flags,\ void\ *context);
+
+ssize_t\ fi_allreduce(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
+\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
+\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
+\ \ \ \ uint64_t\ flags,\ void\ *context);
+
+ssize_t\ fi_reduce_scatter(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
+\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
+\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
+\ \ \ \ uint64_t\ flags,\ void\ *context);
+
+ssize_t\ fi_alltoall(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
+\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
+\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,
+\ \ \ \ uint64_t\ flags,\ void\ *context);
+
+ssize_t\ fi_allgather(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
+\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
+\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,
+\ \ \ \ uint64_t\ flags,\ void\ *context);
+
+int\ fi_query_collective(struct\ fid_domain\ *domain,
+\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
+\ \ \ \ struct\ fi_collective_attr\ *attr,\ uint64_t\ flags);
+\f[]
+.fi
+.SH ARGUMENTS
+.TP
+.B \f[I]ep\f[]
+Fabric endpoint on which to initiate collective operation.
+.RS
+.RE
+.TP
+.B \f[I]set\f[]
+Address vector set defining the collective membership.
+.RS
+.RE
+.TP
+.B \f[I]mc\f[]
+Multicast group associated with the collective.
+.RS
+.RE
+.TP
+.B \f[I]buf\f[]
+Local data buffer that specifies first operand of collective operation
+.RS
+.RE
+.TP
+.B \f[I]datatype\f[]
+Datatype associated with atomic operands
+.RS
+.RE
+.TP
+.B \f[I]op\f[]
+Atomic operation to perform
+.RS
+.RE
+.TP
+.B \f[I]result\f[]
+Local data buffer to store the result of the collective operation.
+.RS
+.RE
+.TP
+.B \f[I]desc / result_desc\f[]
+Data descriptor associated with the local data buffer and local result
+buffer, respectively.
+.RS
+.RE
+.TP
+.B \f[I]coll_addr\f[]
+Address referring to the collective group of endpoints.
+.RS
+.RE
+.TP
+.B \f[I]flags\f[]
+Additional flags to apply for the atomic operation
+.RS
+.RE
+.TP
+.B \f[I]context\f[]
+User specified pointer to associate with the operation.
+This parameter is ignored if the operation will not generate a
+successful completion, unless an op flag specifies the context parameter
+be used for required input.
+.RS
+.RE
+.SH DESCRIPTION
+.PP
+In general collective operations can be thought of as coordinated atomic
+operations between a set of peer endpoints.
+Readers should refer to the \f[C]fi_atomic\f[](3) man page for details
+on the atomic operations and datatypes defined by libfabric.
+.PP
+A collective operation is a group communication exchange.
+It involves multiple peers exchanging data with other peers
+participating in the collective call.
+Collective operations require close coordination by all participating
+members.
+All participants must invoke the same collective call before any single
+member can complete its operation locally.
+As a result, collective calls can strain the fabric, as well as local
+and remote data buffers.
+.PP
+Libfabric collective interfaces target fabrics that support offloading
+portions of the collective communication into network switches, NICs,
+and other devices.
+However, no implementation requirement is placed on the provider.
+.PP
+The first step in using a collective call is identifying the peer
+endpoints that will participate.
+Collective membership follows one of two models, both supported by
+libfabric.
+In the first model, the application manages the membership.
+This usually means that the application is performing a collective
+operation itself using point to point communication to identify the
+members who will participate.
+Additionally, the application may be interacting with a fabric resource
+manager to reserve network resources needed to execute collective
+operations.
+In this model, the application will inform libfabric that the membership
+has already been established.
+.PP
+A separate model moves the membership management under libfabric and
+directly into the provider.
+In this model, the application must identify which peer addresses will
+be members.
+That information is conveyed to the libfabric provider, which is then
+responsible for coordinating the creation of the collective group.
+In the provider managed model, the provider will usually perform the
+necessary collective operation to establish the communication group and
+interact with any fabric management agents.
+.PP
+In both models, the collective membership is communicated to the
+provider by creating and configuring an address vector set (AV set).
+An AV set represents an ordered subset of addresses in an address vector
+(AV).
+Details on creating and configuring an AV set are available in
+\f[C]fi_av_set\f[](3).
+.PP
+Once an AV set has been programmed with the collective membership
+information, an endpoint is joined to the set.
+This uses the fi_join_collective operation and operates asynchronously.
+This differs from how an endpoint is associated synchronously with an AV
+using the fi_ep_bind() call.
+Upon completion of the fi_join_collective operation, an fi_addr is
+provided that is used as the target address when invoking a collective
+operation.
+.PP
+For developer convenience, a set of collective APIs are defined.
+However, these are inline wrappers around the atomic interfaces.
+Collective APIs differ from message and RMA interfaces in that the
+format of the data is known to the provider, and the collective may
+perform an operation on that data.
+This aligns collective operations closely with the atomic interfaces.
+.SS Join Collective (fi_join_collective)
+.PP
+This call attaches an endpoint to a collective membership group.
+Libfabric treats collective members as a multicast group, and the
+fi_join_collective call attaches the endpoint to that multicast group.
+By default, the endpoint will join the group based on the data transfer
+capabilities of the endpoint.
+For example, if the endpoint has been configured to both send and
+receive data, then the endpoint will be able to initiate and receive
+transfers to and from the collective.
+The input flags may be used to restrict access to the collective group,
+subject to endpoint capability limitations.
+.PP
+Join collective operations complete asynchronously, and may involve
+fabric transfers, dependent on the provider implementation.
+An endpoint must be bound to an event queue prior to calling
+fi_join_collective.
+The result of the join operation will be reported to the EQ as an
+FI_JOIN_COMPLETE event.
+Applications cannot issue collective transfers until receiving
+notification that the join operation has completed.
+Note that an endpoint may begin receiving messages from the collective
+group as soon as the join completes, which can occur prior to the
+FI_JOIN_COMPLETE event being generated.
+.PP
+The join collective operation is itself a collective operation.
+All participating peers must call fi_join_collective before any
+individual peer will report that the join has completed.
+Application managed collective memberships are an exception.
+With application managed memberships, the fi_join_collective call may be
+completed locally without fabric communication.
+For provider managed memberships, the join collective call requires as
+input a coll_addr that refers to an existing collective group.
+The fi_join_collective call will create a new collective subgroup.
+If there is no existing collective group (e.g.
+this is the first group being created), or if application managed
+memberships are used, coll_addr should be set to FI_ADDR_UNAVAIL.
+For provider managed memberships, this will result in using all entries
+in the associated AV as the base.
+.PP
+Applications must call fi_close on the collective group to disconnect
+the endpoint from the group.
+After a join operation has completed, the fi_mc_addr call may be used to
+retrieve the address associated with the multicast group.
+See \f[C]fi_cm\f[](3) for additional details on fi_mc_addr().
+.SS Barrier (fi_barrier)
+.PP
+The fi_barrier operation provides a mechanism to synchronize peers.
+Barrier does not result in any data being transferred at the application
+level.
+A barrier does not complete locally until all peers have invoked the
+barrier call.
+This signifies to the local application that work by peers that
+completed prior to them calling barrier has finished.
+.SS Broadcast (fi_broadcast)
+.PP
+fi_broadcast transfers an array of data from a single sender to all
+other members of the collective group.
+The sender of the broadcast data must specify the FI_SEND flag, while
+receivers use the FI_RECV flag.
+The input buf parameter is treated as either the transmit buffer, if
+FI_SEND is set, or the receive buffer, if FI_RECV is set.
+Either the FI_SEND or FI_RECV flag must be set.
+The broadcast operation acts as an atomic write or read to a data array.
+As a result, the format of the data in buf is specified through the
+datatype parameter.
+Any non\-void datatype may be broadcast.
+.PP
+The following diagram shows an example of broadcast being used to
+transfer an array of integers to a group of peers.
+.IP
+.nf
+\f[C]
+[1]\ \ [1]\ \ [1]
+[5]\ \ [5]\ \ [5]
+[9]\ \ [9]\ \ [9]
+\ |____^\ \ \ \ ^
+\ |_________|
+\ broadcast
+\f[]
+.fi
+.SS All Reduce (fi_allreduce)
+.PP
+fi_allreduce can be described as all peers providing input into an
+atomic operation, with the result copied back to each peer.
+Conceptually, this can be viewed as each peer issuing a multicast atomic
+operation to all other peers, fetching the results, and combining them.
+The combining of the results is referred to as the reduction.
+The fi_allreduce() operation takes as input an array of data and the
+specified atomic operation to perform.
+The results of the reduction are written into the result buffer.
+.PP
+Any non\-void datatype may be specified.
+Valid atomic operations are listed below in the fi_query_collective
+call.
+The following diagram shows an example of an all reduce operation
+involving summing an array of integers between three peers.
+.IP
+.nf
+\f[C]
+\ [1]\ \ [1]\ \ [1]
+\ [5]\ \ [5]\ \ [5]
+\ [9]\ \ [9]\ \ [9]
+\ \ \ \\\ \ \ |\ \ \ /
+\ \ \ \ \ \ sum
+\ \ \ /\ \ \ |\ \ \ \\
+\ [3]\ \ [3]\ \ [3]
+[15]\ [15]\ [15]
+[27]\ [27]\ [27]
+\ \ All\ Reduce
+\f[]
+.fi
+.SS All to All (fi_alltoall)
+.PP
+The fi_alltoall collective involves distributing (or scattering)
+different portions of an array of data to peers.
+It is best explained using an example.
+Here three peers perform an all to all collective to exchange different
+entries in an integer array.
+.IP
+.nf
+\f[C]
+[1]\ \ \ [2]\ \ \ [3]
+[5]\ \ \ [6]\ \ \ [7]
+[9]\ \ [10]\ \ [11]
+\ \ \ \\\ \ \ |\ \ \ /
+\ \ \ All\ to\ all
+\ \ \ /\ \ \ |\ \ \ \\
+[1]\ \ \ [5]\ \ \ [9]
+[5]\ \ \ [6]\ \ \ [7]
+[9]\ \ [10]\ \ [11]
+\f[]
+.fi
+.PP
+All to all operations may be performed on any non\-void datatype.
+However, all to all does not perform an operation on the data itself, so
+no operation is specified.
+.SS Reduce\-Scatter (fi_reduce_scatter)
+.PP
+The fi_reduce_scatter collective is similar to an fi_allreduce
+operation, followed by all to all.
+With reduce scatter, all peers provide input into an atomic operation,
+similar to all reduce.
+However, rather than the full result being copied to each peer, each
+participant receives only a slice of the result.
+.PP
+This is shown by the following example:
+.IP
+.nf
+\f[C]
+[1]\ \ [1]\ \ [1]
+[5]\ \ [5]\ \ [5]
+[9]\ \ [9]\ \ [9]
+\ \ \\\ \ \ |\ \ \ /
+\ \ \ \ \ sum\ (reduce)
+\ \ \ \ \ \ |
+\ \ \ \ \ [3]
+\ \ \ \ [15]
+\ \ \ \ [27]
+\ \ \ \ \ \ |
+\ \ \ scatter
+\ \ /\ \ \ |\ \ \ \\
+[3]\ [15]\ [27]
+\f[]
+.fi
+.PP
+The reduce scatter call supports the same datatype and atomic operation
+as fi_allreduce.
+.SS All Gather (fi_allgather)
+.PP
+Conceptually, all gather can be viewed as the opposite of the scatter
+component from reduce\-scatter.
+All gather collects data from all peers into a single array, then copies
+that array back to each peer.
+.IP
+.nf
+\f[C]
+[1]\ \ [5]\ \ [9]
+\ \ \\\ \ \ |\ \ \ /
+\ All\ gather
+\ \ /\ \ \ |\ \ \ \\
+[1]\ \ [1]\ \ [1]
+[5]\ \ [5]\ \ [5]
+[9]\ \ [9]\ \ [9]
+\f[]
+.fi
+.PP
+All gather may be performed on any non\-void datatype.
+However, all gather does not perform an operation on the data itself, so
+no operation is specified.
+.SS Query Collective Attributes (fi_query_collective)
+.PP
+The fi_query_collective call reports which collective operations are
+supported by the underlying provider, for suitably configured endpoints.
+Collective operations needed by an application that are not supported by
+the provider must be implemented by the application.
+The query call checks whether a provider supports a specific collective
+operation for a given datatype and operation, if applicable.
+.PP
+The datatype and operation of the collective are provided as input into
+fi_query_collective.
+For operations that do not exchange application data, such as
+fi_barrier, the datatype should be set to FI_VOID.
+The op parameter may reference one of these atomic opcodes: FI_MIN,
+FI_MAX, FI_SUM, FI_PROD, FI_LOR, FI_LAND, FI_BOR, FI_BAND, FI_LXOR,
+FI_BXOR, or a collective operation: FI_BARRIER, FI_BROADCAST,
+FI_ALLTOALL, FI_ALLGATHER.
+The use of an atomic opcode will indicate if the provider supports the
+fi_allreduce() call for the given operation and datatype, unless the
+FI_SCATTER flag has been specified.
+If FI_SCATTER has been set, query will return if the provider supports
+the fi_reduce_scatter() call for the given operation and datatype.
+Specifying a collective operation for the op parameter queries support
+for the corresponding collective.
+.PP
+On success, fi_query_collective will provide information about the
+supported limits through the struct fi_collective_attr parameter.
+.IP
+.nf
+\f[C]
+struct\ fi_collective_attr\ {
+\ \ \ \ struct\ fi_atomic_attr\ datatype_attr;
+\ \ \ \ size_t\ max_members;
+\ \ \ \ uint64_t\ mode;
+};
+\f[]
+.fi
+.PP
+For a description of struct fi_atomic_attr, see \f[C]fi_atomic\f[](3).
+.TP
+.B \f[I]datatype_attr.count\f[]
+The maximum number of elements that may be used with the collective.
+.RS
+.RE
+.TP
+.B \f[I]datatype.size\f[]
+The size of the datatype as supported by the provider.
+Applications should validate the size of datatypes that differ based on
+the platform, such as FI_LONG_DOUBLE.
+.RS
+.RE
+.TP
+.B \f[I]max_members\f[]
+The maximum number of peers that may participate in a collective
+operation.
+.RS
+.RE
+.TP
+.B \f[I]mode\f[]
+This field is reserved and should be 0.
+.RS
+.RE
+.PP
+If a collective operation is supported, the query call will return 0,
+along with attributes on the limits for using that collective operation
+through the provider.
+.SS Completions
+.PP
+Collective operations map to underlying fi_atomic operations.
+For a discussion of atomic completion semantics, see
+\f[C]fi_atomic\f[](3).
+The completion, ordering, and atomicity of collective operations match
+those defined for point to point atomic operations.
+.SH FLAGS
+.PP
+The following flags are defined for the specified operations.
+.TP
+.B \f[I]FI_SEND\f[]
+Applies to fi_broadcast() operations.
+This indicates that the caller is the transmitter of the broadcast data.
+There should only be a single transmitter for each broadcast collective
+operation.
+.RS
+.RE
+.TP
+.B \f[I]FI_RECV\f[]
+Applies to fi_broadcast() operation.
+This indicates that the caller is the receiver of broadcase data.
+.RS
+.RE
+.TP
+.B \f[I]FI_SCATTER\f[]
+Applies to fi_query_collective.
+When set, requests attribute information on the reduce\-scatter
+collective operation.
+.RS
+.RE
+.SH RETURN VALUE
+.PP
+Returns 0 on success.
+On error, a negative value corresponding to fabric errno is returned.
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+.SH ERRORS
+.TP
+.B \f[I]\-FI_EAGAIN\f[]
+See \f[C]fi_msg\f[](3) for a detailed description of handling FI_EAGAIN.
+.RS
+.RE
+.TP
+.B \f[I]\-FI_EOPNOTSUPP\f[]
+The requested atomic operation is not supported on this endpoint.
+.RS
+.RE
+.TP
+.B \f[I]\-FI_EMSGSIZE\f[]
+The number of collective operations in a single request exceeds that
+supported by the underlying provider.
+.RS
+.RE
+.SH NOTES
+.PP
+Collective operations map to atomic operations.
+As such, they follow most of the conventions and restrictions as peer to
+peer atomic operations.
+This includes data atomicity, data alignment, and message ordering
+semantics.
+See \f[C]fi_atomic\f[](3) for additional information on the datatypes
+and operations defined for atomic and collective operations.
+.SH SEE ALSO
+.PP
+\f[C]fi_getinfo\f[](3), \f[C]fi_av\f[](3), \f[C]fi_atomic\f[](3),
+\f[C]fi_cm\f[](3)
+.SH AUTHORS
+OpenFabrics.
diff --git a/man/man3/fi_join_collective.3 b/man/man3/fi_join_collective.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_join_collective.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_query_atomic.3 b/man/man3/fi_query_atomic.3
new file mode 100644
index 000000000..35f83b015
--- /dev/null
+++ b/man/man3/fi_query_atomic.3
@@ -0,0 +1 @@
+.so man3/fi_atomic.3
diff --git a/man/man3/fi_query_collective.3 b/man/man3/fi_query_collective.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_query_collective.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man3/fi_reduce_scatter.3 b/man/man3/fi_reduce_scatter.3
new file mode 100644
index 000000000..a6792b0b5
--- /dev/null
+++ b/man/man3/fi_reduce_scatter.3
@@ -0,0 +1 @@
+.so man3/fi_collective.3
diff --git a/man/man7/fi_hook.7 b/man/man7/fi_hook.7
index 1fab08471..c01a43c71 100644
--- a/man/man7/fi_hook.7
+++ b/man/man7/fi_hook.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_hook" "7" "2018\-10\-16" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_hook" "7" "2019\-07\-19" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -28,7 +28,7 @@ the provider name.
 .PP
 Known hooking providers include the following:
 .TP
-.B \f[I]ofi_perf_hook\f[]
+.B \f[I]ofi_hook_perf\f[]
 This hooks \[aq]fast path\[aq] data operation calls.
 Performance data is captured on call entrance and exit, in order to
 provide an average of how long each call takes to complete.
diff --git a/man/man7/fi_verbs.7 b/man/man7/fi_verbs.7
index 495d7b521..545307bde 100644
--- a/man/man7/fi_verbs.7
+++ b/man/man7/fi_verbs.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_verbs" "7" "2019\-06\-21" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_verbs" "7" "2019\-07\-18" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -193,12 +193,6 @@ Set min_rnr_timer QP attribute (0 \- 31) (default: 12)
 .RS
 .RE
 .TP
-.B \f[I]FI_VERBS_USE_ODP\f[]
-Enable On\-Demand\-Paging (ODP) experimental feature.
-The feature is supported only on Mellanox OFED (default: 0)
-.RS
-.RE
-.TP
 .B \f[I]FI_VERBS_CQREAD_BUNCH_SIZE\f[]
 The number of entries to be read from the verbs completion queue at a
 time (default: 8).
diff --git a/prov/efa/src/efa_mr.c b/prov/efa/src/efa_mr.c
index e3ffa89ad..b9f72fd97 100644
--- a/prov/efa/src/efa_mr.c
+++ b/prov/efa/src/efa_mr.c
@@ -67,8 +67,8 @@ int efa_mr_cache_entry_reg(struct ofi_mr_cache *cache,
 	md->mr_fid.fid.fclass = FI_CLASS_MR;
 	md->mr_fid.fid.context = NULL;
 
-	md->mr = efa_cmd_reg_mr(md->domain->pd, entry->iov.iov_base,
-				entry->iov.iov_len, fi_ibv_access);
+	md->mr = efa_cmd_reg_mr(md->domain->pd, entry->info.iov.iov_base,
+				entry->info.iov.iov_len, fi_ibv_access);
 	if (!md->mr) {
 		EFA_WARN_ERRNO(FI_LOG_MR, "efa_cmd_reg_mr", errno);
 		return -errno;
diff --git a/prov/efa/src/efa_verbs/efa_ib_cmd.c b/prov/efa/src/efa_verbs/efa_ib_cmd.c
index b73224e3c..d5013dec4 100644
--- a/prov/efa/src/efa_verbs/efa_ib_cmd.c
+++ b/prov/efa/src/efa_verbs/efa_ib_cmd.c
@@ -2,7 +2,7 @@
  * Copyright (c) 2005 Topspin Communications.  All rights reserved.
  * Copyright (c) 2005 PathScale, Inc.  All rights reserved.
  * Copyright (c) 2006 Cisco Systems, Inc.  All rights reserved.
- * Copyright (c) 2017-2018 Amazon.com, Inc. or its affiliates. All rights reserved.
+ * Copyright (c) 2017-2019 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -270,12 +270,52 @@ int efa_ib_cmd_dealloc_pd(struct ibv_pd *pd)
 	return 0;
 }
 
+/* Madvise requires page aligned addresses and lengths */
+static int efa_madvise(void *addr, size_t length, int advice)
+{
+	int i;
+
+	for (i = 0; i < num_page_sizes; i++) {
+		if (!(madvise(ofi_get_page_start(addr, page_sizes[i]),
+			      ofi_get_page_bytes(addr, length, page_sizes[i]),
+			      advice))) {
+			return 0;
+		}
+	}
+
+	EFA_WARN_ERRNO(FI_LOG_MR, "Failed to set madvise", errno);
+	return -errno;
+}
+
 int efa_ib_cmd_reg_mr(struct ibv_pd *pd, void *addr, size_t length,
 		      uint64_t hca_va, int access,
 		      struct ibv_mr *mr, struct ibv_reg_mr *cmd,
 		      size_t cmd_size,
 		      struct ib_uverbs_reg_mr_resp *resp, size_t resp_size)
 {
+	int err;
+
+	/*
+	 * Linux copy-on-write semantics mean that following a fork() call,
+	 * parent and child processes will have page table entries pointing to
+	 * the same physical page. Since these pages are write protected, if
+	 * either the parent or child writes to the page, the hardware will
+	 * trap the event. The kernel will then allocate a new page and copy
+	 * the contents from the original page, breaking the virtual to physical
+	 * page link for that process.
+	 *
+	 * To prevent this case, marking pinned memory with MADV_DONTFORK
+	 * only allows the memory range to be seen by the parent process, and
+	 * the copy-on-write semantics no longer apply to this memory range.
+	 *
+	 * Since the rdma-core library already does all this work for us,
+	 * when we add rdma-core, we will move the logic down a layer and
+	 * remove it from here.
+	 */
+	err = efa_madvise(addr, length, MADV_DONTFORK);
+	if (err)
+		return err;
+
 	IBV_INIT_CMD_RESP(cmd, cmd_size, REG_MR, resp, resp_size);
 
 	cmd->ibcmd.start	  = (uintptr_t)addr;
@@ -284,8 +324,16 @@ int efa_ib_cmd_reg_mr(struct ibv_pd *pd, void *addr, size_t length,
 	cmd->ibcmd.pd_handle	  = pd->handle;
 	cmd->ibcmd.access_flags = access;
 
-	if (write(pd->context->cmd_fd, cmd, cmd_size) != cmd_size)
-		return -errno;
+	if (write(pd->context->cmd_fd, cmd, cmd_size) != cmd_size) {
+		err = -errno;
+		/*
+		 * We drop the efa madvise error after printing a warn
+		 * since we care more about the write error. Since
+		 * madvise will overwrite errno, we set it before hand.
+		 */
+		efa_madvise(addr, length, MADV_DOFORK);
+		return err;
+	}
 
 	VALGRIND_MAKE_MEM_DEFINED(resp, resp_size);
 
@@ -300,6 +348,7 @@ int efa_ib_cmd_reg_mr(struct ibv_pd *pd, void *addr, size_t length,
 int efa_ib_cmd_dereg_mr(struct ibv_mr *mr)
 {
 	struct ibv_dereg_mr cmd;
+	int err;
 
 	IBV_INIT_CMD(&cmd, sizeof(cmd), DEREG_MR);
 	cmd.ibcmd.mr_handle = mr->handle;
@@ -307,7 +356,15 @@ int efa_ib_cmd_dereg_mr(struct ibv_mr *mr)
 	if (write(mr->context->cmd_fd, &cmd, sizeof(cmd)) != sizeof(cmd))
 		return -errno;
 
-	return 0;
+	/*
+	 *  We want to reset the memory to allow default fork behavior
+	 *  after we have released it from pinning.
+	 *
+	 * This behavior will be removed with the switch to rdma-core.
+	 */
+	err = efa_madvise(mr->addr, mr->length, MADV_DOFORK);
+
+	return err;
 }
 
 int efa_ib_cmd_create_cq(struct ibv_context *context, int cqe,
diff --git a/prov/efa/src/rxr/rxr.h b/prov/efa/src/rxr/rxr.h
index 91b27b62a..8201b12ec 100644
--- a/prov/efa/src/rxr/rxr.h
+++ b/prov/efa/src/rxr/rxr.h
@@ -90,7 +90,12 @@ extern const uint32_t rxr_poison_value;
 /* bounds for random RNR backoff timeout */
 #define RXR_RAND_MIN_TIMEOUT		(40)
 #define RXR_RAND_MAX_TIMEOUT		(120)
-#define RXR_DEF_MAX_RX_WINDOW		(16)
+
+/* bounds for flow control */
+#define RXR_DEF_MAX_RX_WINDOW		(128)
+#define RXR_DEF_MAX_TX_CREDITS		(64)
+#define RXR_DEF_MIN_TX_CREDITS		(32)
+
 /*
  * maximum time (microseconds) we will allow available_bufs for large msgs to
  * be exhausted
@@ -120,6 +125,7 @@ extern const uint32_t rxr_poison_value;
 #define RXR_TAGGED		BIT_ULL(0)
 #define RXR_REMOTE_CQ_DATA	BIT_ULL(1)
 #define RXR_REMOTE_SRC_ADDR	BIT_ULL(2)
+
 /*
  * TODO: In future we will send RECV_CANCEL signal to sender,
  * to stop transmitting large message, this flag is also
@@ -140,6 +146,12 @@ extern const uint32_t rxr_poison_value;
 #define RXR_READ_REQ		(1 << 7)
 #define RXR_READ_DATA		(1 << 8)
 
+/*
+ * Used to provide protocol compatibility across versions that include a
+ * credit request along with the RTS and those that do not
+ */
+#define RXR_CREDIT_REQUEST	BIT_ULL(9)
+
 /*
  * OFI flags
  * The 64-bit flag field is used as follows:
@@ -166,6 +178,8 @@ extern struct util_prov rxr_util_prov;
 
 struct rxr_env {
 	int rx_window_size;
+	int tx_min_credits;
+	int tx_max_credits;
 	int tx_queue_size;
 	int enable_sas_ordering;
 	int recvwin_size;
@@ -282,10 +296,15 @@ struct rxr_av {
 };
 
 struct rxr_peer {
+	bool tx_init;			/* tracks initialization of tx state */
+	bool rx_init;			/* tracks initialization of rx state */
 	struct rxr_robuf *robuf;	/* tracks expected msg_id on rx */
 	uint32_t next_msg_id;		/* sender's view of msg_id */
 	enum rxr_peer_state state;	/* state of CM protocol with peer */
 	unsigned int rnr_state;		/* tracks RNR backoff for peer */
+	size_t tx_pending;		/* tracks pending tx ops to this peer */
+	uint16_t tx_credits;		/* available send credits */
+	uint16_t rx_credits;		/* available credits to allocate */
 	uint64_t rnr_ts;		/* timestamp for RNR backoff tracking */
 	int rnr_queued_pkt_cnt;		/* queued RNR packet count */
 	int timeout_interval;		/* initial RNR timeout value */
@@ -319,6 +338,7 @@ struct rxr_rx_entry {
 
 	uint64_t bytes_done;
 	int64_t window;
+	uint16_t credit_request;
 
 	uint64_t total_len;
 
@@ -381,6 +401,8 @@ struct rxr_tx_entry {
 	uint64_t bytes_acked;
 	uint64_t bytes_sent;
 	int64_t window;
+	uint16_t credit_request;
+	uint16_t credit_allocated;
 
 	uint64_t total_len;
 
@@ -572,7 +594,7 @@ struct rxr_base_hdr {
 	uint16_t flags;
 };
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_base_hdr check");
 #endif
 
@@ -584,17 +606,17 @@ struct rxr_rts_hdr {
 	uint8_t version;
 	uint16_t flags;
 	/* end of rxr_base_hdr */
-	/* TODO: need to add msg_id -> tx_id mapping to remove tx_id and pad */
-	uint8_t pad[2];
+	/* TODO: need to add msg_id -> tx_id mapping to remove tx_id */
+	uint16_t credit_request;
 	uint8_t addrlen;
 	uint8_t rma_iov_count;
 	uint32_t tx_id;
 	uint32_t msg_id;
 	uint64_t tag;
 	uint64_t data_len;
-}; /* 24 bytes without tx_id and padding for it */
+};
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 static_assert(sizeof(struct rxr_rts_hdr) == 32, "rxr_rts_hdr check");
 #endif
 
@@ -605,7 +627,7 @@ struct rxr_connack_hdr {
 	/* end of rxr_base_hdr */
 }; /* 4 bytes */
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_connack_hdr check");
 #endif
 
@@ -621,7 +643,7 @@ struct rxr_cts_hdr {
 	uint64_t window;
 };
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 static_assert(sizeof(struct rxr_cts_hdr) == 24, "rxr_cts_hdr check");
 #endif
 
@@ -636,7 +658,7 @@ struct rxr_data_hdr {
 	uint64_t seg_offset;
 };
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 static_assert(sizeof(struct rxr_data_hdr) == 24, "rxr_data_hdr check");
 #endif
 
@@ -651,7 +673,7 @@ struct rxr_readrsp_hdr {
 	uint64_t seg_size;
 };
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 static_assert(sizeof(struct rxr_readrsp_hdr) == sizeof(struct rxr_data_hdr), "rxr_readrsp_hdr check");
 #endif
 
@@ -731,7 +753,7 @@ struct rxr_pkt_entry {
 #endif
 };
 
-#if defined(static_assert) && defined(__X86_64__)
+#if defined(static_assert) && defined(__x86_64__)
 #if ENABLE_DEBUG
 static_assert(sizeof(struct rxr_pkt_entry) == 128, "rxr_pkt_entry check");
 #else
@@ -760,6 +782,27 @@ static inline struct rxr_peer *rxr_ep_get_peer(struct rxr_ep *ep,
 	return &ep->peer[addr];
 }
 
+static inline void rxr_ep_peer_init(struct rxr_ep *ep, struct rxr_peer *peer)
+{
+	assert(!peer->rx_init);
+	peer->robuf = freestack_pop(ep->robuf_fs);
+	peer->robuf = ofi_recvwin_buf_alloc(peer->robuf,
+					    rxr_env.recvwin_size);
+	assert(peer->robuf);
+	dlist_insert_tail(&peer->entry, &ep->peer_list);
+	peer->rx_credits = rxr_env.rx_window_size;
+	peer->rx_init = 1;
+
+	/*
+	 * If the endpoint has never sent a message to this peer thus far,
+	 * initialize tx state as well.
+	 */
+	if (!peer->tx_init) {
+		peer->tx_credits = rxr_env.tx_max_credits;
+		peer->tx_init = 1;
+	}
+}
+
 struct rxr_rx_entry *rxr_ep_get_rx_entry(struct rxr_ep *ep,
 					 const struct iovec *iov,
 					 size_t iov_count, uint64_t tag,
@@ -775,7 +818,8 @@ struct rxr_rx_entry *rxr_ep_rx_entry_init(struct rxr_ep *ep,
 					  fi_addr_t addr, uint32_t op,
 					  uint64_t flags);
 
-void rxr_generic_tx_entry_init(struct rxr_tx_entry *tx_entry,
+void rxr_generic_tx_entry_init(struct rxr_ep *ep,
+			       struct rxr_tx_entry *tx_entry,
 			       const struct iovec *iov,
 			       size_t iov_count,
 			       const struct fi_rma_iov *rma_iov,
@@ -988,6 +1032,28 @@ static inline int rxr_match_tag(uint64_t tag, uint64_t ignore,
 	return ((tag | ignore) == (match_tag | ignore));
 }
 
+static inline void rxr_ep_inc_tx_pending(struct rxr_ep *ep,
+					 struct rxr_peer *peer)
+{
+	ep->tx_pending++;
+	peer->tx_pending++;
+#if ENABLE_DEBUG
+	ep->sends++;
+#endif
+}
+
+static inline void rxr_ep_dec_tx_pending(struct rxr_ep *ep,
+					 struct rxr_peer *peer,
+					 int failed)
+{
+	ep->tx_pending--;
+	peer->tx_pending--;
+#if ENABLE_DEBUG
+	if (failed)
+		ep->failed_send_comps++;
+#endif
+}
+
 /*
  * Helper function to compute the maximum payload of the RTS header based on
  * the RTS header flags. The header may have a length greater than the possible
@@ -1078,12 +1144,12 @@ ssize_t rxr_ep_post_readrsp(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry
 void rxr_ep_init_connack_pkt_entry(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry,
 				   fi_addr_t addr);
-void rxr_ep_calc_cts_window_credits(struct rxr_ep *ep, uint32_t max_window,
-				    uint64_t size, int *window, int *credits);
+void rxr_ep_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
+				    uint64_t size, int request,
+				    int *window, int *credits);
 void rxr_ep_init_cts_pkt_entry(struct rxr_ep *ep,
 			       struct rxr_rx_entry *rx_entry,
 			       struct rxr_pkt_entry *pkt_entry,
-			       uint32_t max_window,
 			       uint64_t size,
 			       int *credits);
 void rxr_ep_init_readrsp_pkt_entry(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
@@ -1108,7 +1174,6 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err);
 ssize_t rxr_cq_post_cts(struct rxr_ep *ep,
 			struct rxr_rx_entry *rx_entry,
-			uint32_t max_window,
 			uint64_t size);
 
 int rxr_cq_handle_rx_completion(struct rxr_ep *ep,
@@ -1265,8 +1330,7 @@ static inline int rxr_ep_post_cts_or_queue(struct rxr_ep *ep,
 	if (rx_entry->state == RXR_RX_QUEUED_CTS)
 		return 0;
 
-	ret = rxr_cq_post_cts(ep, rx_entry, rxr_env.rx_window_size,
-			      bytes_left);
+	ret = rxr_cq_post_cts(ep, rx_entry, bytes_left);
 	if (OFI_UNLIKELY(ret)) {
 		if (ret == -FI_EAGAIN) {
 			rx_entry->state = RXR_RX_QUEUED_CTS;
diff --git a/prov/efa/src/rxr/rxr_cq.c b/prov/efa/src/rxr/rxr_cq.c
index 2a1b0aed5..eb275874d 100644
--- a/prov/efa/src/rxr/rxr_cq.c
+++ b/prov/efa/src/rxr/rxr_cq.c
@@ -300,6 +300,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 	struct rxr_pkt_entry *pkt_entry;
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_tx_entry *tx_entry;
+	struct rxr_peer *peer;
 	ssize_t ret;
 
 	memset(&err_entry, 0, sizeof(err_entry));
@@ -339,6 +340,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 				&err_entry);
 
 	pkt_entry = (struct rxr_pkt_entry *)err_entry.op_context;
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
 
 	/*
 	 * A connack send could fail at the core provider if the peer endpoint
@@ -355,10 +357,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 		 * the flags instead to determine if this is a send or recv.
 		 */
 		if (err_entry.flags & FI_SEND) {
-#if ENABLE_DEBUG
-			ep->failed_send_comps++;
-#endif
-			ep->tx_pending--;
+			rxr_ep_dec_tx_pending(ep, peer, 1);
 			rxr_release_tx_pkt_entry(ep, pkt_entry);
 		} else if (err_entry.flags & FI_RECV) {
 			rxr_release_rx_pkt_entry(ep, pkt_entry);
@@ -385,10 +384,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 	 * packet. Decrement the tx_pending counter and fall through to
 	 * the rx or tx entry handlers.
 	 */
-	ep->tx_pending--;
-#if ENABLE_DEBUG
-	ep->failed_send_comps++;
-#endif
+	rxr_ep_dec_tx_pending(ep, peer, 1);
 	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
 		tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
 		if (err_entry.err != -FI_EAGAIN ||
@@ -501,7 +497,6 @@ static void rxr_cq_post_connack(struct rxr_ep *ep,
 
 ssize_t rxr_cq_post_cts(struct rxr_ep *ep,
 			struct rxr_rx_entry *rx_entry,
-			uint32_t max_window,
 			uint64_t size)
 {
 	ssize_t ret;
@@ -516,15 +511,13 @@ ssize_t rxr_cq_post_cts(struct rxr_ep *ep,
 	if (OFI_UNLIKELY(!pkt_entry))
 		return -FI_EAGAIN;
 
-	rxr_ep_init_cts_pkt_entry(ep, rx_entry, pkt_entry, max_window, size,
-				  &credits);
+	rxr_ep_init_cts_pkt_entry(ep, rx_entry, pkt_entry, size, &credits);
 
 	ret = rxr_ep_send_pkt(ep, pkt_entry, rx_entry->addr);
 	if (OFI_UNLIKELY(ret))
 		goto release_pkt;
 
 	rx_entry->window = rxr_get_cts_hdr(pkt_entry->pkt)->window;
-	assert(ep->available_data_bufs >= credits);
 	ep->available_data_bufs -= credits;
 
 	/*
@@ -927,6 +920,11 @@ static int rxr_cq_process_rts(struct rxr_ep *ep,
 	ep->rx_pending++;
 #endif
 	rx_entry->state = RXR_RX_RECV;
+	if (rts_hdr->flags & RXR_CREDIT_REQUEST)
+		rx_entry->credit_request = rts_hdr->credit_request;
+	else
+		rx_entry->credit_request = rxr_env.tx_min_credits;
+
 	ret = rxr_ep_post_cts_or_queue(ep, rx_entry, bytes_left);
 	if (pkt_entry->type == RXR_PKT_ENTRY_POSTED)
 		ep->rx_bufs_to_post++;
@@ -945,15 +943,11 @@ static int rxr_cq_reorder_msg(struct rxr_ep *ep,
 	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
 
 	/*
-	 * TODO: Do it at the time of AV insertion w/dup detection.
+	 * TODO: Initialize peer state  at the time of AV insertion
+	 * where duplicate detection is available.
 	 */
-	if (!peer->robuf) {
-		peer->robuf = freestack_pop(ep->robuf_fs);
-		peer->robuf = ofi_recvwin_buf_alloc(peer->robuf,
-						    rxr_env.recvwin_size);
-		assert(peer->robuf);
-		dlist_insert_tail(&peer->entry, &ep->peer_list);
-	}
+	if (!peer->rx_init)
+		rxr_ep_peer_init(ep, peer);
 
 #if ENABLE_DEBUG
 	if (rts_hdr->msg_id != ofi_recvwin_next_exp_id(peer->robuf))
@@ -1079,13 +1073,14 @@ static void rxr_cq_handle_rts(struct rxr_ep *ep,
 
 	if (rxr_need_sas_ordering(ep)) {
 		ret = rxr_cq_reorder_msg(ep, peer, pkt_entry);
-		if (ret && ret != -FI_EALREADY) {
+		if (ret == 1) {
+			/* Packet was queued */
 			return;
 		} else if (OFI_UNLIKELY(ret == -FI_EALREADY)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Duplicate RTS packet msg_id: %" PRIu32
-				" next_msg_id: %" PRIu32 "\n",
-			       rts_hdr->msg_id, peer->next_msg_id);
+				" robuf->exp_msg_id: %" PRIu64 "\n",
+			       rts_hdr->msg_id, peer->robuf->exp_msg_id);
 			if (!rts_hdr->addrlen)
 				rxr_eq_write_error(ep, FI_EIO, ret);
 			rxr_release_rx_pkt_entry(ep, pkt_entry);
@@ -1094,6 +1089,12 @@ static void rxr_cq_handle_rts(struct rxr_ep *ep,
 		} else if (OFI_UNLIKELY(ret == -FI_ENOMEM)) {
 			rxr_eq_write_error(ep, FI_ENOBUFS, -FI_ENOBUFS);
 			return;
+		} else if (OFI_UNLIKELY(ret < 0)) {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"Unknown error %d processing RTS packet msg_id: %"
+				PRIu32 "\n", ret, rts_hdr->msg_id);
+			rxr_eq_write_error(ep, FI_EIO, ret);
+			return;
 		}
 
 		/* processing the expected packet */
@@ -1139,9 +1140,12 @@ void rxr_cq_handle_pkt_with_data(struct rxr_ep *ep,
 				 char *data, size_t seg_offset,
 				 size_t seg_size)
 {
+	struct rxr_peer *peer;
 	uint64_t bytes;
 	ssize_t ret;
 
+	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	peer->rx_credits += ofi_div_ceil(seg_size, ep->max_data_payload_size);
 	rx_entry->window -= seg_size;
 
 	if (ep->available_data_bufs < rxr_get_rx_pool_chunk_cnt(ep))
@@ -1199,6 +1203,7 @@ static void rxr_cq_handle_cts(struct rxr_ep *ep,
 			      struct fi_cq_msg_entry *comp,
 			      struct rxr_pkt_entry *pkt_entry)
 {
+	struct rxr_peer *peer;
 	struct rxr_cts_hdr *cts_pkt;
 	struct rxr_tx_entry *tx_entry;
 
@@ -1211,6 +1216,12 @@ static void rxr_cq_handle_cts(struct rxr_ep *ep,
 	tx_entry->rx_id = cts_pkt->rx_id;
 	tx_entry->window = cts_pkt->window;
 
+	/* Return any excess tx_credits that were borrowed for the request */
+	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	tx_entry->credit_allocated = ofi_div_ceil(cts_pkt->window, ep->max_data_payload_size);
+	if (tx_entry->credit_allocated < tx_entry->credit_request)
+		peer->tx_credits += tx_entry->credit_request - tx_entry->credit_allocated;
+
 	rxr_release_rx_pkt_entry(ep, pkt_entry);
 	ep->rx_bufs_to_post++;
 
@@ -1356,15 +1367,16 @@ void rxr_cq_handle_pkt_send_completion(struct rxr_ep *ep, struct fi_cq_msg_entry
 {
 	struct rxr_pkt_entry *pkt_entry;
 	struct rxr_tx_entry *tx_entry = NULL;
-	uint32_t tx_id;
-	int ret;
+	struct rxr_peer *peer;
 	struct rxr_rts_hdr *rts_hdr = NULL;
 	struct rxr_readrsp_hdr *readrsp_hdr = NULL;
+	uint32_t tx_id;
+	int ret;
 
 	pkt_entry = (struct rxr_pkt_entry *)comp->op_context;
-
 	assert(rxr_get_base_hdr(pkt_entry->pkt)->version ==
 	       RXR_PROTOCOL_VERSION);
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
 
 	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
 	case RXR_RTS_PKT:
@@ -1424,6 +1436,8 @@ void rxr_cq_handle_pkt_send_completion(struct rxr_ep *ep, struct fi_cq_msg_entry
 			}
 		}
 
+		peer->tx_credits += tx_entry->credit_allocated;
+
 		if (tx_entry->cq_entry.flags & FI_READ) {
 			/*
 			 * this must be on remote side
@@ -1458,7 +1472,7 @@ void rxr_cq_handle_pkt_send_completion(struct rxr_ep *ep, struct fi_cq_msg_entry
 	}
 
 	rxr_release_tx_pkt_entry(ep, pkt_entry);
-	ep->tx_pending--;
+	rxr_ep_dec_tx_pending(ep, peer, 0);
 	return;
 }
 
diff --git a/prov/efa/src/rxr/rxr_ep.c b/prov/efa/src/rxr/rxr_ep.c
index 3817bafe2..d898d7f9e 100644
--- a/prov/efa/src/rxr/rxr_ep.c
+++ b/prov/efa/src/rxr/rxr_ep.c
@@ -770,9 +770,9 @@ static ssize_t rxr_multi_recv(struct rxr_ep *rxr_ep, const struct iovec *iov,
 			 * long msg completion. Last msg completion will free
 			 * posted rx_entry.
 			 */
-			if (ret != -FI_ENOMSG || ret != 0)
-				return ret;
-			return 0;
+			if (ret == -FI_ENOMSG)
+				return 0;
+			return ret;
 		}
 
 		if (ret == -FI_ENOMSG) {
@@ -921,9 +921,11 @@ static ssize_t rxr_ep_recvv(struct fid_ep *ep, const struct iovec *iov,
 }
 
 
-void rxr_generic_tx_entry_init(struct rxr_tx_entry *tx_entry, const struct iovec *iov, size_t iov_count,
-			       const struct fi_rma_iov *rma_iov, size_t rma_iov_count,
-			       fi_addr_t addr, uint64_t tag, uint64_t data, void *context,
+void rxr_generic_tx_entry_init(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
+			       const struct iovec *iov, size_t iov_count,
+			       const struct fi_rma_iov *rma_iov,
+			       size_t rma_iov_count, fi_addr_t addr,
+			       uint64_t tag, uint64_t data, void *context,
 			       uint32_t op, uint64_t flags)
 {
 	tx_entry->type = RXR_TX_ENTRY;
@@ -1003,8 +1005,9 @@ struct rxr_tx_entry *rxr_ep_tx_entry_init(struct rxr_ep *rxr_ep, const struct io
 	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
 #endif
 
-	rxr_generic_tx_entry_init(tx_entry, iov, iov_count, rma_iov, rma_iov_count,
-				  addr, tag, data, context, op, flags);
+	rxr_generic_tx_entry_init(rxr_ep, tx_entry, iov, iov_count, rma_iov,
+				  rma_iov_count, addr, tag, data, context,
+				  op, flags);
 
 	assert(rxr_ep->util_ep.tx_msg_flags == 0 || rxr_ep->util_ep.tx_msg_flags == FI_COMPLETION);
 	tx_op_flags = rxr_ep->util_ep.tx_op_flags;
@@ -1081,12 +1084,8 @@ ssize_t rxr_ep_send_msg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
 #endif
 	ret = fi_sendmsg(ep->rdm_ep, msg, flags);
 
-	if (OFI_LIKELY(!ret)) {
-		ep->tx_pending++;
-#if ENABLE_DEBUG
-		ep->sends++;
-#endif
-	}
+	if (OFI_LIKELY(!ret))
+		rxr_ep_inc_tx_pending(ep, peer);
 
 	return ret;
 }
@@ -1284,24 +1283,47 @@ ssize_t rxr_ep_post_readrsp(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 	return 0;
 }
 
-void rxr_ep_calc_cts_window_credits(struct rxr_ep *ep, uint32_t max_window, uint64_t size,
+void rxr_ep_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
+				    uint64_t size, int request,
 				    int *window, int *credits)
 {
-	*credits = ofi_div_ceil(size, ep->max_data_payload_size);
+	struct rxr_av *av;
+	int num_peers;
+
+	/*
+	 * Adjust the peer credit pool based on the current AV size, which could
+	 * have grown since the time this peer was initialized.
+	 */
+	av = rxr_ep_av(ep);
+	num_peers = av->rdm_av_used - 1;
+	if (num_peers && ofi_div_ceil(rxr_env.rx_window_size, num_peers) < peer->rx_credits)
+		peer->rx_credits = ofi_div_ceil(peer->rx_credits, num_peers);
+
+	/*
+	 * Allocate credits for this transfer based on the request, the number
+	 * of available data buffers, and the number of outstanding peers this
+	 * endpoint is actively tracking in the AV. Also ensure that a minimum
+	 * number of credits are allocated to the transfer so the sender can
+	 * make progress.
+	 */
 	*credits = MIN(MIN(ep->available_data_bufs, ep->posted_bufs),
-		       MIN(*credits, max_window));
+		       peer->rx_credits);
+	*credits = MIN(request, *credits);
+	*credits = MAX(*credits, rxr_env.tx_min_credits);
 	*window = MIN(size, *credits * ep->max_data_payload_size);
+	if (peer->rx_credits > ofi_div_ceil(*window, ep->max_data_payload_size))
+		peer->rx_credits -= ofi_div_ceil(*window, ep->max_data_payload_size);
 }
 
 void rxr_ep_init_cts_pkt_entry(struct rxr_ep *ep,
 			       struct rxr_rx_entry *rx_entry,
 			       struct rxr_pkt_entry *pkt_entry,
-			       uint32_t max_window,
 			       uint64_t size,
 			       int *credits)
 {
 	int window = 0;
 	struct rxr_cts_hdr *cts_hdr;
+	struct rxr_peer *peer;
 
 	cts_hdr = (struct rxr_cts_hdr *)pkt_entry->pkt;
 
@@ -1315,7 +1337,9 @@ void rxr_ep_init_cts_pkt_entry(struct rxr_ep *ep,
 	cts_hdr->tx_id = rx_entry->tx_id;
 	cts_hdr->rx_id = rx_entry->rx_id;
 
-	rxr_ep_calc_cts_window_credits(ep, max_window, size, &window, credits);
+	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	rxr_ep_calc_cts_window_credits(ep, peer, size, rx_entry->credit_request,
+				       &window, credits);
 	cts_hdr->window = window;
 
 	pkt_entry->pkt_size = RXR_CTS_HDR_SIZE;
@@ -1384,6 +1408,16 @@ void rxr_init_rts_pkt_entry(struct rxr_ep *ep,
 	rts_hdr->tx_id = tx_entry->tx_id;
 	rts_hdr->msg_id = tx_entry->msg_id;
 
+	/*
+	 * Even with protocol versions prior to v3 that did not include a
+	 * request in the RTS, the receiver can test for this flag and decide if
+	 * it should be used as a heuristic for credit calculation. If the
+	 * receiver is on <3 protocol version, the flag and the request just get
+	 * ignored.
+	 */
+	rts_hdr->flags |= RXR_CREDIT_REQUEST;
+	rts_hdr->credit_request = tx_entry->credit_request;
+
 	if (tx_entry->fi_flags & FI_REMOTE_CQ_DATA) {
 		rts_hdr->flags = RXR_REMOTE_CQ_DATA;
 		pkt_entry->pkt_size = RXR_CTRL_HDR_SIZE;
@@ -1482,6 +1516,8 @@ static void rxr_inline_mr_reg(struct rxr_domain *rxr_domain,
 static size_t rxr_ep_post_rts(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 {
 	struct rxr_pkt_entry *pkt_entry;
+	struct rxr_peer *peer;
+	size_t pending = 0;
 	ssize_t ret;
 	uint64_t data_sent, offset;
 	int i;
@@ -1491,6 +1527,35 @@ static size_t rxr_ep_post_rts(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 	if (OFI_UNLIKELY(!pkt_entry))
 		return -FI_EAGAIN;
 
+	/*
+	 * Init tx state for this peer. The rx state and reorder buffers will be
+	 * initialized on the first recv so as to not allocate resources unless
+	 * necessary.
+	 */
+	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
+	if (!peer->tx_init) {
+		peer->tx_credits = rxr_env.tx_max_credits;
+		peer->tx_init = 1;
+	}
+
+	/*
+	 * Divy up available credits to outstanding transfers and request the
+	 * minimum of that and the amount required to finish the current long
+	 * message.
+	 */
+	pending = peer->tx_pending + 1;
+	tx_entry->credit_request = MIN(ofi_div_ceil(peer->tx_credits, pending),
+				       ofi_div_ceil(tx_entry->total_len,
+						    rxr_ep->max_data_payload_size));
+	tx_entry->credit_request = MAX(tx_entry->credit_request,
+				       rxr_env.tx_min_credits);
+	if (peer->tx_credits >= tx_entry->credit_request)
+		peer->tx_credits -= tx_entry->credit_request;
+
+	/* Queue this RTS for later if there are too many outstanding packets */
+	if (!tx_entry->credit_request)
+		return -FI_EAGAIN;
+
 	rxr_init_rts_pkt_entry(rxr_ep, tx_entry, pkt_entry);
 
 	ret = rxr_ep_send_pkt(rxr_ep, pkt_entry, tx_entry->addr);
@@ -1629,12 +1694,13 @@ ssize_t rxr_tx(struct fid_ep *ep, const struct iovec *iov, size_t iov_count,
 			goto out;
 		}
 
-		rxr_ep_calc_cts_window_credits(rxr_ep, rxr_env.rx_window_size,
-					       tx_entry->total_len, &window,
+		rxr_ep_calc_cts_window_credits(rxr_ep, peer,
+					       tx_entry->total_len,
+					       tx_entry->credit_request,
+					       &window,
 					       &credits);
 
 		rx_entry->window = window;
-		assert(rxr_ep->available_data_bufs >= credits);
 		rxr_ep->available_data_bufs -= credits;
 
 		rx_entry->state = RXR_RX_RECV;
@@ -2221,29 +2287,51 @@ static ssize_t rxr_ep_cancel_recv(struct rxr_ep *ep,
 	entry = dlist_remove_first_match(recv_list,
 					 &rxr_ep_cancel_match_recv,
 					 context);
-	if (entry) {
-		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
-		rx_entry->rxr_flags |= RXR_RECV_CANCEL;
-		if (rx_entry->fi_flags & FI_MULTI_RECV)
-			rxr_cq_handle_multi_recv_completion(ep, rx_entry);
+	if (!entry) {
 		fastlock_release(&ep->util_ep.lock);
-		memset(&err_entry, 0, sizeof(err_entry));
-		err_entry.op_context = rx_entry->cq_entry.op_context;
-		err_entry.flags |= rx_entry->cq_entry.flags;
-		err_entry.tag = rx_entry->tag;
-		err_entry.err = FI_ECANCELED;
-		err_entry.prov_errno = -FI_ECANCELED;
-
-		domain = rxr_ep_domain(ep);
-		api_version =
-			 domain->util_domain.fabric->fabric_fid.api_version;
-		if (FI_VERSION_GE(api_version, FI_VERSION(1, 5)))
-			err_entry.err_data_size = 0;
-		return ofi_cq_write_error(ep->util_ep.rx_cq, &err_entry);
+		return 0;
 	}
 
+	rx_entry = container_of(entry, struct rxr_rx_entry, entry);
+	rx_entry->rxr_flags |= RXR_RECV_CANCEL;
+	if (rx_entry->fi_flags & FI_MULTI_RECV &&
+	    rx_entry->rxr_flags & RXR_MULTI_RECV_POSTED) {
+		if (dlist_empty(&rx_entry->multi_recv_consumers)) {
+			/*
+			 * No pending messages for the buffer,
+			 * release it back to the app.
+			 */
+			rx_entry->cq_entry.flags |= FI_MULTI_RECV;
+		} else {
+			rx_entry = container_of(rx_entry->multi_recv_consumers.next,
+						struct rxr_rx_entry,
+						multi_recv_entry);
+			rxr_cq_handle_multi_recv_completion(ep, rx_entry);
+		}
+	} else if (rx_entry->fi_flags & FI_MULTI_RECV &&
+		   rx_entry->rxr_flags & RXR_MULTI_RECV_CONSUMER) {
+			rxr_cq_handle_multi_recv_completion(ep, rx_entry);
+	}
 	fastlock_release(&ep->util_ep.lock);
-	return 0;
+	memset(&err_entry, 0, sizeof(err_entry));
+	err_entry.op_context = rx_entry->cq_entry.op_context;
+	err_entry.flags |= rx_entry->cq_entry.flags;
+	err_entry.tag = rx_entry->tag;
+	err_entry.err = FI_ECANCELED;
+	err_entry.prov_errno = -FI_ECANCELED;
+
+	domain = rxr_ep_domain(ep);
+	api_version =
+		 domain->util_domain.fabric->fabric_fid.api_version;
+	if (FI_VERSION_GE(api_version, FI_VERSION(1, 5)))
+		err_entry.err_data_size = 0;
+	/*
+	 * Other states are currently receiving data. Subsequent messages will
+	 * be sunk (via RXR_RECV_CANCEL flag) and the completion suppressed.
+	 */
+	if (rx_entry->state & (RXR_RX_INIT | RXR_RX_UNEXP | RXR_RX_MATCHED))
+		rxr_release_rx_entry(ep, rx_entry);
+	return ofi_cq_write_error(ep->util_ep.rx_cq, &err_entry);
 }
 
 static ssize_t rxr_ep_cancel(fid_t fid_ep, void *context)
@@ -2618,7 +2706,6 @@ static void rxr_ep_progress_internal(struct rxr_ep *ep)
 				     rx_entry, queued_entry, tmp) {
 		if (rx_entry->state == RXR_RX_QUEUED_CTS)
 			ret = rxr_cq_post_cts(ep, rx_entry,
-					      rxr_env.rx_window_size,
 					      rx_entry->total_len -
 					      rx_entry->bytes_done);
 		else
diff --git a/prov/efa/src/rxr/rxr_init.c b/prov/efa/src/rxr/rxr_init.c
index 1f07b75e0..c06e952a3 100644
--- a/prov/efa/src/rxr/rxr_init.c
+++ b/prov/efa/src/rxr/rxr_init.c
@@ -41,6 +41,8 @@ struct fi_provider *lower_efa_prov;
 
 struct rxr_env rxr_env = {
 	.rx_window_size	= RXR_DEF_MAX_RX_WINDOW,
+	.tx_max_credits = RXR_DEF_MAX_TX_CREDITS,
+	.tx_min_credits = RXR_DEF_MIN_TX_CREDITS,
 	.tx_queue_size = 0,
 	.enable_sas_ordering = 1,
 	.recvwin_size = RXR_RECVWIN_SIZE,
@@ -60,6 +62,8 @@ struct rxr_env rxr_env = {
 static void rxr_init_env(void)
 {
 	fi_param_get_int(&rxr_prov, "rx_window_size", &rxr_env.rx_window_size);
+	fi_param_get_int(&rxr_prov, "tx_max_credits", &rxr_env.tx_max_credits);
+	fi_param_get_int(&rxr_prov, "tx_min_credits", &rxr_env.tx_min_credits);
 	fi_param_get_int(&rxr_prov, "tx_queue_size", &rxr_env.tx_queue_size);
 	fi_param_get_int(&rxr_prov, "enable_sas_ordering", &rxr_env.enable_sas_ordering);
 	fi_param_get_int(&rxr_prov, "recvwin_size", &rxr_env.recvwin_size);
@@ -143,6 +147,11 @@ static int rxr_copy_attr(const struct fi_info *info, struct fi_info *dup)
 				return -FI_ENOMEM;
 		}
 	}
+	if (info->nic) {
+		dup->nic = ofi_nic_dup(info->nic);
+		if (!dup->nic)
+			return -FI_ENOMEM;
+	}
 	return 0;
 }
 
@@ -249,23 +258,24 @@ static int rxr_info_to_rxr(uint32_t version, const struct fi_info *core_info,
 	info->domain_attr->mr_key_size = core_info->domain_attr->mr_key_size;
 
 	/*
-	 * Adapt the info attributes to the application's hints.
-	 * `msg_order` is one such case where packet reordering decision is made
-	 * based on both the application's requirements and the lower-level's
-	 * capability, so it can not be set solely based on rxr_info or
-	 * core_info.
+	 * Handle user-provided hints and adapt the info object passed back up
+	 * based on EFA-specific constraints.
 	 */
-	if (hints && hints->tx_attr) {
-		if (!(hints->tx_attr->msg_order & FI_ORDER_SAS))
-			rxr_env.enable_sas_ordering = 0;
-	}
+	if (hints) {
+		/* Disable packet reordering if the app doesn't need it */
+		if (hints->tx_attr)
+			if (!(hints->tx_attr->msg_order & FI_ORDER_SAS))
+				rxr_env.enable_sas_ordering = 0;
+
+		/* We only support manual progress for RMA operations */
+		if (hints->caps & FI_RMA) {
+			info->domain_attr->control_progress = FI_PROGRESS_MANUAL;
+			info->domain_attr->data_progress = FI_PROGRESS_MANUAL;
+		}
 
-	/*
-	 *  For RMA, we only support PROGRESS_MANUAL
-	 */
-	if (hints && (hints->caps & FI_RMA)) {
-		info->domain_attr->control_progress = FI_PROGRESS_MANUAL;
-		info->domain_attr->data_progress = FI_PROGRESS_MANUAL;
+		/* Use a table for AV if the app has no strong requirement */
+		if (!hints->domain_attr || hints->domain_attr->av_type == FI_AV_UNSPEC)
+			info->domain_attr->av_type = FI_AV_TABLE;
 	}
 
 	rxr_set_rx_tx_size(info, core_info);
@@ -423,7 +433,11 @@ struct fi_provider rxr_prov = {
 EFA_INI
 {
 	fi_param_define(&rxr_prov, "rx_window_size", FI_PARAM_INT,
-			"Defines the maximum window size that a receiver will return for matched large messages. Defaults to the number of available posted receive buffers when the clear to send message is sent (Default: 16).");
+			"Defines the maximum window size that a receiver will return for matched large messages. (Default: 128).");
+	fi_param_define(&rxr_prov, "tx_max_credits", FI_PARAM_INT,
+			"Defines the maximum number of credits a sender requests from a receiver (Default: 64).");
+	fi_param_define(&rxr_prov, "tx_min_credits", FI_PARAM_INT,
+			"Defines the minimum number of credits a sender requests from a receiver (Default: 32).");
 	fi_param_define(&rxr_prov, "tx_queue_size", FI_PARAM_INT,
 			"Defines the maximum number of unacknowledged sends with the NIC.");
 	fi_param_define(&rxr_prov, "enable_sas_ordering", FI_PARAM_INT,
diff --git a/prov/efa/src/rxr/rxr_rma.c b/prov/efa/src/rxr/rxr_rma.c
index 9f50df83a..d02032d52 100644
--- a/prov/efa/src/rxr/rxr_rma.c
+++ b/prov/efa/src/rxr/rxr_rma.c
@@ -88,9 +88,9 @@ struct rxr_tx_entry *rxr_readrsp_tx_entry_init(struct rxr_ep *rxr_ep,
 	 * this tx_entry works similar to a send tx_entry thus its op was
 	 * set to ofi_op_msg. Note this tx_entry will not write a completion
 	 */
-	rxr_generic_tx_entry_init(tx_entry, rx_entry->iov, rx_entry->iov_count,
-				  NULL, 0, rx_entry->addr, 0, 0, NULL,
-				  ofi_op_msg, 0);
+	rxr_generic_tx_entry_init(rxr_ep, tx_entry, rx_entry->iov,
+				  rx_entry->iov_count, NULL, 0, rx_entry->addr,
+				  0, 0, NULL, ofi_op_msg, 0);
 
 	tx_entry->cq_entry.flags |= FI_READ;
 	/* rma_loc_rx_id is for later retrieve of rx_entry
diff --git a/prov/gni/src/gnix_msg.c b/prov/gni/src/gnix_msg.c
index 2e81ed539..08b93fa0e 100644
--- a/prov/gni/src/gnix_msg.c
+++ b/prov/gni/src/gnix_msg.c
@@ -265,20 +265,22 @@ fn_exit:
 static void __gnix_msg_copy_data_to_recv_addr(struct gnix_fab_req *req,
 					      void *data)
 {
+	size_t	len;
+
 	GNIX_DBG_TRACE(FI_LOG_EP_DATA, "\n");
 
+	len = MIN(req->msg.cum_send_len, req->msg.cum_recv_len);
+
 	switch(req->type) {
 	case GNIX_FAB_RQ_RECV:
-		memcpy((void *)req->msg.recv_info[0].recv_addr, data,
-		       req->msg.cum_recv_len);
+		memcpy((void *)req->msg.recv_info[0].recv_addr, data, len);
 		break;
 
 	case GNIX_FAB_RQ_RECVV:
 	case GNIX_FAB_RQ_TRECVV:
 		__gnix_msg_unpack_data_into_iov(req->msg.recv_info,
 						req->msg.recv_iov_cnt,
-						(uint64_t) data,
-						req->msg.cum_recv_len);
+						(uint64_t) data, len);
 		break;
 
 	default:
@@ -409,7 +411,7 @@ static int __recv_completion_src(
 
 	if ((req->msg.recv_flags & FI_COMPLETION) && ep->recv_cq) {
 		if ((src_addr == FI_ADDR_NOTAVAIL) &&
-                    (req->msg.recv_flags & FI_SOURCE_ERR) != 0) {
+                    (ep->caps & FI_SOURCE_ERR) != 0) {
 			if (ep->domain->addr_format == FI_ADDR_STR) {
 				buffer = malloc(GNIX_FI_ADDR_STR_LEN);
 				rc = _gnix_ep_name_to_str(req->vc->gnix_ep_name, (char **)&buffer);
@@ -863,7 +865,7 @@ static int __gnix_rndzv_req_complete(void *arg, gni_return_t tx_status)
 		if (!GNIX_EP_DGM(req->gnix_ep->type)) {
 			GNIX_WARN(FI_LOG_EP_DATA,
 				  "Dropping failed request: %p\n", req);
-			ret = __gnix_msg_send_err(req->gnix_ep,
+			ret = __gnix_msg_recv_err(req->gnix_ep,
 						  req);
 			if (ret != FI_SUCCESS)
 				GNIX_WARN(FI_LOG_EP_DATA,
@@ -3284,7 +3286,7 @@ ssize_t _gnix_send(struct gnix_fid_ep *ep, uint64_t loc_addr, size_t len,
 	ret = _gnix_vc_queue_tx_req(req);
 	connected = (vc->conn_state == GNIX_VC_CONNECTED);
 
-	COND_RELEASE(vc->ep->requires_lock, &vc->ep->vc_lock);
+	COND_RELEASE(ep->requires_lock, &ep->vc_lock);
 
 	/*
 	 * If a new VC was allocated, progress CM before returning.
@@ -3300,7 +3302,7 @@ ssize_t _gnix_send(struct gnix_fid_ep *ep, uint64_t loc_addr, size_t len,
 	return ret;
 
 err_get_vc:
-	COND_RELEASE(vc->ep->requires_lock, &vc->ep->vc_lock);
+	COND_RELEASE(ep->requires_lock, &ep->vc_lock);
 	_gnix_fr_free(ep, req);
 	if (flags & FI_LOCAL_MR)
 		fi_close(&auto_mr->fid);
diff --git a/prov/hook/hook_debug/Makefile.include b/prov/hook/hook_debug/Makefile.include
new file mode 100644
index 000000000..de0b7d999
--- /dev/null
+++ b/prov/hook/hook_debug/Makefile.include
@@ -0,0 +1,13 @@
+if HAVE_HOOK_DEBUG
+_hook_debug_files = \
+	prov/hook/hook_debug/src/hook_debug.c
+
+_hook_debug_headers = \
+	prov/hook/hook_debug/include/hook_debug.h
+
+
+src_libfabric_la_SOURCES  +=	$(_hook_debug_files) \
+				$(_hook_debug_headers)
+src_libfabric_la_CPPFLAGS +=	-I$(top_srcdir)/prov/hook/hook_debug/include
+src_libfabric_la_LIBADD	  +=	$(hook_debug_shm_LIBS)
+endif HAVE_HOOK_DEBUG
diff --git a/prov/hook/hook_debug/configure.m4 b/prov/hook/hook_debug/configure.m4
new file mode 100644
index 000000000..58a6c633c
--- /dev/null
+++ b/prov/hook/hook_debug/configure.m4
@@ -0,0 +1,21 @@
+dnl Configury specific to the libfabrics debug hooking provider
+
+dnl Called to configure this provider
+dnl
+dnl Arguments:
+dnl
+dnl $1: action if configured successfully
+dnl $2: action if not configured successfully
+dnl
+
+AC_DEFUN([FI_HOOK_DEBUG_CONFIGURE],[
+    # Determine if we can support the debug hooking provider
+    hook_debug_happy=0
+    AS_IF([test x"$enable_hook_debug" != x"no"], [hook_debug_happy=1])
+    AS_IF([test x"$hook_debug_dl" == x"1"], [
+	hook_debug_happy=0
+	AC_MSG_ERROR([debug hooking provider cannot be compiled as DL])
+    ])
+    AS_IF([test $hook_debug_happy -eq 1], [$1], [$2])
+
+])
diff --git a/prov/hook/hook_debug/include/hook_debug.h b/prov/hook/hook_debug/include/hook_debug.h
new file mode 100644
index 000000000..fd7e46219
--- /dev/null
+++ b/prov/hook/hook_debug/include/hook_debug.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright (c) 2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL); Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _HOOK_DEBUG_H_
+#define _HOOK_DEBUG_H_
+
+#include "ofi_hook.h"
+#include "ofi.h"
+
+#define HOOK_DEBUG_EAGAIN_LOG 10000000
+#define HOOK_DEBUG_EQ_EVENT_MAX (FI_JOIN_COMPLETE + 1)
+
+extern struct hook_prov_ctx hook_debug_ctx;
+
+struct hook_debug_config {
+	uint64_t trace_exit : 1;
+	uint64_t trace_cq_entry : 1;
+	uint64_t track_sends : 1;
+	uint64_t track_recvs : 1;
+};
+
+struct hook_debug_eq {
+	struct hook_eq hook_eq;
+	ofi_atomic64_t event_cntr[HOOK_DEBUG_EQ_EVENT_MAX];
+};
+
+struct hook_debug_cq {
+	struct hook_cq hook_cq;
+	enum fi_cq_format format;
+	size_t entry_size;
+	size_t eagain_count;
+};
+
+struct hook_debug_txrx_entry {
+	uint64_t 		magic;
+	struct hook_debug_ep 	*ep;
+	uint64_t 		op_flags;
+	void 			*context;
+};
+
+struct hook_debug_ep {
+	struct hook_ep hook_ep;
+	uint64_t tx_op_flags;
+	uint64_t rx_op_flags;
+	struct ofi_bufpool *tx_pool;
+	struct ofi_bufpool *rx_pool;
+	size_t tx_outs;
+	size_t rx_outs;
+	size_t tx_eagain_count;
+	size_t rx_eagain_count;
+};
+
+#endif /* _HOOK_DEBUG_H_ */
diff --git a/prov/hook/hook_debug/src/hook_debug.c b/prov/hook/hook_debug/src/hook_debug.c
new file mode 100644
index 000000000..66ebc0b19
--- /dev/null
+++ b/prov/hook/hook_debug/src/hook_debug.c
@@ -0,0 +1,953 @@
+/*
+ * Copyright (c) 2019 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <inttypes.h>
+
+#include "ofi.h"
+#include "ofi_prov.h"
+#include "ofi_hook.h"
+#include "hook_prov.h"
+#include "ofi_enosys.h"
+
+#include "hook_debug.h"
+
+struct hook_prov_ctx hook_debug_prov_ctx;
+
+struct hook_debug_config config = {
+	.trace_exit = 1,
+	.trace_cq_entry = 1,
+	.track_sends = 1,
+	/* Disable for now: debug hang */
+	.track_recvs = 0,
+};
+
+static struct hook_debug_txrx_entry *
+hook_debug_get_tx_entry(struct hook_debug_ep *myep, void *context,
+			uint64_t flags)
+{
+	struct hook_debug_txrx_entry *tx_entry;
+
+	tx_entry = ofi_buf_alloc(myep->tx_pool);
+	assert(tx_entry);
+	assert(tx_entry->magic == OFI_MAGIC_64);
+
+	tx_entry->op_flags = myep->tx_op_flags | flags;
+	tx_entry->context = context;
+	return tx_entry;
+}
+
+static struct hook_debug_txrx_entry *
+hook_debug_get_rx_entry(struct hook_debug_ep *myep, void *context,
+			uint64_t flags)
+{
+	struct hook_debug_txrx_entry *rx_entry;
+
+	rx_entry = ofi_buf_alloc(myep->rx_pool);
+	assert(rx_entry);
+	assert(rx_entry->magic == OFI_MAGIC_64);
+
+	rx_entry->op_flags = myep->rx_op_flags | flags;
+	rx_entry->context = context;
+	return rx_entry;
+}
+
+static void hook_debug_trace_exit(struct fid *fid, struct fid *hfid,
+				  enum fi_log_subsys subsys, const char *fn,
+				  ssize_t ret, size_t *eagain_count)
+{
+	if (!config.trace_exit)
+		return;
+
+	if (ret > 0) {
+		FI_TRACE(hook_to_hprov(fid), subsys, "%s (fid: %p) returned: "
+			 "%zd\n", fn, hfid, ret);
+		goto out;
+	}
+
+	if (ret != -FI_EAGAIN || !eagain_count ||
+	    !((*eagain_count)++ % HOOK_DEBUG_EAGAIN_LOG))
+		FI_TRACE(hook_to_hprov(fid), subsys, "%s (fid: %p) returned: "
+			 "%zd (%s)\n", fn, fid, ret, fi_strerror(-ret));
+out:
+	if (eagain_count && ret != -FI_EAGAIN)
+		*eagain_count = 0;
+}
+
+static void hook_debug_rx_end(struct hook_debug_ep *ep, char *fn,
+			      ssize_t ret, void *mycontext)
+{
+	struct hook_debug_txrx_entry *rx_entry;
+
+	hook_debug_trace_exit(&ep->hook_ep.ep.fid, &ep->hook_ep.hep->fid,
+			      FI_LOG_EP_DATA, fn, ret, &ep->rx_eagain_count);
+
+	if (config.track_recvs) {
+		if (!ret) {
+			ep->rx_outs++;
+			FI_TRACE(hook_to_hprov(&ep->hook_ep.ep.fid),
+				 FI_LOG_EP_DATA, "ep: %p rx_outs: %zu\n",
+				 ep->hook_ep.hep, ep->rx_outs);
+		} else {
+			rx_entry = mycontext;
+			ofi_buf_free(rx_entry);
+		}
+	}
+}
+
+static int hook_debug_rx_start(struct hook_debug_ep *ep, void *context,
+			       uint64_t flags, void **mycontext)
+{
+	struct hook_debug_txrx_entry *rx_entry;
+
+	if (config.track_recvs) {
+		if (flags & ~(FI_MULTI_RECV | FI_COMPLETION)) {
+			FI_TRACE(&hook_debug_prov_ctx.prov, FI_LOG_EP_DATA,
+				 "unsupported flags: %s\n",
+				 fi_tostr(&flags, FI_TYPE_OP_FLAGS));
+			return -FI_EINVAL;
+		}
+
+		rx_entry = hook_debug_get_rx_entry(ep, context, flags);
+		*mycontext = rx_entry;
+	} else {
+		*mycontext = context;
+	}
+	return 0;
+}
+
+static ssize_t
+hook_debug_recv(struct fid_ep *ep, void *buf, size_t len, void *desc,
+		fi_addr_t src_addr, void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep,
+						  hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_rx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_recv(myep->hook_ep.hep, buf, len, desc, src_addr, mycontext);
+	hook_debug_rx_end(myep, "fi_recv", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_recvmsg(struct fid_ep *ep, const struct fi_msg *msg, uint64_t flags)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	struct fi_msg mymsg = *msg;
+	ssize_t ret;
+
+	ret = hook_debug_rx_start(myep, msg->context, flags, &mymsg.context);
+	if (ret)
+		return ret;
+
+	ret = fi_recvmsg(myep->hook_ep.hep, &mymsg, flags);
+	hook_debug_rx_end(myep, "fi_recvmsg", ret, mymsg.context);
+	return ret;
+}
+
+static ssize_t
+hook_debug_trecv(struct fid_ep *ep, void *buf, size_t len, void *desc,
+		 fi_addr_t src_addr, uint64_t tag, uint64_t ignore,
+		 void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_rx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_trecv(myep->hook_ep.hep, buf, len, desc, src_addr,
+		       tag, ignore, mycontext);
+
+	hook_debug_rx_end(myep, "fi_trecv", ret, mycontext);
+	return ret;
+}
+
+static void hook_debug_tx_end(struct hook_debug_ep *ep, char *fn,
+			      ssize_t ret, void *mycontext)
+{
+	struct hook_debug_txrx_entry *tx_entry;
+
+	hook_debug_trace_exit(&ep->hook_ep.ep.fid, &ep->hook_ep.hep->fid,
+			      FI_LOG_EP_DATA, fn, ret, &ep->tx_eagain_count);
+
+	if (mycontext && config.track_sends) {
+		if (!ret) {
+			ep->tx_outs++;
+			FI_TRACE(hook_to_hprov(&ep->hook_ep.ep.fid),
+				 FI_LOG_EP_DATA, "ep: %p tx_outs: %zu\n",
+				 ep->hook_ep.hep, ep->tx_outs);
+		} else {
+			tx_entry = mycontext;
+			ofi_buf_free(tx_entry);
+		}
+	}
+}
+
+static int hook_debug_tx_start(struct hook_debug_ep *ep, void *context,
+			       uint64_t flags, void **mycontext)
+{
+	struct hook_debug_txrx_entry *tx_entry;
+
+	if (mycontext) {
+		if (config.track_sends) {
+			tx_entry = hook_debug_get_tx_entry(ep, context, flags);
+			*mycontext = tx_entry;
+		} else {
+			*mycontext = context;
+		}
+	}
+	return 0;
+}
+
+static ssize_t
+hook_debug_send(struct fid_ep *ep, const void *buf, size_t len, void *desc,
+	      fi_addr_t dest_addr, void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_send(myep->hook_ep.hep, buf, len, desc, dest_addr, mycontext);
+
+	hook_debug_tx_end(myep, "fi_send", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_sendv(struct fid_ep *ep, const struct iovec *iov, void **desc,
+		 size_t count, fi_addr_t dest_addr, void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_sendv(myep->hook_ep.hep, iov, desc, count, dest_addr, mycontext);
+
+	hook_debug_tx_end(myep, "fi_sendv", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_sendmsg(struct fid_ep *ep, const struct fi_msg *msg, uint64_t flags)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	struct fi_msg mymsg = *msg;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, msg->context, flags, &mymsg.context);
+	if (ret)
+		return ret;
+
+	ret = fi_sendmsg(myep->hook_ep.hep, &mymsg, flags);
+	hook_debug_tx_end(myep, "fi_sendmsg", ret, mymsg.context);
+	return ret;
+}
+
+static ssize_t
+hook_debug_inject(struct fid_ep *ep, const void *buf, size_t len,
+		fi_addr_t dest_addr)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, NULL, 0, NULL);
+	if (ret)
+		return ret;
+
+	ret = fi_inject(myep->hook_ep.hep, buf, len, dest_addr);
+
+	hook_debug_tx_end(myep, "fi_inject", ret, NULL);
+	return ret;
+}
+
+static ssize_t
+hook_debug_senddata(struct fid_ep *ep, const void *buf, size_t len, void *desc,
+		  uint64_t data, fi_addr_t dest_addr, void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_senddata(myep->hook_ep.hep, buf, len, desc, data, dest_addr, mycontext);
+	hook_debug_tx_end(myep, "fi_senddata", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_injectdata(struct fid_ep *ep, const void *buf, size_t len,
+		      uint64_t data, fi_addr_t dest_addr)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, NULL, 0, NULL);
+	if (ret)
+		return ret;
+
+	ret = fi_injectdata(myep->hook_ep.hep, buf, len, data, dest_addr);
+
+	hook_debug_tx_end(myep, "fi_injectdata", ret, NULL);
+	return ret;
+}
+
+static ssize_t
+hook_debug_tsend(struct fid_ep *ep, const void *buf, size_t len, void *desc,
+		 fi_addr_t dest_addr, uint64_t tag, void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_tsend(myep->hook_ep.hep, buf, len, desc, dest_addr, tag, mycontext);
+	hook_debug_tx_end(myep, "fi_tsend", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_tsendv(struct fid_ep *ep, const struct iovec *iov, void **desc,
+		  size_t count, fi_addr_t dest_addr, uint64_t tag, void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_tsendv(myep->hook_ep.hep, iov, desc, count,
+			dest_addr, tag, mycontext);
+
+	hook_debug_tx_end(myep, "fi_tsendv", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_tsendmsg(struct fid_ep *ep, const struct fi_msg_tagged *msg,
+		    uint64_t flags)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	struct fi_msg_tagged mymsg = *msg;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, msg->context, flags, &mymsg.context);
+	if (ret)
+		return ret;
+
+	ret = fi_tsendmsg(myep->hook_ep.hep, &mymsg, flags);
+	hook_debug_tx_end(myep, "fi_tsendmsg", ret, mymsg.context);
+	return ret;
+}
+
+static ssize_t
+hook_debug_tinject(struct fid_ep *ep, const void *buf, size_t len,
+		   fi_addr_t dest_addr, uint64_t tag)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, NULL, 0, NULL);
+	if (ret)
+		return ret;
+
+	ret = fi_tinject(myep->hook_ep.hep, buf, len, dest_addr, tag);
+
+	hook_debug_tx_end(myep, "fi_tinject", ret, NULL);
+	return ret;
+}
+
+static ssize_t
+hook_debug_tsenddata(struct fid_ep *ep, const void *buf, size_t len, void *desc,
+		     uint64_t data, fi_addr_t dest_addr, uint64_t tag,
+		     void *context)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	void *mycontext;
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, context, 0, &mycontext);
+	if (ret)
+		return ret;
+
+	ret = fi_tsenddata(myep->hook_ep.hep, buf, len, desc, data,
+			    dest_addr, tag, mycontext);
+	hook_debug_tx_end(myep, "fi_tsenddata", ret, mycontext);
+	return ret;
+}
+
+static ssize_t
+hook_debug_tinjectdata(struct fid_ep *ep, const void *buf, size_t len,
+		       uint64_t data, fi_addr_t dest_addr, uint64_t tag)
+{
+	struct hook_debug_ep *myep = container_of(ep, struct hook_debug_ep, hook_ep.ep);
+	ssize_t ret;
+
+	ret = hook_debug_tx_start(myep, NULL, 0, NULL);
+	if (ret)
+		return ret;
+
+	ret = fi_tinjectdata(myep->hook_ep.hep, buf, len, data, dest_addr, tag);
+
+	hook_debug_tx_end(myep, "fi_tinjectdata", ret, NULL);
+	return ret;
+}
+
+#define HOOK_DEBUG_TRACE(fabric, subsys, ...) \
+	FI_TRACE(hook_fabric_to_hprov(fabric), subsys, __VA_ARGS__)
+
+#define HOOK_DEBUG_CQ_TRACE(cq, ...) \
+	HOOK_DEBUG_TRACE(cq->hook_cq.domain->fabric, FI_LOG_CQ, __VA_ARGS__)
+
+static void hook_debug_cq_entry_log(struct hook_debug_cq *cq,
+				    struct fi_cq_tagged_entry *entry)
+{
+	if (!config.trace_cq_entry)
+		return;
+
+	HOOK_DEBUG_CQ_TRACE(cq, "cq_entry:\n");
+	HOOK_DEBUG_CQ_TRACE(cq, "\top_context: %p\n", entry->op_context);
+
+	if (cq->format > FI_CQ_FORMAT_CONTEXT) {
+		HOOK_DEBUG_CQ_TRACE(cq, "\tflags: %s\n",
+				    fi_tostr(&entry->flags, FI_TYPE_CAPS));
+
+		if (entry->flags & FI_RECV)
+			HOOK_DEBUG_CQ_TRACE(cq, "\tlen: %zu\n", entry->len);
+
+		if (cq->format == FI_CQ_FORMAT_TAGGED)
+			HOOK_DEBUG_CQ_TRACE(cq, "\ttag: %" PRIx64 "\n", entry->tag);
+	}
+}
+
+static void hook_debug_cq_process_entry(struct hook_debug_cq *mycq,
+					const char *fn, ssize_t ret, char *buf)
+{
+	struct hook_debug_txrx_entry *rx_entry, *tx_entry;
+	struct fi_cq_tagged_entry *cq_entry;
+	int i;
+
+	hook_debug_trace_exit(&mycq->hook_cq.cq.fid, &mycq->hook_cq.hcq->fid,
+			      FI_LOG_CQ, fn, ret, &mycq->eagain_count);
+
+	for (i = 0; i < ret; i++, buf += mycq->entry_size) {
+		cq_entry = (struct fi_cq_tagged_entry *)buf;
+		hook_debug_cq_entry_log(mycq, cq_entry);
+
+		if (config.track_recvs && (cq_entry->flags & FI_RECV)) {
+			rx_entry = cq_entry->op_context;
+			assert(rx_entry->magic == OFI_MAGIC_64);
+
+			cq_entry->op_context = rx_entry->context;
+
+			if (!(rx_entry->op_flags & FI_MULTI_RECV) ||
+			    cq_entry->flags & FI_MULTI_RECV) {
+				rx_entry->ep->rx_outs--;
+				FI_TRACE(hook_to_hprov(&mycq->hook_cq.cq.fid),
+					 FI_LOG_CQ, "ep: %p rx_outs: %zu\n",
+					 rx_entry->ep->hook_ep.hep,
+					 rx_entry->ep->rx_outs);
+				ofi_buf_free(rx_entry);
+			}
+		} else if (config.track_sends && (cq_entry->flags & FI_SEND)) {
+			tx_entry = cq_entry->op_context;
+			assert(tx_entry->magic == OFI_MAGIC_64);
+
+			cq_entry->op_context = tx_entry->context;
+
+			tx_entry->ep->tx_outs--;
+			FI_TRACE(hook_to_hprov(&mycq->hook_cq.cq.fid),
+				 FI_LOG_CQ, "ep: %p tx_outs: %zu\n",
+				 tx_entry->ep->hook_ep.hep,
+				 tx_entry->ep->tx_outs);
+			ofi_buf_free(tx_entry);
+		}
+	}
+}
+
+static ssize_t hook_debug_cq_read(struct fid_cq *cq, void *buf, size_t count)
+{
+	struct hook_debug_cq *mycq = container_of(cq, struct hook_debug_cq,
+						  hook_cq.cq);
+	ssize_t ret;
+
+	ret = fi_cq_read(mycq->hook_cq.hcq, buf, count);
+	hook_debug_cq_process_entry(mycq, "fi_cq_read", ret, buf);
+	return ret;
+}
+
+int hook_debug_cq_close(struct fid *fid)
+{
+	struct hook_debug_cq *mycq =
+		container_of(fid, struct hook_debug_cq, hook_cq.cq.fid);
+	int ret = 0;
+
+	if (mycq->hook_cq.hcq)
+		ret = fi_close(&mycq->hook_cq.hcq->fid);
+	if (!ret)
+		free(mycq);
+	return ret;
+}
+
+// TODO move to common code
+static size_t cq_entry_size[] = {
+	[FI_CQ_FORMAT_UNSPEC] = 0,
+	[FI_CQ_FORMAT_CONTEXT] = sizeof(struct fi_cq_entry),
+	[FI_CQ_FORMAT_MSG] = sizeof(struct fi_cq_msg_entry),
+	[FI_CQ_FORMAT_DATA] = sizeof(struct fi_cq_data_entry),
+	[FI_CQ_FORMAT_TAGGED] = sizeof(struct fi_cq_tagged_entry)
+};
+
+struct fi_ops hook_debug_cq_fid_ops;
+struct fi_ops_cq hook_debug_cq_ops;
+
+static void hook_debug_cq_attr_log(struct hook_domain *dom,
+				   struct fi_cq_attr *attr)
+{
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "fi_cq_attr:\n");
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\tsize: %zu\n", attr->size);
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\tflags: %s\n", "TBD");
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\tformat: %s\n", "TBD");
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\twait_obj: %s\n", "TBD");
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\tsignaling_vector: %d\n",
+			 attr->signaling_vector);
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\twait_cond: %s\n", "TBD");
+	HOOK_DEBUG_TRACE(dom->fabric, FI_LOG_CQ, "\twait_set: %p\n", attr->wait_set);
+}
+
+int hook_debug_cq_open(struct fid_domain *domain_fid, struct fi_cq_attr *attr,
+		       struct fid_cq **cq, void *context)
+{
+	struct hook_domain *domain = container_of(domain_fid, struct hook_domain,
+						  domain);
+	struct hook_debug_cq *mycq;
+	int ret;
+
+	assert(!attr->flags);
+
+	hook_debug_cq_attr_log(domain, attr);
+
+	if ((config.track_sends || config.track_recvs) &&
+	    (attr->format < FI_CQ_FORMAT_MSG)) {
+		FI_WARN(&hook_debug_prov_ctx.prov, FI_LOG_CQ,
+			"need FI_CQ_FORMAT_MSG or higher for tracking sends "
+			"and(or) recvs\n");
+		return -FI_EINVAL;
+	}
+
+	mycq = calloc(1, sizeof *mycq);
+	if (!mycq)
+		return -FI_EAGAIN;
+
+	ret = hook_cq_init(domain_fid, attr, cq, context, &mycq->hook_cq);
+	if (ret)
+		goto err;
+
+	mycq->hook_cq.cq.fid.ops = &hook_debug_cq_fid_ops;
+	mycq->hook_cq.cq.ops = &hook_debug_cq_ops;
+	mycq->format = attr->format;
+	mycq->entry_size = cq_entry_size[attr->format];
+
+	assert(mycq->entry_size);
+
+	return 0;
+err:
+	hook_debug_cq_close(&mycq->hook_cq.cq.fid);
+	return ret;
+}
+
+static int hook_debug_ep_close(struct fid *fid)
+{
+	struct hook_debug_ep *myep =
+		container_of(fid, struct hook_debug_ep, hook_ep.ep.fid);
+	int ret = 0;
+
+	if (myep->tx_pool)
+		ofi_bufpool_destroy(myep->tx_pool);
+
+	if (myep->rx_pool)
+		ofi_bufpool_destroy(myep->rx_pool);
+
+	if (myep->hook_ep.hep)
+		ret = fi_close(&myep->hook_ep.hep->fid);
+	if (!ret)
+		free(myep);
+	return ret;
+}
+
+int hook_debug_ep_bind(struct fid *fid, struct fid *bfid, uint64_t flags)
+{
+	struct fid *hfid, *hbfid;
+	struct hook_cntr *cntr;
+
+	hfid = hook_to_hfid(fid);
+	hbfid = hook_to_hfid(bfid);
+	if (!hfid || !hbfid)
+		return -FI_EINVAL;
+
+	switch (fid->fclass) {
+	case FI_CLASS_CNTR:
+		cntr = container_of(fid, struct hook_cntr, cntr.fid);
+		HOOK_DEBUG_TRACE(cntr->domain->fabric, FI_LOG_EP_CTRL,
+				 "cntr: %p bind flags: %s\n", cntr->hcntr,
+				 fi_tostr(&flags, FI_TYPE_CAPS));
+		break;
+	}
+	return hfid->ops->bind(hfid, hbfid, flags);
+}
+
+static void hook_debug_txrx_entry_init(struct ofi_bufpool_region *region,
+				       void *buf)
+{
+	struct hook_debug_txrx_entry *entry = buf;
+	entry->magic = OFI_MAGIC_64;
+	entry->ep = region->pool->attr.context;
+}
+
+struct fi_ops hook_debug_ep_fid_ops;
+static struct fi_ops_msg hook_debug_msg_ops = {
+	.size 		= sizeof(struct fi_ops_msg),
+	.recv 		= hook_debug_recv,
+	.recvv 		= fi_no_msg_recvv,
+	.recvmsg 	= hook_debug_recvmsg,
+	.send		= hook_debug_send,
+	.senddata 	= hook_debug_senddata,
+	.sendv		= hook_debug_sendv,
+	.sendmsg 	= hook_debug_sendmsg,
+	.inject 	= hook_debug_inject,
+	.injectdata 	= hook_debug_injectdata,
+};
+
+struct fi_ops_tagged hook_debug_tagged_ops = {
+	.recv 		= hook_debug_trecv,
+	.recvv 		= fi_no_tagged_recvv,
+	.recvmsg 	= fi_no_tagged_recvmsg,
+	.send 		= hook_debug_tsend,
+	.senddata 	= hook_debug_tsenddata,
+	.sendv		= hook_debug_tsendv,
+	.sendmsg 	= hook_debug_tsendmsg,
+	.inject 	= hook_debug_tinject,
+	.injectdata	= hook_debug_tinjectdata,
+};
+
+int hook_debug_endpoint(struct fid_domain *domain, struct fi_info *info,
+			struct fid_ep **ep, void *context)
+{
+	struct hook_debug_ep *myep;
+	struct ofi_bufpool_attr bufpool_attr = {
+		.size		= sizeof(struct hook_debug_txrx_entry),
+		.alignment	= 16,
+		.max_cnt	= 0,
+		.init_fn	= hook_debug_txrx_entry_init,
+	};
+
+	int ret = -FI_ENOMEM;
+
+	if (info->domain_attr->threading != FI_THREAD_DOMAIN) {
+		FI_WARN(&hook_debug_prov_ctx.prov, FI_LOG_CQ,
+			"debug hooking provider doesn't support thread safety"
+			"at this time\n");
+		return -FI_EINVAL;
+	}
+
+	FI_TRACE(hook_to_hprov(&domain->fid), FI_LOG_EP_CTRL,
+		 "tx_attr->size: %zu\n", info->tx_attr->size);
+	FI_TRACE(hook_to_hprov(&domain->fid), FI_LOG_EP_CTRL,
+		 "rx_attr->size: %zu\n", info->rx_attr->size);
+
+	myep = calloc(1, sizeof *myep);
+	if (!myep)
+		return ret;
+
+	bufpool_attr.context = myep;
+
+	if (config.track_sends) {
+		bufpool_attr.chunk_cnt = info->tx_attr->size;
+		ret = ofi_bufpool_create_attr(&bufpool_attr, &myep->tx_pool);
+		if (ret)
+			goto err;
+	}
+
+	if (config.track_recvs) {
+		bufpool_attr.chunk_cnt = info->rx_attr->size;
+		ret = ofi_bufpool_create_attr(&bufpool_attr, &myep->rx_pool);
+		if (ret)
+			goto err;
+	}
+
+	ret = hook_endpoint_init(domain, info, ep, context, &myep->hook_ep);
+	if (ret)
+		goto err;
+
+	FI_TRACE(hook_to_hprov(&myep->hook_ep.ep.fid), FI_LOG_EP_CTRL,
+		 "endpoint opened, fid: %p\n", &myep->hook_ep.hep->fid);
+
+	myep->hook_ep.ep.fid.ops = &hook_debug_ep_fid_ops;
+	myep->hook_ep.ep.msg = &hook_debug_msg_ops;
+	myep->hook_ep.ep.tagged = &hook_debug_tagged_ops;
+	myep->tx_op_flags = info->tx_attr->op_flags;
+	myep->rx_op_flags = info->rx_attr->op_flags;
+
+	return 0;
+err:
+	hook_debug_ep_close(&myep->hook_ep.ep.fid);
+	return ret;
+}
+
+/*
+ * EQ
+ */
+
+static ssize_t hook_debug_eq_read(struct fid_eq *eq, uint32_t *event,
+				  void *buf, size_t len, uint64_t flags)
+{
+	struct hook_debug_eq *myeq = container_of(eq, struct hook_debug_eq,
+						  hook_eq.eq);
+	ssize_t ret;
+
+	ret = hook_eq_read(eq, event, buf, len, flags);
+	if (ret > 0)
+		ofi_atomic_inc64(&myeq->event_cntr[*event]);
+
+	return ret;
+}
+
+static ssize_t hook_debug_eq_sread(struct fid_eq *eq, uint32_t *event,
+				   void *buf, size_t len, int timeout,
+				   uint64_t flags)
+{
+	struct hook_debug_eq *myeq = container_of(eq, struct hook_debug_eq,
+						  hook_eq.eq);
+	ssize_t ret;
+
+	ret = hook_eq_sread(eq, event, buf, len, timeout, flags);
+	if (ret > 0)
+		ofi_atomic_inc64(&myeq->event_cntr[*event]);
+
+	return ret;
+}
+
+static int hook_debug_eq_close(struct fid *fid)
+{
+	struct hook_debug_eq *myeq = container_of(fid, struct hook_debug_eq,
+						  hook_eq.eq.fid);
+	int i, ret;
+
+	HOOK_DEBUG_TRACE(myeq->hook_eq.fabric, FI_LOG_EQ, "EQ events:\n");
+
+	for (i = 0; i < HOOK_DEBUG_EQ_EVENT_MAX; i++)
+		HOOK_DEBUG_TRACE(myeq->hook_eq.fabric, FI_LOG_EQ,
+				 "%-20s: %" PRIu64 "\n",
+				 fi_tostr(&i, FI_TYPE_EQ_EVENT),
+				 ofi_atomic_get64(&myeq->event_cntr[i]));
+
+	ret = fi_close(&myeq->hook_eq.heq->fid);
+	if (!ret)
+		free(myeq);
+
+	return ret;
+}
+
+static struct fi_ops_eq hook_debug_eq_ops;
+static struct fi_ops hook_debug_eq_fid_ops;
+
+int hook_debug_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
+		 struct fid_eq **eq, void *context)
+{
+	struct hook_debug_eq *myeq;
+	int i, ret;
+
+	myeq = calloc(1, sizeof *myeq);
+	if (!myeq)
+		return -FI_ENOMEM;
+
+	ret = hook_eq_init(fabric, attr, eq, context, &myeq->hook_eq);
+	if (ret)
+		free(myeq);
+
+	myeq->hook_eq.eq.ops = &hook_debug_eq_ops;
+	myeq->hook_eq.eq.fid.ops = &hook_debug_eq_fid_ops;
+
+	for (i = 0; i < HOOK_DEBUG_EQ_EVENT_MAX; i++)
+		ofi_atomic_initialize64(&myeq->event_cntr[i], 0);
+
+	return 0;
+}
+
+/*
+ * Fabric
+ */
+
+struct fi_ops hook_debug_fabric_fid_ops;
+static struct fi_ops_fabric hook_debug_fabric_ops;
+
+static int hook_debug_fabric(struct fi_fabric_attr *attr,
+			    struct fid_fabric **fabric, void *context)
+{
+	struct fi_provider *hprov = context;
+	struct hook_fabric *fab;
+
+	FI_TRACE(hprov, FI_LOG_FABRIC, "Installing debug hook\n");
+	fab = calloc(1, sizeof *fab);
+	if (!fab)
+		return -FI_ENOMEM;
+
+	hook_fabric_init(fab, HOOK_DEBUG, attr->fabric, hprov,
+			 &hook_debug_fabric_fid_ops, &hook_debug_prov_ctx);
+	*fabric = &fab->fabric;
+	fab->fabric.ops = &hook_debug_fabric_ops;
+	return 0;
+}
+
+struct hook_prov_ctx hook_debug_prov_ctx = {
+	.prov = {
+		.version = FI_VERSION(1,0),
+		/* We're a pass-through provider, so the fi_version is always the latest */
+		.fi_version = FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION),
+		.name = "ofi_hook_debug",
+		.getinfo = NULL,
+		.fabric = hook_debug_fabric,
+		.cleanup = NULL,
+	},
+};
+
+static uint64_t hook_debug_cntr_read(struct fid_cntr *cntr)
+{
+	struct hook_cntr *mycntr = container_of(cntr, struct hook_cntr, cntr);
+	uint64_t ret;
+
+	ret = fi_cntr_read(mycntr->hcntr);
+	hook_debug_trace_exit(&mycntr->cntr.fid, &mycntr->hcntr->fid,
+			      FI_LOG_CNTR, "fi_cntr_read", (ssize_t)ret, NULL);
+	return ret;
+}
+
+
+static int hook_debug_cntr_wait(struct fid_cntr *cntr, uint64_t threshold, int timeout)
+{
+	struct hook_cntr *mycntr = container_of(cntr, struct hook_cntr, cntr);
+	int ret;
+
+	HOOK_DEBUG_TRACE(mycntr->domain->fabric, FI_LOG_CNTR,
+			 "cntr: %p, threshold: %" PRIu64 ", timeout: %d\n",
+			 mycntr->hcntr, threshold, timeout);
+
+	ret = fi_cntr_wait(mycntr->hcntr, threshold, timeout);
+
+	hook_debug_trace_exit(&mycntr->cntr.fid, &mycntr->hcntr->fid,
+			      FI_LOG_CNTR, "fi_cntr_wait", (ssize_t)ret, NULL);
+	return ret;
+}
+
+static struct fi_ops_cntr hook_debug_cntr_ops;
+
+int hook_debug_cntr_init(struct fid *fid)
+{
+	struct hook_cntr *mycntr = container_of(fid, struct hook_cntr, cntr.fid);
+	HOOK_DEBUG_TRACE(mycntr->domain->fabric, FI_LOG_CNTR,
+			 "fi_cntr_open: %p\n", mycntr->hcntr);
+	mycntr->cntr.ops = &hook_debug_cntr_ops;
+	return 0;
+}
+
+static struct fi_ops_domain hook_debug_domain_ops;
+
+int hook_debug_domain_init(struct fid *fid)
+{
+	struct fid_domain *domain = container_of(fid, struct fid_domain, fid);
+	domain->ops = &hook_debug_domain_ops;
+	return 0;
+}
+
+HOOK_DEBUG_INI
+{
+	// TODO explore adding a common hook_ini function that can initialize
+	// the ops to common ones. Then override here.
+	hook_debug_fabric_fid_ops = hook_fid_ops;
+	hook_debug_fabric_ops = hook_fabric_ops;
+	hook_debug_fabric_ops.eq_open = hook_debug_eq_open;
+
+	hook_debug_eq_fid_ops = hook_fid_ops;
+	hook_debug_eq_fid_ops.close = hook_debug_eq_close;
+	hook_debug_eq_ops = hook_eq_ops;
+	hook_debug_eq_ops.read = hook_debug_eq_read;
+	hook_debug_eq_ops.sread = hook_debug_eq_sread;
+
+	hook_debug_domain_ops = hook_domain_ops;
+	hook_debug_domain_ops.cq_open = hook_debug_cq_open;
+	hook_debug_domain_ops.endpoint = hook_debug_endpoint;
+
+	hook_debug_cq_fid_ops = hook_fid_ops;
+	hook_debug_cq_fid_ops.close = hook_debug_cq_close;
+
+	hook_debug_cq_ops = hook_cq_ops;
+	hook_debug_cq_ops.read = hook_debug_cq_read;
+	hook_debug_cq_ops.readfrom = fi_no_cq_readfrom;
+	hook_debug_cq_ops.sread = fi_no_cq_sread;
+	hook_debug_cq_ops.sreadfrom = fi_no_cq_sreadfrom;
+
+	hook_debug_cntr_ops = hook_cntr_ops;
+	hook_debug_cntr_ops.read = hook_debug_cntr_read;
+	hook_debug_cntr_ops.add = fi_no_cntr_add;
+	hook_debug_cntr_ops.set = fi_no_cntr_set;
+	hook_debug_cntr_ops.wait = hook_debug_cntr_wait;
+
+	hook_debug_ep_fid_ops = hook_fid_ops;
+	hook_debug_ep_fid_ops.bind = hook_debug_ep_bind;
+	hook_debug_ep_fid_ops.close = hook_debug_ep_close;
+
+	hook_debug_prov_ctx.ini_fid[FI_CLASS_DOMAIN] = hook_debug_domain_init;
+	hook_debug_prov_ctx.ini_fid[FI_CLASS_CNTR] = hook_debug_cntr_init;
+	return &hook_debug_prov_ctx.prov;
+}
diff --git a/prov/hook/perf/include/hook_perf.h b/prov/hook/perf/include/hook_perf.h
index 56625a8c9..9b86182a1 100644
--- a/prov/hook/perf/include/hook_perf.h
+++ b/prov/hook/perf/include/hook_perf.h
@@ -43,7 +43,7 @@ struct perf_fabric {
 	struct ofi_perfset perf_set;
 };
 
-int perf_hook_destroy(struct fid *fabric);
+int hook_perf_destroy(struct fid *fabric);
 
 
 #define HOOK_FOREACH(DECL)		\
@@ -95,11 +95,4 @@ enum perf_counters {
 
 extern const char *perf_counters_str[];
 
-extern struct fi_ops_msg perf_msg_ops;
-extern struct fi_ops_rma perf_rma_ops;
-extern struct fi_ops_tagged perf_tagged_ops;
-extern struct fi_ops_cq perf_cq_ops;
-extern struct fi_ops_cntr perf_cntr_ops;
-
-
 #endif /* _HOOK_PERF_H_ */
diff --git a/prov/hook/perf/src/hook_perf.c b/prov/hook/perf/src/hook_perf.c
index 154f16393..afa686f4c 100644
--- a/prov/hook/perf/src/hook_perf.c
+++ b/prov/hook/perf/src/hook_perf.c
@@ -378,7 +378,7 @@ perf_msg_injectdata(struct fid_ep *ep, const void *buf, size_t len,
 	return ret;
 }
 
-struct fi_ops_msg perf_msg_ops = {
+static struct fi_ops_msg perf_msg_ops = {
 	.size = sizeof(struct fi_ops_msg),
 	.recv = perf_msg_recv,
 	.recvv = perf_msg_recvv,
@@ -518,7 +518,7 @@ perf_rma_injectdata(struct fid_ep *ep, const void *buf, size_t len,
 	return ret;
 }
 
-struct fi_ops_rma perf_rma_ops = {
+static struct fi_ops_rma perf_rma_ops = {
 	.size = sizeof(struct fi_ops_rma),
 	.read = perf_rma_read,
 	.readv = perf_rma_readv,
@@ -656,7 +656,7 @@ perf_tagged_injectdata(struct fid_ep *ep, const void *buf, size_t len,
 	return ret;
 }
 
-struct fi_ops_tagged perf_tagged_ops = {
+static struct fi_ops_tagged perf_tagged_ops = {
 	.size = sizeof(struct fi_ops_tagged),
 	.recv = perf_tagged_recv,
 	.recvv = perf_tagged_recvv,
@@ -845,13 +845,13 @@ struct fi_ops_cntr perf_cntr_ops = {
 
 static struct fi_ops perf_fabric_fid_ops = {
 	.size = sizeof(struct fi_ops),
-	.close = perf_hook_destroy,
+	.close = hook_perf_destroy,
 	.bind = hook_bind,
 	.control = hook_control,
 	.ops_open = hook_ops_open,
 };
 
-int perf_hook_destroy(struct fid *fid)
+int hook_perf_destroy(struct fid *fid)
 {
 	struct perf_fabric *fab;
 
@@ -863,7 +863,9 @@ int perf_hook_destroy(struct fid *fid)
 	return FI_SUCCESS;
 }
 
-static int perf_hook_fabric(struct fi_fabric_attr *attr,
+struct hook_prov_ctx hook_perf_ctx;
+
+static int hook_perf_fabric(struct fi_fabric_attr *attr,
 			    struct fid_fabric **fabric, void *context)
 {
 	struct fi_provider *hprov = context;
@@ -882,23 +884,58 @@ static int perf_hook_fabric(struct fi_fabric_attr *attr,
 		return ret;
 	}
 
+	/*
+	 * TODO
+	 * comment from GitHub PR #5052:
+	 * "I think we want to try replacing HOOK_PERF with a
+	 * struct hook_provider * (now called struct hook_prov_ctx)."
+	 */
 	hook_fabric_init(&fab->fabric_hook, HOOK_PERF, attr->fabric, hprov,
-			 &perf_fabric_fid_ops);
+			 &perf_fabric_fid_ops, &hook_perf_ctx);
 	*fabric = &fab->fabric_hook.fabric;
 	return 0;
 }
 
-struct fi_provider perf_hook_prov = {
-	.version = FI_VERSION(1,0),
-	/* We're a pass-through provider, so the fi_version is always the latest */
-	.fi_version = FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION),
-	.name = "ofi_perf_hook",
-	.getinfo = NULL,
-	.fabric = perf_hook_fabric,
-	.cleanup = NULL,
+struct hook_prov_ctx hook_perf_ctx = {
+	.prov = {
+		.version = FI_VERSION(1,0),
+		/* We're a pass-through provider, so the fi_version is always the latest */
+		.fi_version = FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION),
+		.name = "ofi_hook_perf",
+		.getinfo = NULL,
+		.fabric = hook_perf_fabric,
+		.cleanup = NULL,
+	},
 };
 
-PERF_HOOK_INI
+static int perf_cq_init(struct fid *fid)
+{
+	struct fid_cq *cq = container_of(fid, struct fid_cq, fid);
+	cq->ops = &perf_cq_ops;
+	return 0;
+}
+
+static int perf_cntr_init(struct fid *fid)
+{
+	struct fid_cntr *cntr = container_of(fid, struct fid_cntr, fid);
+	cntr->ops = &perf_cntr_ops;
+	return 0;
+}
+
+static int perf_endpoint_init(struct fid *fid)
+{
+	struct fid_ep *ep = container_of(fid, struct fid_ep, fid);
+	ep->msg = &perf_msg_ops;
+	ep->rma = &perf_rma_ops;
+	ep->tagged = &perf_tagged_ops;
+	return 0;
+}
+
+
+HOOK_PERF_INI
 {
-	return &perf_hook_prov;
+	hook_perf_ctx.ini_fid[FI_CLASS_CQ] = perf_cq_init;
+	hook_perf_ctx.ini_fid[FI_CLASS_CNTR] = perf_cntr_init;
+	hook_perf_ctx.ini_fid[FI_CLASS_EP] = perf_endpoint_init;
+	return &hook_perf_ctx.prov;
 }
diff --git a/prov/hook/src/hook.c b/prov/hook/src/hook.c
index 05015d9f8..58b6558f1 100644
--- a/prov/hook/src/hook.c
+++ b/prov/hook/src/hook.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018 Intel Corporation. All rights reserved.
+ * Copyright (c) 2018-2019 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -44,6 +44,54 @@ static char **hooks;
 static size_t hook_cnt;
 
 
+struct hook_fabric *hook_to_fabric(const struct fid *fid)
+{
+	switch (fid->fclass) {
+	case FI_CLASS_FABRIC:
+		return (container_of(fid, struct hook_fabric, fabric.fid));
+	case FI_CLASS_DOMAIN:
+		return (container_of(fid, struct hook_domain, domain.fid)->
+			fabric);
+	case FI_CLASS_AV:
+		return (container_of(fid, struct hook_av, av.fid)->
+			domain->fabric);
+	case FI_CLASS_WAIT:
+		return (container_of(fid, struct hook_wait, wait.fid)->
+			fabric);
+	case FI_CLASS_POLL:
+		return (container_of(fid, struct hook_poll, poll.fid)->
+			domain->fabric);
+	case FI_CLASS_EQ:
+		return (container_of(fid, struct hook_eq, eq.fid)->
+			fabric);
+	case FI_CLASS_CQ:
+		return (container_of(fid, struct hook_cq, cq.fid)->
+			domain->fabric);
+	case FI_CLASS_CNTR:
+		return (container_of(fid, struct hook_cntr, cntr.fid)->
+			domain->fabric);
+	case FI_CLASS_SEP:
+	case FI_CLASS_EP:
+	case FI_CLASS_RX_CTX:
+	case FI_CLASS_SRX_CTX:
+	case FI_CLASS_TX_CTX:
+		return (container_of(fid, struct hook_ep, ep.fid)->
+			domain->fabric);
+	case FI_CLASS_PEP:
+		return (container_of(fid, struct hook_pep, pep.fid)->
+			fabric);
+	case FI_CLASS_STX_CTX:
+		return (container_of(fid, struct hook_stx, stx.fid)->
+			domain->fabric);
+	case FI_CLASS_MR:
+		return (container_of(fid, struct hook_mr, mr.fid)->
+			domain->fabric);
+	default:
+		assert(0);
+		return NULL;
+	}
+}
+
 struct fid *hook_to_hfid(const struct fid *fid)
 {
 	switch (fid->fclass) {
@@ -136,12 +184,19 @@ int hook_ops_open(struct fid *fid, const char *name,
 int hook_close(struct fid *fid)
 {
 	struct fid *hfid;
+	struct hook_prov_ctx *prov_ctx;
 	int ret;
 
 	hfid = hook_to_hfid(fid);
 	if (!hfid)
 		return -FI_EINVAL;
 
+	prov_ctx = hook_to_prov_ctx(fid);
+	if (!prov_ctx)
+		return -FI_EINVAL;
+
+	hook_fini_fid(prov_ctx, fid);
+
 	ret = hfid->ops->close(hfid);
 	if (!ret)
 		free(fid);
@@ -157,7 +212,7 @@ struct fi_ops hook_fid_ops = {
 	.ops_open = hook_ops_open,
 };
 
-static struct fi_ops hook_fabric_fid_ops = {
+struct fi_ops hook_fabric_fid_ops = {
 	.size = sizeof(struct fi_ops),
 	.close = hook_close,
 	.bind = hook_bind,
@@ -165,7 +220,7 @@ static struct fi_ops hook_fabric_fid_ops = {
 	.ops_open = hook_ops_open,
 };
 
-static struct fi_ops_fabric hook_fabric_ops = {
+struct fi_ops_fabric hook_fabric_ops = {
 	.size = sizeof(struct fi_ops_fabric),
 	.domain = hook_domain,
 	.passive_ep = hook_passive_ep,
@@ -176,11 +231,12 @@ static struct fi_ops_fabric hook_fabric_ops = {
 
 void hook_fabric_init(struct hook_fabric *fabric, enum ofi_hook_class hclass,
 		      struct fid_fabric *hfabric, struct fi_provider *hprov,
-		      struct fi_ops *f_ops)
+		      struct fi_ops *f_ops, struct hook_prov_ctx *prov_ctx)
 {
 	fabric->hclass = hclass;
 	fabric->hfabric = hfabric;
-	fabric->prov = hprov;
+	fabric->hprov = hprov;
+	fabric->prov_ctx = prov_ctx;
 	fabric->fabric.fid.fclass = FI_CLASS_FABRIC;
 	fabric->fabric.fid.context = hfabric->fid.context;
 	fabric->fabric.fid.ops = f_ops;
@@ -190,7 +246,9 @@ void hook_fabric_init(struct hook_fabric *fabric, enum ofi_hook_class hclass,
 	hfabric->fid.context = fabric;
 }
 
-static int noop_hook_fabric(struct fi_fabric_attr *attr,
+struct hook_prov_ctx hook_noop_ctx;
+
+static int hook_noop_fabric(struct fi_fabric_attr *attr,
 			    struct fid_fabric **fabric, void *context)
 {
 	struct fi_provider *hprov = context;
@@ -202,24 +260,26 @@ static int noop_hook_fabric(struct fi_fabric_attr *attr,
 		return -FI_ENOMEM;
 
 	hook_fabric_init(fab, HOOK_NOOP, attr->fabric, hprov,
-			 &hook_fabric_fid_ops);
+			 &hook_fabric_fid_ops, &hook_noop_ctx);
 	*fabric = &fab->fabric;
 	return 0;
 }
 
-struct fi_provider noop_hook_prov = {
-	.version = FI_VERSION(1,0),
-	/* We're a pass-through provider, so the fi_version is always the latest */
-	.fi_version = FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION),
-	.name = "ofi_noop_hook",
-	.getinfo = NULL,
-	.fabric = noop_hook_fabric,
-	.cleanup = NULL,
+struct hook_prov_ctx hook_noop_ctx = {
+	.prov = {
+		.version = FI_VERSION(1,0),
+		/* We're a pass-through provider, so the fi_version is always the latest */
+		.fi_version = FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION),
+		.name = "ofi_hook_noop",
+		.getinfo = NULL,
+		.fabric = hook_noop_fabric,
+		.cleanup = NULL,
+	},
 };
 
-NOOP_HOOK_INI
+HOOK_NOOP_INI
 {
-	return &noop_hook_prov;
+	return &hook_noop_ctx.prov;
 }
 
 /*
diff --git a/prov/hook/src/hook_cntr.c b/prov/hook/src/hook_cntr.c
index 781bc9489..e4d3cfa3a 100644
--- a/prov/hook/src/hook_cntr.c
+++ b/prov/hook/src/hook_cntr.c
@@ -83,7 +83,7 @@ static int hook_cntr_seterr(struct fid_cntr *cntr, uint64_t value)
 	return fi_cntr_seterr(mycntr->hcntr, value);
 }
 
-static struct fi_ops_cntr hook_cntr_ops = {
+struct fi_ops_cntr hook_cntr_ops = {
 	.size = sizeof(struct fi_ops_cntr),
 	.read = hook_cntr_read,
 	.readerr = hook_cntr_readerr,
@@ -110,15 +110,7 @@ int hook_cntr_open(struct fid_domain *domain, struct fi_cntr_attr *attr,
 	mycntr->cntr.fid.fclass = FI_CLASS_CNTR;
 	mycntr->cntr.fid.context = context;
 	mycntr->cntr.fid.ops = &hook_fid_ops;
-
-	switch (dom->fabric->hclass) {
-	case HOOK_PERF:
-		mycntr->cntr.ops = &perf_cntr_ops;
-		break;
-	default:
-		mycntr->cntr.ops = &hook_cntr_ops;
-		break;
-	}
+	mycntr->cntr.ops = &hook_cntr_ops;
 
 	hattr = *attr;
 	if (attr->wait_obj == FI_WAIT_SET)
@@ -127,9 +119,18 @@ int hook_cntr_open(struct fid_domain *domain, struct fi_cntr_attr *attr,
 	ret = fi_cntr_open(dom->hdomain, &hattr, &mycntr->hcntr,
 			   &mycntr->cntr.fid);
 	if (ret)
-		free(mycntr);
-	else
-		*cntr = &mycntr->cntr;
+		goto err1;
+
+	*cntr = &mycntr->cntr;
 
+	ret = hook_ini_fid(dom->fabric->prov_ctx, &mycntr->cntr.fid);
+	if (ret)
+		goto err2;
+
+	return ret;
+err2:
+	fi_close(&mycntr->hcntr->fid);
+err1:
+	free(mycntr);
 	return ret;
 }
diff --git a/prov/hook/src/hook_cq.c b/prov/hook/src/hook_cq.c
index fb3bb83bc..5a4044be8 100644
--- a/prov/hook/src/hook_cq.c
+++ b/prov/hook/src/hook_cq.c
@@ -34,14 +34,14 @@
 #include "hook_prov.h"
 
 
-static ssize_t hook_cq_read(struct fid_cq *cq, void *buf, size_t count)
+ssize_t hook_cq_read(struct fid_cq *cq, void *buf, size_t count)
 {
 	struct hook_cq *mycq = container_of(cq, struct hook_cq, cq);
 
 	return fi_cq_read(mycq->hcq, buf, count);
 }
 
-static ssize_t
+ssize_t
 hook_cq_readerr(struct fid_cq *cq, struct fi_cq_err_entry *buf, uint64_t flags)
 {
 	struct hook_cq *mycq = container_of(cq, struct hook_cq, cq);
@@ -49,7 +49,7 @@ hook_cq_readerr(struct fid_cq *cq, struct fi_cq_err_entry *buf, uint64_t flags)
 	return fi_cq_readerr(mycq->hcq, buf, flags);
 }
 
-static ssize_t
+ssize_t
 hook_cq_readfrom(struct fid_cq *cq, void *buf, size_t count, fi_addr_t *src_addr)
 {
 	struct hook_cq *mycq = container_of(cq, struct hook_cq, cq);
@@ -57,7 +57,7 @@ hook_cq_readfrom(struct fid_cq *cq, void *buf, size_t count, fi_addr_t *src_addr
 	return fi_cq_readfrom(mycq->hcq, buf, count, src_addr);
 }
 
-static ssize_t
+ssize_t
 hook_cq_sread(struct fid_cq *cq, void *buf, size_t count,
 	      const void *cond, int timeout)
 {
@@ -66,7 +66,7 @@ hook_cq_sread(struct fid_cq *cq, void *buf, size_t count,
 	return fi_cq_sread(mycq->hcq, buf, count, cond, timeout);
 }
 
-static ssize_t
+ssize_t
 hook_cq_sreadfrom(struct fid_cq *cq, void *buf, size_t count,
 		  fi_addr_t *src_addr, const void *cond, int timeout)
 {
@@ -75,7 +75,7 @@ hook_cq_sreadfrom(struct fid_cq *cq, void *buf, size_t count,
 	return fi_cq_sreadfrom(mycq->hcq, buf, count, src_addr, cond, timeout);
 }
 
-static int hook_cq_signal(struct fid_cq *cq)
+int hook_cq_signal(struct fid_cq *cq)
 {
 	struct hook_cq *mycq = container_of(cq, struct hook_cq, cq);
 
@@ -91,7 +91,7 @@ hook_cq_strerror(struct fid_cq *cq, int prov_errno,
 	return fi_cq_strerror(mycq->hcq, prov_errno, err_data, buf,len);
 }
 
-static struct fi_ops_cq hook_cq_ops = {
+struct fi_ops_cq hook_cq_ops = {
 	.size = sizeof(struct fi_ops_cq),
 	.read = hook_cq_read,
 	.readfrom = hook_cq_readfrom,
@@ -102,31 +102,18 @@ static struct fi_ops_cq hook_cq_ops = {
 	.strerror = hook_cq_strerror,
 };
 
-int hook_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
-		 struct fid_cq **cq, void *context)
+int hook_cq_init(struct fid_domain *domain, struct fi_cq_attr *attr,
+		 struct fid_cq **cq, void *context, struct hook_cq *mycq)
 {
 	struct hook_domain *dom = container_of(domain, struct hook_domain, domain);
-	struct hook_cq *mycq;
 	struct fi_cq_attr hattr;
 	int ret;
 
-	mycq = calloc(1, sizeof *mycq);
-	if (!mycq)
-		return -FI_ENOMEM;
-
 	mycq->domain = dom;
 	mycq->cq.fid.fclass = FI_CLASS_CQ;
 	mycq->cq.fid.context = context;
 	mycq->cq.fid.ops = &hook_fid_ops;
-
-	switch (dom->fabric->hclass) {
-	case HOOK_PERF:
-		mycq->cq.ops = &perf_cq_ops;
-		break;
-	default:
-		mycq->cq.ops = &hook_cq_ops;
-		break;
-	}
+	mycq->cq.ops = &hook_cq_ops;
 
 	hattr = *attr;
 	if (attr->wait_obj == FI_WAIT_SET)
@@ -134,9 +121,35 @@ int hook_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 
 	ret = fi_cq_open(dom->hdomain, &hattr, &mycq->hcq, &mycq->cq.fid);
 	if (ret)
-		free(mycq);
-	else
-		*cq = &mycq->cq;
+		return ret;
+
+	*cq = &mycq->cq;
+	return 0;
+}
+
+int hook_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
+		 struct fid_cq **cq, void *context)
+{
+	struct hook_domain *dom = container_of(domain, struct hook_domain, domain);
+	struct hook_cq *mycq;
+	int ret;
+
+	mycq = calloc(1, sizeof *mycq);
+	if (!mycq)
+		return -FI_ENOMEM;
+
+	ret = hook_cq_init(domain, attr, cq, context, mycq);
+	if (ret)
+		goto err1;
+
+	ret = hook_ini_fid(dom->fabric->prov_ctx, &mycq->cq.fid);
+	if (ret)
+		goto err2;
 
+	return 0;
+err2:
+	fi_close(&mycq->hcq->fid);
+err1:
+	free(mycq);
 	return ret;
 }
diff --git a/prov/hook/src/hook_domain.c b/prov/hook/src/hook_domain.c
index 04706665a..4e37d437c 100644
--- a/prov/hook/src/hook_domain.c
+++ b/prov/hook/src/hook_domain.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018 Intel Corporation. All rights reserved.
+ * Copyright (c) 2018-2019 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -34,7 +34,6 @@
 #include <sys/uio.h>
 #include "ofi_hook.h"
 
-
 static int hook_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 			   uint64_t flags, struct fid_mr **mr)
 {
@@ -101,9 +100,7 @@ static struct fi_ops_mr hook_mr_ops = {
 	.regattr = hook_mr_regattr,
 };
 
-
-static int
-hook_query_atomic(struct fid_domain *domain, enum fi_datatype datatype,
+int hook_query_atomic(struct fid_domain *domain, enum fi_datatype datatype,
 		  enum fi_op op, struct fi_atomic_attr *attr, uint64_t flags)
 {
 	struct hook_domain *dom = container_of(domain, struct hook_domain, domain);
@@ -111,7 +108,7 @@ hook_query_atomic(struct fid_domain *domain, enum fi_datatype datatype,
 	return fi_query_atomic(dom->hdomain, datatype, op, attr, flags);
 }
 
-static struct fi_ops_domain hook_domain_ops = {
+struct fi_ops_domain hook_domain_ops = {
 	.size = sizeof(struct fi_ops_domain),
 	.av_open = hook_av_open,
 	.cq_open = hook_cq_open,
@@ -145,9 +142,17 @@ int hook_domain(struct fid_fabric *fabric, struct fi_info *info,
 
 	ret = fi_domain(fab->hfabric, info, &dom->hdomain, &dom->domain.fid);
 	if (ret)
-		free(dom);
-	else
-		*domain = &dom->domain;
+		goto err1;
 
+	*domain = &dom->domain;
+
+	ret = hook_ini_fid(dom->fabric->prov_ctx, &dom->domain.fid);
+	if (ret)
+		goto err2;
+	return 0;
+err2:
+	fi_close(&dom->domain.fid);
+err1:
+	free(dom);
 	return ret;
 }
diff --git a/prov/hook/src/hook_ep.c b/prov/hook/src/hook_ep.c
index 655a1b42b..47c038455 100644
--- a/prov/hook/src/hook_ep.c
+++ b/prov/hook/src/hook_ep.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018 Intel Corporation. All rights reserved.
+ * Copyright (c) 2018-2019 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -33,6 +33,7 @@
 #include <stdlib.h>
 #include <ofi_enosys.h>
 #include "hook_prov.h"
+#include "ofi_hook.h"
 
 
 static int hook_open_tx_ctx(struct fid_ep *sep, int index,
@@ -94,19 +95,9 @@ static void hook_setup_ep(enum ofi_hook_class hclass, struct fid_ep *ep,
 	ep->fid.ops = &hook_fid_ops;
 	ep->ops = &hook_ep_ops;
 	ep->cm = &hook_cm_ops;
-
-	switch (hclass) {
-	case HOOK_PERF:
-		ep->msg = &perf_msg_ops;
-		ep->rma = &perf_rma_ops;
-		ep->tagged = &perf_tagged_ops;
-		break;
-	default:
-		ep->msg = &hook_msg_ops;
-		ep->rma = &hook_rma_ops;
-		ep->tagged = &hook_tagged_ops;
-		break;
-	}
+	ep->msg = &hook_msg_ops;
+	ep->rma = &hook_rma_ops;
+	ep->tagged = &hook_tagged_ops;
 	ep->atomic = &hook_atomic_ops;
 }
 
@@ -256,18 +247,13 @@ int hook_passive_ep(struct fid_fabric *fabric, struct fi_info *info,
 	return ret;
 }
 
-int hook_endpoint(struct fid_domain *domain, struct fi_info *info,
-		  struct fid_ep **ep, void *context)
+int hook_endpoint_init(struct fid_domain *domain, struct fi_info *info,
+		       struct fid_ep **ep, void *context, struct hook_ep *myep)
 {
 	struct hook_domain *dom = container_of(domain, struct hook_domain, domain);
-	struct hook_ep *myep;
 	struct fid *saved_fid;
 	int ret;
 
-	myep = calloc(1, sizeof *myep);
-	if (!myep)
-		return -FI_ENOMEM;
-
 	saved_fid = info->handle;
 	if (saved_fid) {
 		info->handle = hook_to_hfid(info->handle);
@@ -275,13 +261,42 @@ int hook_endpoint(struct fid_domain *domain, struct fi_info *info,
 			info->handle = saved_fid;
 	}
 	myep->domain = dom;
+
 	hook_setup_ep(dom->fabric->hclass, &myep->ep, FI_CLASS_EP, context);
+
 	ret = fi_endpoint(dom->hdomain, info, &myep->hep, &myep->ep.fid);
+	info->handle = saved_fid;
+
 	if (ret)
-		free(myep);
-	else
-		*ep = &myep->ep;
+		return ret;
 
-	info->handle = saved_fid;
+	*ep = &myep->ep;
+	return 0;
+}
+
+int hook_endpoint(struct fid_domain *domain, struct fi_info *info,
+		  struct fid_ep **ep, void *context)
+{
+	struct hook_domain *dom = container_of(domain, struct hook_domain, domain);
+	struct hook_ep *myep;
+	int ret;
+
+	myep = calloc(1, sizeof *myep);
+	if (!myep)
+		return -FI_ENOMEM;
+
+	ret = hook_endpoint_init(domain, info, ep, context, myep);
+	if (ret)
+		goto err1;
+
+	ret = hook_ini_fid(dom->fabric->prov_ctx, &myep->ep.fid);
+	if (ret)
+		goto err2;
+
+	return 0;
+err2:
+	fi_close(&myep->hep->fid);
+err1:
+	free(myep);
 	return ret;
 }
diff --git a/prov/hook/src/hook_eq.c b/prov/hook/src/hook_eq.c
index 8391099e1..4e2f5f659 100644
--- a/prov/hook/src/hook_eq.c
+++ b/prov/hook/src/hook_eq.c
@@ -51,7 +51,7 @@ static void hook_eq_map_fid(void *buf)
 	entry->fid = entry->fid->context;
 }
 
-static ssize_t hook_eq_read(struct fid_eq *eq, uint32_t *event,
+ssize_t hook_eq_read(struct fid_eq *eq, uint32_t *event,
 			    void *buf, size_t len, uint64_t flags)
 {
 	struct hook_eq *myeq = container_of(eq, struct hook_eq, eq);
@@ -85,7 +85,7 @@ static ssize_t hook_eq_write(struct fid_eq *eq, uint32_t event,
 	return fi_eq_write(myeq->heq, event, buf, len, flags);
 }
 
-static ssize_t hook_eq_sread(struct fid_eq *eq, uint32_t *event,
+ssize_t hook_eq_sread(struct fid_eq *eq, uint32_t *event,
 			     void *buf, size_t len, int timeout, uint64_t flags)
 {
 	struct hook_eq *myeq = container_of(eq, struct hook_eq, eq);
@@ -107,7 +107,7 @@ hook_eq_strerror(struct fid_eq *eq, int prov_errno,
 	return fi_eq_strerror(myeq->heq, prov_errno, err_data, buf, len);
 }
 
-static struct fi_ops_eq hook_eq_ops = {
+struct fi_ops_eq hook_eq_ops = {
 	.size = sizeof(struct fi_ops_eq),
 	.read = hook_eq_read,
 	.readerr = hook_eq_readerr,
@@ -116,17 +116,22 @@ static struct fi_ops_eq hook_eq_ops = {
 	.strerror = hook_eq_strerror,
 };
 
-int hook_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
-		 struct fid_eq **eq, void *context)
+int hook_eq_init(struct fid_fabric *fabric, struct fi_eq_attr *attr,
+		 struct fid_eq **eq, void *context, struct hook_eq *myeq)
 {
 	struct hook_fabric *fab = container_of(fabric, struct hook_fabric, fabric);
-	struct hook_eq *myeq;
 	struct fi_eq_attr hattr;
 	int ret;
 
-	myeq = calloc(1, sizeof *myeq);
-	if (!myeq)
-		return -FI_ENOMEM;
+	hattr = *attr;
+	if (attr->wait_obj == FI_WAIT_SET)
+		hattr.wait_set = hook_to_hwait(attr->wait_set);
+
+	ret = fi_eq_open(fab->hfabric, &hattr, &myeq->heq, &myeq->eq.fid);
+	if (ret)
+		return ret;
+
+	*eq = &myeq->eq;
 
 	myeq->fabric = fab;
 	myeq->eq.fid.fclass = FI_CLASS_EQ;
@@ -134,15 +139,17 @@ int hook_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
 	myeq->eq.fid.ops = &hook_fid_ops;
 	myeq->eq.ops = &hook_eq_ops;
 
-	hattr = *attr;
-	if (attr->wait_obj == FI_WAIT_SET)
-		hattr.wait_set = hook_to_hwait(attr->wait_set);
+	return 0;
+}
 
-	ret = fi_eq_open(fab->hfabric, &hattr, &myeq->heq, &myeq->eq.fid);
-	if (ret)
-		free(myeq);
-	else
-		*eq = &myeq->eq;
+int hook_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
+		 struct fid_eq **eq, void *context)
+{
+	struct hook_eq *myeq;
 
-	return ret;
+	myeq = calloc(1, sizeof *myeq);
+	if (!myeq)
+		return -FI_ENOMEM;
+
+	return hook_eq_init(fabric, attr, eq, context, myeq);
 }
diff --git a/prov/mrail/src/mrail.h b/prov/mrail/src/mrail.h
index 54e3a4338..3b4e73e1b 100644
--- a/prov/mrail/src/mrail.h
+++ b/prov/mrail/src/mrail.h
@@ -79,9 +79,11 @@ extern struct fi_fabric_attr mrail_fabric_attr;
 extern struct fi_info *mrail_info_vec[MRAIL_MAX_INFO];
 extern size_t mrail_num_info;
 
-#define MRAIL_POLICY_FIXED		0
-#define MRAIL_POLICY_ROUND_ROBIN	1
-#define MRAIL_POLICY_STRIPING		2
+enum {
+	MRAIL_POLICY_FIXED,
+	MRAIL_POLICY_ROUND_ROBIN,
+	MRAIL_POLICY_STRIPING
+};
 
 #define MRAIL_MAX_CONFIG		8
 
@@ -128,16 +130,45 @@ mrail_match_recv_handle_unexp(struct mrail_recv_queue *recv_queue, uint64_t tag,
 			      uint64_t addr, char *data, size_t len, void *context);
 
 /* mrail protocol */
-#define MRAIL_HDR_VERSION 1
+#define MRAIL_HDR_VERSION 2
+
+enum {
+	MRAIL_PROTO_EAGER,
+	MRAIL_PROTO_RNDV
+};
+
+enum {
+	MRAIL_RNDV_REQ,
+	MRAIL_RNDV_ACK
+};
 
 struct mrail_hdr {
 	uint8_t		version;
 	uint8_t		op;
-	uint8_t		padding[2];
+	uint8_t		protocol;
+	uint8_t		protocol_cmd;
 	uint32_t	seq;
 	uint64_t 	tag;
 };
 
+#define MRAIL_IOV_LIMIT		5
+
+/* bit 60~63 are provider defined */
+#define MRAIL_RNDV_FLAG		(1ULL << 60)
+
+struct mrail_rndv_hdr {
+	uint64_t		context;
+};
+
+struct mrail_rndv_req {
+	size_t			len;
+	size_t			count;
+	size_t			mr_count;
+	struct fi_rma_iov	rma_iov[MRAIL_IOV_LIMIT];
+	size_t			rawkey_size;
+	uint8_t			rawkey[]; /* rawkey + base_addr */
+};
+
 struct mrail_tx_buf {
 	/* context should stay at top and would get overwritten on
 	 * util buf release */
@@ -147,6 +178,9 @@ struct mrail_tx_buf {
 	 * and completion flags (FI_MSG, FI_TAGGED, etc) */
 	uint64_t		flags;
 	struct mrail_hdr	hdr;
+	struct mrail_rndv_hdr	rndv_hdr;
+	struct mrail_rndv_req	*rndv_req;
+	fid_t			rndv_mr_fid;
 };
 
 struct mrail_pkt {
@@ -156,13 +190,19 @@ struct mrail_pkt {
 
 /* TX & RX processing */
 
-#define MRAIL_IOV_LIMIT	5
-
 struct mrail_rx_buf {
 	struct fid_ep		*rail_ep;
 	struct mrail_pkt	pkt;
 };
 
+struct mrail_rndv_recv {
+	void			*context;
+	uint64_t		flags;
+	uint64_t		tag;
+	uint64_t		data;
+	size_t			len;
+};
+
 struct mrail_recv {
 	struct iovec 		iov[MRAIL_IOV_LIMIT];
 	void 			*desc[MRAIL_IOV_LIMIT];
@@ -176,6 +216,7 @@ struct mrail_recv {
 	fi_addr_t 		addr;
 	uint64_t 		tag;
 	uint64_t 		ignore;
+	struct mrail_rndv_recv	rndv;
 };
 DECLARE_FREESTACK(struct mrail_recv, mrail_recv_fs);
 
@@ -400,3 +441,8 @@ static inline void mrail_cntr_incerr(struct util_cntr *cntr)
                cntr->cntr_fid.ops->adderr(&cntr->cntr_fid, 1);
        }
 }
+
+int mrail_send_rndv_ack_blocking(struct mrail_ep *mrail_ep,
+				 struct mrail_cq *mrail_cq,
+				 fi_addr_t dest_addr,
+				 void *context);
diff --git a/prov/mrail/src/mrail_cq.c b/prov/mrail/src/mrail_cq.c
index 31860c54b..481ca14ff 100644
--- a/prov/mrail/src/mrail_cq.c
+++ b/prov/mrail/src/mrail_cq.c
@@ -34,6 +34,37 @@
 
 #include "mrail.h"
 
+static int mrail_cq_write_send_comp(struct util_cq *cq,
+				    struct mrail_tx_buf *tx_buf)
+{
+	int ret = 0;
+
+	ofi_ep_tx_cntr_inc(&tx_buf->ep->util_ep);
+
+	if (tx_buf->flags & FI_COMPLETION) {
+		ret = ofi_cq_write(cq, tx_buf->context,
+				   (tx_buf->flags &
+				    (FI_TAGGED | FI_MSG)) |
+				   FI_SEND, 0, NULL, 0, 0);
+		if (ret) {
+			FI_WARN(&mrail_prov, FI_LOG_CQ,
+				"Unable to write to util cq\n");
+		}
+	}
+
+	if (tx_buf->hdr.protocol == MRAIL_PROTO_RNDV &&
+	    tx_buf->hdr.protocol_cmd == MRAIL_RNDV_REQ) {
+		free(tx_buf->rndv_req);
+		fi_close(tx_buf->rndv_mr_fid);
+	}
+
+	ofi_ep_lock_acquire(&tx_buf->ep->util_ep);
+	ofi_buf_free(tx_buf);
+	ofi_ep_lock_release(&tx_buf->ep->util_ep);
+
+	return ret;
+}
+
 int mrail_cq_write_recv_comp(struct mrail_ep *mrail_ep, struct mrail_hdr *hdr,
 			     struct fi_cq_tagged_entry *comp,
 			     struct mrail_recv *recv)
@@ -51,6 +82,143 @@ int mrail_cq_write_recv_comp(struct mrail_ep *mrail_ep, struct mrail_hdr *hdr,
 			   NULL, comp->data, hdr->tag);
 }
 
+static int mrail_cq_write_rndv_recv_comp(struct mrail_ep *mrail_ep,
+					 struct mrail_recv *recv)
+{
+	FI_DBG(&mrail_prov, FI_LOG_CQ, "finish rndv recv: length: %zu "
+	       "tag: 0x%" PRIx64 "\n", recv->rndv.len, recv->rndv.tag);
+	ofi_ep_rx_cntr_inc(&mrail_ep->util_ep);
+	if (!(recv->flags & FI_COMPLETION))
+		return 0;
+	return ofi_cq_write(mrail_ep->util_ep.rx_cq, recv->context,
+			   recv->comp_flags | recv->rndv.flags,
+			   recv->rndv.len, NULL, recv->rndv.data,
+			   recv->rndv.tag);
+}
+
+static void mrail_finish_rndv_recv(struct util_cq *cq,
+				   struct mrail_req *req,
+				   struct fi_cq_tagged_entry *comp)
+{
+	struct mrail_cq *mrail_cq = container_of(cq, struct mrail_cq, util_cq);
+	struct mrail_recv *recv = req->comp.op_context;
+	int ret;
+
+	ret = mrail_cq_write_rndv_recv_comp(req->mrail_ep, recv);
+	if (ret) {
+		FI_WARN(&mrail_prov, FI_LOG_CQ,
+			"Cannot write to recv cq\n");
+		assert(0);
+	}
+
+	ret = mrail_send_rndv_ack_blocking(req->mrail_ep, mrail_cq, recv->addr,
+					   (void *)recv->rndv.context);
+	if (ret) {
+		FI_WARN(&mrail_prov, FI_LOG_CQ,
+			"Cannot send rndv ack: %s\n", fi_strerror(-ret));
+		assert(0);
+	}
+
+	mrail_free_req(req->mrail_ep, req);
+	mrail_push_recv(recv);
+}
+
+static int mrail_cq_process_rndv_req(struct fi_cq_tagged_entry *comp,
+				     struct mrail_recv *recv)
+{
+	struct fi_recv_context *recv_ctx = comp->op_context;
+	struct fi_msg msg = {
+		.context = recv_ctx,
+	};
+	struct mrail_ep *mrail_ep;
+	struct mrail_pkt *mrail_pkt;
+	struct mrail_rndv_hdr *rndv_hdr;
+	struct mrail_rndv_req *rndv_req;
+	struct fi_msg_rma rma_msg;
+	uint64_t *base_addrs;
+	size_t key_size;
+	size_t offset;
+	int ret, retv = 0;
+	int i;
+
+	mrail_ep = recv_ctx->ep->fid.context;
+	mrail_pkt = (struct mrail_pkt *)comp->buf;
+	rndv_hdr = (struct mrail_rndv_hdr *)&mrail_pkt[1];
+	rndv_req = (struct mrail_rndv_req *)&rndv_hdr[1];
+	recv->rndv.context = (void *)rndv_hdr->context;
+	recv->rndv.flags = comp->flags & FI_REMOTE_CQ_DATA;
+	recv->rndv.len = rndv_req->len;
+	recv->rndv.tag = mrail_pkt->hdr.tag;
+	recv->rndv.data = comp->data;
+
+	base_addrs = (uint64_t *)(rndv_req->rawkey + rndv_req->rawkey_size);
+	for (offset = 0, i = 0; i < rndv_req->count; i++) {
+		if (i < rndv_req->mr_count) {
+			key_size = rndv_req->rma_iov[i].key;
+			ret = fi_mr_map_raw(&mrail_ep->util_ep.domain->domain_fid,
+					    base_addrs[i],
+					    rndv_req->rawkey + offset,
+					    key_size,
+					    &rndv_req->rma_iov[i].key,
+					    0);
+			assert(!ret);
+			offset += key_size;
+		} else {
+			rndv_req->rma_iov[i].key = rndv_req->rma_iov[0].key;
+		}
+	}
+
+	rma_msg.msg_iov		= recv->iov + 1;
+	rma_msg.desc		= recv->desc + 1;
+	rma_msg.iov_count	= recv->count - 1;
+	rma_msg.addr		= recv->addr;
+	rma_msg.rma_iov		= rndv_req->rma_iov;
+	rma_msg.rma_iov_count	= rndv_req->count;
+	rma_msg.context		= recv;
+
+	ret = fi_readmsg(&mrail_ep->util_ep.ep_fid, &rma_msg,
+			 MRAIL_RNDV_FLAG | FI_COMPLETION);
+	if (ret)
+		retv = ret;
+
+	ret = fi_recvmsg(recv_ctx->ep, &msg, FI_DISCARD);
+	if (ret) {
+		FI_WARN(&mrail_prov, FI_LOG_CQ,
+			"Unable to discard buffered recv\n");
+		retv = retv ? retv : ret;
+	}
+
+	return retv;
+}
+
+static int mrail_cq_process_rndv_ack(struct fi_cq_tagged_entry *comp)
+{
+	struct fi_recv_context *recv_ctx = comp->op_context;
+	struct fi_msg msg = {
+		.context = recv_ctx,
+	};
+	struct mrail_pkt *mrail_pkt;
+	struct mrail_rndv_hdr *rndv_hdr;
+	struct mrail_tx_buf *tx_buf;
+	int ret, retv = 0;
+
+	mrail_pkt = (struct mrail_pkt *)comp->buf;
+	rndv_hdr = (struct mrail_rndv_hdr *)&mrail_pkt[1];
+	tx_buf = (struct mrail_tx_buf *)rndv_hdr->context;
+	ret = mrail_cq_write_send_comp(tx_buf->ep->util_ep.tx_cq, tx_buf);
+	if (ret)
+		retv = ret;
+
+	ret = fi_recvmsg(recv_ctx->ep, &msg, FI_DISCARD);
+	if (ret) {
+		FI_WARN(&mrail_prov, FI_LOG_CQ,
+			"Unable to discard buffered recv\n");
+		retv = retv ? retv : ret;
+	}
+
+	return retv;
+}
+
 int mrail_cq_process_buf_recv(struct fi_cq_tagged_entry *comp,
 			      struct mrail_recv *recv)
 {
@@ -83,6 +251,9 @@ int mrail_cq_process_buf_recv(struct fi_cq_tagged_entry *comp,
 	mrail_ep = recv_ctx->ep->fid.context;
 	mrail_pkt = (struct mrail_pkt *)comp->buf;
 
+	if (mrail_pkt->hdr.protocol == MRAIL_PROTO_RNDV)
+		return mrail_cq_process_rndv_req(comp, recv);
+
 	len = comp->len - sizeof(*mrail_pkt);
 
 	size = ofi_copy_to_iov(&recv->iov[1], recv->count - 1, 0,
@@ -111,7 +282,7 @@ out:
 	if (ret) {
 		FI_WARN(&mrail_prov, FI_LOG_CQ,
 			"Unable to discard buffered recv\n");
-		retv = ret;
+		retv = retv ? retv : ret;
 	}
 	mrail_push_recv(recv);
 	return retv;
@@ -257,6 +428,10 @@ static int mrail_handle_recv_completion(struct fi_cq_tagged_entry *comp,
 	// TODO make rxm send buffered recv amount of data for large message
 	assert(hdr->version == MRAIL_HDR_VERSION);
 
+	if (hdr->protocol == MRAIL_PROTO_RNDV &&
+	    hdr->protocol_cmd == MRAIL_RNDV_ACK)
+		return mrail_cq_process_rndv_ack(comp);
+
 	seq_no = ntohl(hdr->seq);
 	peer_info = ofi_av_get_addr(mrail_ep->util_ep.av, (int) src_addr);
 	FI_DBG(&mrail_prov, FI_LOG_CQ,
@@ -305,7 +480,7 @@ static int mrail_cq_close(fid_t fid)
 
 	ret = ofi_cq_cleanup(&mrail_cq->util_cq);
 	if (ret)
-		retv = ret;
+		retv = retv ? retv : ret;
 
 	free(mrail_cq);
 	return retv;
@@ -343,6 +518,11 @@ static void mrail_handle_rma_completion(struct util_cq *cq,
 	req = subreq->parent;
 
 	if (ofi_atomic_dec32(&req->expected_subcomps) == 0) {
+		if (req->comp.flags & MRAIL_RNDV_FLAG) {
+			mrail_finish_rndv_recv(cq, req, comp);
+			return;
+		}
+
 		ret = ofi_cq_write(cq, req->comp.op_context, req->comp.flags,
 				req->comp.len, req->comp.buf, req->comp.data,
 				req->comp.tag);
@@ -386,34 +566,30 @@ void mrail_poll_cq(struct util_cq *cq)
 			FI_WARN(&mrail_prov, FI_LOG_CQ,
 				"Unable to read rail completion: %s\n",
 				fi_strerror(-ret));
-			goto err1;
+			goto err;
 		}
 		// TODO handle variable length message
 		if (comp.flags & FI_RECV) {
 			ret = mrail_cq->process_comp(&comp, src_addr);
 			if (ret)
-				goto err1;
+				goto err;
 		} else if (comp.flags & (FI_READ | FI_WRITE)) {
 			mrail_handle_rma_completion(cq, &comp);
 		} else if (comp.flags & FI_SEND) {
 			tx_buf = comp.op_context;
-
-			ofi_ep_tx_cntr_inc(&tx_buf->ep->util_ep);
-
-			if (tx_buf->flags & FI_COMPLETION) {
-				ret = ofi_cq_write(cq, tx_buf->context,
-						   (tx_buf->flags &
-						    (FI_TAGGED | FI_MSG)) |
-						   FI_SEND, 0, NULL, 0, 0);
-				if (ret) {
-					FI_WARN(&mrail_prov, FI_LOG_CQ,
-						"Unable to write to util cq\n");
-					goto err2;
+			if (tx_buf->hdr.protocol == MRAIL_PROTO_RNDV) {
+				if (tx_buf->hdr.protocol_cmd == MRAIL_RNDV_REQ) {
+					/* buf will be freed when ACK comes */
+				} else if (tx_buf->hdr.protocol_cmd == MRAIL_RNDV_ACK) {
+					ofi_ep_lock_acquire(&tx_buf->ep->util_ep);
+					ofi_buf_free(tx_buf);
+					ofi_ep_lock_release(&tx_buf->ep->util_ep);
 				}
+			} else {
+				ret = mrail_cq_write_send_comp(cq, tx_buf);
+				if (ret)
+					goto err;
 			}
-			ofi_ep_lock_acquire(&tx_buf->ep->util_ep);
-			ofi_buf_free(tx_buf);
-			ofi_ep_lock_release(&tx_buf->ep->util_ep);
 		} else {
 			/* We currently cannot support FI_REMOTE_READ and
 			 * FI_REMOTE_WRITE because RMA operations are split
@@ -432,11 +608,7 @@ void mrail_poll_cq(struct util_cq *cq)
 
 	return;
 
-err2:
-	ofi_ep_lock_acquire(&tx_buf->ep->util_ep);
-	ofi_buf_free(tx_buf);
-	ofi_ep_lock_release(&tx_buf->ep->util_ep);
-err1:
+err:
 	// TODO write error to cq
 	assert(0);
 }
diff --git a/prov/mrail/src/mrail_ep.c b/prov/mrail/src/mrail_ep.c
index dbc6ae5e4..deaad5c8e 100644
--- a/prov/mrail/src/mrail_ep.c
+++ b/prov/mrail/src/mrail_ep.c
@@ -337,70 +337,149 @@ static struct mrail_tx_buf *mrail_get_tx_buf(struct mrail_ep *mrail_ep,
 	return tx_buf;
 }
 
-static ssize_t
-mrail_send_common(struct fid_ep *ep_fid, const struct iovec *iov, void **desc,
-		  size_t count, size_t len, fi_addr_t dest_addr, uint64_t data,
-		  void *context, uint64_t flags)
+/*
+ * This is an internal send that doesn't use seq_no and doesn't update
+ * the counters. The call doesn't return -FI_EAGAIN.
+ */
+int mrail_send_rndv_ack_blocking(struct mrail_ep *mrail_ep,
+				 struct mrail_cq *mrail_cq,
+				 fi_addr_t dest_addr, void *context)
 {
-	struct mrail_ep *mrail_ep = container_of(ep_fid, struct mrail_ep,
-						 util_ep.ep_fid.fid);
-	struct mrail_peer_info *peer_info;
-	struct iovec *iov_dest = alloca(sizeof(*iov_dest) * (count + 1));
+	struct iovec iov_dest;
 	struct mrail_tx_buf *tx_buf;
-	int policy = mrail_get_policy(len);
-	uint32_t rail = mrail_get_tx_rail(mrail_ep, policy);
+	size_t rndv_pkt_size = sizeof(tx_buf->hdr) + sizeof(tx_buf->rndv_hdr);
+	int policy = mrail_get_policy(rndv_pkt_size);
+	uint32_t i = mrail_get_tx_rail(mrail_ep, policy);
 	struct fi_msg msg;
 	ssize_t ret;
-
-	peer_info = ofi_av_get_addr(mrail_ep->util_ep.av, (int) dest_addr);
+	uint64_t flags = FI_COMPLETION;
 
 	ofi_ep_lock_acquire(&mrail_ep->util_ep);
 
-	tx_buf = mrail_get_tx_buf(mrail_ep, context, peer_info->seq_no++,
-				  ofi_op_msg, flags | FI_MSG);
-	if (OFI_UNLIKELY(!tx_buf)) {
-		ret = -FI_ENOMEM;
-		goto err1;
-	}
-	mrail_copy_iov_hdr(&tx_buf->hdr, iov_dest, iov, count);
+	tx_buf = mrail_get_tx_buf(mrail_ep, context, 0, ofi_op_tagged, 0);
+	if (OFI_UNLIKELY(!tx_buf))
+		return -FI_ENOMEM;
+
+	tx_buf->hdr.protocol = MRAIL_PROTO_RNDV;
+	tx_buf->hdr.protocol_cmd = MRAIL_RNDV_ACK;
+	tx_buf->rndv_hdr.context = (uint64_t)context;
+
+	iov_dest.iov_base = &tx_buf->hdr;
+	iov_dest.iov_len = rndv_pkt_size;
 
-	msg.msg_iov 	= iov_dest;
-	msg.desc    	= desc;
-	msg.iov_count	= count + 1;
+	msg.msg_iov 	= &iov_dest;
+	msg.desc    	= NULL;
+	msg.iov_count	= 1;
 	msg.addr	= dest_addr;
 	msg.context	= tx_buf;
-	msg.data	= data;
 
-	if (len + iov_dest[0].iov_len <
-	    mrail_ep->rails[rail].info->tx_attr->inject_size)
+	if (iov_dest.iov_len < mrail_ep->rails[i].info->tx_attr->inject_size)
 		flags |= FI_INJECT;
 
-	FI_DBG(&mrail_prov, FI_LOG_EP_DATA, "Posting send of length: %" PRIu64
-	       " dest_addr: 0x%" PRIx64 "  seq: %d on rail: %d\n",
-	       len, dest_addr, peer_info->seq_no - 1, rail);
+	FI_DBG(&mrail_prov, FI_LOG_EP_DATA, "Posting rdnv ack "
+	       " dest_addr: 0x%" PRIx64 " on rail: %d\n", dest_addr, i);
+
+	do {
+		ret = fi_sendmsg(mrail_ep->rails[i].ep, &msg, flags);
+		if (ret == -FI_EAGAIN) {
+			FI_DBG(&mrail_prov, FI_LOG_EP_DATA,
+			        "Resource busy when trying to fi_sendmsg on rail: %"
+				PRIu32 ", retrying.\n", i);
+			fi_cq_read(mrail_cq->cqs[i], NULL, 0);
+		}
+	} while (ret == -FI_EAGAIN);
 
-	ret = fi_sendmsg(mrail_ep->rails[rail].ep, &msg, flags | FI_COMPLETION);
 	if (ret) {
 		FI_WARN(&mrail_prov, FI_LOG_EP_DATA,
-			"Unable to fi_sendmsg on rail: %" PRIu32 "\n", rail);
-		goto err2;
-	} else if (!(flags & FI_COMPLETION)) {
-		ofi_ep_tx_cntr_inc(&mrail_ep->util_ep);
+			"Unable to fi_sendmsg on rail: %" PRIu32 "\n", i);
+		ofi_buf_free(tx_buf);
 	}
-	ofi_ep_lock_release(&mrail_ep->util_ep);
-	return ret;
-err2:
-	ofi_buf_free(tx_buf);
-err1:
-	peer_info->seq_no--;
+
 	ofi_ep_lock_release(&mrail_ep->util_ep);
 	return ret;
 }
 
 static ssize_t
-mrail_tsend_common(struct fid_ep *ep_fid, const struct iovec *iov, void **desc,
-		   size_t count, size_t len, fi_addr_t dest_addr, uint64_t tag,
-		   uint64_t data, void *context, uint64_t flags)
+mrail_prepare_rndv_req(struct mrail_ep *mrail_ep, struct mrail_tx_buf *tx_buf,
+		       const struct iovec *iov, void **desc, size_t count,
+		       size_t len, struct iovec *iov_dest)
+{
+	size_t mr_count;
+	struct fid_mr *mr;
+	uint64_t addr, *base_addrs;
+	size_t key_size, offset;
+	size_t total_key_size = 0;
+	ssize_t ret;
+	int i;
+
+	tx_buf->hdr.protocol = MRAIL_PROTO_RNDV;
+	tx_buf->hdr.protocol_cmd = MRAIL_RNDV_REQ;
+	tx_buf->rndv_hdr.context = (uint64_t)tx_buf;
+	tx_buf->rndv_req = NULL;
+
+	if (!desc || !desc[0]) {
+		ret = fi_mr_regv(&mrail_ep->util_ep.domain->domain_fid,
+				 iov, count, FI_REMOTE_READ, 0, 0, 0, &mr, 0);
+		if (ret)
+			return ret;
+		total_key_size = 0;
+		ret = fi_mr_raw_attr(mr, &addr, NULL, &total_key_size, 0);
+		assert(ret == -FI_ETOOSMALL);
+		mr_count = 1;
+		tx_buf->rndv_mr_fid = &mr->fid;
+	} else {
+		total_key_size = 0;
+		for (i = 0; i < count; i++) {
+			mr = &((struct mrail_mr *)desc[i])->mr_fid;
+			key_size = 0;
+			ret = fi_mr_raw_attr(mr, &addr, NULL, &key_size, 0);
+			assert(ret == -FI_ETOOSMALL);
+			total_key_size += key_size;
+		}
+		mr_count = count;
+		tx_buf->rndv_mr_fid = NULL;
+	}
+
+	tx_buf->rndv_req = malloc(sizeof(*tx_buf->rndv_req) + total_key_size +
+				  sizeof(*base_addrs) * mr_count);
+	if (!tx_buf->rndv_req)
+		return -FI_ENOMEM;
+
+	tx_buf->rndv_req->len = len;
+	tx_buf->rndv_req->count = count;
+	tx_buf->rndv_req->mr_count = mr_count;
+	tx_buf->rndv_req->rawkey_size = total_key_size;
+
+	base_addrs = (uint64_t *)(tx_buf->rndv_req->rawkey + total_key_size);
+	for (offset = 0, i = 0; i < count; i++) {
+		if (i < mr_count) {
+			if (mr_count > 1)
+				mr = &((struct mrail_mr *)desc[i])->mr_fid;
+			key_size = total_key_size - offset;
+			ret = fi_mr_raw_attr(mr, &base_addrs[i],
+					     tx_buf->rndv_req->rawkey + offset,
+					     &key_size, 0);
+			assert(!ret);
+			offset += key_size;
+		}
+		tx_buf->rndv_req->rma_iov[i].addr = (uint64_t)iov[i].iov_base;
+		tx_buf->rndv_req->rma_iov[i].len = iov[i].iov_len;
+		tx_buf->rndv_req->rma_iov[i].key = key_size; /* otherwise unused */
+	}
+
+	iov_dest[0].iov_base = &tx_buf->hdr;
+	iov_dest[0].iov_len = sizeof(tx_buf->hdr) + sizeof(tx_buf->rndv_hdr);
+	iov_dest[1].iov_base = tx_buf->rndv_req;
+	iov_dest[1].iov_len = sizeof(*tx_buf->rndv_req) + total_key_size +
+			      sizeof(uint64_t) * mr_count;
+
+	return 0;
+}
+
+static ssize_t
+mrail_send_common(struct fid_ep *ep_fid, const struct iovec *iov, void **desc,
+		  size_t count, size_t len, fi_addr_t dest_addr, uint64_t tag,
+		  uint64_t data, void *context, uint64_t flags, uint64_t op)
 {
 	struct mrail_ep *mrail_ep = container_of(ep_fid, struct mrail_ep,
 						 util_ep.ep_fid.fid);
@@ -411,32 +490,50 @@ mrail_tsend_common(struct fid_ep *ep_fid, const struct iovec *iov, void **desc,
 	uint32_t rail = mrail_get_tx_rail(mrail_ep, policy);
 	struct fi_msg msg;
 	ssize_t ret;
+	size_t total_len;
 
 	peer_info = ofi_av_get_addr(mrail_ep->util_ep.av, (int) dest_addr);
 
 	ofi_ep_lock_acquire(&mrail_ep->util_ep);
 
 	tx_buf = mrail_get_tx_buf(mrail_ep, context, peer_info->seq_no++,
-				  ofi_op_tagged, flags | FI_TAGGED);
+				  ofi_op_tagged, flags | op);
 	if (OFI_UNLIKELY(!tx_buf)) {
 		ret = -FI_ENOMEM;
 		goto err1;
 	}
 	tx_buf->hdr.tag = tag;
-	mrail_copy_iov_hdr(&tx_buf->hdr, iov_dest, iov, count);
 
-	msg.msg_iov 	= iov_dest;
-	msg.desc    	= desc;
-	msg.iov_count	= count + 1;
-	msg.addr	= dest_addr;
-	msg.context	= tx_buf;
-	msg.data	= data;
+	if (policy == MRAIL_POLICY_STRIPING) {
+		ret = mrail_prepare_rndv_req(mrail_ep, tx_buf, iov, desc,
+					     count, len, iov_dest);
+		if (ret)
+			goto err2;
+
+		msg.msg_iov 	= iov_dest;
+		msg.desc    	= NULL;	/* it's fine since FI_MR_LOCAL is unsupported */
+		msg.iov_count	= 2;
+		msg.addr	= dest_addr;
+		msg.context	= tx_buf;
+		msg.data	= data;
+		total_len = iov_dest[0].iov_len + iov_dest[1].iov_len;
+	} else {
+		tx_buf->hdr.protocol = MRAIL_PROTO_EAGER;
+		mrail_copy_iov_hdr(&tx_buf->hdr, iov_dest, iov, count);
+
+		msg.msg_iov 	= iov_dest;
+		msg.desc    	= desc;	/* doesn't matter since FI_MR_LOCAL is unsupported */
+		msg.iov_count	= count + 1;
+		msg.addr	= dest_addr;
+		msg.context	= tx_buf;
+		msg.data	= data;
+		total_len = len + iov_dest[0].iov_len;
+	}
 
-	if (len + iov_dest[0].iov_len <
-	    mrail_ep->rails[rail].info->tx_attr->inject_size)
+	if (total_len < mrail_ep->rails[rail].info->tx_attr->inject_size)
 		flags |= FI_INJECT;
 
-	FI_DBG(&mrail_prov, FI_LOG_EP_DATA, "Posting tsend of length: %" PRIu64
+	FI_DBG(&mrail_prov, FI_LOG_EP_DATA, "Posting send of length: %" PRIu64
 	       " dest_addr: 0x%" PRIx64 " tag: 0x%" PRIx64 " seq: %d"
 	       " on rail: %d\n", len, dest_addr, tag, peer_info->seq_no - 1, rail);
 
@@ -451,6 +548,10 @@ mrail_tsend_common(struct fid_ep *ep_fid, const struct iovec *iov, void **desc,
 	ofi_ep_lock_release(&mrail_ep->util_ep);
 	return ret;
 err2:
+	if (tx_buf->hdr.protocol == MRAIL_PROTO_RNDV) {
+		free(tx_buf->rndv_req);
+		fi_close(tx_buf->rndv_mr_fid);
+	}
 	ofi_buf_free(tx_buf);
 err1:
 	peer_info->seq_no--;
@@ -463,8 +564,8 @@ static ssize_t mrail_sendmsg(struct fid_ep *ep_fid, const struct fi_msg *msg,
 {
 	return mrail_send_common(ep_fid, msg->msg_iov, msg->desc, msg->iov_count,
 				 ofi_total_iov_len(msg->msg_iov, msg->iov_count),
-				 msg->addr, msg->data, msg->context,
-				 flags | mrail_comp_flag(ep_fid));
+				 msg->addr, 0, msg->data, msg->context,
+				 flags | mrail_comp_flag(ep_fid), FI_MSG);
 }
 
 static ssize_t mrail_send(struct fid_ep *ep_fid, const void *buf, size_t len,
@@ -474,8 +575,8 @@ static ssize_t mrail_send(struct fid_ep *ep_fid, const void *buf, size_t len,
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_send_common(ep_fid, &iov, &desc, 1, len, dest_addr, 0,
-				 context, mrail_comp_flag(ep_fid));
+	return mrail_send_common(ep_fid, &iov, &desc, 1, len, dest_addr, 0, 0,
+				 context, mrail_comp_flag(ep_fid), FI_MSG);
 }
 
 static ssize_t mrail_inject(struct fid_ep *ep_fid, const void *buf, size_t len,
@@ -485,8 +586,8 @@ static ssize_t mrail_inject(struct fid_ep *ep_fid, const void *buf, size_t len,
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_send_common(ep_fid, &iov, NULL, 1, len, dest_addr, 0,
-				 NULL, mrail_inject_flags(ep_fid));
+	return mrail_send_common(ep_fid, &iov, NULL, 1, len, dest_addr, 0, 0,
+				 NULL, mrail_inject_flags(ep_fid), FI_MSG);
 }
 
 static ssize_t mrail_injectdata(struct fid_ep *ep_fid, const void *buf,
@@ -496,19 +597,19 @@ static ssize_t mrail_injectdata(struct fid_ep *ep_fid, const void *buf,
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_send_common(ep_fid, &iov, NULL, 1, len, dest_addr, data,
+	return mrail_send_common(ep_fid, &iov, NULL, 1, len, dest_addr, 0, data,
 				 NULL, (mrail_inject_flags(ep_fid) |
-					FI_REMOTE_CQ_DATA));
+					FI_REMOTE_CQ_DATA), FI_MSG);
 }
 
 static ssize_t
 mrail_tsendmsg(struct fid_ep *ep_fid, const struct fi_msg_tagged *msg,
 	       uint64_t flags)
 {
-	return mrail_tsend_common(ep_fid, msg->msg_iov, msg->desc, msg->iov_count,
-				  ofi_total_iov_len(msg->msg_iov, msg->iov_count),
-				  msg->addr, msg->tag, msg->data, msg->context,
-				  flags | mrail_comp_flag(ep_fid));
+	return mrail_send_common(ep_fid, msg->msg_iov, msg->desc, msg->iov_count,
+				 ofi_total_iov_len(msg->msg_iov, msg->iov_count),
+				 msg->addr, msg->tag, msg->data, msg->context,
+				 flags | mrail_comp_flag(ep_fid), FI_TAGGED);
 }
 
 static ssize_t mrail_tsend(struct fid_ep *ep_fid, const void *buf, size_t len,
@@ -519,8 +620,8 @@ static ssize_t mrail_tsend(struct fid_ep *ep_fid, const void *buf, size_t len,
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_tsend_common(ep_fid, &iov, &desc, 1, len, dest_addr, tag,
-				  0, context, mrail_comp_flag(ep_fid));
+	return mrail_send_common(ep_fid, &iov, &desc, 1, len, dest_addr, tag,
+				 0, context, mrail_comp_flag(ep_fid), FI_TAGGED);
 }
 
 static ssize_t mrail_tsenddata(struct fid_ep *ep_fid, const void *buf, size_t len,
@@ -531,9 +632,9 @@ static ssize_t mrail_tsenddata(struct fid_ep *ep_fid, const void *buf, size_t le
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_tsend_common(ep_fid, &iov, &desc, 1, len, dest_addr, tag,
-				  data, context, (mrail_comp_flag(ep_fid) |
-						  FI_REMOTE_CQ_DATA));
+	return mrail_send_common(ep_fid, &iov, &desc, 1, len, dest_addr, tag,
+				 data, context, (mrail_comp_flag(ep_fid) |
+						 FI_REMOTE_CQ_DATA), FI_TAGGED);
 }
 
 static ssize_t mrail_tinject(struct fid_ep *ep_fid, const void *buf, size_t len,
@@ -543,8 +644,8 @@ static ssize_t mrail_tinject(struct fid_ep *ep_fid, const void *buf, size_t len,
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_tsend_common(ep_fid, &iov, NULL, 1, len, dest_addr, tag,
-				  0, NULL, mrail_inject_flags(ep_fid));
+	return mrail_send_common(ep_fid, &iov, NULL, 1, len, dest_addr, tag,
+				 0, NULL, mrail_inject_flags(ep_fid), FI_TAGGED);
 }
 
 static ssize_t mrail_tinjectdata(struct fid_ep *ep_fid, const void *buf,
@@ -555,9 +656,9 @@ static ssize_t mrail_tinjectdata(struct fid_ep *ep_fid, const void *buf,
 		.iov_base 	= (void *)buf,
 		.iov_len 	= len,
 	};
-	return mrail_tsend_common(ep_fid, &iov, NULL, 1, len, dest_addr, tag,
-				  data, NULL, (mrail_inject_flags(ep_fid) |
-					       FI_REMOTE_CQ_DATA));
+	return mrail_send_common(ep_fid, &iov, NULL, 1, len, dest_addr, tag,
+				 data, NULL, (mrail_inject_flags(ep_fid) |
+					      FI_REMOTE_CQ_DATA), FI_TAGGED);
 }
 
 static struct mrail_unexp_msg_entry *
diff --git a/prov/psm2/src/psmx2_domain.c b/prov/psm2/src/psmx2_domain.c
index 741e79dc9..c99ef9579 100644
--- a/prov/psm2/src/psmx2_domain.c
+++ b/prov/psm2/src/psmx2_domain.c
@@ -375,7 +375,6 @@ int psmx2_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 			domain_priv->av_lock_fn = psmx2_lock_disabled;
 			domain_priv->trx_ctxt_lock_fn = psmx2_lock_disabled;
 			domain_priv->trigger_queue_lock_fn = psmx2_lock_disabled;
-			domain_priv->peer_lock_fn = psmx2_lock_disabled;
 			domain_priv->sep_lock_fn = psmx2_lock_disabled;
 			domain_priv->trigger_lock_fn = psmx2_lock_disabled;
 			domain_priv->cq_lock_fn = psmx2_lock_disabled;
@@ -386,7 +385,6 @@ int psmx2_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 			domain_priv->av_unlock_fn = psmx2_lock_disabled;
 			domain_priv->trx_ctxt_unlock_fn = psmx2_lock_disabled;
 			domain_priv->trigger_queue_unlock_fn = psmx2_lock_disabled;
-			domain_priv->peer_unlock_fn = psmx2_lock_disabled;
 			domain_priv->sep_unlock_fn = psmx2_lock_disabled;
 			domain_priv->trigger_unlock_fn = psmx2_lock_disabled;
 			domain_priv->cq_unlock_fn = psmx2_lock_disabled;
@@ -394,11 +392,15 @@ int psmx2_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 			domain_priv->context_unlock_fn = psmx2_lock_disabled;
 			domain_priv->poll_unlock_fn = psmx2_lock_disabled;
 
+			/* Enable lock accessed by the disconnection thread */
+			domain_priv->peer_lock_fn = psmx2_lock_enabled;
+			domain_priv->peer_unlock_fn = psmx2_unlock_enabled;
+
 			/*
 			 * If FI_RMA or FI_ATOMIC caps are enabled, then locks are
-			 * required for the CQ, am_req_poll, & rma_queue
+			 * required for the CQ, am_req_pool, & rma_queue
 			 * due to the PSM2 Recv thread.
-			 * NOTE: am_req_poll & rma_queue are only used when FI_RMA
+			 * NOTE: am_req_pool & rma_queue are only used when FI_RMA
 			 * and FI_ATOMIC capabilities are enabled.
 			 */
 			if ((info->caps & FI_RMA) || (info->caps & FI_ATOMIC)) {
@@ -409,6 +411,32 @@ int psmx2_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 				domain_priv->am_req_pool_unlock_fn = psmx2_unlock_enabled;
 				domain_priv->rma_queue_unlock_fn = psmx2_unlock_enabled;
 			}
+
+			/*
+			 * Locks accessed by the progress thread are required because
+			 * they are outside the scope of domain access serialization
+			 * implied by FI_THREAD_DOMAIN.
+			 */
+			if (domain_priv->progress_thread_enabled) {
+				domain_priv->trx_ctxt_lock_fn = psmx2_lock_enabled;
+				domain_priv->poll_trylock_fn = psmx2_trylock_enabled;
+				domain_priv->cq_lock_fn = psmx2_lock_enabled;
+				domain_priv->trx_ctxt_unlock_fn = psmx2_unlock_enabled;
+				domain_priv->poll_unlock_fn = psmx2_unlock_enabled;
+				domain_priv->cq_unlock_fn = psmx2_unlock_enabled;
+				if (info->caps & FI_TRIGGER) {
+					domain_priv->trigger_queue_lock_fn = psmx2_lock_enabled;
+					domain_priv->trigger_lock_fn = psmx2_lock_enabled;
+					domain_priv->av_lock_fn = psmx2_lock_enabled;
+					domain_priv->mr_lock_fn = psmx2_lock_enabled;
+					domain_priv->context_lock_fn = psmx2_lock_enabled;
+					domain_priv->trigger_queue_unlock_fn = psmx2_unlock_enabled;
+					domain_priv->trigger_unlock_fn = psmx2_unlock_enabled;
+					domain_priv->av_unlock_fn = psmx2_unlock_enabled;
+					domain_priv->mr_unlock_fn = psmx2_unlock_enabled;
+					domain_priv->context_unlock_fn = psmx2_unlock_enabled;
+				}
+			}
 			break;
 		default:
 			/* Otherwise, enable all locks */
diff --git a/prov/psm2/src/psmx2_trx_ctxt.c b/prov/psm2/src/psmx2_trx_ctxt.c
index 726a9e7dd..74fbb52b4 100644
--- a/prov/psm2/src/psmx2_trx_ctxt.c
+++ b/prov/psm2/src/psmx2_trx_ctxt.c
@@ -47,32 +47,40 @@ int psmx2_trx_ctxt_cnt = 0;
  */
 
 struct disconnect_args {
-	psm2_ep_t	ep;
-	psm2_epaddr_t	epaddr;
+	struct psmx2_trx_ctxt	*trx_ctxt;
+	psm2_epaddr_t		epaddr;
 };
 
+static int psmx2_peer_match(struct dlist_entry *item, const void *arg)
+{
+	struct psmx2_epaddr_context *peer;
+
+	peer = container_of(item, struct psmx2_epaddr_context, entry);
+	return  (peer->epaddr == arg);
+}
+
 static void *disconnect_func(void *args)
 {
 	struct disconnect_args *disconn = args;
+	struct psmx2_trx_ctxt *trx_ctxt = disconn->trx_ctxt;
 	psm2_error_t errors;
 
 	FI_INFO(&psmx2_prov, FI_LOG_CORE,
-		"psm2_ep: %p, epaddr: %p\n", disconn->ep, disconn->epaddr);
+		"psm2_ep: %p, epaddr: %p\n", trx_ctxt->psm2_ep, disconn->epaddr);
 
-	psm2_ep_disconnect2(disconn->ep, 1, &disconn->epaddr, NULL,
+	trx_ctxt->domain->peer_lock_fn(&trx_ctxt->peer_lock, 2);
+	dlist_remove_first_match(&trx_ctxt->peer_list,
+				 psmx2_peer_match, disconn->epaddr);
+	trx_ctxt->domain->peer_unlock_fn(&trx_ctxt->peer_lock, 2);
+	if (trx_ctxt->ep && trx_ctxt->ep->av)
+		psmx2_av_remove_conn(trx_ctxt->ep->av, trx_ctxt, disconn->epaddr);
+
+	psm2_ep_disconnect2(trx_ctxt->psm2_ep, 1, &disconn->epaddr, NULL,
 			    &errors, PSM2_EP_DISCONNECT_FORCE, 0);
 	free(args);
 	return NULL;
 }
 
-static int psmx2_peer_match(struct dlist_entry *item, const void *arg)
-{
-	struct psmx2_epaddr_context *peer;
-
-	peer = container_of(item, struct psmx2_epaddr_context, entry);
-	return  (peer->epaddr == arg);
-}
-
 int psmx2_am_trx_ctxt_handler(psm2_am_token_t token, psm2_amarg_t *args,
 			      int nargs, void *src, uint32_t len, void *hctx)
 {
@@ -93,16 +101,14 @@ int psmx2_am_trx_ctxt_handler(psm2_am_token_t token, psm2_amarg_t *args,
 		 * we can't call psm2_ep_disconnect from the AM
 		 * handler. instead, create a thread to do the work.
 		 * the performance of this operation is not important.
+		 *
+		 * also put the av cleanup operations into the thread
+		 * to avoid deadlock because the AM handler may be
+		 * called with the av lock held.
 		 */
 		disconn = malloc(sizeof(*disconn));
 		if (disconn) {
-			trx_ctxt->domain->peer_lock_fn(&trx_ctxt->peer_lock, 2);
-			dlist_remove_first_match(&trx_ctxt->peer_list,
-						 psmx2_peer_match, epaddr);
-			trx_ctxt->domain->peer_unlock_fn(&trx_ctxt->peer_lock, 2);
-			if (trx_ctxt->ep && trx_ctxt->ep->av)
-				psmx2_av_remove_conn(trx_ctxt->ep->av, trx_ctxt, epaddr);
-			disconn->ep = trx_ctxt->psm2_ep;
+			disconn->trx_ctxt = trx_ctxt;
 			disconn->epaddr = epaddr;
 			pthread_create(&disconnect_thread, NULL,
 				       disconnect_func, disconn);
diff --git a/prov/rxd/src/rxd.h b/prov/rxd/src/rxd.h
index 7f1f2e0c1..82e70f92b 100644
--- a/prov/rxd/src/rxd.h
+++ b/prov/rxd/src/rxd.h
@@ -172,6 +172,17 @@ struct rxd_cq {
 	rxd_cq_write_fn write_fn;
 };
 
+enum rxd_pool_type {
+	RXD_BUF_POOL_RX,
+	RXD_BUF_POOL_TX,
+};
+
+struct rxd_buf_pool {
+	enum rxd_pool_type type;
+	struct ofi_bufpool *pool;
+	struct rxd_ep *rxd_ep;
+};
+
 struct rxd_ep {
 	struct util_ep util_ep;
 	struct fid_ep *dg_ep;
@@ -181,7 +192,6 @@ struct rxd_ep {
 	size_t tx_size;
 	size_t tx_prefix_size;
 	size_t rx_prefix_size;
-	uint32_t posted_bufs;
 	size_t min_multi_recv_size;
 	int do_local_mr;
 	int next_retry;
@@ -194,12 +204,12 @@ struct rxd_ep {
 	size_t tx_rma_avail;
 	size_t rx_rma_avail;
 
-	struct ofi_bufpool *tx_pkt_pool;
-	struct ofi_bufpool *rx_pkt_pool;
+	struct rxd_buf_pool tx_pkt_pool;
+	struct rxd_buf_pool rx_pkt_pool;
 	struct slist rx_pkt_list;
 
-	struct ofi_bufpool *tx_entry_pool;
-	struct ofi_bufpool *rx_entry_pool;
+	struct rxd_buf_pool tx_entry_pool;
+	struct rxd_buf_pool rx_entry_pool;
 
 	struct dlist_entry unexp_list;
 	struct dlist_entry unexp_tag_list;
@@ -291,6 +301,7 @@ struct rxd_pkt_entry {
 	uint64_t timestamp;
 	struct fi_context context;
 	struct fid_mr *mr;
+	void *desc;
 	fi_addr_t peer;
 	void *pkt;
 };
@@ -353,6 +364,25 @@ static inline void *rxd_pkt_start(struct rxd_pkt_entry *pkt_entry)
 	return (void *) ((char *) pkt_entry + sizeof(*pkt_entry));
 }
 
+static inline size_t rxd_pkt_size(struct rxd_ep *ep, struct rxd_base_hdr *base_hdr,
+				   void *ptr)
+{
+	return ((char *) ptr - (char *) base_hdr) + ep->tx_prefix_size;
+}
+
+static inline void rxd_remove_free_pkt_entry(struct rxd_pkt_entry *pkt_entry)
+{
+	dlist_remove(&pkt_entry->d_entry);
+	ofi_buf_free(pkt_entry);
+}
+
+static inline void rxd_free_unexp_msg(struct rxd_unexp_msg *unexp_msg)
+{
+	ofi_buf_free(unexp_msg->pkt_entry);
+	dlist_remove(&unexp_msg->entry);
+	free(unexp_msg);
+}
+
 struct rxd_match_attr {
 	fi_addr_t	peer;
 	uint64_t	tag;
@@ -396,7 +426,6 @@ int rxd_av_insert_dg_addr(struct rxd_av *av, const void *addr,
 
 /* Pkt resource functions */
 int rxd_ep_post_buf(struct rxd_ep *ep);
-void rxd_release_repost_rx(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry);
 void rxd_ep_send_ack(struct rxd_ep *rxd_ep, fi_addr_t peer);
 struct rxd_pkt_entry *rxd_get_tx_pkt(struct rxd_ep *ep);
 struct rxd_x_entry *rxd_get_tx_entry(struct rxd_ep *ep, uint32_t op);
@@ -433,7 +462,8 @@ static inline void rxd_check_init_cq_data(void **ptr, struct rxd_x_entry *tx_ent
 /* Tx/Rx entry sub-functions */
 struct rxd_x_entry *rxd_tx_entry_init_common(struct rxd_ep *ep, fi_addr_t addr,
 			uint32_t op, const struct iovec *iov, size_t iov_count,
-			uint64_t tag, uint64_t data, uint32_t flags, void *context);
+			uint64_t tag, uint64_t data, uint32_t flags, void *context,
+			struct rxd_base_hdr **base_hdr, void **ptr);
 struct rxd_x_entry *rxd_rx_entry_init(struct rxd_ep *ep,
 			const struct iovec *iov, size_t iov_count, uint64_t tag,
 			uint64_t ignore, void *context, fi_addr_t addr,
@@ -478,6 +508,7 @@ struct rxd_x_entry *rxd_progress_multi_recv(struct rxd_ep *ep,
 					    struct rxd_x_entry *rx_entry,
 					    size_t total_size);
 void rxd_ep_progress(struct util_ep *util_ep);
+void rxd_cleanup_unexp_msg(struct rxd_unexp_msg *unexp_msg);
 
 /* CQ sub-functions */
 void rxd_cq_report_error(struct rxd_cq *cq, struct fi_cq_err_entry *err_entry);
diff --git a/prov/rxd/src/rxd_atomic.c b/prov/rxd/src/rxd_atomic.c
index a60e15015..9d75d474b 100644
--- a/prov/rxd/src/rxd_atomic.c
+++ b/prov/rxd/src/rxd_atomic.c
@@ -54,14 +54,10 @@ static struct rxd_x_entry *rxd_tx_entry_init_atomic(struct rxd_ep *ep, fi_addr_t
 	OFI_UNUSED(len);
 
 	tx_entry = rxd_tx_entry_init_common(ep, addr, op, iov, iov_count, 0,
-					    data, flags, context);
+					    data, flags, context, &base_hdr, &ptr);
 	if (!tx_entry)
 		return NULL;
 
-	base_hdr = rxd_get_base_hdr(tx_entry->pkt);
-	ptr = (void *) base_hdr;
-	rxd_init_base_hdr(ep, &ptr, tx_entry);
-
 	if (res_count) {
 		tx_entry->res_count = res_count;
 		memcpy(&tx_entry->res_iov[0], res_iov, sizeof(*res_iov) * res_count);
@@ -98,8 +94,8 @@ static struct rxd_x_entry *rxd_tx_entry_init_atomic(struct rxd_ep *ep, fi_addr_t
 			assert(len == tx_entry->bytes_done);
 		}
 	}
-	tx_entry->pkt->pkt_size = ((char *) ptr - (char *) base_hdr) +
-				ep->tx_prefix_size;
+
+	tx_entry->pkt->pkt_size = rxd_pkt_size(ep, base_hdr, ptr);
 
 	return tx_entry;
 }
diff --git a/prov/rxd/src/rxd_av.c b/prov/rxd/src/rxd_av.c
index 48446d22c..d817c9487 100644
--- a/prov/rxd/src/rxd_av.c
+++ b/prov/rxd/src/rxd_av.c
@@ -158,13 +158,14 @@ int rxd_av_insert_dg_addr(struct rxd_av *av, const void *addr,
 
 	*rxd_addr = rxd_set_rxd_addr(av, dg_addr);
 
-	ret = ofi_rbmap_insert(&av->rbmap, (void *) addr, (void *) (*rxd_addr));
-	if (ret && ret != -FI_EALREADY) {
+	ret = ofi_rbmap_insert(&av->rbmap, (void *) addr, (void *) (*rxd_addr),
+			       NULL);
+	if (ret) {
+		assert(ret != -FI_EALREADY);
 		fi_av_remove(av->dg_av, &dg_addr, 1, flags);
-		return ret;
 	}
 
-	return 0;
+	return ret;
 }
 
 static int rxd_av_insert(struct fid_av *av_fid, const void *addr, size_t count,
diff --git a/prov/rxd/src/rxd_cq.c b/prov/rxd/src/rxd_cq.c
index 80915ddc7..0384ead08 100644
--- a/prov/rxd/src/rxd_cq.c
+++ b/prov/rxd/src/rxd_cq.c
@@ -107,12 +107,6 @@ static void rxd_remove_rx_pkt(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry
 	}
 }
 
-void rxd_release_repost_rx(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
-{
-	ofi_buf_free(pkt_entry);
-	rxd_ep_post_buf(ep);
-}
-
 static void rxd_complete_rx(struct rxd_ep *ep, struct rxd_x_entry *rx_entry)
 {
 	struct fi_cq_err_entry err_entry;
@@ -277,6 +271,7 @@ void rxd_progress_tx_list(struct rxd_ep *ep, struct rxd_peer *peer)
 	struct dlist_entry *tmp_entry;
 	struct rxd_x_entry *tx_entry;
 	uint64_t head_seq = peer->last_rx_ack;
+	int ret = 0, inc = 0;
 
 	if (!dlist_empty(&peer->unacked)) {
 		head_seq = rxd_get_base_hdr(container_of(
@@ -307,9 +302,25 @@ void rxd_progress_tx_list(struct rxd_ep *ep, struct rxd_peer *peer)
 			}
 			continue;
 		}
+				
+		if (tx_entry->op == RXD_DATA_READ && !tx_entry->bytes_done) {
+			if (ep->peers[tx_entry->peer].unacked_cnt >=
+		    	    ep->peers[tx_entry->peer].tx_window) {
+				break;
+			} 
+			tx_entry->start_seq = ep->peers[tx_entry->peer].tx_seq_no;
+			ep->peers[tx_entry->peer].tx_seq_no = tx_entry->start_seq +
+							      tx_entry->num_segs;
+			inc = 1;
+		}
 
-		if (!rxd_ep_post_data_pkts(ep, tx_entry))
+		ret = rxd_ep_post_data_pkts(ep, tx_entry);
+		if (ret) {
+			if (ret == -FI_ENOMEM && inc)
+				ep->peers[tx_entry->peer].tx_seq_no -=
+							  tx_entry->num_segs;
 			break;
+		}
 	}
 
 	if (dlist_empty(&peer->tx_list))
@@ -346,10 +357,8 @@ static int rxd_send_cts(struct rxd_ep *rxd_ep, struct rxd_rts_pkt *rts_pkt,
 
 	dlist_insert_tail(&pkt_entry->d_entry, &rxd_ep->ctrl_pkts);
 	ret = rxd_ep_send_pkt(rxd_ep, pkt_entry);
-	if (ret) {
-		dlist_remove(&pkt_entry->d_entry);
-		ofi_buf_free(pkt_entry);
-	}
+	if (ret)
+		rxd_remove_free_pkt_entry(pkt_entry);
 
 	return ret;
 }
@@ -867,10 +876,10 @@ static struct rxd_x_entry *rxd_get_data_x_entry(struct rxd_ep *ep,
 			struct rxd_data_pkt *data_pkt)
 {
 	if (data_pkt->base_hdr.type == RXD_DATA)
-		return ofi_bufpool_get_ibuf(ep->rx_entry_pool,
+		return ofi_bufpool_get_ibuf(ep->rx_entry_pool.pool,
 			     ep->peers[data_pkt->base_hdr.peer].curr_rx_id);
 
-	return ofi_bufpool_get_ibuf(ep->tx_entry_pool, data_pkt->ext_hdr.tx_id);
+	return ofi_bufpool_get_ibuf(ep->tx_entry_pool.pool, data_pkt->ext_hdr.tx_id);
 }
 
 static void rxd_progress_buf_pkts(struct rxd_ep *ep, fi_addr_t peer)
@@ -914,8 +923,7 @@ static void rxd_progress_buf_pkts(struct rxd_ep *ep, fi_addr_t peer)
 					FI_WARN(&rxd_prov, FI_LOG_EP_CTRL,
 						"could not write error entry\n");
 				ep->peers[base_hdr->peer].rx_seq_no++;
-				dlist_remove(&pkt_entry->d_entry);
-				rxd_release_repost_rx(ep, pkt_entry);
+				rxd_remove_free_pkt_entry(pkt_entry);
 				continue;
 			}
 			if (!rx_entry) {
@@ -933,8 +941,7 @@ static void rxd_progress_buf_pkts(struct rxd_ep *ep, fi_addr_t peer)
 		}
 
 		ep->peers[base_hdr->peer].rx_seq_no++;
-		dlist_remove(&pkt_entry->d_entry);
-		rxd_release_repost_rx(ep, pkt_entry);
+		rxd_remove_free_pkt_entry(pkt_entry);
 	}
 }
 
@@ -960,7 +967,6 @@ static void rxd_handle_data(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 				ep->peers[pkt->base_hdr.peer].curr_unexp = NULL;
 				rxd_ep_send_ack(ep, pkt->base_hdr.peer);
 			}
-			rxd_remove_rx_pkt(ep, pkt_entry);
 			return;
 		}
 		x_entry = rxd_get_data_x_entry(ep, pkt);
@@ -968,7 +974,6 @@ static void rxd_handle_data(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 		if (!dlist_empty(&ep->peers[pkt->base_hdr.peer].buf_pkts))
 			rxd_progress_buf_pkts(ep, pkt->base_hdr.peer);
 	} else if (!rxd_env.retry) {
-		rxd_remove_rx_pkt(ep, pkt_entry);
 		dlist_insert_order(&ep->peers[pkt->base_hdr.peer].buf_pkts,
 				   &rxd_comp_pkt_seq_no, &pkt_entry->d_entry);
 		return;
@@ -976,8 +981,7 @@ static void rxd_handle_data(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 		rxd_ep_send_ack(ep, pkt->base_hdr.peer);
 	}
 free:
-	rxd_remove_rx_pkt(ep, pkt_entry);
-	rxd_release_repost_rx(ep, pkt_entry);
+	ofi_buf_free(pkt_entry);
 }
 
 static void rxd_handle_op(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
@@ -995,7 +999,6 @@ static void rxd_handle_op(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 
 	if (base_hdr->seq_no != ep->peers[base_hdr->peer].rx_seq_no) {
 		if (!rxd_env.retry) {
-			rxd_remove_rx_pkt(ep, pkt_entry);
 			dlist_insert_order(&ep->peers[base_hdr->peer].buf_pkts,
 					   &rxd_comp_pkt_seq_no, &pkt_entry->d_entry);
 			return;
@@ -1021,7 +1024,6 @@ static void rxd_handle_op(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 				goto ack;
 
 			ep->peers[base_hdr->peer].rx_seq_no++;
-			rxd_remove_rx_pkt(ep, pkt_entry);
 
 			if (!sar_hdr)
 				ep->peers[base_hdr->peer].curr_unexp = NULL;
@@ -1044,8 +1046,7 @@ static void rxd_handle_op(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 ack:
 	rxd_ep_send_ack(ep, base_hdr->peer);
 release:
-	rxd_remove_rx_pkt(ep, pkt_entry);
-	rxd_release_repost_rx(ep, pkt_entry);
+	ofi_buf_free(pkt_entry);
 }
 
 static void rxd_handle_cts(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
@@ -1092,9 +1093,8 @@ static void rxd_handle_ack(struct rxd_ep *ep, struct rxd_pkt_entry *ack_entry)
 						 struct rxd_pkt_entry, d_entry);
 			continue;
 		}
-		dlist_remove(&pkt_entry->d_entry);
-		ofi_buf_free(pkt_entry);
-	     	ep->peers[peer].unacked_cnt--;
+		rxd_remove_free_pkt_entry(pkt_entry);
+		ep->peers[peer].unacked_cnt--;
 		ep->peers[peer].retry_cnt = 0;
 
 		pkt_entry = container_of((&ep->peers[peer].unacked)->next,
@@ -1117,15 +1117,13 @@ void rxd_handle_send_comp(struct rxd_ep *ep, struct fi_cq_msg_entry *comp)
 	switch (rxd_pkt_type(pkt_entry)) {
 	case RXD_CTS:
 	case RXD_ACK:
-		dlist_remove(&pkt_entry->d_entry);
-		ofi_buf_free(pkt_entry);
+		rxd_remove_free_pkt_entry(pkt_entry);
 		break;
 	default:
 		if (pkt_entry->flags & RXD_PKT_ACKED) {
 			peer = pkt_entry->peer;
-			dlist_remove(&pkt_entry->d_entry);
-			ofi_buf_free(pkt_entry);
-	     		ep->peers[peer].unacked_cnt--;
+			rxd_remove_free_pkt_entry(pkt_entry);
+			ep->peers[peer].unacked_cnt--;
 			rxd_progress_tx_list(ep, &ep->peers[peer]);
 		} else {
 			pkt_entry->flags &= ~RXD_PKT_IN_USE;
@@ -1142,7 +1140,8 @@ void rxd_handle_recv_comp(struct rxd_ep *ep, struct fi_cq_msg_entry *comp)
 	       "got recv completion (type: %s)\n",
 	       rxd_pkt_type_str[(rxd_pkt_type(pkt_entry))]);
 
-	ep->posted_bufs--;
+	rxd_ep_post_buf(ep);
+	rxd_remove_rx_pkt(ep, pkt_entry);
 
 	pkt_entry->pkt_size = comp->len;
 	switch (rxd_pkt_type(pkt_entry)) {
@@ -1159,19 +1158,16 @@ void rxd_handle_recv_comp(struct rxd_ep *ep, struct fi_cq_msg_entry *comp)
 	case RXD_DATA_READ:
 		rxd_handle_data(ep, pkt_entry);
 		/* don't need to perform action below:
-		 * - remove RX packet
 		 * - release/repost RX packet */
 		return;
 	default:
 		rxd_handle_op(ep, pkt_entry);
 		/* don't need to perform action below:
-		 * - remove RX packet
 		 * - release/repost RX packet */
 		return;
 	}
 
-	rxd_remove_rx_pkt(ep, pkt_entry);
-	rxd_release_repost_rx(ep, pkt_entry);
+	ofi_buf_free(pkt_entry);
 }
 
 void rxd_handle_error(struct rxd_ep *ep)
diff --git a/prov/rxd/src/rxd_ep.c b/prov/rxd/src/rxd_ep.c
index b2d0e4056..2dfb6a518 100644
--- a/prov/rxd/src/rxd_ep.c
+++ b/prov/rxd/src/rxd_ep.c
@@ -39,36 +39,13 @@
 struct rxd_pkt_entry *rxd_get_tx_pkt(struct rxd_ep *ep)
 {
 	struct rxd_pkt_entry *pkt_entry;
-	void *mr = NULL;
 
-	pkt_entry = ep->do_local_mr ?
-		    ofi_buf_alloc_ex(ep->tx_pkt_pool, &mr) :
-		    ofi_buf_alloc(ep->tx_pkt_pool);
+	pkt_entry = ofi_buf_alloc(ep->tx_pkt_pool.pool);
 
 	if (!pkt_entry)
 		return NULL;
 
-	pkt_entry->mr = (struct fid_mr *) mr;
 	pkt_entry->flags = 0;
-	rxd_set_tx_pkt(ep, pkt_entry);
-
-	return pkt_entry;
-}
-
-static struct rxd_pkt_entry *rxd_get_rx_pkt(struct rxd_ep *ep)
-{
-	struct rxd_pkt_entry *pkt_entry;
-	void *mr = NULL;
-
-	pkt_entry = ep->do_local_mr ?
-		    ofi_buf_alloc_ex(ep->rx_pkt_pool, &mr) :
-		    ofi_buf_alloc(ep->rx_pkt_pool);
-
-	if (!pkt_entry)
-		return NULL;
-
-	pkt_entry->mr = (struct fid_mr *) mr;
-	rxd_set_rx_pkt(ep, pkt_entry);
 
 	return pkt_entry;
 }
@@ -84,11 +61,10 @@ struct rxd_x_entry *rxd_get_tx_entry(struct rxd_ep *ep, uint32_t op)
 		return NULL;
 	}
 
-	tx_entry = ofi_ibuf_alloc(ep->tx_entry_pool);
+	tx_entry = ofi_ibuf_alloc(ep->tx_entry_pool.pool);
 	if (!tx_entry)
 		return NULL;
 
-	tx_entry->tx_id = ofi_buf_index(tx_entry);
 	(*avail)--;
 
 	return tx_entry;
@@ -105,11 +81,10 @@ struct rxd_x_entry *rxd_get_rx_entry(struct rxd_ep *ep, uint32_t op)
 		return NULL;
 	}
 
-	rx_entry = ofi_ibuf_alloc(ep->rx_entry_pool);
+	rx_entry = ofi_ibuf_alloc(ep->rx_entry_pool.pool);
 	if (!rx_entry)
 		return NULL;
 
-	rx_entry->rx_id = ofi_buf_index(rx_entry);
 	(*avail)--;
 
 	return rx_entry;
@@ -248,31 +223,25 @@ struct rxd_x_entry *rxd_rx_entry_init(struct rxd_ep *ep,
 	return rx_entry;
 }
 
-static inline void *rxd_mr_desc(struct fid_mr *mr, struct rxd_ep *ep)
-{
-	return (ep->do_local_mr) ? fi_mr_desc(mr) : NULL;
-}
-
 int rxd_ep_post_buf(struct rxd_ep *ep)
 {
 	struct rxd_pkt_entry *pkt_entry;
 	ssize_t ret;
 
-	pkt_entry = rxd_get_rx_pkt(ep);
+	pkt_entry = ofi_buf_alloc(ep->rx_pkt_pool.pool);
 	if (!pkt_entry)
 		return -FI_ENOMEM;
 
 	ret = fi_recv(ep->dg_ep, rxd_pkt_start(pkt_entry),
 		      rxd_ep_domain(ep)->max_mtu_sz,
-		      rxd_mr_desc(pkt_entry->mr, ep),
-		      FI_ADDR_UNSPEC, &pkt_entry->context);
+		      pkt_entry->desc, FI_ADDR_UNSPEC,
+		      &pkt_entry->context);
 	if (ret) {
 		ofi_buf_free(pkt_entry);
 		FI_WARN(&rxd_prov, FI_LOG_EP_CTRL, "failed to repost\n");
 		return ret;
 	}
 
-	ep->posted_bufs++;
 	slist_insert_tail(&pkt_entry->s_entry, &ep->rx_pkt_list);
 
 	return 0;
@@ -351,7 +320,8 @@ void rxd_init_data_pkt(struct rxd_ep *ep, struct rxd_x_entry *tx_entry,
 
 struct rxd_x_entry *rxd_tx_entry_init_common(struct rxd_ep *ep, fi_addr_t addr,
 			uint32_t op, const struct iovec *iov, size_t iov_count,
-			uint64_t tag, uint64_t data, uint32_t flags, void *context)
+			uint64_t tag, uint64_t data, uint32_t flags, void *context,
+			struct rxd_base_hdr **base_hdr, void **ptr)
 {
 	struct rxd_x_entry *tx_entry;
 
@@ -385,6 +355,10 @@ struct rxd_x_entry *rxd_tx_entry_init_common(struct rxd_ep *ep, fi_addr_t addr,
 
 	tx_entry->pkt->peer = tx_entry->peer;
 
+	*base_hdr = rxd_get_base_hdr(tx_entry->pkt);
+	*ptr = (void *) *base_hdr;
+	rxd_init_base_hdr(ep, &(*ptr), tx_entry);
+
 	dlist_insert_tail(&tx_entry->entry,
 			  &ep->peers[tx_entry->peer].tx_list);
 
@@ -421,12 +395,6 @@ ssize_t rxd_ep_post_data_pkts(struct rxd_ep *ep, struct rxd_x_entry *tx_entry)
 		if (!pkt_entry)
 			return -FI_ENOMEM;
 
-		if (tx_entry->op == RXD_DATA_READ && !tx_entry->bytes_done) {
-			tx_entry->start_seq = ep->peers[tx_entry->peer].tx_seq_no;
-			ep->peers[tx_entry->peer].tx_seq_no = tx_entry->start_seq +
-							      tx_entry->num_segs;
-		}
-
 		rxd_init_data_pkt(ep, tx_entry, pkt_entry);
 
 		data = (struct rxd_data_pkt *) (pkt_entry->pkt);
@@ -439,7 +407,7 @@ ssize_t rxd_ep_post_data_pkts(struct rxd_ep *ep, struct rxd_x_entry *tx_entry)
 		rxd_insert_unacked(ep, tx_entry->peer, pkt_entry);
 	}
 
-	return ep->peers[tx_entry->peer].unacked_cnt <
+	return ep->peers[tx_entry->peer].unacked_cnt >=
 	       ep->peers[tx_entry->peer].tx_window;
 }
 
@@ -450,7 +418,7 @@ int rxd_ep_send_pkt(struct rxd_ep *ep, struct rxd_pkt_entry *pkt_entry)
 	pkt_entry->timestamp = fi_gettime_ms();
 
 	ret = fi_send(ep->dg_ep, (const void *) rxd_pkt_start(pkt_entry),
-		      pkt_entry->pkt_size, rxd_mr_desc(pkt_entry->mr, ep),
+		      pkt_entry->pkt_size, pkt_entry->desc,
 		      rxd_ep_av(ep)->rxd_addr_table[pkt_entry->peer].dg_addr,
 		      &pkt_entry->context);
 	if (ret) {
@@ -607,18 +575,23 @@ void rxd_ep_send_ack(struct rxd_ep *rxd_ep, fi_addr_t peer)
 	rxd_ep->peers[peer].last_tx_ack = ack->base_hdr.seq_no;
 
 	dlist_insert_tail(&pkt_entry->d_entry, &rxd_ep->ctrl_pkts);
-	if (rxd_ep_send_pkt(rxd_ep, pkt_entry)) {
-		dlist_remove(&pkt_entry->d_entry);
-		ofi_buf_free(pkt_entry);
-	}
+	if (rxd_ep_send_pkt(rxd_ep, pkt_entry))
+		rxd_remove_free_pkt_entry(pkt_entry);
 }
 
 static void rxd_ep_free_res(struct rxd_ep *ep)
 {
-	ofi_bufpool_destroy(ep->tx_pkt_pool);
-	ofi_bufpool_destroy(ep->rx_pkt_pool);
-	ofi_bufpool_destroy(ep->tx_entry_pool);
-	ofi_bufpool_destroy(ep->rx_entry_pool);
+	if (ep->tx_pkt_pool.pool)
+		ofi_bufpool_destroy(ep->tx_pkt_pool.pool);
+
+	if (ep->rx_pkt_pool.pool)
+		ofi_bufpool_destroy(ep->rx_pkt_pool.pool);
+
+	if (ep->tx_entry_pool.pool)
+		ofi_bufpool_destroy(ep->tx_entry_pool.pool);
+
+	if (ep->rx_entry_pool.pool)
+		ofi_bufpool_destroy(ep->rx_entry_pool.pool);
 }
 
 static void rxd_close_peer(struct rxd_ep *ep, struct rxd_peer *peer)
@@ -655,21 +628,26 @@ static void rxd_close_peer(struct rxd_ep *ep, struct rxd_peer *peer)
 	peer->active = 0;
 }
 
-static void rxd_cleanup_unexp_msg(struct dlist_entry *list)
+void rxd_cleanup_unexp_msg(struct rxd_unexp_msg *unexp_msg)
 {
-	struct rxd_unexp_msg *unexp_msg;
 	struct rxd_pkt_entry *pkt_entry;
+	while (!dlist_empty(&unexp_msg->pkt_list)) {
+		dlist_pop_front(&unexp_msg->pkt_list, struct rxd_pkt_entry,
+				pkt_entry, d_entry);
+		ofi_buf_free(pkt_entry);
+	}
+
+	rxd_free_unexp_msg(unexp_msg);
+}
+
+static void rxd_cleanup_unexp_msg_list(struct dlist_entry *list)
+{
+	struct rxd_unexp_msg *unexp_msg;
 
 	while (!dlist_empty(list)) {
 		dlist_pop_front(list, struct rxd_unexp_msg,
 				unexp_msg, entry);
-		while (!dlist_empty(&unexp_msg->pkt_list)) {
-			dlist_pop_front(&unexp_msg->pkt_list, struct rxd_pkt_entry,
-					pkt_entry, d_entry);
-			ofi_buf_free(pkt_entry);
-		}
-		ofi_buf_free(unexp_msg->pkt_entry);
-		free(unexp_msg);
+		rxd_cleanup_unexp_msg(unexp_msg);
 	}
 }
 
@@ -702,8 +680,8 @@ static int rxd_ep_close(struct fid *fid)
 		ofi_buf_free(pkt_entry);
 	}
 
-	rxd_cleanup_unexp_msg(&ep->unexp_list);
-	rxd_cleanup_unexp_msg(&ep->unexp_tag_list);
+	rxd_cleanup_unexp_msg_list(&ep->unexp_list);
+	rxd_cleanup_unexp_msg_list(&ep->unexp_tag_list);
 
 	while (!dlist_empty(&ep->ctrl_pkts)) {
 		dlist_pop_front(&ep->ctrl_pkts, struct rxd_pkt_entry,
@@ -1007,70 +985,141 @@ void rxd_ep_progress(struct util_ep *util_ep)
 	}
 
 out:
-	while (ep->posted_bufs < ep->rx_size) {
-		ret = rxd_ep_post_buf(ep);
-		if (ret)
-			break;
-	}
-
 	fastlock_release(&ep->util_ep.lock);
 }
 
 static int rxd_buf_region_alloc_fn(struct ofi_bufpool_region *region)
 {
-	struct rxd_domain *domain = region->pool->attr.context;
+	struct rxd_buf_pool *pool = region->pool->attr.context;
 	struct fid_mr *mr;
 	int ret;
 
-	ret = fi_mr_reg(domain->dg_domain, region->mem_region,
+	if (!pool->rxd_ep->do_local_mr) {
+		region->context = NULL;
+		return 0;
+	}
+
+	ret = fi_mr_reg(rxd_ep_domain(pool->rxd_ep)->dg_domain, region->mem_region,
 			region->pool->region_size,
 			FI_SEND | FI_RECV, 0, 0, 0, &mr, NULL);
+
 	region->context = mr;
 	return ret;
 }
 
+static void rxd_pkt_init_fn(struct ofi_bufpool_region *region, void *buf)
+{
+	struct rxd_pkt_entry *pkt_entry = (struct rxd_pkt_entry *) buf;
+	struct rxd_buf_pool *pool = (struct rxd_buf_pool *) region->pool->attr.context;
+
+ 	if (pool->rxd_ep->do_local_mr)
+		pkt_entry->desc = fi_mr_desc((struct fid_mr *) region->context);
+	else
+		pkt_entry->desc = NULL;
+
+ 	pkt_entry->mr = (struct fid_mr *) region->context;
+	if (pool->type == RXD_BUF_POOL_RX)
+		rxd_set_rx_pkt(pool->rxd_ep, pkt_entry);
+	else
+		rxd_set_tx_pkt(pool->rxd_ep, pkt_entry);
+}
+
+ static void rxd_entry_init_fn(struct ofi_bufpool_region *region, void *buf)
+{
+	struct rxd_x_entry *entry = (struct rxd_x_entry *) buf;
+	struct rxd_buf_pool *pool = (struct rxd_buf_pool *) region->pool->attr.context;
+
+ 	if (pool->type == RXD_BUF_POOL_TX)
+		entry->tx_id = ofi_buf_index(entry);
+	else
+		entry->rx_id = ofi_buf_index(entry);
+}
+
 static void rxd_buf_region_free_fn(struct ofi_bufpool_region *region)
 {
-	fi_close(region->context);
+	struct rxd_buf_pool *pool = region->pool->attr.context;
+
+	if (pool->rxd_ep->do_local_mr)
+		fi_close(region->context);
 }
 
-int rxd_ep_init_res(struct rxd_ep *ep, struct fi_info *fi_info)
+static int rxd_pool_create_attrs(struct rxd_ep *ep, struct rxd_buf_pool *pool,
+					struct ofi_bufpool_attr attr,
+					enum rxd_pool_type type)
 {
-	struct rxd_domain *rxd_domain = rxd_ep_domain(ep);
-	struct ofi_bufpool_attr pkt_attr = { 0 };
-	struct ofi_bufpool_attr entry_attr = { 0 };
 	int ret;
+	pool->rxd_ep = ep;
+	pool->type = type;
 
-	pkt_attr.size = rxd_domain->max_mtu_sz + sizeof(struct rxd_pkt_entry);
-	pkt_attr.alignment = RXD_BUF_POOL_ALIGNMENT;
-	pkt_attr.chunk_cnt = RXD_TX_POOL_CHUNK_CNT;
-	pkt_attr.alloc_fn = ep->do_local_mr ? rxd_buf_region_alloc_fn : NULL;
-	pkt_attr.free_fn = ep->do_local_mr ? rxd_buf_region_free_fn : NULL;
-	pkt_attr.context = rxd_domain;
-	pkt_attr.flags = OFI_BUFPOOL_HUGEPAGES;
-
-	ret = ofi_bufpool_create_attr(&pkt_attr, &ep->tx_pkt_pool);
+	ret = ofi_bufpool_create_attr(&attr, &pool->pool);
 	if (ret)
-		return ret;
+		FI_WARN(&rxd_prov, FI_LOG_EP_CTRL,
+			"Unable to create buf pool\n");
+	return ret;
+}
+
+static int rxd_pkt_pool_create(struct rxd_ep *ep,
+			       size_t chunk_cnt, struct rxd_buf_pool *pool,
+			       enum rxd_pool_type type)
+{
+	struct ofi_bufpool_attr attr = {
+		.size		= rxd_ep_domain(ep)->max_mtu_sz +
+				  sizeof(struct rxd_pkt_entry),
+		.alignment	= RXD_BUF_POOL_ALIGNMENT,
+		.max_cnt	= 0,
+		.chunk_cnt	= chunk_cnt,
+		.alloc_fn	= rxd_buf_region_alloc_fn,
+		.free_fn	= rxd_buf_region_free_fn,
+		.init_fn	= rxd_pkt_init_fn,
+		.context	= pool,
+		.flags		= OFI_BUFPOOL_HUGEPAGES,
+	};
+
+	return rxd_pool_create_attrs(ep, pool, attr, type);
+}
+
+
+static int rxd_entry_pool_create(struct rxd_ep *ep,
+				 size_t chunk_cnt, struct rxd_buf_pool *pool,
+				 enum rxd_pool_type type)
+{
+	struct ofi_bufpool_attr attr = {
+		.size		= sizeof(struct rxd_x_entry),
+		.alignment	= RXD_BUF_POOL_ALIGNMENT,
+		.max_cnt	= (size_t) ((uint16_t) (~0)),
+		.chunk_cnt	= chunk_cnt,
+		.alloc_fn	= NULL,
+		.free_fn	= NULL,
+		.init_fn	= rxd_entry_init_fn,
+		.context	= pool,
+		.flags		= OFI_BUFPOOL_INDEXED | OFI_BUFPOOL_NO_TRACK |
+				  OFI_BUFPOOL_HUGEPAGES,
+	};
+
+	return rxd_pool_create_attrs(ep, pool, attr, type);
+}
+
+int rxd_ep_init_res(struct rxd_ep *ep, struct fi_info *fi_info)
+{
+	int ret;
 
-	pkt_attr.chunk_cnt = RXD_RX_POOL_CHUNK_CNT;
-	ret = ofi_bufpool_create_attr(&pkt_attr, &ep->rx_pkt_pool);
+	ret = rxd_pkt_pool_create(ep, RXD_TX_POOL_CHUNK_CNT,
+				  &ep->tx_pkt_pool, RXD_BUF_POOL_TX);
 	if (ret)
 		goto err;
 
-	entry_attr.size = sizeof(struct rxd_x_entry);
-	entry_attr.alignment = RXD_BUF_POOL_ALIGNMENT;
-	entry_attr.max_cnt = (size_t) ((uint16_t) (~0));
-	entry_attr.chunk_cnt = ep->tx_size;
-	entry_attr.flags = OFI_BUFPOOL_INDEXED | OFI_BUFPOOL_NO_TRACK |
-			   OFI_BUFPOOL_HUGEPAGES;
+	ret = rxd_pkt_pool_create(ep, RXD_RX_POOL_CHUNK_CNT,
+				  &ep->rx_pkt_pool, RXD_BUF_POOL_RX);
+	if (ret)
+		goto err;
 
-	ret = ofi_bufpool_create_attr(&entry_attr, &ep->tx_entry_pool);
+	ret = rxd_entry_pool_create(ep, ep->tx_size,
+				    &ep->tx_entry_pool, RXD_BUF_POOL_TX);
 	if (ret)
 		goto err;
 
-	entry_attr.chunk_cnt = ep->rx_size;
-	ret = ofi_bufpool_create_attr(&entry_attr, &ep->rx_entry_pool);
+	ret = rxd_entry_pool_create(ep, ep->rx_size,
+				    &ep->rx_entry_pool, RXD_BUF_POOL_RX);
 	if (ret)
 		goto err;
 
@@ -1085,17 +1134,7 @@ int rxd_ep_init_res(struct rxd_ep *ep, struct fi_info *fi_info)
 
 	return 0;
 err:
-	if (ep->tx_pkt_pool)
-		ofi_bufpool_destroy(ep->tx_pkt_pool);
-
-	if (ep->rx_pkt_pool)
-		ofi_bufpool_destroy(ep->rx_pkt_pool);
-
-	if (ep->tx_entry_pool)
-		ofi_bufpool_destroy(ep->tx_entry_pool);
-
-	if (ep->rx_entry_pool)
-		ofi_bufpool_destroy(ep->rx_entry_pool);
+	rxd_ep_free_res(ep);
 
 	return ret;
 }
diff --git a/prov/rxd/src/rxd_msg.c b/prov/rxd/src/rxd_msg.c
index 53edbbaee..5ad6be747 100644
--- a/prov/rxd/src/rxd_msg.c
+++ b/prov/rxd/src/rxd_msg.c
@@ -100,9 +100,7 @@ static void rxd_progress_unexp_msg(struct rxd_ep *ep, struct rxd_x_entry *rx_ent
 			ep->peers[unexp_msg->base_hdr->peer].curr_unexp = NULL;
 	}
 
-	ofi_buf_free(unexp_msg->pkt_entry);
-	dlist_remove(&unexp_msg->entry);
-	free(unexp_msg);
+	rxd_free_unexp_msg(unexp_msg);
 }
 
 static int rxd_progress_unexp_list(struct rxd_ep *ep,
@@ -139,7 +137,6 @@ static int rxd_progress_unexp_list(struct rxd_ep *ep,
 static int rxd_ep_discard_recv(struct rxd_ep *rxd_ep, void *context,
 			       struct rxd_unexp_msg *unexp_msg)
 {
-	struct rxd_pkt_entry *pkt_entry;
 	uint64_t seq = unexp_msg->base_hdr->seq_no;
 	int ret;
 
@@ -155,15 +152,7 @@ static int rxd_ep_discard_recv(struct rxd_ep *rxd_ep, void *context,
 			   unexp_msg->data_hdr->cq_data : 0,
 			   unexp_msg->tag_hdr->tag);
 
-	while (!dlist_empty(&unexp_msg->pkt_list)) {
-		dlist_pop_front(&unexp_msg->pkt_list, struct rxd_pkt_entry,
-				pkt_entry, d_entry);
-		ofi_buf_free(pkt_entry);
-	}
-
-	ofi_buf_free(unexp_msg->pkt_entry);
-	dlist_remove(&unexp_msg->entry);
-	free(unexp_msg);
+	rxd_cleanup_unexp_msg(unexp_msg);
 
 	return ret;
 }
@@ -320,14 +309,10 @@ static struct rxd_x_entry *rxd_tx_entry_init_msg(struct rxd_ep *ep, fi_addr_t ad
 	void *ptr;
 
 	tx_entry = rxd_tx_entry_init_common(ep, addr, op, iov, iov_count,
-					    tag, data, flags, context);
+					    tag, data, flags, context, &base_hdr, &ptr);
 	if (!tx_entry)
 		return NULL;
 
-	base_hdr = rxd_get_base_hdr(tx_entry->pkt);
-	ptr = (void *) base_hdr;
-	rxd_init_base_hdr(ep, &ptr, tx_entry);
-
 	max_inline = rxd_domain->max_inline_msg;
 
 	if (tx_entry->flags & RXD_TAG_HDR) {
@@ -352,8 +337,7 @@ static struct rxd_x_entry *rxd_tx_entry_init_msg(struct rxd_ep *ep, fi_addr_t ad
 					    tx_entry->cq_entry.len,
 					    max_inline);
 
-	tx_entry->pkt->pkt_size = ((char *) ptr - (char *) base_hdr) +
-				ep->tx_prefix_size;
+	tx_entry->pkt->pkt_size = rxd_pkt_size(ep, base_hdr, ptr);
 
 	return tx_entry;
 }
diff --git a/prov/rxd/src/rxd_rma.c b/prov/rxd/src/rxd_rma.c
index ea4794d73..bb93d8085 100644
--- a/prov/rxd/src/rxd_rma.c
+++ b/prov/rxd/src/rxd_rma.c
@@ -48,14 +48,10 @@ static struct rxd_x_entry *rxd_tx_entry_init_rma(struct rxd_ep *ep, fi_addr_t ad
 	void *ptr;
 
 	tx_entry = rxd_tx_entry_init_common(ep, addr, op, iov, iov_count, 0,
-					    data, flags, context);
+					    data, flags, context, &base_hdr, &ptr);
 	if (!tx_entry)
 		return NULL;
 
-	base_hdr = rxd_get_base_hdr(tx_entry->pkt);
-	ptr = (void *) base_hdr;
-	rxd_init_base_hdr(ep, &ptr, tx_entry);
-
 	if (tx_entry->cq_entry.flags & FI_READ) {
 		tx_entry->num_segs = ofi_div_ceil(tx_entry->cq_entry.len,
 						  rxd_domain->max_seg_sz);
@@ -81,8 +77,8 @@ static struct rxd_x_entry *rxd_tx_entry_init_rma(struct rxd_ep *ep, fi_addr_t ad
 			tx_entry->iov_count, tx_entry->cq_entry.len,
 			max_inline);
 	}
-	tx_entry->pkt->pkt_size = ((char *) ptr - (char *) base_hdr) +
-				ep->tx_prefix_size;
+
+	tx_entry->pkt->pkt_size = rxd_pkt_size(ep, base_hdr, ptr);
 
 	return tx_entry;
 }
diff --git a/prov/rxm/src/rxm_conn.c b/prov/rxm/src/rxm_conn.c
index 1cbaec8c3..3d14773a3 100644
--- a/prov/rxm/src/rxm_conn.c
+++ b/prov/rxm/src/rxm_conn.c
@@ -448,7 +448,6 @@ void rxm_cmap_process_shutdown(struct rxm_cmap *cmap,
 			"Invalid handle on shutdown event\n");
 	} else if (handle->state != RXM_CMAP_SHUTDOWN) {
 		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Got remote shutdown\n");
-		rxm_cmap_del_handle(handle);
 	} else {
 		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Got local shutdown\n");
 	}
@@ -1536,8 +1535,11 @@ rxm_conn_connect(struct util_ep *util_ep, struct rxm_cmap_handle *handle,
 	rxm_ep->msg_info->dest_addrlen = rxm_ep->msg_info->src_addrlen;
 
 	rxm_ep->msg_info->dest_addr = mem_dup(addr, rxm_ep->msg_info->dest_addrlen);
-	if (!rxm_ep->msg_info->dest_addr)
+	if (!rxm_ep->msg_info->dest_addr) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "mem_dup failed, len %zu\n",
+			rxm_ep->msg_info->dest_addrlen);
 		return -FI_ENOMEM;
+	}
 
 	ret = rxm_msg_ep_open(rxm_ep, rxm_ep->msg_info, rxm_conn, &rxm_conn->handle);
 	if (ret)
diff --git a/prov/tcp/src/tcpx_ep.c b/prov/tcp/src/tcpx_ep.c
index 18e37c35a..9ca5d1b07 100644
--- a/prov/tcp/src/tcpx_ep.c
+++ b/prov/tcp/src/tcpx_ep.c
@@ -66,20 +66,6 @@ void tcpx_hdr_bswap(struct tcpx_base_hdr *hdr)
 	}
 }
 
-static int tcpx_setup_socket_nodelay(SOCKET sock)
-{
-	int ret, optval = 1;
-
-	ret = setsockopt(sock, IPPROTO_TCP, TCP_NODELAY, (char *) &optval,
-			 sizeof(optval));
-	if (ret) {
-		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,"setsockopt nodelay failed\n");
-		return ret;
-	}
-
-	return ret;
-}
-
 static int tcpx_setup_socket(SOCKET sock)
 {
 	int ret, optval = 1;
@@ -253,20 +239,14 @@ static int tcpx_pep_sock_create(struct tcpx_pep *pep)
 			strerror(ofi_sockerr()));
 		return -FI_EIO;
 	}
-
+	ret = tcpx_setup_socket(pep->sock);
+	if (ret) {
+		goto err;
+	}
 	if (ofi_addr_get_port(pep->info->src_addr) != 0 || port_range.high == 0) {
-		ret = tcpx_setup_socket(pep->sock);
-		if (ret) {
-			goto err;
-		}
 		ret = bind(pep->sock, pep->info->src_addr,
-		      (socklen_t) pep->info->src_addrlen);
+			  (socklen_t) pep->info->src_addrlen);
 	} else {
-		ret = tcpx_setup_socket_nodelay(pep->sock);
-		if (ret) {
-			goto err;
-		}
-
 		ret = tcpx_bind_to_port_range(pep->sock, pep->info->src_addr,
 					      pep->info->src_addrlen);
 	}
@@ -439,6 +419,7 @@ static int tcpx_ep_ctrl(struct fid *fid, int command, void *arg)
 	}
 	return 0;
 }
+
 static int tcpx_ep_bind(struct fid *fid, struct fid *bfid, uint64_t flags)
 {
 	struct tcpx_ep *tcpx_ep;
@@ -462,6 +443,7 @@ static struct fi_ops tcpx_ep_fi_ops = {
 	.control = tcpx_ep_ctrl,
 	.ops_open = fi_no_ops_open,
 };
+
 static int tcpx_ep_getopt(fid_t fid, int level, int optname,
 			  void *optval, size_t *optlen)
 {
@@ -681,7 +663,6 @@ static int tcpx_pep_setname(fid_t fid, void *addr, size_t addrlen)
 		tcpx_pep->info->src_addrlen = 0;
 	}
 
-
 	tcpx_pep->info->src_addr = mem_dup(addr, addrlen);
 	if (!tcpx_pep->info->src_addr)
 		return -FI_ENOMEM;
@@ -795,7 +776,6 @@ static struct fi_ops_ep tcpx_pep_ops = {
 	.tx_size_left = fi_no_tx_size_left,
 };
 
-
 int tcpx_passive_ep(struct fid_fabric *fabric, struct fi_info *info,
 		    struct fid_pep **pep, void *context)
 {
diff --git a/prov/util/src/util_buf.c b/prov/util/src/util_buf.c
index e6375ad73..2be9be877 100644
--- a/prov/util/src/util_buf.c
+++ b/prov/util/src/util_buf.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2016 Intel Corporation. All rights reserved.
+ * Copyright (c) 2016-2019 Intel Corporation. All rights reserved.
  * Copyright (c) 2018-2019 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -54,11 +54,11 @@ int ofi_bufpool_grow(struct ofi_bufpool *pool)
 	size_t i;
 
 	if (pool->attr.max_cnt && pool->entry_cnt >= pool->attr.max_cnt)
-		return -FI_EINVAL;
+		return -FI_ENOMEM;
 
 	buf_region = calloc(1, sizeof(*buf_region));
 	if (!buf_region)
-		return -FI_ENOSPC;
+		return -FI_ENOMEM;
 
 	buf_region->pool = pool;
 	dlist_init(&buf_region->free_list);
@@ -113,7 +113,8 @@ retry:
 	for (i = 0; i < pool->attr.chunk_cnt; i++) {
 		buf = (buf_region->mem_region + i * pool->entry_size);
 		buf_hdr = ofi_buf_hdr(buf);
-
+		buf_hdr->region = buf_region;
+		buf_hdr->index = pool->entry_cnt + i;
 		if (pool->attr.init_fn) {
 #if ENABLE_DEBUG
 			if (pool->attr.flags & OFI_BUFPOOL_INDEXED) {
@@ -135,9 +136,6 @@ retry:
 			pool->attr.init_fn(buf_region, buf);
 #endif
 		}
-
-		buf_hdr->region = buf_region;
-		buf_hdr->index = pool->entry_cnt + i;
 		if (pool->attr.flags & OFI_BUFPOOL_INDEXED) {
 			dlist_insert_tail(&buf_hdr->entry.dlist,
 					  &buf_region->free_list);
diff --git a/prov/util/src/util_mem_hooks.c b/prov/util/src/util_mem_hooks.c
new file mode 100644
index 000000000..4f96b1b58
--- /dev/null
+++ b/prov/util/src/util_mem_hooks.c
@@ -0,0 +1,569 @@
+/*
+ * Copyright (c) 2016 Los Alamos National Security, LLC. All rights reserved.
+ * Copyright (c) 2019 Intel Corporation, Inc.  All rights reserved.
+ *
+ * License text from Open-MPI (www.open-mpi.org/community/license.php)
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ * - Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ *
+ * - Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer listed
+ * in this license in the documentation and/or other materials
+ * provided with the distribution.
+ *
+ * - Neither the name of the copyright holders nor the names of its
+ * contributors may be used to endorse or promote products derived from
+ * this software without specific prior written permission.
+ *
+ * The copyright holders provide no reassurances that the source code
+ * provided does not infringe any patent, copyright, or any other
+ * intellectual property rights of third parties.  The copyright holders
+ * disclaim any liability to any recipient for claims brought against
+ * recipient by any third party for infringement of that parties
+ * intellectual property rights.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <ofi_mr.h>
+
+struct ofi_memhooks memhooks;
+struct ofi_mem_monitor *memhooks_monitor = &memhooks.monitor;
+
+
+#if defined(HAVE_ELF_H) && defined(HAVE_SYS_AUXV_H)
+
+#include <elf.h>
+#include <sys/auxv.h>
+#include <sys/mman.h>
+#include <sys/syscall.h>
+#include <sys/types.h>
+#include <sys/shm.h>
+#include <unistd.h>
+#include <dlfcn.h>
+#include <fcntl.h>
+#include <link.h>
+
+
+struct ofi_intercept {
+	struct dlist_entry 		entry;
+	const char			*symbol;
+	void				*our_func;
+	struct dlist_entry		dl_intercept_list;
+};
+
+struct ofi_dl_intercept {
+	struct dlist_entry 		entry;
+	void 				**dl_func_addr;
+	void				*dl_func;
+};
+
+enum {
+	OFI_INTERCEPT_DLOPEN,
+	OFI_INTERCEPT_MMAP,
+	OFI_INTERCEPT_MUNMAP,
+	OFI_INTERCEPT_MREMAP,
+	OFI_INTERCEPT_MADVISE,
+	OFI_INTERCEPT_SHMAT,
+	OFI_INTERCEPT_SHMDT,
+	OFI_INTERCEPT_BRK,
+	OFI_INTERCEPT_MAX
+};
+
+static void *ofi_intercept_dlopen(const char *filename, int flag);
+static void *ofi_intercept_mmap(void *start, size_t length,
+				int prot, int flags, int fd, off_t offset);
+static int ofi_intercept_munmap(void *start, size_t length);
+static void *ofi_intercept_mremap(void *old_address, size_t old_size,
+		size_t new_size, int flags, void *new_address);
+static int ofi_intercept_madvise(void *addr, size_t length, int advice);
+static void *ofi_intercept_shmat(int shmid, const void *shmaddr, int shmflg);
+static int ofi_intercept_shmdt(const void *shmaddr);
+static int ofi_intercept_brk(const void *brkaddr);
+
+static struct ofi_intercept intercepts[] = {
+	[OFI_INTERCEPT_DLOPEN] = { .symbol = "dlopen",
+				.our_func = ofi_intercept_dlopen},
+	[OFI_INTERCEPT_MMAP] = { .symbol = "mmap",
+				.our_func = ofi_intercept_mmap},
+	[OFI_INTERCEPT_MUNMAP] = { .symbol = "munmap",
+				.our_func = ofi_intercept_munmap},
+	[OFI_INTERCEPT_MREMAP] = { .symbol = "mremap",
+				.our_func = ofi_intercept_mremap},
+	[OFI_INTERCEPT_MADVISE] = { .symbol = "madvise",
+				.our_func = ofi_intercept_madvise},
+	[OFI_INTERCEPT_SHMAT] = { .symbol = "shmat",
+				.our_func = ofi_intercept_shmat},
+	[OFI_INTERCEPT_SHMDT] = { .symbol = "shmdt",
+				.our_func = ofi_intercept_shmdt},
+	[OFI_INTERCEPT_BRK] = { .symbol = "brk",
+				.our_func = ofi_intercept_brk},
+};
+
+struct ofi_mem_calls {
+	void *(*dlopen) (const char *, int);
+	void *(*mmap)(void *, size_t, int, int, int, off_t);
+	int (*munmap)(void *, size_t);
+	void *(*mremap)(void *old_address, size_t old_size,
+			size_t new_size, int flags, ... /* void *new_address */ );
+	int (*madvise)(void *addr, size_t length, int advice);
+	void *(*shmat)(int shmid, const void *shmaddr, int shmflg);
+	int (*shmdt)(const void *shmaddr);
+	int (*brk)(const void *brkaddr);
+};
+
+static struct ofi_mem_calls real_calls;
+
+
+static const ElfW(Phdr) *
+ofi_get_phdr_dynamic(const ElfW(Phdr) *phdr, uint16_t phnum, int phent)
+{
+	uint16_t i;
+
+	for (i = 0 ; i < phnum; i++) {
+		if (phdr->p_type == PT_DYNAMIC)
+			return phdr;
+		phdr = (ElfW(Phdr)*) ((intptr_t) phdr + phent);
+	}
+
+	return NULL;
+}
+
+static void *ofi_get_dynentry(ElfW(Addr) base, const ElfW(Phdr) *pdyn,
+			      ElfW(Sxword) type)
+{
+	ElfW(Dyn) *dyn;
+
+	for (dyn = (ElfW(Dyn)*) (base + pdyn->p_vaddr); dyn->d_tag; ++dyn) {
+		if (dyn->d_tag == type)
+			return (void *) (uintptr_t) dyn->d_un.d_val;
+	}
+
+	return NULL;
+}
+
+#if SIZE_MAX > UINT_MAX
+#define OFI_ELF_R_SYM ELF64_R_SYM
+#else
+#define OFI_ELF_R_SYM ELF32_R_SYM
+#endif
+
+static void *ofi_dl_func_addr(ElfW(Addr) base, const ElfW(Phdr) *phdr,
+			      int16_t phnum, int phent, const char *symbol)
+{
+	const ElfW(Phdr) *dphdr;
+	ElfW(Rela) *reloc;
+	void *jmprel, *strtab;
+	char *elf_sym;
+	uint32_t relsymidx;
+	ElfW(Sym) *symtab;
+	size_t pltrelsz;
+
+	dphdr = ofi_get_phdr_dynamic(phdr, phnum, phent);
+	jmprel = ofi_get_dynentry(base, dphdr, DT_JMPREL);
+	symtab = (ElfW(Sym) *) ofi_get_dynentry(base, dphdr, DT_SYMTAB);
+	strtab = ofi_get_dynentry (base, dphdr, DT_STRTAB);
+	pltrelsz = (uintptr_t) ofi_get_dynentry(base, dphdr, DT_PLTRELSZ);
+
+	for (reloc = jmprel; (intptr_t) reloc < (intptr_t) jmprel + pltrelsz;
+	     reloc++) {
+		relsymidx = OFI_ELF_R_SYM(reloc->r_info);
+		elf_sym = (char *) strtab + symtab[relsymidx].st_name;
+		if (!strcmp(symbol, elf_sym))
+			return (void *) (base + reloc->r_offset);
+        }
+
+        return NULL;
+}
+
+static int ofi_intercept_dl_calls(ElfW(Addr) base, const ElfW(Phdr) *phdr,
+				  const char *phname, int16_t phnum, int phent,
+				  struct ofi_intercept *intercept)
+{
+	struct ofi_dl_intercept *dl_entry;
+	long page_size = ofi_get_page_size();
+	void **func_addr, *page;
+	int ret;
+
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepting symbol %s from dl\n", intercept->symbol);
+	func_addr = ofi_dl_func_addr(base, phdr, phnum, phent, intercept->symbol);
+	if (!func_addr)
+		return FI_SUCCESS;
+
+	page = (void *) ((intptr_t) func_addr & ~(page_size - 1));
+	ret = mprotect(page, page_size, PROT_READ | PROT_WRITE);
+	if (ret < 0)
+		return -FI_ENOSYS;
+
+	if (*func_addr != intercept->our_func) {
+		dl_entry = malloc(sizeof(*dl_entry));
+		if (!dl_entry)
+			return -FI_ENOMEM;
+
+		dl_entry->dl_func_addr = func_addr;
+		dl_entry->dl_func = *func_addr;
+		*func_addr = intercept->our_func;
+		dlist_insert_tail(&dl_entry->entry, &intercept->dl_intercept_list);
+	}
+
+	return FI_SUCCESS;
+}
+
+static int ofi_intercept_phdr_handler(struct dl_phdr_info *info,
+                                    size_t size, void *data)
+{
+	struct ofi_intercept *intercept = data;
+	int phent, ret;
+
+	phent = getauxval(AT_PHENT);
+	if (phent <= 0) {
+		FI_DBG(&core_prov, FI_LOG_MR, "failed to read phent size");
+		return -FI_EINVAL;
+	}
+
+	ret = ofi_intercept_dl_calls(info->dlpi_addr, info->dlpi_phdr,
+				     info->dlpi_name, info->dlpi_phnum,
+				     phent, intercept);
+	return ret;
+}
+
+static void *ofi_intercept_dlopen(const char *filename, int flag)
+{
+	struct ofi_intercept  *intercept;
+	void *handle;
+
+	handle = real_calls.dlopen(filename, flag);
+	if (!handle)
+		return NULL;
+
+	fastlock_acquire(&memhooks_monitor->lock);
+	dlist_foreach_container(&memhooks.intercept_list, struct ofi_intercept,
+		intercept, entry) {
+		dl_iterate_phdr(ofi_intercept_phdr_handler, intercept);
+	}
+	fastlock_release(&memhooks_monitor->lock);
+	return handle;
+}
+
+static int ofi_restore_dl_calls(ElfW(Addr) base, const ElfW(Phdr) *phdr,
+				const char *phname, int16_t phnum, int phent,
+				struct ofi_intercept *intercept)
+{
+	struct ofi_dl_intercept *dl_entry;
+	long page_size = ofi_get_page_size();
+	void **func_addr, *page;
+	int ret;
+
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "releasing symbol %s from dl\n", intercept->symbol);
+	func_addr = ofi_dl_func_addr(base, phdr, phnum, phent, intercept->symbol);
+	if (!func_addr)
+		return FI_SUCCESS;
+
+	page = (void *) ((intptr_t) func_addr & ~(page_size - 1));
+	ret = mprotect(page, page_size, PROT_READ | PROT_WRITE);
+	if (ret < 0)
+		return -FI_ENOSYS;
+
+	dlist_foreach_container_reverse(&intercept->dl_intercept_list,
+		struct ofi_dl_intercept, dl_entry, entry) {
+
+		if (dl_entry->dl_func_addr != func_addr)
+			continue;
+
+		assert(*func_addr == intercept->our_func);
+		*func_addr = dl_entry->dl_func;
+		dlist_remove(&dl_entry->entry);
+		free(dl_entry);
+		FI_DBG(&core_prov, FI_LOG_MR,
+		       "dl symbol %s restored\n", intercept->symbol);
+		break;
+	}
+
+	return FI_SUCCESS;
+}
+
+static int ofi_restore_phdr_handler(struct dl_phdr_info *info,
+                                    size_t size, void *data)
+{
+	struct ofi_intercept *intercept = data;
+	int phent, ret;
+
+	phent = getauxval(AT_PHENT);
+	if (phent <= 0) {
+		FI_DBG(&core_prov, FI_LOG_MR, "failed to read phent size");
+		return -FI_EINVAL;
+	}
+
+	ret = ofi_restore_dl_calls(info->dlpi_addr, info->dlpi_phdr,
+				   info->dlpi_name, info->dlpi_phnum,
+				   phent, intercept);
+	return ret;
+}
+
+static void ofi_restore_intercepts(void)
+{
+	struct ofi_intercept *intercept;
+
+	dlist_foreach_container(&memhooks.intercept_list, struct ofi_intercept,
+		intercept, entry) {
+		dl_iterate_phdr(ofi_restore_phdr_handler, intercept);
+	}
+}
+
+static int ofi_intercept_symbol(struct ofi_intercept *intercept, void **real_func)
+{
+	int ret;
+
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepting symbol %s\n", intercept->symbol);
+	ret = dl_iterate_phdr(ofi_intercept_phdr_handler, intercept);
+	if (ret)
+		return ret;
+
+	*real_func = dlsym(RTLD_DEFAULT, intercept->symbol);
+	if (*real_func == intercept->our_func) {
+		(void) dlerror();
+		*real_func = dlsym(RTLD_NEXT, intercept->symbol);
+	}
+
+	if (!*real_func) {
+		FI_DBG(&core_prov, FI_LOG_MR,
+		       "could not find symbol %s\n", intercept->symbol);
+		ret = -FI_ENOMEM;
+		return ret;
+	}
+	dlist_insert_tail(&intercept->entry, &memhooks.intercept_list);
+
+	return ret;
+}
+
+void ofi_intercept_handler(const void *addr, size_t len)
+{
+	fastlock_acquire(&memhooks_monitor->lock);
+	ofi_monitor_notify(memhooks_monitor, addr, len);
+	fastlock_release(&memhooks_monitor->lock);
+}
+
+static void *ofi_intercept_mmap(void *start, size_t length,
+                            int prot, int flags, int fd, off_t offset)
+{
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepted mmap start %p len %zu\n", start, length);
+	ofi_intercept_handler(start, length);
+
+	return real_calls.mmap(start, length, prot, flags, fd, offset);
+}
+
+static int ofi_intercept_munmap(void *start, size_t length)
+{
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepted munmap start %p len %zu\n", start, length);
+	ofi_intercept_handler(start, length);
+
+	return real_calls.munmap(start, length);
+}
+
+static void *ofi_intercept_mremap(void *old_address, size_t old_size,
+		size_t new_size, int flags, void *new_address)
+{
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepted mremap old_addr %p old_size %zu\n",
+	       old_address, old_size);
+	ofi_intercept_handler(old_address, old_size);
+
+	return real_calls.mremap(old_address, old_size, new_size, flags,
+				 new_address);
+}
+
+static int ofi_intercept_madvise(void *addr, size_t length, int advice)
+{
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepted madvise addr %p len %zu\n", addr, length);
+	ofi_intercept_handler(addr, length);
+
+	return real_calls.madvise(addr, length, advice);
+}
+
+static void *ofi_intercept_shmat(int shmid, const void *shmaddr, int shmflg)
+{
+	struct shmid_ds ds;
+	const void *start;
+	size_t len;
+	int ret;
+
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepted shmat addr %p\n", shmaddr);
+
+	if (shmflg & SHM_REMAP) {
+		ret = shmctl(shmid, IPC_STAT, &ds);
+		len = (ret < 0) ? 0 : ds.shm_segsz;
+
+		if (shmflg & SHM_RND) {
+			start = (char *) shmaddr + ((uintptr_t) shmaddr) % SHMLBA;
+			len += ((uintptr_t) shmaddr) % SHMLBA;
+		} else {
+			start = shmaddr;
+		}
+
+		ofi_intercept_handler(start, len);
+	}
+
+	return real_calls.shmat(shmid, shmaddr, shmflg);
+}
+
+static int ofi_intercept_shmdt(const void *shmaddr)
+{
+	FI_DBG(&core_prov, FI_LOG_MR,
+	       "intercepted shmdt addr %p\n", shmaddr);
+	/* Overly aggressive, but simple.  Invalidate everything after shmaddr */
+	ofi_intercept_handler(shmaddr, SIZE_MAX - (uintptr_t) shmaddr);
+
+	return real_calls.shmdt(shmaddr);
+}
+
+static int ofi_intercept_brk(const void *brkaddr)
+{
+	void *old_addr;
+
+	FI_DBG(&core_prov, FI_LOG_MR,
+	      "intercepted brk addr %p\n", brkaddr);
+
+	old_addr = sbrk (0);
+
+	if(brkaddr > old_addr) {
+		ofi_intercept_handler(brkaddr, (intptr_t) brkaddr -
+							  (intptr_t) old_addr);
+	}
+
+	return real_calls.brk(brkaddr);
+}
+
+static int ofi_memhooks_subscribe(struct ofi_mem_monitor *monitor,
+				 const void *addr, size_t len)
+{
+	/* no-op */
+	return FI_SUCCESS;
+}
+
+static void ofi_memhooks_unsubscribe(struct ofi_mem_monitor *monitor,
+				    const void *addr, size_t len)
+{
+	/* no-op */
+}
+
+int ofi_memhooks_init(void)
+{
+	int i, ret;
+
+	/* TODO: remove once cleanup is written */
+	if (memhooks_monitor->subscribe == ofi_memhooks_subscribe)
+		return 0;
+
+	memhooks_monitor->subscribe = ofi_memhooks_subscribe;
+	memhooks_monitor->unsubscribe = ofi_memhooks_unsubscribe;
+	dlist_init(&memhooks.intercept_list);
+
+	for (i = 0; i < OFI_INTERCEPT_MAX; ++i)
+		dlist_init(&intercepts[i].dl_intercept_list);
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_DLOPEN],
+				   (void **) &real_calls.dlopen);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept dlopen failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MMAP],
+				   (void **) &real_calls.mmap);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept mmap failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MUNMAP],
+				   (void **) &real_calls.munmap);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept munmap failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MREMAP],
+				   (void **) &real_calls.mremap);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept mremap failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MADVISE],
+				   (void **) &real_calls.madvise);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept madvise failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_SHMAT],
+				   (void **) &real_calls.shmat);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept shmat failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_SHMDT],
+				   (void **) &real_calls.shmdt);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept shmdt failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	ret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_BRK],
+				   (void **) &real_calls.brk);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+		       "intercept brk failed %d %s\n", ret, fi_strerror(ret));
+		return ret;
+	}
+
+	return 0;
+}
+
+void ofi_memhooks_cleanup(void)
+{
+	ofi_restore_intercepts();
+}
+
+#else
+
+int ofi_memhooks_init(void)
+{
+	return -FI_ENOSYS;
+}
+
+void ofi_memhooks_cleanup(void)
+{
+}
+
+#endif
diff --git a/prov/util/src/util_mem_monitor.c b/prov/util/src/util_mem_monitor.c
index 5899d2007..47a5d7afe 100644
--- a/prov/util/src/util_mem_monitor.c
+++ b/prov/util/src/util_mem_monitor.c
@@ -34,10 +34,11 @@
 
 #include <ofi_mr.h>
 
-
 static struct ofi_uffd uffd;
 struct ofi_mem_monitor *uffd_monitor = &uffd.monitor;
 
+struct ofi_mem_monitor *default_monitor;
+
 
 /*
  * Initialize all available memory monitors
@@ -47,6 +48,17 @@ void ofi_monitor_init(void)
 	fastlock_init(&uffd_monitor->lock);
 	dlist_init(&uffd_monitor->list);
 
+	fastlock_init(&memhooks_monitor->lock);
+	dlist_init(&memhooks_monitor->list);
+
+#if HAVE_UFFD_UNMAP
+        default_monitor = uffd_monitor;
+#elif defined(HAVE_ELF_H) && defined(HAVE_SYS_AUXV_H)
+        default_monitor = memhooks_monitor;
+#else
+        default_monitor = NULL;
+#endif
+
 	fi_param_define(NULL, "mr_cache_max_size", FI_PARAM_SIZE_T,
 			"Defines the total number of bytes for all memory"
 			" regions that may be tracked by the MR cache."
@@ -65,20 +77,43 @@ void ofi_monitor_init(void)
 			" region.  Merging regions can reduce the cache"
 			" memory footprint, but can negatively impact"
 			" performance in some situations.  (default: false)");
+	fi_param_define(NULL, "mr_cache_monitor", FI_PARAM_STRING,
+			"Define a default memory registration monitor."
+			" The monitor checks for virtual to physical memory"
+			" address changes.  Options are: userfaultfd, memhooks"
+			" and disabled.  Userfaultfd is a Linux kernel feature."
+			" Memhooks operates by intercepting memory allocation"
+			" and free calls.  Userfaultfd is the default if"
+			" available on the system. 'disabled' option disables"
+			" memory caching.");
 
 	fi_param_get_size_t(NULL, "mr_cache_max_size", &cache_params.max_size);
 	fi_param_get_size_t(NULL, "mr_cache_max_count", &cache_params.max_cnt);
 	fi_param_get_bool(NULL, "mr_cache_merge_regions",
 			  &cache_params.merge_regions);
+	fi_param_get_str(NULL, "mr_cache_monitor", &cache_params.monitor);
 
 	if (!cache_params.max_size)
 		cache_params.max_size = SIZE_MAX;
+
+	if (cache_params.monitor != NULL) {
+		if (!strcmp(cache_params.monitor, "userfaultfd") &&
+		    default_monitor == uffd_monitor)
+			default_monitor = uffd_monitor;
+		else if (!strcmp(cache_params.monitor, "memhooks"))
+			default_monitor = memhooks_monitor;
+		else if (!strcmp(cache_params.monitor, "disabled"))
+			default_monitor = NULL;
+	}
 }
 
 void ofi_monitor_cleanup(void)
 {
 	assert(dlist_empty(&uffd_monitor->list));
 	fastlock_destroy(&uffd_monitor->lock);
+
+	assert(dlist_empty(&memhooks_monitor->list));
+	fastlock_destroy(&memhooks_monitor->lock);
 }
 
 int ofi_monitor_add_cache(struct ofi_mem_monitor *monitor,
@@ -86,10 +121,15 @@ int ofi_monitor_add_cache(struct ofi_mem_monitor *monitor,
 {
 	int ret = 0;
 
+	if (!monitor)
+		return -FI_ENOSYS;
+
 	fastlock_acquire(&monitor->lock);
 	if (dlist_empty(&monitor->list)) {
 		if (monitor == uffd_monitor)
 			ret = ofi_uffd_init();
+		else if (monitor == memhooks_monitor)
+			ret = ofi_memhooks_init();
 		else
 			ret = -FI_ENOSYS;
 
@@ -107,14 +147,17 @@ void ofi_monitor_del_cache(struct ofi_mr_cache *cache)
 {
 	struct ofi_mem_monitor *monitor = cache->monitor;
 
-	if (!monitor)
-		return;
-
+	assert(monitor);
 	fastlock_acquire(&monitor->lock);
 	dlist_remove(&cache->notify_entry);
 
-	if (dlist_empty(&monitor->list) && (monitor == uffd_monitor))
-		ofi_uffd_cleanup();
+	if (dlist_empty(&monitor->list)) {
+		if (monitor == uffd_monitor)
+			ofi_uffd_cleanup();
+		else if (monitor == memhooks_monitor)
+			ofi_memhooks_cleanup();
+	}
+
 	fastlock_release(&monitor->lock);
 }
 
@@ -125,7 +168,7 @@ void ofi_monitor_notify(struct ofi_mem_monitor *monitor,
 	struct ofi_mr_cache *cache;
 
 	dlist_foreach_container(&monitor->list, struct ofi_mr_cache,
-			cache, notify_entry) {
+				cache, notify_entry) {
 		ofi_mr_cache_notify(cache, addr, len);
 	}
 }
diff --git a/prov/util/src/util_mr_cache.c b/prov/util/src/util_mr_cache.c
index 8a3b5d31a..939766b8d 100644
--- a/prov/util/src/util_mr_cache.c
+++ b/prov/util/src/util_mr_cache.c
@@ -47,34 +47,34 @@ struct ofi_mr_cache_params cache_params = {
 static int util_mr_find_within(struct ofi_rbmap *map, void *key, void *data)
 {
 	struct ofi_mr_entry *entry = data;
-	struct iovec *iov = key;
+	struct ofi_mr_info *info = key;
 
-	if (ofi_iov_shifted_left(iov, &entry->iov))
+	if (ofi_iov_shifted_left(&info->iov, &entry->info.iov))
 		return -1;
-	else if (ofi_iov_shifted_right(iov, &entry->iov))
+	if (ofi_iov_shifted_right(&info->iov, &entry->info.iov))
 		return 1;
-	else
-		return 0;
+
+	return 0;
 }
 
 static int util_mr_find_overlap(struct ofi_rbmap *map, void *key, void *data)
 {
 	struct ofi_mr_entry *entry = data;
-	struct iovec *iov = key;
+	struct ofi_mr_info *info = key;
 
-	if (ofi_iov_left(iov, &entry->iov))
+	if (ofi_iov_left(&info->iov, &entry->info.iov))
 		return -1;
-	else if (ofi_iov_right(iov, &entry->iov))
+	if (ofi_iov_right(&info->iov, &entry->info.iov))
 		return 1;
-	else
-		return 0;
+
+	return 0;
 }
 
 static void util_mr_free_entry(struct ofi_mr_cache *cache,
 			       struct ofi_mr_entry *entry)
 {
 	FI_DBG(cache->domain->prov, FI_LOG_MR, "free %p (len: %" PRIu64 ")\n",
-	       entry->iov.iov_base, entry->iov.iov_len);
+	       entry->info.iov.iov_base, entry->info.iov.iov_len);
 
 	assert(!entry->cached);
 	/* If regions are not being merged, then we can't safely
@@ -84,22 +84,36 @@ static void util_mr_free_entry(struct ofi_mr_cache *cache,
 	 * notification events, but is harmless to correct operation.
 	 */
 	if (entry->subscribed && cache_params.merge_regions) {
-		ofi_monitor_unsubscribe(cache->monitor, entry->iov.iov_base,
-					entry->iov.iov_len);
+		ofi_monitor_unsubscribe(cache->monitor, entry->info.iov.iov_base,
+					entry->info.iov.iov_len);
 		entry->subscribed = 0;
 	}
 	cache->delete_region(cache, entry);
 	ofi_buf_free(entry);
 }
 
-static void util_mr_uncache_entry(struct ofi_mr_cache *cache,
-				  struct ofi_mr_entry *entry)
+static void util_mr_uncache_entry_storage(struct ofi_mr_cache *cache,
+					  struct ofi_mr_entry *entry)
 {
 	assert(entry->cached);
 	cache->storage.erase(&cache->storage, entry);
 	entry->cached = 0;
 	cache->cached_cnt--;
-	cache->cached_size -= entry->iov.iov_len;
+	cache->cached_size -= entry->info.iov.iov_len;
+}
+
+static void util_mr_uncache_entry(struct ofi_mr_cache *cache,
+				  struct ofi_mr_entry *entry)
+{
+	util_mr_uncache_entry_storage(cache, entry);
+
+	if (entry->use_cnt == 0) {
+		dlist_remove_init(&entry->lru_entry);
+		util_mr_free_entry(cache, entry);
+	} else {
+		cache->uncached_cnt++;
+		cache->uncached_size += entry->info.iov.iov_len;
+	}
 }
 
 /* Caller must hold ofi_mem_monitor lock */
@@ -113,15 +127,9 @@ void ofi_mr_cache_notify(struct ofi_mr_cache *cache, const void *addr, size_t le
 	iov.iov_len = len;
 
 	for (entry = cache->storage.overlap(&cache->storage, &iov); entry;
-	     entry = cache->storage.overlap(&cache->storage, &iov)) {
+	     entry = cache->storage.overlap(&cache->storage, &iov))
 		util_mr_uncache_entry(cache, entry);
 
-		if (entry->use_cnt == 0) {
-			dlist_remove_init(&entry->lru_entry);
-			util_mr_free_entry(cache, entry);
-		}
-	}
-
 	/* See comment in util_mr_free_entry.  If we're not merging address
 	 * ranges, we can only safely unsubscribe for the reported range.
 	 */
@@ -140,9 +148,9 @@ static bool mr_cache_flush(struct ofi_mr_cache *cache)
 			entry, lru_entry);
 	dlist_init(&entry->lru_entry);
 	FI_DBG(cache->domain->prov, FI_LOG_MR, "flush %p (len: %" PRIu64 ")\n",
-	       entry->iov.iov_base, entry->iov.iov_len);
+	       entry->info.iov.iov_base, entry->info.iov.iov_len);
 
-	util_mr_uncache_entry(cache, entry);
+	util_mr_uncache_entry_storage(cache, entry);
 	util_mr_free_entry(cache, entry);
 	return true;
 }
@@ -160,7 +168,7 @@ bool ofi_mr_cache_flush(struct ofi_mr_cache *cache)
 void ofi_mr_cache_delete(struct ofi_mr_cache *cache, struct ofi_mr_entry *entry)
 {
 	FI_DBG(cache->domain->prov, FI_LOG_MR, "delete %p (len: %" PRIu64 ")\n",
-	       entry->iov.iov_base, entry->iov.iov_len);
+	       entry->info.iov.iov_base, entry->info.iov.iov_len);
 
 	fastlock_acquire(&cache->monitor->lock);
 	cache->delete_cnt++;
@@ -170,7 +178,7 @@ void ofi_mr_cache_delete(struct ofi_mr_cache *cache, struct ofi_mr_entry *entry)
 			dlist_insert_tail(&entry->lru_entry, &cache->lru_list);
 		} else {
 			cache->uncached_cnt--;
-			cache->uncached_size -= entry->iov.iov_len;
+			cache->uncached_size -= entry->info.iov.iov_len;
 			util_mr_free_entry(cache, entry);
 		}
 	}
@@ -190,7 +198,7 @@ util_mr_cache_create(struct ofi_mr_cache *cache, const struct iovec *iov,
 	if (OFI_UNLIKELY(!*entry))
 		return -FI_ENOMEM;
 
-	(*entry)->iov = *iov;
+	(*entry)->info.iov = *iov;
 	(*entry)->use_cnt = 1;
 
 	ret = cache->add_region(cache, *entry);
@@ -212,7 +220,7 @@ util_mr_cache_create(struct ofi_mr_cache *cache, const struct iovec *iov,
 		cache->uncached_size += iov->iov_len;
 	} else {
 		if (cache->storage.insert(&cache->storage,
-					  &(*entry)->iov, *entry)) {
+					  &(*entry)->info, *entry)) {
 			ret = -FI_ENOMEM;
 			goto err;
 		}
@@ -239,41 +247,37 @@ static int
 util_mr_cache_merge(struct ofi_mr_cache *cache, const struct fi_mr_attr *attr,
 		    struct ofi_mr_entry *old_entry, struct ofi_mr_entry **entry)
 {
-	struct iovec iov, *old_iov;
+	struct ofi_mr_info info, *old_info;
 
-	iov = *attr->mr_iov;
+	info.iov = *attr->mr_iov;
 	do {
 		FI_DBG(cache->domain->prov, FI_LOG_MR,
 		       "merging %p (len: %" PRIu64 ") with %p (len: %" PRIu64 ")\n",
-		       iov.iov_base, iov.iov_len,
-		       old_entry->iov.iov_base, old_entry->iov.iov_len);
-		old_iov = &old_entry->iov;
-
-		iov.iov_len = ((uintptr_t)
-			MAX(ofi_iov_end(&iov), ofi_iov_end(old_iov))) + 1 -
-			((uintptr_t) MIN(iov.iov_base, old_iov->iov_base));
-		iov.iov_base = MIN(iov.iov_base, old_iov->iov_base);
+		       info.iov.iov_base, info.iov.iov_len,
+		       old_entry->info.iov.iov_base, old_entry->info.iov.iov_len);
+		old_info = &old_entry->info;
+
+		info.iov.iov_len = ((uintptr_t)
+			MAX(ofi_iov_end(&info.iov), ofi_iov_end(&old_info->iov))) + 1 -
+			((uintptr_t) MIN(info.iov.iov_base, old_info->iov.iov_base));
+		info.iov.iov_base = MIN(info.iov.iov_base, old_info->iov.iov_base);
 		FI_DBG(cache->domain->prov, FI_LOG_MR, "merged %p (len: %" PRIu64 ")\n",
-		       iov.iov_base, iov.iov_len);
+		       info.iov.iov_base, info.iov.iov_len);
 
 		/* New entry will expand range of subscription */
 		old_entry->subscribed = 0;
 
 		util_mr_uncache_entry(cache, old_entry);
 
-		if (old_entry->use_cnt == 0) {
-			dlist_remove_init(&old_entry->lru_entry);
-			util_mr_free_entry(cache, old_entry); 
-		}
-
-	} while ((old_entry = cache->storage.find(&cache->storage, &iov)));
+	} while ((old_entry = cache->storage.find(&cache->storage, &info)));
 
-	return util_mr_cache_create(cache, &iov, attr->access, entry);
+	return util_mr_cache_create(cache, &info.iov, attr->access, entry);
 }
 
 int ofi_mr_cache_search(struct ofi_mr_cache *cache, const struct fi_mr_attr *attr,
 			struct ofi_mr_entry **entry)
 {
+	struct ofi_mr_info info;
 	int ret = 0;
 
 	assert(attr->iov_count == 1);
@@ -288,15 +292,19 @@ int ofi_mr_cache_search(struct ofi_mr_cache *cache, const struct fi_mr_attr *att
 	       mr_cache_flush(cache))
 		;
 
-	*entry = cache->storage.find(&cache->storage, attr->mr_iov);
+	info.iov = *attr->mr_iov;
+	*entry = cache->storage.find(&cache->storage, &info);
 	if (!*entry) {
 		ret = util_mr_cache_create(cache, attr->mr_iov,
 					   attr->access, entry);
 		goto unlock;
 	}
 
-	/* This branch is always false if the merging entries wasn't requested */
-	if (!ofi_iov_within(attr->mr_iov, &(*entry)->iov)) {
+	/* This branch may be taken even if user hasn't enabled merging regions.
+	 * e.g. a new region encloses previously cached smaller region. Cache
+	 * find function (util_mr_find_within) would match the enclosed region.
+	 */
+	if (!ofi_iov_within(attr->mr_iov, &(*entry)->info.iov)) {
 		ret = util_mr_cache_merge(cache, attr, *entry, entry);
 		goto unlock;
 	}
@@ -329,8 +337,6 @@ void ofi_mr_cache_cleanup(struct ofi_mr_cache *cache)
 				     entry, lru_entry, tmp) {
 		assert(entry->use_cnt == 0);
 		util_mr_uncache_entry(cache, entry);
-		dlist_remove_init(&entry->lru_entry);
-		util_mr_free_entry(cache, entry);
 	}
 	fastlock_release(&cache->monitor->lock);
 
@@ -350,7 +356,7 @@ static void ofi_mr_rbt_destroy(struct ofi_mr_storage *storage)
 }
 
 static struct ofi_mr_entry *ofi_mr_rbt_find(struct ofi_mr_storage *storage,
-					    const struct iovec *key)
+					    const struct ofi_mr_info *key)
 {
 	struct ofi_rbnode *node;
 
@@ -362,7 +368,7 @@ static struct ofi_mr_entry *ofi_mr_rbt_find(struct ofi_mr_storage *storage,
 }
 
 static struct ofi_mr_entry *ofi_mr_rbt_overlap(struct ofi_mr_storage *storage,
-					    const struct iovec *key)
+					       const struct iovec *key)
 {
 	struct ofi_rbnode *node;
 
@@ -375,11 +381,11 @@ static struct ofi_mr_entry *ofi_mr_rbt_overlap(struct ofi_mr_storage *storage,
 }
 
 static int ofi_mr_rbt_insert(struct ofi_mr_storage *storage,
-			     struct iovec *key,
+			     struct ofi_mr_info *key,
 			     struct ofi_mr_entry *entry)
 {
-	return ofi_rbmap_insert(storage->storage, (void *) &entry->iov,
-			        (void *) entry);
+	return ofi_rbmap_insert(storage->storage, (void *) key, (void *) entry,
+				NULL);
 }
 
 static int ofi_mr_rbt_erase(struct ofi_mr_storage *storage,
@@ -387,7 +393,7 @@ static int ofi_mr_rbt_erase(struct ofi_mr_storage *storage,
 {
 	struct ofi_rbnode *node;
 
-	node = ofi_rbmap_find(storage->storage, &entry->iov);
+	node = ofi_rbmap_find(storage->storage, &entry->info);
 	assert(node);
 	ofi_rbmap_delete(storage->storage, node);
 	return 0;
diff --git a/prov/util/src/util_mr_map.c b/prov/util/src/util_mr_map.c
index ffad5d1ac..2157b702b 100644
--- a/prov/util/src/util_mr_map.c
+++ b/prov/util/src/util_mr_map.c
@@ -59,6 +59,7 @@ int ofi_mr_map_insert(struct ofi_mr_map *map, const struct fi_mr_attr *attr,
 		      uint64_t *key, void *context)
 {
 	struct fi_mr_attr *item;
+	int ret;
 
 	item = dup_mr_attr(attr);
 	if (!item)
@@ -67,20 +68,22 @@ int ofi_mr_map_insert(struct ofi_mr_map *map, const struct fi_mr_attr *attr,
 	if (!(map->mode & FI_MR_VIRT_ADDR))
 		item->offset = (uintptr_t) attr->mr_iov[0].iov_base;
 
-	if (!(map->mode & FI_MR_PROV_KEY)) {
-		if (ofi_rbmap_find(map->rbtree, &item->requested_key)) {
-			free(item);
-			return -FI_ENOKEY;
-		}
-	} else {
+	if (map->mode & FI_MR_PROV_KEY)
 		item->requested_key = map->key++;
-	}
 
-	ofi_rbmap_insert(map->rbtree, &item->requested_key, item);
+	ret = ofi_rbmap_insert(map->rbtree, &item->requested_key, item, NULL);
+	if (ret) {
+		if (ret == -FI_EALREADY)
+			ret = -FI_ENOKEY;
+		goto err;
+	}
 	*key = item->requested_key;
 	item->context = context;
 
 	return 0;
+err:
+	free(item);
+	return ret;
 }
 
 void *ofi_mr_map_get(struct ofi_mr_map *map, uint64_t key)
diff --git a/prov/verbs/configure.m4 b/prov/verbs/configure.m4
index 3257aeca6..3ae5c6c4f 100644
--- a/prov/verbs/configure.m4
+++ b/prov/verbs/configure.m4
@@ -11,7 +11,6 @@ AC_DEFUN([FI_VERBS_CONFIGURE],[
 	# Determine if we can support the verbs provider
 	verbs_ibverbs_happy=0
 	verbs_rdmacm_happy=0
-	verbs_ibverbs_exp_happy=0
 	AS_IF([test x"$enable_verbs" != x"no"],
 	      [FI_CHECK_PACKAGE([verbs_ibverbs],
 				[infiniband/verbs.h],
@@ -23,16 +22,6 @@ AC_DEFUN([FI_VERBS_CONFIGURE],[
 				[FI_VERBS_DOUBLE_CHECK_LIBIBVERBS],
 				[verbs_ibverbs_happy=0])
 
-	      FI_CHECK_PACKAGE([verbs_ibverbs],
-				[infiniband/verbs_exp.h],
-				[ibverbs],
-				[ibv_open_device],
-				[],
-				[$verbs_PREFIX],
-				[$verbs_LIBDIR],
-				[verbs_ibverbs_exp_happy=1],
-				[verbs_ibverbs_exp_happy=0])
-
 	       FI_CHECK_PACKAGE([verbs_rdmacm],
 				[rdma/rdma_cma.h],
 				[rdmacm],
@@ -55,27 +44,9 @@ AC_DEFUN([FI_VERBS_CONFIGURE],[
 
 	      ])
 
-	AC_COMPILE_IFELSE([AC_LANG_PROGRAM(
-			   [
-			    #include <infiniband/verbs_exp.h>
-			   ],
-			   [
-			    return (IBV_EXP_DEVICE_ATTR_ODP | IBV_EXP_DEVICE_ODP);
-			   ])
-			  ],
-			  [verbs_ibverbs_exp_happy=1],
-			  [verbs_ibverbs_exp_happy=0])
-
 	AS_IF([test $verbs_ibverbs_happy -eq 1 && \
 	       test $verbs_rdmacm_happy -eq 1], [$1], [$2])
 
-	AS_IF([test $verbs_ibverbs_happy -eq 1 && \
-	       test $verbs_rdmacm_happy -eq 1 && \
-	       test $verbs_ibverbs_exp_happy -eq 1],
-		[AC_DEFINE([HAVE_VERBS_EXP_H], [1],
-			   [Experimental verbs features support])],
-		[])
-
 	#See if we have XRC support
 	VERBS_HAVE_XRC=0
 	AS_IF([test $verbs_ibverbs_happy -eq 1 && \
diff --git a/prov/verbs/src/fi_verbs.c b/prov/verbs/src/fi_verbs.c
index 58768e924..5e3b9f038 100644
--- a/prov/verbs/src/fi_verbs.c
+++ b/prov/verbs/src/fi_verbs.c
@@ -49,10 +49,6 @@ struct fi_ibv_gl_data fi_ibv_gl_data = {
 	.def_rx_iov_limit	= 4,
 	.def_inline_size	= 256,
 	.min_rnr_timer		= VERBS_DEFAULT_MIN_RNR_TIMER,
-	/* Disable by default. Because this feature may corrupt
-	 * data due to IBV_EXP_ACCESS_RELAXED flag. But usage
-	 * this feature w/o this flag leads to poor bandwidth */
-	.use_odp		= 0,
 	.cqread_bunch_size	= 8,
 	.iface			= NULL,
 	.gid_idx		= 0,
@@ -203,42 +199,44 @@ err1:
 	return ret;
 }
 
-int fi_ibv_create_ep(const char *node, const char *service,
-		     uint64_t flags, const struct fi_info *hints,
-		     struct rdma_addrinfo **rai, struct rdma_cm_id **id)
+int fi_ibv_create_ep(const struct fi_info *hints, struct rdma_cm_id **id)
 {
-	struct rdma_addrinfo *_rai = NULL;
+	struct rdma_addrinfo *rai = NULL;
 	int ret;
 
-	ret = fi_ibv_get_rdma_rai(node, service, flags, hints, &_rai);
+	ret = fi_ibv_get_rdma_rai(NULL, NULL, 0, hints, &rai);
 	if (ret) {
 		return ret;
 	}
 
-	ret = rdma_create_ep(id, _rai, NULL, NULL);
-	if (ret) {
-		VERBS_INFO_ERRNO(FI_LOG_FABRIC, "rdma_create_ep", errno);
+	if (rdma_create_id(NULL, id, NULL, RDMA_PS_TCP)) {
 		ret = -errno;
+		FI_WARN(&fi_ibv_prov, FI_LOG_FABRIC, "rdma_create_id failed: "
+			"%s (%d)\n", strerror(-ret), -ret);
 		goto err1;
 	}
 
-	if (rai) {
-		*rai = _rai;
-	} else {
-		rdma_freeaddrinfo(_rai);
+	/* TODO convert this call to non-blocking (use event channel) as well:
+	 * This may likely be needed for better scaling when running large
+	 * MPI jobs.
+	 * Making this non-blocking would mean we can't create QP at EP enable
+	 * time. We need to wait for RDMA_CM_EVENT_ADDR_RESOLVED event before
+	 * creating the QP using rdma_create_qp. It would also require a SW
+	 * receive queue to store recvs posted by app after enabling the EP.
+	 */
+	if (rdma_resolve_addr(*id, rai->ai_src_addr, rai->ai_dst_addr,
+			      VERBS_RESOLVE_TIMEOUT)) {
+		ret = -errno;
+		FI_WARN(&fi_ibv_prov, FI_LOG_EP_CTRL, "rdma_resolve_addr failed: %s (%d)\n",
+			strerror(-ret), -ret);
+		goto err2;
 	}
-
-	return ret;
+	return 0;
+err2:
+	rdma_destroy_id(*id);
 err1:
-	rdma_freeaddrinfo(_rai);
-
-	return ret;
-}
-
-void fi_ibv_destroy_ep(struct rdma_addrinfo *rai, struct rdma_cm_id **id)
-{
 	rdma_freeaddrinfo(rai);
-	rdma_destroy_ep(*id);
+	return ret;
 }
 
 static int fi_ibv_param_define(const char *param_name, const char *param_str,
@@ -554,15 +552,6 @@ static int fi_ibv_read_params(void)
 		return -FI_EINVAL;
 	}
 
-	if (fi_ibv_get_param_bool("use_odp", "Enable on-demand paging experimental feature. "
-				  "Currently this feature may corrupt data. "
-				  "Use it on your own risk.",
-				  &fi_ibv_gl_data.use_odp)) {
-		VERBS_WARN(FI_LOG_CORE,
-			   "Invalid value of use_odp\n");
-		return -FI_EINVAL;
-	}
-
 	if (fi_ibv_get_param_bool("prefer_xrc", "Order XRC transport fi_infos"
 				  "ahead of RC. Default orders RC first.",
 				  &fi_ibv_gl_data.msg.prefer_xrc)) {
diff --git a/prov/verbs/src/fi_verbs.h b/prov/verbs/src/fi_verbs.h
index ce6548bf9..0dae282ca 100644
--- a/prov/verbs/src/fi_verbs.h
+++ b/prov/verbs/src/fi_verbs.h
@@ -72,12 +72,9 @@
 #include "ofi_tree.h"
 #include "ofi_indexer.h"
 
-#ifdef HAVE_VERBS_EXP_H
-#include <infiniband/verbs_exp.h>
-#endif /* HAVE_VERBS_EXP_H */
-
 #include "ofi_verbs_priv.h"
 
+
 #ifndef AF_IB
 #define AF_IB 27
 #endif
@@ -86,6 +83,8 @@
 #define RAI_FAMILY              0x00000008
 #endif
 
+#define VERBS_RESOLVE_TIMEOUT 2000	// ms
+
 #define VERBS_PROV_NAME "verbs"
 #define VERBS_PROV_VERS FI_VERSION(1,0)
 
@@ -145,7 +144,6 @@ extern struct fi_ibv_gl_data {
 	int	def_rx_iov_limit;
 	int	def_inline_size;
 	int	min_rnr_timer;
-	int	use_odp;
 	int	cqread_bunch_size;
 	char	*iface;
 	int	gid_idx;
@@ -306,13 +304,6 @@ struct fi_ibv_pep {
 
 struct fi_ops_cm *fi_ibv_pep_ops_cm(struct fi_ibv_pep *pep);
 
-struct fi_ibv_mem_desc;
-struct fi_ibv_domain;
-typedef int(*fi_ibv_mr_reg_cb)(struct fi_ibv_domain *domain, void *buf,
-			       size_t len, uint64_t access,
-			       struct fi_ibv_mem_desc *md);
-typedef int(*fi_ibv_mr_dereg_cb)(struct fi_ibv_mem_desc *md);
-
 struct fi_ibv_domain {
 	struct util_domain		util_domain;
 	struct ibv_context		*verbs;
@@ -339,10 +330,7 @@ struct fi_ibv_domain {
 	} xrc ;
 
 	/* MR stuff */
-	int				use_odp;
 	struct ofi_mr_cache		cache;
-	fi_ibv_mr_reg_cb		internal_mr_reg;
-	fi_ibv_mr_dereg_cb		internal_mr_dereg;
 	int 				(*post_send)(struct ibv_qp *qp,
 						     struct ibv_send_wr *wr,
 						     struct ibv_send_wr **bad_wr);
@@ -398,33 +386,13 @@ struct fi_ibv_mem_desc {
 	struct ofi_mr_entry	*entry;
 };
 
-static inline uint64_t
-fi_ibv_mr_internal_rkey(struct fi_ibv_mem_desc *md)
-{
-	return md->mr->rkey;
-}
-
-static inline uint64_t
-fi_ibv_mr_internal_lkey(struct fi_ibv_mem_desc *md)
-{
-	return md->mr->lkey;
-}
-
-struct fi_ibv_mr_internal_ops {
-	struct fi_ops_mr	*fi_ops;
-	fi_ibv_mr_reg_cb	internal_mr_reg;
-	fi_ibv_mr_dereg_cb	internal_mr_dereg;
-};
-
-
-extern struct fi_ibv_mr_internal_ops fi_ibv_mr_internal_ops;
-extern struct fi_ibv_mr_internal_ops fi_ibv_mr_internal_cache_ops;
-extern struct fi_ibv_mr_internal_ops fi_ibv_mr_internal_ex_ops;
+extern struct fi_ops_mr fi_ibv_mr_ops;
+extern struct fi_ops_mr fi_ibv_mr_cache_ops;
 
-int fi_ibv_mr_cache_entry_reg(struct ofi_mr_cache *cache,
-			      struct ofi_mr_entry *entry);
-void fi_ibv_mr_cache_entry_dereg(struct ofi_mr_cache *cache,
-				 struct ofi_mr_entry *entry);
+int fi_ibv_mr_cache_add_region(struct ofi_mr_cache *cache,
+			       struct ofi_mr_entry *entry);
+void fi_ibv_mr_cache_delete_region(struct ofi_mr_cache *cache,
+				   struct ofi_mr_entry *entry);
 
 /*
  * An XRC SRQ cannot be created until the associated RX CQ is known,
@@ -503,8 +471,11 @@ struct fi_ibv_ini_shared_conn {
 
 	/* The physical INI/TGT QPN connection. Virtual connections to the
 	 * same remote peer and TGT QPN will share this connection, with
-	 * the remote end opening the specified XRC TGT QPN for sharing. */
+	 * the remote end opening the specified XRC TGT QPN for sharing
+	 * During the physical connection setup, phys_conn_id identifies
+	 * the RDMA CM ID (and MSG_EP) associated with the operation. */
 	enum fi_ibv_ini_qp_state	state;
+	struct rdma_cm_id		*phys_conn_id;
 	struct ibv_qp			*ini_qp;
 	uint32_t			tgt_qpn;
 
@@ -520,7 +491,8 @@ enum fi_ibv_xrc_ep_conn_state {
 	FI_IBV_XRC_ORIG_CONNECTING,
 	FI_IBV_XRC_ORIG_CONNECTED,
 	FI_IBV_XRC_RECIP_CONNECTING,
-	FI_IBV_XRC_CONNECTED
+	FI_IBV_XRC_CONNECTED,
+	FI_IBV_XRC_ERROR
 };
 
 /*
@@ -584,6 +556,8 @@ struct fi_ibv_ep {
 		struct ibv_sge		sge;
 	} *wrs;
 	size_t				rx_size;
+	struct rdma_conn_param		conn_param;
+	struct fi_ibv_cm_data_hdr	*cm_hdr;
 };
 
 #define VERBS_XRC_EP_MAGIC		0x1F3D5B79
@@ -613,10 +587,7 @@ int fi_ibv_open_ep(struct fid_domain *domain, struct fi_info *info,
 		   struct fid_ep **ep, void *context);
 int fi_ibv_passive_ep(struct fid_fabric *fabric, struct fi_info *info,
 		      struct fid_pep **pep, void *context);
-int fi_ibv_create_ep(const char *node, const char *service,
-		     uint64_t flags, const struct fi_info *hints,
-		     struct rdma_addrinfo **rai, struct rdma_cm_id **id);
-void fi_ibv_destroy_ep(struct rdma_addrinfo *rai, struct rdma_cm_id **id);
+int fi_ibv_create_ep(const struct fi_info *hints, struct rdma_cm_id **id);
 int fi_ibv_dgram_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 			 struct fid_av **av_fid, void *context);
 static inline
diff --git a/prov/verbs/src/verbs_cm.c b/prov/verbs/src/verbs_cm.c
index 7e8714087..98b71d4d3 100644
--- a/prov/verbs/src/verbs_cm.c
+++ b/prov/verbs/src/verbs_cm.c
@@ -71,13 +71,14 @@ static int fi_ibv_msg_ep_setname(fid_t ep_fid, void *addr, size_t addrlen)
 
 	ep->info->src_addr = malloc(ep->info->src_addrlen);
 	if (!ep->info->src_addr) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "memory allocation failure\n");
 		ret = -FI_ENOMEM;
 		goto err1;
 	}
 
 	memcpy(ep->info->src_addr, addr, ep->info->src_addrlen);
 
-	ret = fi_ibv_create_ep(NULL, NULL, 0, ep->info, NULL, &id);
+	ret = fi_ibv_create_ep(ep->info, &id);
 	if (ret)
 		goto err2;
 
@@ -136,49 +137,44 @@ fi_ibv_ep_prepare_rdma_cm_param(struct rdma_conn_param *conn_param,
 }
 
 static int
-fi_ibv_msg_ep_connect(struct fid_ep *ep, const void *addr,
+fi_ibv_msg_ep_connect(struct fid_ep *ep_fid, const void *addr,
 		      const void *param, size_t paramlen)
 {
-	struct rdma_conn_param conn_param = { 0 };
-	struct sockaddr *src_addr, *dst_addr;
+	struct fi_ibv_ep *ep =
+		container_of(ep_fid, struct fi_ibv_ep, util_ep.ep_fid);
 	int ret;
-	struct fi_ibv_cm_data_hdr *cm_hdr;
-	struct fi_ibv_ep *_ep =
-		container_of(ep, struct fi_ibv_ep, util_ep.ep_fid);
 
 	if (OFI_UNLIKELY(paramlen > VERBS_CM_DATA_SIZE))
 		return -FI_EINVAL;
 
-	if (!_ep->id->qp) {
-		ret = fi_control(&ep->fid, FI_ENABLE, NULL);
+	if (!ep->id->qp) {
+		ret = fi_control(&ep_fid->fid, FI_ENABLE, NULL);
 		if (ret)
 			return ret;
 	}
 
-	cm_hdr = alloca(sizeof(*cm_hdr) + paramlen);
-	fi_ibv_msg_ep_prepare_cm_data(param, paramlen, cm_hdr);
-	fi_ibv_ep_prepare_rdma_cm_param(&conn_param, cm_hdr,
-					sizeof(*cm_hdr) + paramlen);
-	conn_param.retry_count = 15;
-
-	if (_ep->srq_ep)
-		conn_param.srq = 1;
-
-	src_addr = rdma_get_local_addr(_ep->id);
-	if (src_addr) {
-		VERBS_INFO(FI_LOG_CORE, "src_addr: %s:%d\n",
-			   inet_ntoa(((struct sockaddr_in *)src_addr)->sin_addr),
-			   ntohs(((struct sockaddr_in *)src_addr)->sin_port));
-	}
+	ep->cm_hdr = malloc(sizeof(*(ep->cm_hdr)) + paramlen);
+	if (!ep->cm_hdr)
+		return -FI_ENOMEM;
 
-	dst_addr = rdma_get_peer_addr(_ep->id);
-	if (dst_addr) {
-		VERBS_INFO(FI_LOG_CORE, "dst_addr: %s:%d\n",
-			   inet_ntoa(((struct sockaddr_in *)dst_addr)->sin_addr),
-			   ntohs(((struct sockaddr_in *)dst_addr)->sin_port));
+	fi_ibv_msg_ep_prepare_cm_data(param, paramlen, ep->cm_hdr);
+	fi_ibv_ep_prepare_rdma_cm_param(&ep->conn_param, ep->cm_hdr,
+					sizeof(*(ep->cm_hdr)) + paramlen);
+	ep->conn_param.retry_count = 15;
+
+	if (ep->srq_ep)
+		ep->conn_param.srq = 1;
+
+	if (rdma_resolve_route(ep->id, VERBS_RESOLVE_TIMEOUT)) {
+		ret = -errno;
+		FI_WARN(&fi_ibv_prov, FI_LOG_EP_CTRL,
+			"rdma_resolve_route failed: %s (%d)\n",
+			strerror(-ret), -ret);
+		free(ep->cm_hdr);
+		ep->cm_hdr = NULL;
+		return ret;
 	}
-
-	return rdma_connect(_ep->id, &conn_param) ? -errno : 0;
+	return 0;
 }
 
 static int
@@ -272,6 +268,8 @@ fi_ibv_msg_ep_reject(struct fid_pep *pep, fid_t handle,
 	struct fi_ibv_connreq *connreq =
 		container_of(handle, struct fi_ibv_connreq, handle);
 	struct fi_ibv_cm_data_hdr *cm_hdr;
+	struct fi_ibv_pep *_pep = container_of(pep, struct fi_ibv_pep,
+					       pep_fid);
 	int ret;
 
 	if (OFI_UNLIKELY(paramlen > VERBS_CM_DATA_SIZE))
@@ -280,13 +278,15 @@ fi_ibv_msg_ep_reject(struct fid_pep *pep, fid_t handle,
 	cm_hdr = alloca(sizeof(*cm_hdr) + paramlen);
 	fi_ibv_msg_ep_prepare_cm_data(param, paramlen, cm_hdr);
 
+	fastlock_acquire(&_pep->eq->lock);
 	if (connreq->is_xrc)
 		ret = fi_ibv_msg_xrc_ep_reject(connreq, cm_hdr,
 				(uint8_t)(sizeof(*cm_hdr) + paramlen));
-
 	else
 		ret = rdma_reject(connreq->id, cm_hdr,
 			(uint8_t)(sizeof(*cm_hdr) + paramlen)) ? -errno : 0;
+	fastlock_release(&_pep->eq->lock);
+
 	free(connreq);
 	return ret;
 }
@@ -354,28 +354,37 @@ fi_ibv_msg_xrc_ep_connect(struct fid_ep *ep, const void *addr,
 	if (ret)
 		return ret;
 
-	cm_hdr = alloca(sizeof(*cm_hdr) + paramlen);
+	cm_hdr = malloc(sizeof(*cm_hdr) + paramlen);
+	if (!cm_hdr)
+		return -FI_ENOMEM;
+
 	fi_ibv_msg_ep_prepare_cm_data(param, paramlen, cm_hdr);
 	paramlen += sizeof(*cm_hdr);
 
 	ret = fi_ibv_msg_alloc_xrc_params(&adjusted_param, cm_hdr, &paramlen);
-	if (ret)
+	if (ret) {
+		free(cm_hdr);
 		return ret;
+	}
 
 	xrc_ep->conn_setup = calloc(1, sizeof(*xrc_ep->conn_setup));
 	if (!xrc_ep->conn_setup) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "Unable to allocate connection setup memory\n");
 		free(adjusted_param);
+		free(cm_hdr);
 		return -FI_ENOMEM;
 	}
 
 	fastlock_acquire(&xrc_ep->base_ep.eq->lock);
 	xrc_ep->conn_setup->conn_tag = VERBS_CONN_TAG_INVALID;
 	fi_ibv_eq_set_xrc_conn_tag(xrc_ep);
-	fastlock_release(&xrc_ep->base_ep.eq->lock);
-
 	dst_addr = rdma_get_peer_addr(_ep->id);
 	ret = fi_ibv_connect_xrc(xrc_ep, dst_addr, 0, adjusted_param, paramlen);
+	fastlock_release(&xrc_ep->base_ep.eq->lock);
+
 	free(adjusted_param);
+	free(cm_hdr);
 	return ret;
 }
 
@@ -402,7 +411,10 @@ fi_ibv_msg_xrc_ep_accept(struct fid_ep *ep, const void *param, size_t paramlen)
 	if (ret)
 		return ret;
 
+	fastlock_acquire(&xrc_ep->base_ep.eq->lock);
 	ret = fi_ibv_accept_xrc(xrc_ep, 0, adjusted_param, paramlen);
+	fastlock_release(&xrc_ep->base_ep.eq->lock);
+
 	free(adjusted_param);
 	return ret;
 }
diff --git a/prov/verbs/src/verbs_cm_xrc.c b/prov/verbs/src/verbs_cm_xrc.c
index 94f6198fa..40af6bebf 100644
--- a/prov/verbs/src/verbs_cm_xrc.c
+++ b/prov/verbs/src/verbs_cm_xrc.c
@@ -49,6 +49,7 @@ void fi_ibv_next_xrc_conn_state(struct fi_ibv_xrc_ep *ep)
 		ep->conn_state = FI_IBV_XRC_CONNECTED;
 		break;
 	case FI_IBV_XRC_CONNECTED:
+	case FI_IBV_XRC_ERROR:
 		break;
 	default:
 		assert(0);
@@ -74,6 +75,8 @@ void fi_ibv_prev_xrc_conn_state(struct fi_ibv_xrc_ep *ep)
 	case FI_IBV_XRC_CONNECTED:
 		ep->conn_state = FI_IBV_XRC_RECIP_CONNECTING;
 		break;
+	case FI_IBV_XRC_ERROR:
+		break;
 	default:
 		assert(0);
 		VERBS_WARN(FI_LOG_EP_CTRL, "Unkown XRC connection state %d\n",
@@ -168,31 +171,27 @@ void fi_ibv_free_xrc_conn_setup(struct fi_ibv_xrc_ep *ep, int disconnect)
 {
 	assert(ep->conn_setup);
 
-	/* Free shared connection reserved QP number resources. If
-	 * a disconnect is requested and required then initiate a
-	 * disconnect sequence (the XRC INI QP side disconnect is
-	 * initiated when the remote target disconnect is received).
-	 * If disconnecting, the QP resources will be destroyed when
-	 * the timewait state has been exited or the EP is closed. */
+	/* If a disconnect is requested then the XRC bidirectional connection
+	 * has completed and a disconnect sequence is started (the XRC INI QP
+	 * side disconnect is initiated when the remote target disconnect is
+	 * received). XRC temporary QP resources will be released when the
+	 * timewait state is exited. */
 	if (ep->conn_setup->rsvd_ini_qpn && !disconnect) {
-		assert(ep->base_ep.id);
-		assert(!ep->base_ep.id->qp);
-
 		ibv_destroy_qp(ep->conn_setup->rsvd_ini_qpn);
 		ep->conn_setup->rsvd_ini_qpn = NULL;
 	}
 
-	if (ep->conn_setup->rsvd_tgt_qpn) {
+	if (disconnect) {
 		assert(ep->tgt_id);
 		assert(!ep->tgt_id->qp);
 
-		if (disconnect && ep->conn_setup->tgt_connected) {
+		if (ep->conn_setup->tgt_connected) {
 			rdma_disconnect(ep->tgt_id);
 			ep->conn_setup->tgt_connected = 0;
-		} else {
-			ibv_destroy_qp(ep->conn_setup->rsvd_tgt_qpn);
-			ep->conn_setup->rsvd_tgt_qpn = NULL;
 		}
+	} else if (ep->conn_setup->rsvd_tgt_qpn) {
+		ibv_destroy_qp(ep->conn_setup->rsvd_tgt_qpn);
+		ep->conn_setup->rsvd_tgt_qpn = NULL;
 	}
 
 	if (ep->conn_setup->conn_tag != VERBS_CONN_TAG_INVALID)
@@ -257,7 +256,8 @@ void fi_ibv_ep_ini_conn_done(struct fi_ibv_xrc_ep *ep, uint32_t peer_srqn,
 	/* If this was a physical INI/TGT QP connection, remove the QP
 	 * from control of the RDMA CM. We don't want the shared INI QP
 	 * to be destroyed if this endpoint closes. */
-	if (ep->base_ep.id->qp) {
+	if (ep->base_ep.id == ep->ini_conn->phys_conn_id) {
+		ep->ini_conn->phys_conn_id = NULL;
 		ep->ini_conn->state = FI_IBV_INI_QP_CONNECTED;
 		ep->ini_conn->tgt_qpn = tgt_qpn;
 		ep->base_ep.id->qp = NULL;
@@ -281,10 +281,8 @@ void fi_ibv_ep_ini_conn_rejected(struct fi_ibv_xrc_ep *ep)
 
 	fastlock_acquire(&domain->xrc.ini_mgmt_lock);
 	fi_ibv_log_ep_conn(ep, "INI Connection Rejected");
-
-	if (ep->ini_conn->state == FI_IBV_INI_QP_CONNECTING)
-		ep->ini_conn->state = FI_IBV_INI_QP_UNCONNECTED;
 	fi_ibv_put_shared_ini_conn(ep);
+	ep->conn_state = FI_IBV_XRC_ERROR;
 	fastlock_release(&domain->xrc.ini_mgmt_lock);
 }
 
@@ -369,8 +367,11 @@ int fi_ibv_process_xrc_connreq(struct fi_ibv_ep *ep,
 	assert(ep->info->dest_addr);
 
 	xrc_ep->conn_setup = calloc(1, sizeof(*xrc_ep->conn_setup));
-	if (!xrc_ep->conn_setup)
+	if (!xrc_ep->conn_setup) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			  "Unable to allocate connection setup memory\n");
 		return -FI_ENOMEM;
+	}
 
 	/* This endpoint was created on the passive side of a connection
 	 * request. The reciprocal connection request will go back to the
@@ -378,7 +379,7 @@ int fi_ibv_process_xrc_connreq(struct fi_ibv_ep *ep,
 	ofi_addr_set_port(ep->info->src_addr, 0);
 	ofi_addr_set_port(ep->info->dest_addr, connreq->xrc.port);
 
-	ret = fi_ibv_create_ep(NULL, NULL, 0, ep->info, NULL, &ep->id);
+	ret = fi_ibv_create_ep(ep->info, &ep->id);
 	if (ret) {
 		VERBS_WARN(FI_LOG_EP_CTRL,
 			   "Creation of INI cm_id failed %d\n", ret);
diff --git a/prov/verbs/src/verbs_cq.c b/prov/verbs/src/verbs_cq.c
index 27dde9bb2..ef2a7c252 100644
--- a/prov/verbs/src/verbs_cq.c
+++ b/prov/verbs/src/verbs_cq.c
@@ -271,19 +271,25 @@ void fi_ibv_cleanup_cq(struct fi_ibv_ep *ep)
 {
 	int ret;
 
-	ep->util_ep.rx_cq->cq_fastlock_acquire(&ep->util_ep.rx_cq->cq_lock);
-	do {
-		ret = fi_ibv_poll_outstanding_cq(ep, container_of(ep->util_ep.rx_cq,
-								  struct fi_ibv_cq, util_cq));
-	} while (ret > 0);
-	ep->util_ep.rx_cq->cq_fastlock_release(&ep->util_ep.rx_cq->cq_lock);
-
-	ep->util_ep.tx_cq->cq_fastlock_acquire(&ep->util_ep.tx_cq->cq_lock);
-	do {
-		ret = fi_ibv_poll_outstanding_cq(ep, container_of(ep->util_ep.tx_cq,
-								  struct fi_ibv_cq, util_cq));
-	} while (ret > 0);
-	ep->util_ep.tx_cq->cq_fastlock_release(&ep->util_ep.tx_cq->cq_lock);
+	if (ep->util_ep.rx_cq) {
+		ep->util_ep.rx_cq->cq_fastlock_acquire(&ep->util_ep.rx_cq->cq_lock);
+		do {
+			ret = fi_ibv_poll_outstanding_cq(
+				ep, container_of(ep->util_ep.rx_cq,
+						 struct fi_ibv_cq, util_cq));
+		} while (ret > 0);
+		ep->util_ep.rx_cq->cq_fastlock_release(&ep->util_ep.rx_cq->cq_lock);
+	}
+
+	if (ep->util_ep.tx_cq) {
+		ep->util_ep.tx_cq->cq_fastlock_acquire(&ep->util_ep.tx_cq->cq_lock);
+		do {
+			ret = fi_ibv_poll_outstanding_cq(ep,
+				container_of(ep->util_ep.tx_cq,
+					     struct fi_ibv_cq, util_cq));
+		} while (ret > 0);
+		ep->util_ep.tx_cq->cq_fastlock_release(&ep->util_ep.tx_cq->cq_lock);
+	}
 }
 
 /* Must call with cq->lock held */
diff --git a/prov/verbs/src/verbs_domain.c b/prov/verbs/src/verbs_domain.c
index 017aa7d45..8534db3a2 100644
--- a/prov/verbs/src/verbs_domain.c
+++ b/prov/verbs/src/verbs_domain.c
@@ -193,26 +193,6 @@ static struct fi_ops_domain fi_ibv_dgram_domain_ops = {
 	.query_atomic = fi_no_query_atomic,
 };
 
-static void fi_ibv_domain_process_exp(struct fi_ibv_domain *domain)
-{
-#ifdef HAVE_VERBS_EXP_H
-	struct ibv_exp_device_attr exp_attr= {
-		.comp_mask = IBV_EXP_DEVICE_ATTR_ODP |
-			     IBV_EXP_DEVICE_ATTR_EXP_CAP_FLAGS,
-	};
-	domain->use_odp = (!ibv_exp_query_device(domain->verbs, &exp_attr) &&
-			   exp_attr.exp_device_cap_flags & IBV_EXP_DEVICE_ODP);
-#else /* HAVE_VERBS_EXP_H */
-	domain->use_odp = 0;
-#endif /* HAVE_VERBS_EXP_H */
-	if (!domain->use_odp && fi_ibv_gl_data.use_odp) {
-		VERBS_WARN(FI_LOG_CORE,
-			   "ODP is not supported on this configuration, ignore \n");
-		return;
-	}
-	domain->use_odp = fi_ibv_gl_data.use_odp;
-}
-
 static int
 fi_ibv_post_send_track_credits(struct ibv_qp *qp, struct ibv_send_wr *wr,
 			       struct ibv_send_wr **bad_wr)
@@ -299,22 +279,15 @@ fi_ibv_domain(struct fid_fabric *fabric, struct fi_info *info,
 	_domain->util_domain.domain_fid.fid.context = context;
 	_domain->util_domain.domain_fid.fid.ops = &fi_ibv_fid_ops;
 
-	fi_ibv_domain_process_exp(_domain);
-
 	_domain->cache.entry_data_size = sizeof(struct fi_ibv_mem_desc);
-	_domain->cache.add_region = fi_ibv_mr_cache_entry_reg;
-	_domain->cache.delete_region = fi_ibv_mr_cache_entry_dereg;
-	ret = ofi_mr_cache_init(&_domain->util_domain, uffd_monitor,
+	_domain->cache.add_region = fi_ibv_mr_cache_add_region;
+	_domain->cache.delete_region = fi_ibv_mr_cache_delete_region;
+	ret = ofi_mr_cache_init(&_domain->util_domain, default_monitor,
 				&_domain->cache);
-	if (!ret) {
-		_domain->util_domain.domain_fid.mr = fi_ibv_mr_internal_cache_ops.fi_ops;
-		_domain->internal_mr_reg = fi_ibv_mr_internal_cache_ops.internal_mr_reg;
-		_domain->internal_mr_dereg = fi_ibv_mr_internal_cache_ops.internal_mr_dereg;
-	} else {
-		_domain->util_domain.domain_fid.mr = fi_ibv_mr_internal_ops.fi_ops;
-		_domain->internal_mr_reg = fi_ibv_mr_internal_ops.internal_mr_reg;
-		_domain->internal_mr_dereg = fi_ibv_mr_internal_ops.internal_mr_dereg;
-	}
+	if (!ret)
+		_domain->util_domain.domain_fid.mr = &fi_ibv_mr_cache_ops;
+	else
+		_domain->util_domain.domain_fid.mr = &fi_ibv_mr_ops;
 
 	switch (_domain->ep_type) {
 	case FI_EP_DGRAM:
diff --git a/prov/verbs/src/verbs_domain_xrc.c b/prov/verbs/src/verbs_domain_xrc.c
index 7dc3547b1..5c9b86822 100644
--- a/prov/verbs/src/verbs_domain_xrc.c
+++ b/prov/verbs/src/verbs_domain_xrc.c
@@ -131,12 +131,17 @@ int fi_ibv_get_shared_ini_conn(struct fi_ibv_xrc_ep *ep,
 
 	*ini_conn = NULL;
 	conn = calloc(1, sizeof(*conn));
-	if (!conn)
+	if (!conn) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "Unable to allocate INI connection memory\n");
 		return -FI_ENOMEM;
+	}
 
 	conn->tgt_qpn = FI_IBV_NO_INI_TGT_QPNUM;
 	conn->peer_addr = mem_dup(key.addr, ofi_sizeofaddr(key.addr));
 	if (!conn->peer_addr) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "mem_dup of peer address failed\n");
 		free(conn);
 		return -FI_ENOMEM;
 	}
@@ -147,7 +152,7 @@ int fi_ibv_get_shared_ini_conn(struct fi_ibv_xrc_ep *ep,
 	ofi_atomic_initialize32(&conn->ref_cnt, 1);
 
 	ret = ofi_rbmap_insert(domain->xrc.ini_conn_rbmap,
-			       (void *) &key, (void *) conn);
+			       (void *) &key, (void *) conn, NULL);
 	assert(ret != -FI_EALREADY);
 	if (ret) {
 		VERBS_WARN(FI_LOG_EP_CTRL, "INI QP RBTree insert failed %d\n",
@@ -183,6 +188,15 @@ void fi_ibv_put_shared_ini_conn(struct fi_ibv_xrc_ep *ep)
 	if (ep->base_ep.id)
 		ep->base_ep.id->qp = NULL;
 
+	/* If XRC physical QP connection was not completed, make sure
+	 * any pending connection to that destination will get scheduled. */
+	if (ep->base_ep.id && ep->base_ep.id == ini_conn->phys_conn_id) {
+		if (ini_conn->state == FI_IBV_INI_QP_CONNECTING)
+			ini_conn->state = FI_IBV_INI_QP_UNCONNECTED;
+
+		ini_conn->phys_conn_id = NULL;
+	}
+
 	/* Tear down physical INI/TGT when no longer being used */
 	if (!ofi_atomic_dec32(&ini_conn->ref_cnt)) {
 		if (ini_conn->ini_qp && ibv_destroy_qp(ini_conn->ini_qp))
@@ -190,6 +204,7 @@ void fi_ibv_put_shared_ini_conn(struct fi_ibv_xrc_ep *ep)
 				   "Destroy of XRC physical INI QP failed %d\n",
 				   errno);
 
+		assert(dlist_empty(&ini_conn->pending_list));
 		fi_ibv_set_ini_conn_key(ep, &key);
 		node = ofi_rbmap_find(domain->xrc.ini_conn_rbmap, &key);
 		assert(node);
@@ -246,6 +261,8 @@ void fi_ibv_sched_ini_conn(struct fi_ibv_ini_shared_conn *ini_conn)
 				  &ep->ini_conn->active_list);
 		last_state = ep->ini_conn->state;
 		if (last_state == FI_IBV_INI_QP_UNCONNECTED) {
+			assert(!ep->ini_conn->phys_conn_id && ep->base_ep.id);
+
 			if (ep->ini_conn->ini_qp &&
 			    ibv_destroy_qp(ep->ini_conn->ini_qp)) {
 				VERBS_WARN(FI_LOG_EP_CTRL, "Failed to destroy "
@@ -259,16 +276,17 @@ void fi_ibv_sched_ini_conn(struct fi_ibv_ini_shared_conn *ini_conn)
 			}
 			ep->ini_conn->ini_qp = ep->base_ep.id->qp;
 			ep->ini_conn->state = FI_IBV_INI_QP_CONNECTING;
+			ep->ini_conn->phys_conn_id = ep->base_ep.id;
 		} else {
-			if (!ep->base_ep.id->qp) {
-				ret = fi_ibv_reserve_qpn(ep,
-						 &ep->conn_setup->rsvd_ini_qpn);
-				if (ret) {
-					VERBS_WARN(FI_LOG_EP_CTRL,
-						   "Failed to create rsvd INI "
-						   "QP %d\n", ret);
-					goto err;
-				}
+			assert(!ep->base_ep.id->qp);
+
+			ret = fi_ibv_reserve_qpn(ep,
+					&ep->conn_setup->rsvd_ini_qpn);
+			if (ret) {
+				VERBS_WARN(FI_LOG_EP_CTRL,
+					   "Failed to create rsvd INI "
+					   "QP %d\n", ret);
+				goto err;
 			}
 		}
 
@@ -296,7 +314,6 @@ int fi_ibv_process_ini_conn(struct fi_ibv_xrc_ep *ep,int reciprocal,
 			    void *param, size_t paramlen)
 {
 	struct fi_ibv_xrc_cm_data *cm_data = param;
-	struct rdma_conn_param conn_param = { 0 };
 	int ret;
 
 	assert(ep->base_ep.ibv_qp);
@@ -304,30 +321,34 @@ int fi_ibv_process_ini_conn(struct fi_ibv_xrc_ep *ep,int reciprocal,
 	fi_ibv_set_xrc_cm_data(cm_data, reciprocal, ep->conn_setup->conn_tag,
 			       ep->base_ep.eq->xrc.pep_port,
 			       ep->ini_conn->tgt_qpn);
-	conn_param.private_data = cm_data;
-	conn_param.private_data_len = paramlen;
-	conn_param.responder_resources = RDMA_MAX_RESP_RES;
-	conn_param.initiator_depth = RDMA_MAX_INIT_DEPTH;
-	conn_param.flow_control = 1;
-	conn_param.retry_count = 15;
-	conn_param.rnr_retry_count = 7;
-	conn_param.srq = 1;
+	ep->base_ep.conn_param.private_data = cm_data;
+	ep->base_ep.conn_param.private_data_len = paramlen;
+	ep->base_ep.conn_param.responder_resources = RDMA_MAX_RESP_RES;
+	ep->base_ep.conn_param.initiator_depth = RDMA_MAX_INIT_DEPTH;
+	ep->base_ep.conn_param.flow_control = 1;
+	ep->base_ep.conn_param.retry_count = 15;
+	ep->base_ep.conn_param.rnr_retry_count = 7;
+	ep->base_ep.conn_param.srq = 1;
 
 	/* Shared connections use reserved temporary QP numbers to
 	 * avoid the appearance of stale/duplicate CM messages */
 	if (!ep->base_ep.id->qp)
-		conn_param.qp_num = ep->conn_setup->rsvd_ini_qpn->qp_num;
+		ep->base_ep.conn_param.qp_num =
+				ep->conn_setup->rsvd_ini_qpn->qp_num;
 
 	assert(ep->conn_state == FI_IBV_XRC_UNCONNECTED ||
 	       ep->conn_state == FI_IBV_XRC_ORIG_CONNECTED);
 	fi_ibv_next_xrc_conn_state(ep);
 
-	ret = rdma_connect(ep->base_ep.id, &conn_param) ? -errno : 0;
+	ret = rdma_resolve_route(ep->base_ep.id, VERBS_RESOLVE_TIMEOUT);
 	if (ret) {
 		ret = -errno;
-		VERBS_WARN(FI_LOG_EP_CTRL, "rdma_connect failed %d\n", -ret);
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "rdma_resolve_route failed %s (%d)\n",
+			   strerror(-ret), -ret);
 		fi_ibv_prev_xrc_conn_state(ep);
 	}
+
 	return ret;
 }
 
diff --git a/prov/verbs/src/verbs_ep.c b/prov/verbs/src/verbs_ep.c
index 4ce3abf37..3dca09427 100644
--- a/prov/verbs/src/verbs_ep.c
+++ b/prov/verbs/src/verbs_ep.c
@@ -34,8 +34,6 @@
 
 #include "fi_verbs.h"
 
-#define VERBS_RESOLVE_TIMEOUT 2000	// ms
-
 static struct fi_ops_msg fi_ibv_srq_msg_ops;
 
 static inline int fi_ibv_msg_ep_cmdata_size(fid_t fid)
@@ -210,6 +208,7 @@ static int fi_ibv_close_free_ep(struct fi_ibv_ep *ep)
 
 	free(ep->util_ep.ep_fid.msg);
 	ep->util_ep.ep_fid.msg = NULL;
+	free(ep->cm_hdr);
 
 	ret = ofi_endpoint_close(&ep->util_ep);
 	if (ret)
@@ -319,14 +318,23 @@ static int fi_ibv_ep_bind(struct fid *fid, struct fid *bfid, uint64_t flags)
 			break;
 		case FI_CLASS_EQ:
 			ep->eq = container_of(bfid, struct fi_ibv_eq, eq_fid.fid);
+
+			/* Make sure EQ channel is not polled during migrate */
+			fastlock_acquire(&ep->eq->lock);
 			ret = rdma_migrate_id(ep->id, ep->eq->channel);
-			if (ret)
+			if (ret)  {
+				fastlock_release(&ep->eq->lock);
 				return -errno;
+			}
 			if (fi_ibv_is_xrc(ep->info)) {
 				ret = fi_ibv_ep_xrc_set_tgt_chan(ep);
-				if (ret)
+				if (ret) {
+					fastlock_release(&ep->eq->lock);
 					return -errno;
+				}
 			}
+			fastlock_release(&ep->eq->lock);
+
 			break;
 		case FI_CLASS_SRX_CTX:
 			ep->srq_ep = container_of(bfid, struct fi_ibv_srq_ep, ep_fid.fid);
@@ -842,8 +850,11 @@ int fi_ibv_open_ep(struct fid_domain *domain, struct fi_info *info,
 	}
 
 	ep = fi_ibv_alloc_init_ep(info, dom, context);
-	if (!ep)
+	if (!ep) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "Unable to allocate/init EP memory\n");
 		return -FI_ENOMEM;
+	}
 
 	ep->inject_limit = ep->info->tx_attr->inject_size;
 
@@ -872,7 +883,7 @@ int fi_ibv_open_ep(struct fid_domain *domain, struct fi_info *info,
 		}
 
 		if (!info->handle) {
-			ret = fi_ibv_create_ep(NULL, NULL, 0, info, NULL, &ep->id);
+			ret = fi_ibv_create_ep(info, &ep->id);
 			if (ret)
 				goto err1;
 		} else if (info->handle->fclass == FI_CLASS_CONNREQ) {
@@ -903,12 +914,6 @@ int fi_ibv_open_ep(struct fid_domain *domain, struct fi_info *info,
 				VERBS_INFO(FI_LOG_DOMAIN, "Unable to rdma_resolve_addr\n");
 				goto err2;
 			}
-
-			if (rdma_resolve_route(ep->id, VERBS_RESOLVE_TIMEOUT)) {
-				ret = -errno;
-				VERBS_INFO(FI_LOG_DOMAIN, "Unable to rdma_resolve_route\n");
-				goto err2;
-			}
 		} else {
 			ret = -FI_ENOSYS;
 			goto err1;
diff --git a/prov/verbs/src/verbs_eq.c b/prov/verbs/src/verbs_eq.c
index c691cfee6..2e73b3d6b 100644
--- a/prov/verbs/src/verbs_eq.c
+++ b/prov/verbs/src/verbs_eq.c
@@ -110,11 +110,20 @@ struct fi_ibv_xrc_ep *fi_ibv_eq_xrc_conn_tag2ep(struct fi_ibv_eq *eq,
 
 	index = ofi_key2idx(&eq->xrc.conn_key_idx, (uint64_t)conn_tag);
 	ep = ofi_idx_lookup(eq->xrc.conn_key_map, index);
-	if (!ep || !ep->conn_setup || (ep->conn_setup->conn_tag != conn_tag)) {
+	if (!ep || ep->magic != VERBS_XRC_EP_MAGIC) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "XRC EP is not valid\n");
+		return NULL;
+	}
+	if (!ep->conn_setup) {
 		VERBS_WARN(FI_LOG_EP_CTRL,
-			   "Invalid/stale XRC connection tag\n");
-		return ep;
+			   "Bad state, no connection data\n");
+		return NULL;
+	}
+	if (ep->conn_setup->conn_tag != conn_tag) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "Connection tag mismatch\n");
+		return NULL;
 	}
+
 	ofi_idx_remove(eq->xrc.conn_key_map, index);
 	ep->conn_setup->conn_tag = VERBS_CONN_TAG_INVALID;
 
@@ -167,8 +176,10 @@ fi_ibv_eq_cm_getinfo(struct rdma_cm_event *event, struct fi_info *pep_info,
 	const char *devname = ibv_get_device_name(event->id->verbs->device);
 	int ret = -FI_ENOMEM;
 
-	if (!(hints = fi_dupinfo(pep_info)))
+	if (!(hints = fi_dupinfo(pep_info))) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "dupinfo failure\n");
 		return -FI_ENOMEM;
+	}
 
 	/* Free src_addr info from pep to avoid addr reuse errors */
 	free(hints->src_addr);
@@ -217,8 +228,11 @@ fi_ibv_eq_cm_getinfo(struct rdma_cm_event *event, struct fi_info *pep_info,
 	ofi_straddr_dbg(&fi_ibv_prov, FI_LOG_EQ, "dst", (*info)->dest_addr);
 
 	connreq = calloc(1, sizeof *connreq);
-	if (!connreq)
+	if (!connreq) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "Unable to allocate connreq memory\n");
 		goto err2;
+	}
 
 	connreq->handle.fclass = FI_CLASS_CONNREQ;
 	connreq->id = event->id;
@@ -291,9 +305,11 @@ fi_ibv_eq_xrc_connreq_event(struct fi_ibv_eq *eq, struct fi_eq_cm_entry *entry,
 	ep = fi_ibv_eq_xrc_conn_tag2ep(eq, connreq->xrc.conn_tag);
 	if (!ep) {
 		VERBS_WARN(FI_LOG_EP_CTRL,
-			   "Reciprocal XRC connection tag not found\n");
+			   "Reciprocal XRC connection tag 0x%x not found\n",
+			   connreq->xrc.conn_tag);
 		goto done;
 	}
+	assert(ep->conn_state == FI_IBV_XRC_ORIG_CONNECTED);
 
 	ep->tgt_id = connreq->id;
 	ep->tgt_id->context = &ep->base_ep.util_ep.ep_fid.fid;
@@ -337,7 +353,7 @@ fi_ibv_eq_xrc_conn_event(struct fi_ibv_xrc_ep *ep,
 	size_t priv_datalen = cma_event->param.conn.private_data_len;
 	int ret;
 
-	VERBS_DBG(FI_LOG_FABRIC, "EP %p INITIAL CONNECTION DONE state %d\n",
+	VERBS_DBG(FI_LOG_EP_CTRL, "EP %p INITIAL CONNECTION DONE state %d\n",
 		  ep, ep->conn_state);
 	fi_ibv_next_xrc_conn_state(ep);
 
@@ -354,16 +370,17 @@ fi_ibv_eq_xrc_conn_event(struct fi_ibv_xrc_ep *ep,
 			goto err;
 		}
 		ep->peer_srqn = xrc_info.conn_data;
-		fi_ibv_ep_ini_conn_done(ep, xrc_info.conn_data,
-					xrc_info.conn_param.qp_num);
 		fi_ibv_eq_skip_xrc_cm_data(&priv_data, &priv_datalen);
 		fi_ibv_save_priv_data(ep, priv_data, priv_datalen);
+		fi_ibv_ep_ini_conn_done(ep, xrc_info.conn_data,
+					xrc_info.conn_param.qp_num);
 	} else {
 		fi_ibv_ep_tgt_conn_done(ep);
 		ret = fi_ibv_connect_xrc(ep, NULL, FI_IBV_RECIP_CONN, &cm_data,
 					 sizeof(cm_data));
 		if (ret) {
 			fi_ibv_prev_xrc_conn_state(ep);
+			ep->tgt_id->qp = NULL;
 			rdma_disconnect(ep->tgt_id);
 			goto err;
 		}
@@ -385,7 +402,7 @@ fi_ibv_eq_xrc_recip_conn_event(struct fi_ibv_eq *eq,
 	int ret;
 
 	fi_ibv_next_xrc_conn_state(ep);
-	VERBS_DBG(FI_LOG_FABRIC, "EP %p RECIPROCAL CONNECTION DONE state %d\n",
+	VERBS_DBG(FI_LOG_EP_CTRL, "EP %p RECIPROCAL CONNECTION DONE state %d\n",
 		  ep, ep->conn_state);
 
 	/* If this is the reciprocal active side notification */
@@ -404,7 +421,7 @@ fi_ibv_eq_xrc_recip_conn_event(struct fi_ibv_eq *eq,
 		fi_ibv_ep_ini_conn_done(ep, xrc_info.conn_data,
 					xrc_info.conn_param.qp_num);
 	} else {
-			fi_ibv_ep_tgt_conn_done(ep);
+		fi_ibv_ep_tgt_conn_done(ep);
 	}
 
 	/* The internal reciprocal XRC connection has completed. Return the
@@ -428,11 +445,18 @@ fi_ibv_eq_xrc_rej_event(struct fi_ibv_eq *eq, struct rdma_cm_event *cma_event)
 	enum fi_ibv_xrc_ep_conn_state state;
 
 	ep = container_of(fid, struct fi_ibv_xrc_ep, base_ep.util_ep.ep_fid);
-	state = ep->conn_state;
+	if (ep->magic != VERBS_XRC_EP_MAGIC) {
+		VERBS_WARN(FI_LOG_EP_CTRL,
+			   "CM ID context not valid\n");
+		return -FI_EAGAIN;
+	}
 
-	if (ep->base_ep.id != cma_event->id || state == FI_IBV_XRC_CONNECTED) {
+	state = ep->conn_state;
+	if (ep->base_ep.id != cma_event->id ||
+	    (state != FI_IBV_XRC_ORIG_CONNECTING &&
+	     state != FI_IBV_XRC_RECIP_CONNECTING)) {
 		VERBS_WARN(FI_LOG_EP_CTRL,
-			   "Stale CM Reject %d received\n", cma_event->status);
+			   "Stale/invalid CM reject %d received\n", cma_event->status);
 		return -FI_EAGAIN;
 	}
 
@@ -459,12 +483,46 @@ fi_ibv_eq_xrc_rej_event(struct fi_ibv_eq *eq, struct rdma_cm_event *cma_event)
 	return state == FI_IBV_XRC_ORIG_CONNECTING ? FI_SUCCESS : -FI_EAGAIN;
 }
 
+/* Caller must hold eq:lock */                                                                                  
+static inline int
+fi_ibv_eq_xrc_cm_err_event(struct fi_ibv_eq *eq,
+                           struct rdma_cm_event *cma_event)
+{
+	struct fi_ibv_xrc_ep *ep;
+	fid_t fid = cma_event->id->context;
+
+	ep = container_of(fid, struct fi_ibv_xrc_ep, base_ep.util_ep.ep_fid);
+	if (ep->magic != VERBS_XRC_EP_MAGIC) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "CM ID context invalid\n");
+		return -FI_EAGAIN;
+	}
+
+	/* Connect errors can be reported on active or passive side, all other
+	 * errors considered are reported on the active side only */
+	if ((ep->base_ep.id != cma_event->id) &&
+	    (cma_event->event == RDMA_CM_EVENT_CONNECT_ERROR &&
+	     ep->tgt_id != cma_event->id)) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "CM error not valid for EP\n");
+		return -FI_EAGAIN;
+	}
+
+	VERBS_WARN(FI_LOG_EP_CTRL, "CM error event %s, status %d\n",
+		   rdma_event_str(cma_event->event), cma_event->status);
+	if (ep->base_ep.info->src_addr)
+		ofi_straddr_log(&fi_ibv_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
+				"Src ", ep->base_ep.info->src_addr);
+	if (ep->base_ep.info->dest_addr)
+		ofi_straddr_log(&fi_ibv_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
+				"Dest ", ep->base_ep.info->dest_addr);
+        ep->conn_state = FI_IBV_XRC_ERROR;
+        return FI_SUCCESS;
+}
+
 /* Caller must hold eq:lock */
 static inline int
 fi_ibv_eq_xrc_connected_event(struct fi_ibv_eq *eq,
 			      struct rdma_cm_event *cma_event,
-			      struct fi_eq_cm_entry *entry, size_t len,
-			      int *acked)
+			      struct fi_eq_cm_entry *entry, size_t len)
 {
 	struct fi_ibv_xrc_ep *ep;
 	fid_t fid = cma_event->id->context;
@@ -480,11 +538,9 @@ fi_ibv_eq_xrc_connected_event(struct fi_ibv_eq *eq,
 
 	ret = fi_ibv_eq_xrc_recip_conn_event(eq, ep, cma_event, entry, len);
 
-	/* Bidirectional connection setup is complete, disconnect RDMA CM
-	 * ID(s) and release shared QP reservations/hardware resources
-	 * that were needed for shared connection setup only. */
-	*acked = 1;
-	rdma_ack_cm_event(cma_event);
+	/* Bidirectional connection setup is complete, release RDMA CM ID resources.
+	 * Note this will initiate release of shared QP reservation/hardware resources
+	 * that were needed for XRC shared connection setup as well. */
 	fi_ibv_free_xrc_conn_setup(ep, 1);
 
 	return ret;
@@ -501,23 +557,26 @@ fi_ibv_eq_xrc_timewait_event(struct fi_ibv_eq *eq,
 	assert(ep->magic == VERBS_XRC_EP_MAGIC);
 	assert(ep->conn_setup);
 
-	if (cma_event->id == ep->tgt_id && ep->conn_setup->rsvd_tgt_qpn) {
+	if (cma_event->id == ep->tgt_id) {
 		*acked = 1;
 		rdma_ack_cm_event(cma_event);
-		ibv_destroy_qp(ep->conn_setup->rsvd_tgt_qpn);
-		ep->conn_setup->rsvd_tgt_qpn = NULL;
+		if (ep->conn_setup->rsvd_tgt_qpn) {
+			ibv_destroy_qp(ep->conn_setup->rsvd_tgt_qpn);
+			ep->conn_setup->rsvd_tgt_qpn = NULL;
+		}
 		rdma_destroy_id(ep->tgt_id);
 		ep->tgt_id = NULL;
-	} else if (cma_event->id == ep->base_ep.id &&
-		   ep->conn_setup->rsvd_ini_qpn) {
+	} else if (cma_event->id == ep->base_ep.id) {
 		*acked = 1;
 		rdma_ack_cm_event(cma_event);
-		ibv_destroy_qp(ep->conn_setup->rsvd_ini_qpn);
-		ep->conn_setup->rsvd_ini_qpn = NULL;
+		if (ep->conn_setup->rsvd_ini_qpn) {
+			ibv_destroy_qp(ep->conn_setup->rsvd_ini_qpn);
+			ep->conn_setup->rsvd_ini_qpn = NULL;
+		}
 		rdma_destroy_id(ep->base_ep.id);
 		ep->base_ep.id = NULL;
 	}
-	if (!ep->conn_setup->rsvd_ini_qpn && !ep->conn_setup->rsvd_tgt_qpn)
+	if (!ep->base_ep.id && !ep->tgt_id)
 		fi_ibv_free_xrc_conn_setup(ep, 0);
 }
 
@@ -531,8 +590,7 @@ fi_ibv_eq_xrc_disconnect_event(struct fi_ibv_eq *eq,
 						base_ep.util_ep.ep_fid);
 	assert(ep->magic == VERBS_XRC_EP_MAGIC);
 
-	if (ep->conn_setup && cma_event->id == ep->base_ep.id &&
-	    ep->conn_setup->rsvd_ini_qpn) {
+	if (ep->conn_setup && cma_event->id == ep->base_ep.id) {
 		*acked = 1;
 		rdma_ack_cm_event(cma_event);
 		rdma_disconnect(ep->base_ep.id);
@@ -554,15 +612,37 @@ fi_ibv_eq_cm_process_event(struct fi_ibv_eq *eq,
 	struct fi_ibv_pep *pep =
 		container_of(fid, struct fi_ibv_pep, pep_fid);
 	struct fi_ibv_ep *ep;
+	struct fi_ibv_xrc_ep *xrc_ep;
+	struct fi_ibv_domain *domain;
 
 	*acked = 0;
 
 	switch (cma_event->event) {
+	case RDMA_CM_EVENT_ROUTE_RESOLVED:
+		ep = container_of(fid, struct fi_ibv_ep, util_ep.ep_fid);
+		if (rdma_connect(ep->id, &ep->conn_param)) {
+			ret = -errno;
+			FI_WARN(&fi_ibv_prov, FI_LOG_EP_CTRL,
+				"rdma_connect failed: %s (%d)\n",
+				strerror(-ret), -ret);
+			if (fi_ibv_is_xrc(ep->info)) {
+				xrc_ep = container_of(fid, struct fi_ibv_xrc_ep,
+						      base_ep.util_ep.ep_fid);
+				domain = fi_ibv_ep_to_domain(ep);
+				fastlock_acquire(&domain->xrc.ini_mgmt_lock);
+				fi_ibv_put_shared_ini_conn(xrc_ep);
+				fastlock_release(&domain->xrc.ini_mgmt_lock);
+			}
+			return ret;
+		}
+		return -FI_EAGAIN;
 	case RDMA_CM_EVENT_CONNECT_REQUEST:
 		*event = FI_CONNREQ;
 
 		ret = fi_ibv_eq_cm_getinfo(cma_event, pep->info, &entry->info);
 		if (ret) {
+			VERBS_WARN(FI_LOG_EP_CTRL,
+				   "CM getinfo error %d\n", ret);
 			fastlock_acquire(&eq->lock);
 			rdma_destroy_id(cma_event->id);
 			eq->err.err = -ret;
@@ -591,7 +671,7 @@ fi_ibv_eq_cm_process_event(struct fi_ibv_eq *eq,
 		if (fi_ibv_is_xrc(ep->info)) {
 			fastlock_acquire(&eq->lock);
 			ret = fi_ibv_eq_xrc_connected_event(eq, cma_event,
-							    entry, len, acked);
+							    entry, len);
 			fastlock_release(&eq->lock);
 			return ret;
 		}
@@ -621,7 +701,22 @@ fi_ibv_eq_cm_process_event(struct fi_ibv_eq *eq,
 	case RDMA_CM_EVENT_CONNECT_ERROR:
 	case RDMA_CM_EVENT_UNREACHABLE:
 		fastlock_acquire(&eq->lock);
-		eq->err.err = -cma_event->status;
+		ep = container_of(fid, struct fi_ibv_ep, util_ep.ep_fid);
+		assert(ep->info);
+		if (fi_ibv_is_xrc(ep->info)) {
+			ret = fi_ibv_eq_xrc_cm_err_event(eq, cma_event);
+			if (ret == -FI_EAGAIN) {
+				fastlock_release(&eq->lock);
+				return ret;
+			}
+		}
+		eq->err.err = ETIMEDOUT;
+		eq->err.prov_errno = -cma_event->status;
+		if (eq->err.err_data) {
+			free(eq->err.err_data);
+			eq->err.err_data = NULL;
+			eq->err.err_data_size = 0;
+		}
 		goto err;
 	case RDMA_CM_EVENT_REJECTED:
 		ep = container_of(fid, struct fi_ibv_ep, util_ep.ep_fid);
@@ -690,8 +785,10 @@ ssize_t fi_ibv_eq_write_event(struct fi_ibv_eq *eq, uint32_t event,
 	struct fi_ibv_eq_entry *entry;
 
 	entry = calloc(1, sizeof(struct fi_ibv_eq_entry) + len);
-	if (!entry)
+	if (!entry) {
+		VERBS_WARN(FI_LOG_EP_CTRL, "Unable to allocate EQ entry\n");
 		return -FI_ENOMEM;
+	}
 
 	entry->event = event;
 	entry->len = len;
diff --git a/prov/verbs/src/verbs_mr.c b/prov/verbs/src/verbs_mr.c
index c4e0051a4..6a88d7116 100644
--- a/prov/verbs/src/verbs_mr.c
+++ b/prov/verbs/src/verbs_mr.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017 Intel Corporation, Inc.  All rights reserved.
+ * Copyright (c) 2017-2019 Intel Corporation, Inc.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -33,76 +33,31 @@
 #include <ofi_util.h>
 #include "fi_verbs.h"
 
-#define FI_IBV_DEFINE_MR_REG_OPS(type)							\
-											\
-static int										\
-fi_ibv_mr ## type ## regv(struct fid *fid, const struct iovec *iov,			\
-			  size_t count, uint64_t access, uint64_t offset,		\
-			  uint64_t requested_key, uint64_t flags,			\
-			  struct fid_mr **mr, void *context)				\
-{											\
-	const struct fi_mr_attr attr = {						\
-		.mr_iov		= iov,							\
-		.iov_count	= count,						\
-		.access		= access,						\
-		.offset		= offset,						\
-		.requested_key	= requested_key,					\
-		.context	= context,						\
-	};										\
-	return fi_ibv_mr ## type ## regattr(fid, &attr, flags, mr);			\
-}											\
-											\
-static int										\
-fi_ibv_mr ## type ## reg(struct fid *fid, const void *buf, size_t len,			\
-			 uint64_t access, uint64_t offset, uint64_t requested_key,	\
-			 uint64_t flags, struct fid_mr **mr, void *context)		\
-{											\
-	const struct iovec iov = {							\
-		.iov_base	= (void *)buf,						\
-		.iov_len	= len,							\
-	};										\
-	return fi_ibv_mr ## type ## regv(fid, &iov, 1, access, offset,			\
-					 requested_key, flags, mr, context);		\
-}											\
-											\
-static struct fi_ops_mr fi_ibv_domain_mr ##type## ops = {				\
-	.size = sizeof(struct fi_ops_mr),						\
-	.reg = fi_ibv_mr ## type ## reg,						\
-	.regv = fi_ibv_mr ## type ## regv,						\
-	.regattr = fi_ibv_mr ## type ## regattr,					\
-};											\
-											\
-struct fi_ibv_mr_internal_ops fi_ibv_mr_internal ##type## ops = {			\
-	.fi_ops = &fi_ibv_domain_mr ##type## ops,					\
-	.internal_mr_reg = fi_ibv_mr_internal ##type## reg,				\
-	.internal_mr_dereg = fi_ibv_mr_internal ##type## dereg,				\
-};
 
-static inline struct ibv_mr *
-fi_ibv_mr_reg_ibv_mr(struct fi_ibv_domain *domain, void *buf,
-		     size_t len, int fi_ibv_access)
-{	
-#if defined HAVE_VERBS_EXP_H
-	struct ibv_exp_reg_mr_in in = {
-		.pd		= domain->pd,
-		.addr		= buf,
-		.length		= len,
-		.exp_access 	= fi_ibv_access,
-		.comp_mask	= 0,
-	};
-	if (domain->use_odp)
-		in.exp_access |= IBV_EXP_ACCESS_RELAXED |
-				 IBV_EXP_ACCESS_ON_DEMAND;
-	return ibv_exp_reg_mr(&in);
-#else /* HAVE_VERBS_EXP_H */
-	return ibv_reg_mr(domain->pd, buf, len, fi_ibv_access);
-#endif /* HAVE_VERBS_EXP_H */
+static int
+fi_ibv_mr_regv(struct fid *fid, const struct iovec *iov,
+	       size_t count, uint64_t access, uint64_t offset,
+	       uint64_t requested_key, uint64_t flags,
+	       struct fid_mr **mr, void *context)
+{
+	struct fid_domain *domain = container_of(fid, struct fid_domain, fid);
+
+	if (OFI_UNLIKELY(count > 1))
+		return -FI_EINVAL;
+
+	return count ? fi_mr_reg(domain, (const void *) iov->iov_base,
+				 iov->iov_len, access, offset, requested_key,
+				 flags, mr, context) :
+		       fi_mr_reg(domain, NULL, 0, access, offset, requested_key,
+				 flags, mr, context);
 }
 
-static inline
-int fi_ibv_mr_dereg_ibv_mr(struct ibv_mr *mr)
+static int fi_ibv_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
+			     uint64_t flags, struct fid_mr **mr)
 {
-	return -ibv_dereg_mr(mr);
+	return fi_ibv_mr_regv(fid, attr->mr_iov, attr->iov_count, attr->access,
+			      attr->offset, attr->requested_key, flags, mr,
+			      attr->context);
 }
 
 static int fi_ibv_mr_close(fid_t fid)
@@ -111,13 +66,16 @@ static int fi_ibv_mr_close(fid_t fid)
 	int ret;
 
 	mr = container_of(fid, struct fi_ibv_mem_desc, mr_fid.fid);
-	ret = fi_ibv_mr_dereg_ibv_mr(mr->mr);
+	if (!mr->mr)
+		return 0;
+
+	ret = -ibv_dereg_mr(mr->mr);
 	if (!ret)
 		free(mr);
 	return ret;
 }
 
-static struct fi_ops fi_ibv_mr_ops = {
+static struct fi_ops fi_ibv_mr_fi_ops = {
 	.size = sizeof(struct fi_ops),
 	.close = fi_ibv_mr_close,
 	.bind = fi_no_bind,
@@ -133,12 +91,17 @@ int fi_ibv_mr_reg_common(struct fi_ibv_mem_desc *md, int fi_ibv_access,
 	md->mr_fid.fid.fclass = FI_CLASS_MR;
 	md->mr_fid.fid.context = context;
 
-	md->mr = fi_ibv_mr_reg_ibv_mr(md->domain, (void *)buf, len, fi_ibv_access);
-	if (!md->mr)
-		return -errno;
-
-	md->mr_fid.mem_desc = (void *)(uintptr_t)md->mr->lkey;
-	md->mr_fid.key = md->mr->rkey;
+	md->mr = ibv_reg_mr(md->domain->pd, (void *) buf, len, fi_ibv_access);
+	if (!md->mr) {
+		if (len)
+			return -errno;
+		else
+			/* Ignore failure for zero length memory registration */
+			assert(errno == FI_EINVAL);
+	} else {
+		md->mr_fid.mem_desc = (void *)(uintptr_t)md->mr->lkey;
+		md->mr_fid.key = md->mr->rkey;
+	}
 
 	if (md->domain->eq_flags & FI_REG_MR) {
 		struct fi_eq_entry entry = {
@@ -156,24 +119,6 @@ int fi_ibv_mr_reg_common(struct fi_ibv_mem_desc *md, int fi_ibv_access,
 	return FI_SUCCESS;
 }
 
-static inline
-int fi_ibv_mr_regattr_check_args(struct fid *fid,
-				 const struct fi_mr_attr *attr,
-				 uint64_t flags)
-{
-	if (OFI_UNLIKELY(flags))
-		return -FI_EBADFLAGS;
-	if (OFI_UNLIKELY(fid->fclass != FI_CLASS_DOMAIN))
-		return -FI_EINVAL;
-	if (OFI_UNLIKELY(attr->iov_count > VERBS_MR_IOV_LIMIT)) {
-		VERBS_WARN(FI_LOG_FABRIC,
-			   "iov count > %d not supported\n",
-			   VERBS_MR_IOV_LIMIT);
-		return -FI_EINVAL;
-	}
-	return FI_SUCCESS;
-}
-
 static inline int
 fi_ibv_mr_ofi2ibv_access(uint64_t ofi_access, struct fi_ibv_domain *domain)
 {
@@ -206,93 +151,16 @@ fi_ibv_mr_ofi2ibv_access(uint64_t ofi_access, struct fi_ibv_domain *domain)
 	return ibv_access;
 }
 
-static inline struct fi_ibv_mem_desc *
-fi_ibv_mr_common_cache_reg(struct fi_ibv_domain *domain,
-			 struct fi_mr_attr *attr)
-{
-	struct fi_ibv_mem_desc *md;
-	struct ofi_mr_entry *entry;
-	int ret;
-
-	ret = ofi_mr_cache_search(&domain->cache, attr, &entry);
-	if (OFI_UNLIKELY(ret))
-		return NULL;
-
-	md = (struct fi_ibv_mem_desc *)entry->data;
-	md->entry = entry;
-
-	return md;
-}
-
-static inline
-void fi_ibv_common_cache_dereg(struct fi_ibv_mem_desc *md)
-{
-	ofi_mr_cache_delete(&md->domain->cache, md->entry);
-}
-
-static inline
-int fi_ibv_mr_internal_reg(struct fi_ibv_domain *domain, void *buf,
-			   size_t len, uint64_t access,
-			   struct fi_ibv_mem_desc *md)
-{
-	md->domain = domain;
-	md->len = len;
-	md->mr = fi_ibv_mr_reg_ibv_mr(domain, buf, len,
-				      fi_ibv_mr_ofi2ibv_access(access,
-							       domain));
-	if (OFI_UNLIKELY(!md->mr))
-		return -errno;
-	return FI_SUCCESS;
-}
-
-static inline
-int fi_ibv_mr_internal_dereg(struct fi_ibv_mem_desc *md)
-{
-	int ret = fi_ibv_mr_dereg_ibv_mr(md->mr);
-	md->mr = NULL;
-	return ret;
-}
-
-static inline
-int fi_ibv_mr_internal_cache_reg(struct fi_ibv_domain *domain, void *buf,
-				 size_t len, uint64_t access,
-				 struct fi_ibv_mem_desc *md)
-{
-	const struct iovec iov = {
-		.iov_base	= buf,
-		.iov_len	= len,
-	};
-	struct fi_mr_attr attr = {
-		.mr_iov		= &iov,
-		.iov_count	= 1,
-		.access		= access,
-	};
-	struct fi_ibv_mem_desc *mdesc =
-		fi_ibv_mr_common_cache_reg(domain, &attr);
-	if (OFI_UNLIKELY(!mdesc))
-		return -FI_EAVAIL;
-	*md = *mdesc;
-	md->len = len;
-	return FI_SUCCESS;
-}
-
-static inline
-int fi_ibv_mr_internal_cache_dereg(struct fi_ibv_mem_desc *md)
-{
-	fi_ibv_common_cache_dereg(md);
-	md->mr = NULL;
-	return FI_SUCCESS;
-}
-
-static int fi_ibv_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
-			     uint64_t flags, struct fid_mr **mr)
+static int
+fi_ibv_mr_reg(struct fid *fid, const void *buf, size_t len,
+	      uint64_t access, uint64_t offset, uint64_t requested_key,
+	      uint64_t flags, struct fid_mr **mr, void *context)
 {
 	struct fi_ibv_mem_desc *md;
 	int ret;
 
-	ret = fi_ibv_mr_regattr_check_args(fid, attr, flags);
-	if (OFI_UNLIKELY(ret))
-		return ret;
+	if (OFI_UNLIKELY(flags))
+		return -FI_EBADFLAGS;
 
 	md = calloc(1, sizeof(*md));
 	if (OFI_UNLIKELY(!md))
@@ -300,12 +168,10 @@ static int fi_ibv_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 
 	md->domain = container_of(fid, struct fi_ibv_domain,
 				  util_domain.domain_fid.fid);
-	md->mr_fid.fid.ops = &fi_ibv_mr_ops;
+	md->mr_fid.fid.ops = &fi_ibv_mr_fi_ops;
 
-	ret = fi_ibv_mr_reg_common(md, fi_ibv_mr_ofi2ibv_access(attr->access,
-								md->domain),
-				   attr->mr_iov[0].iov_base,
-				   attr->mr_iov[0].iov_len, attr->context);
+	ret = fi_ibv_mr_reg_common(md, fi_ibv_mr_ofi2ibv_access(access, md->domain),
+				   buf, len, context);
 	if (OFI_UNLIKELY(ret))
 		goto err;
 
@@ -316,19 +182,23 @@ err:
 	return ret;
 }
 
-FI_IBV_DEFINE_MR_REG_OPS(_)
-
 static int fi_ibv_mr_cache_close(fid_t fid)
 {
 	struct fi_ibv_mem_desc *md =
 		container_of(fid, struct fi_ibv_mem_desc, mr_fid.fid);
 	
-	fi_ibv_common_cache_dereg(md);
-
+	ofi_mr_cache_delete(&md->domain->cache, md->entry);
 	return FI_SUCCESS;
 }
 
-static struct fi_ops fi_ibv_mr_cache_ops = {
+struct fi_ops_mr fi_ibv_mr_ops = {
+	.size = sizeof(struct fi_ops_mr),
+	.reg = fi_ibv_mr_reg,
+	.regv = fi_ibv_mr_regv,
+	.regattr = fi_ibv_mr_regattr,
+};
+
+static struct fi_ops fi_ibv_mr_cache_fi_ops = {
 	.size = sizeof(struct fi_ops),
 	.close = fi_ibv_mr_cache_close,
 	.bind = fi_no_bind,
@@ -336,46 +206,69 @@ static struct fi_ops fi_ibv_mr_cache_ops = {
 	.ops_open = fi_no_ops_open,
 };
 
-int fi_ibv_mr_cache_entry_reg(struct ofi_mr_cache *cache,
-			      struct ofi_mr_entry *entry)
+int fi_ibv_mr_cache_add_region(struct ofi_mr_cache *cache,
+			       struct ofi_mr_entry *entry)
 {
-	int fi_ibv_access = IBV_ACCESS_LOCAL_WRITE |
-			    IBV_ACCESS_REMOTE_WRITE |
-			    IBV_ACCESS_REMOTE_ATOMIC |
-			    IBV_ACCESS_REMOTE_READ;
-	struct fi_ibv_mem_desc *md = (struct fi_ibv_mem_desc *)entry->data;
+	struct fi_ibv_mem_desc *md = (struct fi_ibv_mem_desc *) entry->data;
+
 	md->domain = container_of(cache->domain, struct fi_ibv_domain, util_domain);
-	md->mr_fid.fid.ops = &fi_ibv_mr_cache_ops;
-	return fi_ibv_mr_reg_common(md, fi_ibv_access, entry->iov.iov_base,
-				    entry->iov.iov_len, NULL);
+	md->mr_fid.fid.ops = &fi_ibv_mr_cache_fi_ops;
+	md->entry = entry;
+
+	return fi_ibv_mr_reg_common(md, IBV_ACCESS_LOCAL_WRITE |
+			IBV_ACCESS_REMOTE_WRITE | IBV_ACCESS_REMOTE_ATOMIC |
+			IBV_ACCESS_REMOTE_READ, entry->info.iov.iov_base,
+			entry->info.iov.iov_len, NULL);
 }
 
-void fi_ibv_mr_cache_entry_dereg(struct ofi_mr_cache *cache,
-				 struct ofi_mr_entry *entry)
+void fi_ibv_mr_cache_delete_region(struct ofi_mr_cache *cache,
+				   struct ofi_mr_entry *entry)
 {
 	struct fi_ibv_mem_desc *md = (struct fi_ibv_mem_desc *)entry->data;
-	(void)fi_ibv_mr_dereg_ibv_mr(md->mr);
+	if (md->mr)
+		(void)ibv_dereg_mr(md->mr);
 }
 
-static int fi_ibv_mr_cache_regattr(struct fid *fid, const struct fi_mr_attr *attr,
-				   uint64_t flags, struct fid_mr **mr)
+static int
+fi_ibv_mr_cache_reg(struct fid *fid, const void *buf, size_t len,
+		    uint64_t access, uint64_t offset, uint64_t requested_key,
+		    uint64_t flags, struct fid_mr **mr, void *context)
 {
 	struct fi_ibv_domain *domain;
 	struct fi_ibv_mem_desc *md;
+	struct ofi_mr_entry *entry;
+	struct fi_mr_attr attr;
+	struct iovec iov;
 	int ret;
 
-	ret = fi_ibv_mr_regattr_check_args(fid, attr, flags);
-	if (OFI_UNLIKELY(ret))
-		return ret;
+	if (OFI_UNLIKELY(flags))
+		return -FI_EBADFLAGS;
 
 	domain = container_of(fid, struct fi_ibv_domain,
 			      util_domain.domain_fid.fid);
 
-	md = fi_ibv_mr_common_cache_reg(domain, (struct fi_mr_attr *)attr);
-	if (OFI_UNLIKELY(!md))
-		return -FI_EAVAIL;
+	attr.access = access;
+	attr.context = context;
+	attr.iov_count = 1;
+	iov.iov_base = (void *) buf;
+	iov.iov_len = len;
+	attr.mr_iov = &iov;
+	attr.offset = offset;
+	attr.requested_key = requested_key;
+	attr.auth_key_size = 0;
+
+	ret = ofi_mr_cache_search(&domain->cache, &attr, &entry);
+	if (OFI_UNLIKELY(ret))
+		return ret;
+
+	md = (struct fi_ibv_mem_desc *) entry->data;
 	*mr = &md->mr_fid;
 	return FI_SUCCESS;
 }
 
-FI_IBV_DEFINE_MR_REG_OPS(_cache_)
+struct fi_ops_mr fi_ibv_mr_cache_ops = {
+	.size = sizeof(struct fi_ops_mr),
+	.reg = fi_ibv_mr_cache_reg,
+	.regv = fi_ibv_mr_regv,
+	.regattr = fi_ibv_mr_regattr,
+};
diff --git a/src/fabric.c b/src/fabric.c
index 851e36363..bd1ade955 100644
--- a/src/fabric.c
+++ b/src/fabric.c
@@ -250,7 +250,7 @@ struct fi_provider *ofi_get_hook(const char *name)
 
 	prov = ofi_getprov(name, strlen(name));
 	if (!prov) {
-		ret = asprintf(&try_name, "ofi_%s_hook", name);
+		ret = asprintf(&try_name, "ofi_hook_%s", name);
 		if (ret > 0)
 			prov = ofi_getprov(try_name, ret);
 		else
@@ -337,7 +337,7 @@ static void ofi_ordered_provs_init(void)
 		/* These are hooking providers only.  Their order
 		 * doesn't matter
 		 */
-		"ofi_perf_hook", "ofi_noop_hook",
+		"ofi_hook_perf", "ofi_hook_debug", "ofi_hook_noop",
 	};
 	int num_provs = sizeof(ordered_prov_names)/sizeof(ordered_prov_names[0]), i;
 
@@ -621,8 +621,9 @@ libdl_done:
 	ofi_register_provider(SOCKETS_INIT, NULL);
 	ofi_register_provider(TCP_INIT, NULL);
 
-	ofi_register_provider(PERF_HOOK_INIT, NULL);
-	ofi_register_provider(NOOP_HOOK_INIT, NULL);
+	ofi_register_provider(HOOK_PERF_INIT, NULL);
+	ofi_register_provider(HOOK_DEBUG_INIT, NULL);
+	ofi_register_provider(HOOK_NOOP_INIT, NULL);
 
 	ofi_init = 1;
 
diff --git a/src/fi_tostr.c b/src/fi_tostr.c
index b82276adf..8c99cb12c 100644
--- a/src/fi_tostr.c
+++ b/src/fi_tostr.c
@@ -197,6 +197,7 @@ static void ofi_tostr_caps(char *buf, uint64_t caps)
 	IFFLAGSTR(caps, FI_TAGGED);
 	IFFLAGSTR(caps, FI_ATOMIC);
 	IFFLAGSTR(caps, FI_MULTICAST);
+	IFFLAGSTR(caps, FI_COLLECTIVE);
 
 	IFFLAGSTR(caps, FI_READ);
 	IFFLAGSTR(caps, FI_WRITE);
@@ -584,6 +585,7 @@ static void ofi_tostr_atomic_type(char *buf, enum fi_datatype type)
 	CASEENUMSTR(FI_DOUBLE_COMPLEX);
 	CASEENUMSTR(FI_LONG_DOUBLE);
 	CASEENUMSTR(FI_LONG_DOUBLE_COMPLEX);
+	CASEENUMSTR(FI_VOID);
 	default:
 		ofi_strcatf(buf, "Unknown");
 		break;
@@ -612,6 +614,10 @@ static void ofi_tostr_atomic_op(char *buf, enum fi_op op)
 	CASEENUMSTR(FI_CSWAP_GE);
 	CASEENUMSTR(FI_CSWAP_GT);
 	CASEENUMSTR(FI_MSWAP);
+	CASEENUMSTR(FI_BARRIER);
+	CASEENUMSTR(FI_BROADCAST);
+	CASEENUMSTR(FI_ALLTOALL);
+	CASEENUMSTR(FI_ALLGATHER);
 	default:
 		ofi_strcatf(buf, "Unknown");
 		break;
@@ -633,6 +639,7 @@ static void ofi_tostr_eq_event(char *buf, int type)
 	CASEENUMSTR(FI_SHUTDOWN);
 	CASEENUMSTR(FI_MR_COMPLETE);
 	CASEENUMSTR(FI_AV_COMPLETE);
+	CASEENUMSTR(FI_JOIN_COMPLETE);
 	default:
 		ofi_strcatf(buf, "Unknown");
 		break;
diff --git a/src/mem.c b/src/mem.c
index 91836a79c..23617a0a4 100644
--- a/src/mem.c
+++ b/src/mem.c
@@ -84,7 +84,7 @@ void ofi_mem_init(void)
 		num_page_sizes = 1;
 	}
 
-	while (n--) {
+	while (n-- > 0) {
 		if (sscanf(pglist[n]->d_name, "hugepages-%zukB", &hpsize) == 1) {
 			hpsize *= 1024;
 			if (hpsize != page_sizes[OFI_DEF_HUGEPAGE_SIZE])
diff --git a/src/tree.c b/src/tree.c
index 98c780859..77e29378b 100644
--- a/src/tree.c
+++ b/src/tree.c
@@ -192,7 +192,8 @@ ofi_insert_rebalance(struct ofi_rbmap *map, struct ofi_rbnode *x)
 	map->root->color = BLACK;
 }
 
-int ofi_rbmap_insert(struct ofi_rbmap *map, void *key, void *data)
+int ofi_rbmap_insert(struct ofi_rbmap *map, void *key, void *data,
+		     struct ofi_rbnode **ret_node)
 {
 	struct ofi_rbnode *current, *parent, *node;
 	int ret;
@@ -229,6 +230,8 @@ int ofi_rbmap_insert(struct ofi_rbmap *map, void *key, void *data)
 	}
 
 	ofi_insert_rebalance(map, node);
+	if (ret_node)
+		*ret_node = node;
 	return 0;
 }
 
diff --git a/src/windows/osd.c b/src/windows/osd.c
index f387a281d..a8d4efbef 100644
--- a/src/windows/osd.c
+++ b/src/windows/osd.c
@@ -43,7 +43,6 @@
 #include "ofi_file.h"
 #include "ofi_list.h"
 #include "ofi_util.h"
-
 #include "rdma/providers/fi_log.h"
 
 extern struct ofi_common_locks common_locks;
@@ -404,65 +403,95 @@ fn_nomem:
 
 int getifaddrs(struct ifaddrs **ifap)
 {
-	DWORD i;
-	MIB_IPADDRTABLE _iptbl;
-	MIB_IPADDRTABLE *iptbl = &_iptbl;
-	ULONG ips = 1;
-	ULONG res = GetIpAddrTable(iptbl, &ips, 0);
-	int ret = -1;
+	ULONG subnet = 0;
+	PULONG mask = &subnet;
+	DWORD size, res, i = 0;
+	int ret;
+	PIP_ADAPTER_ADDRESSES adapter_addresses, aa;
+	PIP_ADAPTER_UNICAST_ADDRESS ua;
 	struct ifaddrs *head = NULL;
+	struct sockaddr_in *pInAddr = NULL;
+	SOCKADDR *pSockAddr = NULL;
+	struct ifaddrs *fa;
+
+	res = GetAdaptersAddresses(AF_UNSPEC, GAA_FLAG_INCLUDE_PREFIX,
+				   NULL, NULL, &size);
+	if (res != ERROR_BUFFER_OVERFLOW)
+		return -FI_ENOMEM;
+
+	adapter_addresses = (PIP_ADAPTER_ADDRESSES)malloc(size);
+	res = GetAdaptersAddresses(AF_UNSPEC, GAA_FLAG_INCLUDE_PREFIX,
+				   NULL, adapter_addresses, &size);
+	if (res != ERROR_SUCCESS)
+		return -FI_ENOMEM;
+
+	for (aa = adapter_addresses; aa != NULL; aa = aa->Next) {
+		if (aa->OperStatus != 1)
+			continue;
+
+		for (ua = aa->FirstUnicastAddress; ua != NULL; ua = ua->Next) {
+			pSockAddr = ua->Address.lpSockaddr;
+			if (pSockAddr->sa_family != AF_INET &&
+				pSockAddr->sa_family != AF_INET6)
+				continue;
+			fa = calloc(sizeof(*fa), 1);
+			if (!fa) {
+				ret = -FI_ENOMEM;
+				goto out;
+			}
 
-	assert(ifap);
-
-	if (res == ERROR_INSUFFICIENT_BUFFER) {
-		iptbl = malloc(ips);
-		if (!iptbl)
-			goto failed_no_mem;
-		res = GetIpAddrTable(iptbl, &ips, 0);
-		if (res != NO_ERROR)
-			goto failed_get_addr;
-	} else if (res != NO_ERROR) {
-		goto failed;
-	}
+			fa->ifa_next = head;
+			head = fa;
 
-	for (i = 0; i < iptbl->dwNumEntries; i++) {
-		if (iptbl->table[i].dwAddr && iptbl->table[i].dwAddr != ntohl(INADDR_LOOPBACK)) {
-			struct ifaddrs *fa = calloc(sizeof(*fa), 1);
-			if (!fa)
-				goto failed_cant_allocate;
 			fa->ifa_flags = IFF_UP;
-			fa->ifa_addr = (struct sockaddr *)&fa->in_addr;
-			fa->ifa_netmask = (struct sockaddr *)&fa->in_netmask;
-			fa->ifa_name = fa->ad_name;
+			if (aa->IfType == IF_TYPE_SOFTWARE_LOOPBACK)
+				fa->ifa_flags |= IFF_LOOPBACK;
 
-			fa->in_addr.sin_family = fa->in_netmask.sin_family = AF_INET;
-			fa->in_addr.sin_addr.s_addr = iptbl->table[i].dwAddr;
-			fa->in_netmask.sin_addr.s_addr = iptbl->table[i].dwMask;
-			/* on Windows there is no Unix-like interface names,
-			   so, let's generate fake names */
-			sprintf_s(fa->ad_name, sizeof(fa->ad_name), "eth%d", i);
+			fa->ifa_addr = (struct sockaddr *) &fa->in_addrs;
+			fa->ifa_netmask = (struct sockaddr *) &fa->in_netmasks;
+			fa->ifa_name = fa->ad_name;
 
-			fa->ifa_next = head;
-			head = fa;
+			if (pSockAddr->sa_family == AF_INET) {
+				subnet = 0;
+				mask = &subnet;
+				if (ConvertLengthToIpv4Mask(ua->OnLinkPrefixLength, mask) !=
+					NO_ERROR) {
+					ret = -FI_ENODATA;
+					goto out;
+				}
+				struct sockaddr_in *addr4 = (struct sockaddr_in *)
+							    &fa->in_addrs;
+				struct sockaddr_in *netmask4 = (struct sockaddr_in *)
+								&fa->in_netmasks;
+				netmask4->sin_family = pSockAddr->sa_family;
+				addr4->sin_family = pSockAddr->sa_family;
+				netmask4->sin_addr.S_un.S_addr = mask;
+				pInAddr = (struct sockaddr_in *) pSockAddr;
+				addr4->sin_addr = pInAddr->sin_addr;
+			} else {
+				struct sockaddr_in6 *addr6 = (struct sockaddr_in6 *)
+							      &fa->in_addrs;
+				(*addr6) = *(struct sockaddr_in6 *) pSockAddr;
+			}
+			fa->speed = aa->TransmitLinkSpeed;
+			/* Generate fake Unix-like device names */
+			sprintf_s(fa->ad_name, sizeof(fa->ad_name), "eth%d", i++);
 		}
 	}
-
-	if (iptbl != &_iptbl)
-		free(iptbl);
 	ret = 0;
-	if (ifap)
+out:
+	free(adapter_addresses);
+	if (ret && head)
+		free(head);
+	else if (ifap)
 		*ifap = head;
-complete:
+
 	return ret;
+}
 
-failed_cant_allocate:
-	if(head)
-		freeifaddrs(head);
-failed_get_addr:
-	free(iptbl);
-failed_no_mem:
-failed:
-	goto complete;
+size_t ofi_ifaddr_get_speed(struct ifaddrs *ifa)
+{
+	return ifa->speed;
 }
 
 void freeifaddrs(struct ifaddrs *ifa)
