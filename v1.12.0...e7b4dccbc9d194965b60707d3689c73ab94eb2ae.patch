diff --git a/.github/workflows/nroff-elves.sh b/.github/workflows/nroff-elves.sh
new file mode 100755
index 0000000..a7176ba
--- /dev/null
+++ b/.github/workflows/nroff-elves.sh
@@ -0,0 +1,104 @@
+#!/bin/bash
+
+set -euxo pipefail
+
+# Only do anything meaningful in the main ofiwg/libfabric repo.  If
+# we're not in that repo, then don't do anything because it confuses
+# people who fork the libfabric repo if Nroff Elves commits show up in
+# their fork with different git hashes than the Nroff Elves commits on
+# the ofiwg/libfabric repo.
+if test -n "$REPO"; then
+    first=`echo $REPO | cut -d/ -f1`
+    second=`echo $REPO | cut -d/ -f2`
+
+    if test "$first" != "ofiwg" -o "$second" != "libfabric"; then
+        cat <<EOF
+
+The Nroff Elves are contractually obligated to only operate on the
+ofiwg/libfabric repository.
+
+Exiting without doing anything.
+
+EOF
+        exit 0
+    fi
+fi
+
+# In June of 2021, ofiwg/libfabric changed its default branch from
+# "master" to "main".  This confuses "hub" (because it still falls
+# back to "master" when it can't figure out the target branch name).
+# In nroff-elves.yaml, we load $BASE_REPO with
+# ${{github.event.repository.default_branch}} and use that to tell
+# "hub" what the base branch should be.  This works great... except
+# sometimes $BASE_REPO is blank (when it should have a valid branch
+# name in it).  So if we get here and $BASE_REPO is blank, just assume
+# that it should be "main".  This is lame and shouldn't be necessary,
+# but it prevents us all from getting Github Action failure emails.
+# Sigh.
+if test -z "$BASE_REF"; then
+    BASE_REF=main
+fi
+
+# If we're here, we want to generate some nroff.  Woo hoo!
+for file in `ls man/*.md`; do
+    perl config/md2nroff.pl --source=$file
+done
+
+git config --global user.name "OFIWG Bot"
+git config --global user.email "ofiwg@lists.openfabrics.org"
+
+branch_name=pr/update-nroff-generated-man-pages
+git checkout -b $branch_name
+
+set +e
+git commit -as -m 'Updated nroff-generated man pages'
+st=$?
+set -e
+
+if test $st -ne 0; then
+    echo "Nothing to commit -- nothing to do!"
+    exit 0
+fi
+
+# Yes, we committed something.  Push the branch and make a PR.
+# Extract the PR number.
+git push --set-upstream origin $branch_name
+url=`hub pull-request -b $BASE_REF -m 'Update nroff-generated man pages'`
+pr_num=`echo $url | cut -d/ -f7`
+
+# Wait for the required "DCO" CI to complete
+i=0
+sleep_time=5
+max_seconds=300
+i_max=`expr $max_seconds / $sleep_time`
+
+echo "Waiting up to $max_seconds seconds for DCO CI to complete..."
+while test $i -lt $i_max; do
+    date
+    set +e
+    status=`hub ci-status --format "%t %S%n" | egrep '^DCO' | awk '{ print $2 }'`
+    set -e
+    if test "$status" = "success"; then
+        echo "DCO CI is complete!"
+        break
+    fi
+    sleep $sleep_time
+    i=`expr $i + 1`
+done
+
+status=0
+if test $i -lt $i_max; then
+    # Sadly, there is no "hub" command to merge a PR.  So do it by
+    # hand.
+    curl \
+        -XPUT \
+        -H "Authorization: token $GITHUB_TOKEN" \
+        https://api.github.com/repos/$GITHUB_REPOSITORY/pulls/$pr_num/merge
+else
+    echo "Sad panda; DCO CI didn't complete -- did not merge $url"
+    status=1
+fi
+
+# Delete the remote branch
+git push origin --delete $branch_name
+exit $status
diff --git a/.github/workflows/nroff-elves.yaml b/.github/workflows/nroff-elves.yaml
new file mode 100644
index 0000000..ae1040a
--- /dev/null
+++ b/.github/workflows/nroff-elves.yaml
@@ -0,0 +1,31 @@
+name: GitHub Action Schedule
+
+on:
+  schedule:
+    - cron: '0 * * * *'
+  workflow_dispatch:
+
+jobs:
+    nroff-elves-scheduled:
+        name: The Nroff Elves
+        runs-on: ubuntu-latest
+        steps:
+          - name: Debug information
+            env:
+              GITHUB_DATA: ${{ toJSON(github) }}
+            run: |
+              echo This is information that may be useful for debugging.
+              echo "$GITHUB_DATA"
+
+          - name: Check out the git repo
+            uses: actions/checkout@v2
+
+          - name: Get the required packages
+            run: sudo apt install -y pandoc hub
+
+          - name: Build the nroff man pages
+            run: .github/workflows/nroff-elves.sh
+            env:
+              GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+              REPO: ${{ github.event.repository.full_name }}
+              BASE_REF: ${{ github.event.repository.default_branch }}
diff --git a/.github/workflows/pr-ci.yml b/.github/workflows/pr-ci.yml
new file mode 100644
index 0000000..7e66800
--- /dev/null
+++ b/.github/workflows/pr-ci.yml
@@ -0,0 +1,136 @@
+name: Build Checks
+on: [push, pull_request]
+env:
+  APT_PACKAGES: >-
+    abi-compliance-checker
+    abi-dumper
+    build-essential
+    debhelper
+    dh-systemd
+    fakeroot
+    gcc
+    git
+    libnl-3-200 libnl-3-dev libnl-route-3-200 libnl-route-3-dev
+    libnuma-dev
+    libudev-dev
+    uuid-dev
+    make
+    ninja-build
+    pandoc
+    pkg-config
+    python
+    rpm
+    sparse
+    valgrind
+    wget
+  OFI_PROVIDER_FLAGS: >-
+    --enable-efa=rdma-core/build
+    --enable-mrail
+    --enable-psm3=rdma-core/build
+    --enable-rxd
+    --enable-rxm
+    --enable-shm
+    --enable-tcp
+    --enable-udp
+    --enable-usnic
+    --enable-verbs=rdma-core/build
+  RDMA_CORE_PATH: 'rdma-core/build'
+  RDMA_CORE_VERSION: v34.1
+jobs:
+  linux:
+    runs-on: '${{ matrix.os }}'
+    strategy:
+      matrix:
+        os:
+          - ubuntu-18.04
+          - ubuntu-20.04
+        cc:
+          - gcc
+          - clang
+      fail-fast: false
+    steps:
+      - name: Install dependencies (Linux)
+        run: |
+          sudo apt-get update
+          sudo apt-get install -y ${{ env.APT_PACKAGES }}
+      - uses: actions/checkout@v2
+      - name: Build Check
+        run: |
+          set -x
+          git clone --depth 1 -b ${{ env.RDMA_CORE_VERSION }} https://github.com/linux-rdma/rdma-core.git
+          pushd rdma-core; bash build.sh; popd
+          export LD_LIBRARY_PATH="${{ env.RDMA_CORE_PATH }}/lib:$LD_LIBRARY_PATH"
+          ./autogen.sh
+          ./configure --prefix=$PWD/install ${{ env.OFI_PROVIDER_FLAGS }} CC=${{ matrix.cc }}
+          make -j 2; make install
+          $PWD/install/bin/fi_info -l
+      - name: Upload build logs
+        if: failure()
+        uses: actions/upload-artifact@v2
+        with:
+          name: config.log
+          path: config.log
+  hmem:
+    runs-on: ubuntu-20.04
+    steps:
+      - name: Install dependencies (Linux)
+        run: |
+          sudo apt-get update
+          sudo apt-get install -y ${{ env.APT_PACKAGES }}
+      - name: Install CUDA
+        run: |
+          sudo apt-get install -y nvidia-cuda-toolkit
+      - name: Install ROCm
+        run: |
+          echo "Installing ROCm SDK"
+          # TODO: Install ROCm dependencies and add --with-rocm to build in next step
+      - name: Install Ze
+        run: |
+          echo "Installing Ze SDK"
+          sudo apt-get install -y gpg-agent wget
+          wget -qO - https://repositories.intel.com/graphics/intel-graphics.key | sudo apt-key add -
+          sudo apt-add-repository 'deb [arch=amd64] https://repositories.intel.com/graphics/ubuntu focal main'
+          sudo apt-get update
+          sudo apt-get install -y level-zero level-zero-dev
+      - uses: actions/checkout@v2
+      - name: HMEM Checks
+        run: |
+          set -x
+          # We could use 'upload-artifact' and persist the rdma-core build
+          # across jobs, but this is just as quick.
+          git clone --depth 1 -b ${{ env.RDMA_CORE_VERSION }} https://github.com/linux-rdma/rdma-core.git
+          pushd rdma-core; bash build.sh; popd
+          export LD_LIBRARY_PATH="${{ env.RDMA_CORE_PATH }}/lib:$LD_LIBRARY_PATH"
+          ./autogen.sh
+          ./configure --prefix=$PWD/install ${{ env.OFI_PROVIDER_FLAGS }} \
+                                            --with-cuda=/usr/local/cuda --with-ze \
+                                            CC=${{ matrix.cc }}
+          make -j 2; make install
+          $PWD/install/bin/fi_info -l
+          $PWD/install/bin/fi_info -c FI_HMEM
+      - name: Upload build logs
+        if: failure()
+        uses: actions/upload-artifact@v2
+        with:
+          name: config.log
+          path: config.log
+  macos:
+    runs-on: macos-10.15
+    steps:
+      - name: Install dependencies (Mac OS)
+        run: |
+           brew install automake
+           brew install libtool
+      - uses: actions/checkout@v2
+      - name: Build Check
+        run: |
+          ./autogen.sh
+          ./configure --prefix=$PWD/install
+          make -j 2; make install
+          $PWD/install/bin/fi_info -l
+      - name: Upload build logs
+        if: failure()
+        uses: actions/upload-artifact@v2
+        with:
+          name: config.log
+          path: config.log
diff --git a/.travis.yml b/.travis.yml
index 2f22683..ed73084 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -35,18 +35,13 @@ addons:
             - abi-compliance-checker
             - abi-dumper
     coverity_scan:
-      project:
-        name: "ofiwg/libfabric"
-        description: "Libfabric project coverity scans"
-      notification_email: sean.hefty@intel.com
-      build_command_prepend: "./autogen.sh; ./configure"
-      build_command: "make -j2"
-      # It might be overkill to run a full scan across the compiler test matrix
-      # for every PR to master. The coverity addon can not selectively run for
-      # certain OSes or compilers. Once a couple runs succeed, change this to a
-      # coverity-scan branch that we push to on-demand during releases or as
-      # needed..
-      branch_pattern: master
+        project:
+            name: "ofiwg/libfabric"
+            description: "Libfabric project coverity scans"
+        notification_email: sean.hefty@intel.com
+        build_command_prepend: "./autogen.sh; ./configure --enable-efa=$RDMA_CORE_PATH --enable-psm2 --enable-psm3=$RDMA_CORE_PATH --enable-usnic --enable-verbs=$RDMA_CORE_PATH"
+        build_command: "make -j2"
+        branch_pattern: main
 
 env:
     global:
diff --git a/AUTHORS b/AUTHORS
index 4e85a18..3ed3b58 100644
--- a/AUTHORS
+++ b/AUTHORS
@@ -4,6 +4,7 @@ aingerson <aingerson@gmail.com>
 aingerson <alexia.ingerson@intel.com>
 Ajay Kulkarni <ajaykulk@cisco.com>
 aleksandra.justa <ajusta@gklab-125-155.igk.intel.com>
+Alexia Ingerson <alexia.ingerson@intel.com>
 Alex McKinley <alex.mckinley@intel.com>
 Alex McKinley <alex@mckpals.com>
 Amith Abraham <aabraham@cray.com>
@@ -19,6 +20,9 @@ Arun Ilango <arun.ilango@intel.com>
 Ashley Pittman <ampittma@ampittma-mac02.pittman.co.uk.20.20.172.in-addr.arpa>
 Ashley Pittman <ashley.m.pittman@intel.com>
 Automated bot for the OFIWG organization <ofiwg-bot@users.noreply.github.com>
+AWS ParallelCluster user <centos@ip-172-31-23-100.ec2.internal>
+AWS ParallelCluster user <ec2-user@ip-172-31-0-240.us-east-2.compute.internal>
+AWS ParallelCluster user <ec2-user@ip-172-31-15-28.ec2.internal>
 Benjamin Drung <bdrung@debian.org>
 Ben Menadue <ben.menadue@nci.org.au>
 Ben Turrubiates <bturrubiates@lanl.gov>
@@ -55,6 +59,7 @@ Gengbin Zheng <gengbin.zheng@intel.com>
 germanafro <andreasberger86@hotmail.de>
 Gilles Gouaillardet <gilles.gouaillardet@iferc.org>
 Gilles Gouaillardet <gilles@rist.or.jp>
+github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
 Goldman, Adam <adam.goldman@intel.com>
 Hefty <sean.hefty@intel.com>
 Holger Hoffstätte <holger@applied-asynchrony.com>
@@ -111,7 +116,9 @@ Michael Blocksome <michael.blocksome@intel.com>
 Michael Chuvelev <michael.chuvelev@intel.com>
 Mikhail Khalilov <miharulidze@gmail.com>
 Mikhail Khalilov <mikhail.khalilov@intel.com>
+Min Si <msi@anl.gov>
 Mohan Gandhi <mohgan@amazon.com>
+muttormark <mike.uttormark@hpe.com>
 Neil Spruit <neil.r.spruit@intel.com>
 Nicolas Morey-Chaisemartin <nmoreychaisemartin@suse.com>
 nikhilnanal <nikhilnanal1@gmail.com>
@@ -123,6 +130,7 @@ Nikola Dancejic <dancejic@amazon.com>
 Oblomov, Sergey <hoopoepg@gmail.com>
 Oblomov, Sergey <sergey.oblomov@intel.com>
 OFIWG Bot <ofiwg@lists.openfabrics.org>
+orbea <orbea@riseup.net>
 Paolo Inaudi <p91paul@gmail.com>
 patrickbueb <70724661+patrickbueb@users.noreply.github.com>
 Patrick Bueb <patrick.bueb@hpe.com>
@@ -137,6 +145,8 @@ Philip Davis <philipdavis01@gmail.com>
 Pierre Roux <piroux@cisco.com>
 Prankur Gupta <prankgup@cisco.com>
 Raghu Raja <craghun@amazon.com>
+Raghu Raja <raghu@enfabrica.net>
+Raghu Raja <rajachan@protonmail.com>
 Raghu Raja <rajachan@users.noreply.github.com>
 Reese Faucette <rfaucett@cisco.com>
 Richard Halkyard <rhalkyard@cray.com>
@@ -167,6 +177,7 @@ Todd Rimmer <todd.rimmer@intel.com>
 Tony Zinger <ajz@cray.com>
 tonyzinger <ajz@cray.com>
 Trevor Hendricks <trevorhe@amazon.com>
+Ubuntu <ubuntu@ip-172-31-15-224.ec2.internal>
 Venkata Krishna Nimmagadda <nvkrishna85@gmail.com>
 Venkata Krishna Nimmagadda <venkata.krishna.nimmagadda@intel.com>
 Wei Zhang <wzam@amazonc.com>
diff --git a/Makefile.am b/Makefile.am
index 187b5dd..d9e4268 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -76,6 +76,7 @@ common_srcs =				\
 	prov/util/src/util_mr_cache.c	\
 	prov/util/src/cuda_mem_monitor.c \
 	prov/util/src/rocr_mem_monitor.c \
+	prov/util/src/ze_mem_monitor.c \
 	prov/util/src/util_coll.c
 
 
@@ -185,7 +186,7 @@ src_libfabric_la_LIBADD =
 src_libfabric_la_DEPENDENCIES = libfabric.map
 
 if !EMBEDDED
-src_libfabric_la_LDFLAGS += -version-info 16:0:15
+src_libfabric_la_LDFLAGS += -version-info 17:0:16
 endif
 src_libfabric_la_LDFLAGS += -export-dynamic \
 			   $(libfabric_version_script)
@@ -196,6 +197,7 @@ rdmainclude_HEADERS += \
 	$(top_srcdir)/include/rdma/fi_collective.h \
 	$(top_srcdir)/include/rdma/fi_domain.h \
 	$(top_srcdir)/include/rdma/fi_eq.h \
+	$(top_srcdir)/include/rdma/fi_ext.h \
 	$(top_srcdir)/include/rdma/fi_rma.h \
 	$(top_srcdir)/include/rdma/fi_endpoint.h \
 	$(top_srcdir)/include/rdma/fi_errno.h \
@@ -224,6 +226,7 @@ real_man_pages = \
         man/man1/fi_info.1 \
         man/man1/fi_pingpong.1 \
         man/man1/fi_strerror.1 \
+        man/man3/fi_atomic.3 \
         man/man3/fi_av.3 \
         man/man3/fi_av_set.3 \
         man/man3/fi_cm.3 \
@@ -236,6 +239,7 @@ real_man_pages = \
         man/man3/fi_errno.3 \
         man/man3/fi_eq.3 \
         man/man3/fi_fabric.3 \
+        man/man3/fi_provider.3 \
         man/man3/fi_getinfo.3 \
         man/man3/fi_mr.3 \
         man/man3/fi_msg.3 \
@@ -256,7 +260,6 @@ dummy_man_pages = \
         man/man3/fi_allgather.3 \
         man/man3/fi_allreduce.3 \
         man/man3/fi_alltoall.3 \
-        man/man3/fi_atomic.3 \
         man/man3/fi_atomic_valid.3 \
         man/man3/fi_atomicmsg.3 \
         man/man3/fi_atomicv.3 \
@@ -425,6 +428,7 @@ include prov/hook/hook_debug/Makefile.include
 man_MANS = $(real_man_pages) $(prov_install_man_pages) $(dummy_man_pages)
 
 EXTRA_DIST += \
+        autogen.sh \
         NEWS.md \
         libfabric.spec.in \
         config/distscript.pl \
diff --git a/NEWS.md b/NEWS.md
index cb996d1..6897468 100644
--- a/NEWS.md
+++ b/NEWS.md
@@ -6,6 +6,220 @@ bug fixes (and other actions) for each version of Libfabric since
 version 1.0.  New major releases include all fixes from minor
 releases with earlier release dates.
 
+v1.13.0, Thu Jul 1, 2021
+========================
+
+## Core
+
+- Fix behavior of fi_param_get parsing an invalid boolean value
+- Add new APIs to open, export, and import specialized fid's
+- Define ability to import a monitor into the registration cache
+- Add API support for INT128/UINT128 atomics
+- Fix incorrect check for provider name in getinfo filtering path
+- Allow core providers to return default attributes which are lower then
+  maximum supported attributes in getinfo call
+- Add option prefer external providers (in order discovered) over internal
+  providers, regardless of provider version
+- Separate Ze (level-0) and DRM dependencies
+- Always maintain a list of all discovered providers
+- Fix incorrect CUDA warnings
+- Fix bug in cuda init/cleanup checking for gdrcopy support
+- Shift order providers are called from in fi_getinfo, move psm2 ahead of
+  psm3 and efa ahead of psmX
+
+## EFA
+
+- Minor code optimizations and bug fixed
+- Add support for fi_inject for RDM messages
+- Improve handling of RNR NACKs from NIC
+- Improve handling of zero copy receive case, especially when sender does not
+  post receive buffer
+- Numerous RMA read bug fixes
+- Add unexpected receive queue for each peer
+- Fixed issue releasing rx entries
+- Decrease the initial size of the out-of-order packeet pool allocation size
+  to reduce the common-case memory footprint
+- Handle FI_ADDR_NOTAVAIL in rxr_ep_get_peer
+- Identify and handle QP reuse
+- Use the memory monitor specified by the user
+- Replace provider code with common code in select places
+- Update efa_av_lookup to return correct address
+- Update rdm endpoint directly poll cq from ibv_cq
+- Avoid possible duplicate completions
+- Add reference counting for peer tracking
+- Fix EFA usage of util AV causing incorrect refcounting
+- Do not allow endpoints to share address vectors
+- Improve fork support; users can set the FI_EFA_FORK_SAFE environment variable
+  for applications which call fork()
+- Adjust the timing of clearing deferred memory registration list
+- Do not use eager protocol for cuda message and local peer
+- Fixes for shm support
+- Enable MR cache for CUDA
+- Disable shm when application requests FI_HMEM
+
+## PSM3
+
+- Added CUDA Support, GPU Direct through RV kernel module
+- Changed PSM3 Provider Version to match IEFS version
+- Expanded Multi-Rail support
+- Enhanced debug logging
+- Removed internal copy of libuuid, added as linked lib
+- Various Bug Fixes
+
+## RxD
+
+- Fix peer connection and address cleanup
+- Maintain peer connection after AV removal to send ACKs
+
+## RxM
+
+- Fix rx buffer leak in error case
+- Dynamically allocate buffer space for large unexpected messages
+- Separate the eager protocol size from allocated receive buffers
+  to reduce memory footprint
+- Make eager limit a per ep value, rather than global for all peers
+- Separate definitions and use of buffer, eager, and packet sizes
+- Fix calling fi_getinfo to the msg provider with FI_SOURCE set but
+  null parameters
+- General code cleanups, simplifications, and optimizations
+- Fix retrieving tag from dynamic receive buffer path
+- Enable dynamic receive buffer path over tcp by default
+- Use correct check to select between tagged and untagged rx queues
+- Repost rx buffers immediately to fix situation where applications can hang
+- Update help text for several environment variables
+- Fix use_srx check to enable srx by default layering over tcp provider
+- Reduce default tx/rx sizes to shrink memory footprint
+- Fix leaving stale peer entries in the AV
+- Handle error completions from the msg provider properly, and avoid passing
+  internal transfers up to the application
+- Reduce memory footprint by combining inject packets into one
+- Reduce inject copy overhead by using memcpy instead of hmem copy routines
+- Restrict the number of outstanding user transfers to prevent memory
+  overflow
+- Enable direct send feature by default for the tcp provider
+- Fix initialization of atomic headers
+- Only ignore interrupts in wait calls (e.g. poll) in debug builds, otherwise
+  return control to the caller
+- Combine and simplify internal buffer pools to reduce memory footprint
+- Remove request for huge pages for internal buffer pools
+- Add optimized tagged message path over tcp provider, removing need for
+  rxm header overhead
+- Several optimizations around supporting rxm over tcp provider
+
+## SHM
+
+- Use signal to reduce lock contention between processes
+- Fix communication with a peer that was restarted
+- Code cleanup to handle issues reported by coverity
+- Add check that IPC protocol is accessing device only memory
+- Fix interface selection used for IPC transfers
+- Change address to use a global ep index to support apps that open
+  multiple fabrics
+- Add environment variable to disable CMA transfers, to handle environments
+  where CMA checks may succeed, but CMA may not be usable
+- Add missing lock in ofi_av_insert_addr
+- Add support for GPU memory in inject operations.
+
+## Sockets
+
+- Fix possible ring buffer overflow calculating atomic lengths
+- Use correct address length (IPv6 vs 4) walking through address array
+
+## TCP
+
+- Add send side coalescing buffer to improve small message handling
+- Add receive side prefetch buffer to reduce kernel transitions
+- Fix initializing the mr_iov_limit domain attribute
+- Add support for zero copy transfers, with configurable threshold settings.
+  Disable zero copy by default due to negative impact on overall performance
+- Add environment variable overrides for default tx/rx sizes
+- Simplify and optimize handling of protocol headers
+- Add a priority transmit queue for internally generated messages (e.g. ACKs)
+- Check that the endpoint state is valid before attempting to drive progress
+  on the underlying socket
+- Limit the number of outstanding transmit and receive operations that a
+  user may post to an ep
+- Remove limitations on allocating internally generated messages to prevent
+  application hangs
+- Combine multiple internal buffer pools to one to reduce memory footprint
+- Optimize socket progress based on signaled events
+- Optimize pollfd abstraction to replace linear searches with direct indexing
+- Update both rx and tx cq's socket poll list to prevent application hangs
+- Optimize reading in extra headers to reduce loop overhead
+- Continue progressing transmit data until socket is full to reduce progress
+  overhead
+- Add msg id field to protocol headers (debug only) for protocol debugging
+- Drive rx progress when there's an unmatched 0-byte received message to
+  avoid application hangs
+- Avoid kernel transitions that are likely to do not work (return EAGAIN)
+- Fail try_wait call if there's data already queued in user space prefetch
+  buffers to avoid possible hangs
+- Fix possible access to freed tx entry
+- Optimize socket receive calls in progress function to skip progress loop
+  and immediately handle a received header.  This also fixes an application
+  hang handling 0-byte messages
+- Broad code cleanups, rework, and simplifications aimed at reducing
+  overhead and improving code stability
+- Improve handling of socket disconnect or fatal protocol errors
+- Fix reporting failures of internal messages to the user
+- Disable endpoints on fatal protocol errors
+- Validate response messages are what is expected
+- Simplify and align transmit, receive, and response handling to improve code
+  maintainability and simplify related data structures
+- Copy small messages through a coalescing buffer to avoid passing SGL to
+  the kernel
+- Fix race handling a disconnected event during the CM handshake
+- Report default attributes that are lower than the supported maximums
+- Remove use of huge pages, which aren't needed by tcp, to reserve them for
+  the user
+- Increase default inject size to be larger than the rxm header
+- Add tagged message protocol header for sending tagged messages using the
+  tcp headers only
+- Separate definition of maximum header size from maximum inject size
+
+## Util
+
+- Added lock validation checks to debug builds
+- Fix MR cache flush LRU behavior
+- Always remove dead memory regions from the MR cache immediately
+- Update buffer pools to handle an alignment of 0
+- Fail memory registration calls for HMEM if the interface isn't available
+- Pass through failures when a requested memory monitor fails to start
+- Always process deferred work list from pollfd wait abstraction
+
+## Verbs
+
+- Fixed checks setting CQ signaling vector
+- Internal code cleanups and clarifications
+- Fixed XRC MOFED 5.2 incompatibility
+- Add dmabuf MR support for GPU P2P transfers
+
+v1.12.1, Thu Apr 1, 2021
+========================
+
+## Core
+
+- Fix initialization checks for CUDA HMEM support
+- Fail if a memory monitor is requested but not available
+- Adjust priority of psm3 provider to prefer HW specific providers,
+  such as efa and psm2
+
+## EFA
+- Adjust timing clearing the deferred MR list to fix memory leak
+- Repost handshake packets on EAGAIN failure
+- Enable mr cache for CUDA memory
+- Support FI_HMEM and FI_LOCAL_COMM when used together
+- Skip using shm provider when FI_HMEM is requested
+
+## PSM3
+- Fix AVX2 configure check
+- Fix conflict with with-psm2-src build option to prevent duplicate
+  symbols
+- Fix checksum generation to support different builddir
+- Remove dependency on librdmacm header files
+- Use AR variable instead of calling ar directly in automake tools
+- Add missing PACK_SUFFIX to header
+
 v1.12.0, Mon Mar 8, 2021
 =========================
 
diff --git a/config/cron-run-all-md2nroff.pl b/config/cron-run-all-md2nroff.pl
deleted file mode 100755
index f1f0c78..0000000
--- a/config/cron-run-all-md2nroff.pl
+++ /dev/null
@@ -1,412 +0,0 @@
-#!/usr/bin/env perl
-
-# Script to pull down the latest markdown man pages from the libfabric
-# git repo.  Iterate over them, converting each to an nroff man page
-# and also copying+committing them to the gh-pages branch.  Finally,
-# git push them back upstream (so that Github will render + serve them
-# up as web pages).
-
-use strict;
-use warnings;
-
-use POSIX;
-use File::Basename;
-use Getopt::Long;
-use File::Temp;
-use JSON;
-use Data::Dumper;
-
-my $repo_arg;
-my $source_branch_arg;
-my $pages_branch_arg;
-my $logfile_dir_arg = "/tmp";
-my $pat_file_arg;
-my $verbose_arg;
-my $help_arg;
-
-my $ok = Getopt::Long::GetOptions("repo=s" => \$repo_arg,
-                                  "source-branch=s" => \$source_branch_arg,
-                                  "pages-branch=s" => \$pages_branch_arg,
-                                  "logfile-dir=s" => \$logfile_dir_arg,
-                                  "pat=s" => \$pat_file_arg,
-                                  "help|h" => \$help_arg,
-                                  "verbose" => \$verbose_arg,
-                                  );
-
-# Sanity checks
-die "Must specify a git repo"
-    if (!defined($repo_arg));
-die "Must specify a git source branch"
-    if (!defined($source_branch_arg));
-die "Must specify a Github Personal Access Token (PAT) file"
-    if (!defined($pat_file_arg));
-die "Github Personal Access Token (PAT) file unreadable"
-    if (! -r $pat_file_arg);
-
-#####################################################################
-
-open(FILE, $pat_file_arg) || die "Can't open Github Personal Access Token (PAT) file";
-my $pat = <FILE>;
-chomp($pat);
-close(FILE);
-
-$repo_arg =~ m/:(.+)\/(.+)\.git$/;
-my $gh_org = $1;
-my $gh_repo = $2;
-
-#####################################################################
-
-my $logfile_dir = $logfile_dir_arg;
-my $logfile_counter = 1;
-
-sub doit {
-    my $allowed_to_fail = shift;
-    my $cmd = shift;
-    my $stdout_file = shift;
-
-    # Redirect stdout if requested
-    if (defined $stdout_file) {
-        # Put a prefix on the logfiles so that we know that they
-        # belong to this script, and put a counter so that we know the
-        # sequence of logfiles
-        $stdout_file = "runall-md2nroff-$logfile_counter-$stdout_file";
-        ++$logfile_counter;
-
-        $stdout_file = "$logfile_dir/$stdout_file.log";
-        unlink($stdout_file);
-        $cmd .= " >$stdout_file";
-    } elsif (!$verbose_arg && $cmd !~ />/) {
-        $cmd .= " >/dev/null";
-    }
-    $cmd .= " 2>&1";
-
-    my $rc = system($cmd);
-    if (0 != $rc && !$allowed_to_fail) {
-        my_die("Command $cmd failed: exit status $rc");
-    }
-
-    system("cat $stdout_file")
-        if ($verbose_arg && defined($stdout_file) && -f $stdout_file);
-}
-
-sub verbose {
-    print @_
-        if ($verbose_arg);
-}
-
-sub my_die {
-    # Move out of our current cwd so that temp directories can be
-    # automatically cleaned up at close.
-    chdir("/");
-
-    die @_;
-}
-
-sub read_json_file {
-    my $filename = shift;
-    my $unlink_file = shift;
-
-    open(FILE, $filename);
-    my $contents;
-    while (<FILE>) {
-        $contents .= $_;
-    }
-    close(FILE);
-
-    unlink($filename)
-        if ($unlink_file);
-
-    return decode_json($contents);
-}
-
-#####################################################################
-
-# Setup a logfile dir just for this run
-my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) =
-    localtime(time);
-$logfile_dir =
-    sprintf("%s/cron-run-all-md2nroff-logs-%04d-%02d-%02d-%02d%02d",
-            $logfile_dir_arg, $year + 1900, $mon + 1, $mday,
-            $hour, $min);
-my $rc = system("mkdir $logfile_dir");
-if ($rc != 0 || ! -d $logfile_dir || ! -w $logfile_dir) {
-    my_die "mkdir of $logfile_dir failed, or can't write to it";
-}
-
-my $tmpdir = File::Temp->newdir();
-verbose("*** Working in: $tmpdir\n");
-chdir($tmpdir);
-
-# First, git clone the source branch of the repo
-verbose("*** Cloning repo: $repo_arg / $source_branch_arg...\n");
-doit(0, "git clone --single-branch --branch $source_branch_arg $repo_arg source", "git-clone-source");
-
-# Next, git clone the pages branch of repo
-if (defined($pages_branch_arg)) {
-    verbose("*** Cloning repo: $repo_arg / $pages_branch_arg...\n");
-    doit(0, "git clone --single-branch --branch $pages_branch_arg $repo_arg pages", "git-clone-pages");
-}
-
-#####################################################################
-# Look for all markdown man pages
-#####################################################################
-
-# Find all libfabric *.\d.md files
-verbose("*** Finding libfabric markdown man pages...\n");
-opendir(DIR, "source/man");
-my @libfabric_markdown_files =
-    map { "source/man/" . $_ }
-    grep { /\.\d\.md$/ && -f "source/man/$_" } readdir(DIR);
-closedir(DIR);
-verbose("Found: @libfabric_markdown_files\n");
-
-# Find all fabtests *.\d.md files
-verbose("*** Finding fabtests markdown man pages...\n");
-opendir(DIR, "source/fabtests/man");
-my @fabtests_markdown_files =
-    map { "source/fabtests/man/" . $_ }
-    grep { /\.\d\.md$/ && -f "source/fabtests/man/$_" } readdir(DIR);
-closedir(DIR);
-verbose("Found: @fabtests_markdown_files\n");
-
-#####################################################################
-# Publish any changes to man pages to the gh-pages branch
-# (only libfabric -- not fabtests)
-#####################################################################
-
-# Copy each of the markdown files to the pages branch checkout
-if (defined($pages_branch_arg)) {
-    chdir("pages/master");
-    foreach my $file (@libfabric_markdown_files) {
-        my $base = basename($file);
-        doit(0, "cp $tmpdir/$file man/$base", "loop-cp");
-
-        # Is there a new man page?  If so, we need to "git add" it.
-        my $out = `git status --porcelain man/$base`;
-        doit(0, "git add man/$base", "loop-git-add")
-            if ($out =~ /^\?\?/);
-    }
-
-    # Generate a new index.md with all the files that we just
-    # published.  First, read in the header stub.
-    open(IN, "man/index-head.txt") ||
-        my_die("failed to open index-head.txt");
-    my $str;
-    $str .= $_
-        while (<IN>);
-    close(IN);
-
-    # Write out the header stub into index.md itself
-    open(OUT, ">man/index.md") ||
-        my_die("failed to write to new index.md file");
-    print OUT $str;
-
-    # Now write out all the pages
-    my @headings;
-    push(@headings, { section=>7, title=>"General information" });
-    push(@headings, { section=>3, title=>"API documentation" });
-    foreach my $h (@headings) {
-        print OUT "\n* $h->{title}\n";
-        foreach my $file (sort(@libfabric_markdown_files)) {
-            my $base = basename($file);
-            if ($base =~ /\.$h->{section}\.md$/) {
-                $base =~ m/^(.+)\.$h->{section}\.md$/;
-                my $shortname = $1;
-                print OUT "  * [$shortname($h->{section})]($shortname.$h->{section}.html)\n";
-            }
-        }
-    }
-    close(OUT);
-
-    # Git commit those files in the pages repo and push them to the
-    # upstream repo so that they go live.  If nothing changed, the commit
-    # and push will be no-ops.
-    chdir("..");
-    doit(1, "git commit -s --no-verify -a -m \"Updated Markdown man pages from $source_branch_arg\"",
-         "git-commit-pages");
-    doit(1, "git push", "git-push-pages");
-}
-
-#####################################################################
-# Look for changes to .md files and generate nroff files on master
-#####################################################################
-
-my @markdown_files = (@libfabric_markdown_files,
-                      @fabtests_markdown_files);
-
-# Now process each of the Markdown files in the source repo and
-# generate new nroff man pages.
-chdir("$tmpdir");
-foreach my $file (@markdown_files) {
-    doit(0, "$tmpdir/source/config/md2nroff.pl --source $file", "loop2-md2nroff");
-}
-
-#####################################################################
-
-# Similar to above: commit the newly-generated nroff pages and push
-# them back upstream.  If nothing changed, these will be no-ops.  Note
-# that there are mandatory CI checks on master, which means we can't
-# push directly.  Instead, we must make a pull request.  Hence, don't
-# git commit directly to the pages branch here; make a branch and
-# commit there.
-
-# Try to delete the old pr branch first (it's ok to fail -- i.e., if
-# it wasn't there).
-chdir("$tmpdir/source");
-my $pr_branch_name = "pr/update-nroff-generated-man-pages";
-doit(1, "git branch -D $pr_branch_name");
-doit(0, "git checkout -b $pr_branch_name");
-
-# Do the commit.  Save the git HEAD hash before and after so that we
-# can tell if the "git commit" command actually resulted in a new
-# commit.
-my $old_head=`git rev-parse HEAD`;
-doit(1, "git commit -s --no-verify -a -m \"Updated nroff-generated man pages\"",
-     "git-commit-source-generated-man-pages");
-my $new_head=`git rev-parse HEAD`;
-
-# See if the commit was a no op or not.
-if ($old_head ne $new_head) {
-    chomp($new_head);
-
-    # Push the branch up to github
-    doit(0, "git push --force", "git-push-source-generated-man-pages");
-
-    # Get the list of files
-    open(GIT, 'git diff-tree --no-commit-id --name-only -r HEAD|') ||
-        my_die "Cannot git diff-tree";
-    my @files;
-    while (<GIT>) {
-        chomp;
-        push(@files, $_);
-    }
-    close(GIT);
-
-    # Create a new pull request
-    my $cmd_base = "curl --silent ";
-    $cmd_base .= "-H 'Content-Type: application/json' ";
-    $cmd_base .= "-H 'Authorization: token $pat' ";
-    $cmd_base .= "-H 'User-Agent: OFIWG-bot' ";
-
-    my $outfile = 'curl-out.json';
-    unlink($outfile);
-
-    my $body;
-    $body = "The Nroff Elves created these man pages, just for you:\n\n";
-    foreach my $f (@files) {
-        $body .= "* `$f`\n";
-    }
-
-    my $json = {
-        title => 'Update nroff-generated man pages',
-        body  => $body,
-        head  => $pr_branch_name,
-        base  => 'master',
-    };
-    my $json_encoded = encode_json($json);
-
-    my $cmd = $cmd_base;
-    $cmd .= "--request POST ";
-    $cmd .= "--data '$json_encoded' ";
-    $cmd .= "https://api.github.com/repos/$gh_org/$gh_repo/pulls ";
-    $cmd .= "-o $outfile";
-    doit(0, $cmd, "github-create-pr");
-
-    # Read the resulting file to find whether the PR creation
-    # succeeded, and if so, what the URL of the new PR is.
-    $json = read_json_file($outfile, 1);
-    if (!exists($json->{'id'}) || !exists($json->{'url'})) {
-        my_die "Failed to create PR";
-    }
-
-    my $pr_url = $json->{'url'};
-    my $pr_num = $json->{'number'};
-    verbose("Created PR #$pr_num\n");
-
-    # Wait for the required DCO check to complete on the git hash for
-    # the latest commit.
-    $outfile = "github-ci-status-check.json";
-
-    $cmd = $cmd_base;
-    $cmd .= "-o $outfile ";
-    $cmd .= "-H 'Accept: application/vnd.github.antiope-preview+json' ";
-    $cmd .= "https://api.github.com/repos/$gh_org/$gh_repo/commits/$new_head/check-runs";
-
-    my $count = 0;
-    my $max_count = 30;
-    my $happy = 0;
-    verbose("Waiting for DCO check to complete\n");
-
-    # Only wait for $max_count iterations
-    while (!$happy && $count < $max_count) {
-        # Give the DCO hook time to run
-        sleep(1);
-
-        unlink($outfile);
-        doit(0, $cmd, "github-check-run-status");
-        my $json = read_json_file($outfile, 1);
-
-        if ($json and $#{$json->{"check_runs"}} >= 0) {
-            # If we got any statuses back, check them to see if we can
-            # find a successful DCO signoff.  That would indicate that
-            # the required check test ran.
-            foreach my $j (@{$json->{"check_runs"}}) {
-                if ($j->{"name"} eq "DCO") {
-                    verbose("Found DCO status on SHA $new_head\n");
-                    if ($j->{"status"} eq "completed") {
-                        if ($j->{"conclusion"} eq "success") {
-                            verbose("DCO is happy!\n");
-                            $happy = 1;
-                            last;
-                        } else {
-                            verbose("DCO is not happy -- how did that happen?\n");
-                            $happy = 0;
-                            last;
-                        }
-                    }
-                }
-            }
-        }
-
-        $count += 1;
-    }
-
-    my_die("Could not find a happy DCO status on $new_head")
-        if (!$happy);
-
-    # If we get here, it means the DCO CI is done/happy, so we can
-    # merge the PR.
-    $json = {
-        commit_title   => "Merge pull request #$pr_num",
-        commit_message => "More tasty nroff man pages for you, fresh out of the oven!",
-        sha            => $new_head,
-        merge_method   => "merge",
-    };
-    $json_encoded = encode_json($json);
-
-    $outfile = "github-per-merge.json";
-    unlink($outfile);
-
-    $cmd = $cmd_base;
-    $cmd .= "--request PUT ";
-    $cmd .= "--data '$json_encoded' ";
-    $cmd .= "-o $outfile ";
-    $cmd .= "$pr_url/merge";
-    doit(0, $cmd, "github-create-pr");
-
-    # Remove the remote branch
-    doit(1, "git push origin --delete $pr_branch_name", 'git-remove-remote-branch');
-}
-
-# Delete the local pull request branch
-doit(0, "git checkout master");
-doit(1, "git branch -D $pr_branch_name");
-
-# chdir out of the tmpdir so that it can be removed
-chdir("/");
-
-# If we get here, we finished successfully, so there's no need to keep
-# the logfile dir around
-system("rm -rf $logfile_dir");
-
-exit(0);
diff --git a/config/distscript.pl b/config/distscript.pl
index 25b4483..fd49943 100755
--- a/config/distscript.pl
+++ b/config/distscript.pl
@@ -29,17 +29,8 @@ sub subst {
     close(IN);
 
     my $copy = $orig;
-    $copy =~ s/\@VERSION\@/Libfabric v$version/g;
-    $copy =~ s/\@DATE\@/$today/g;
-
-    # Note that there appears to be a bug in some versions of Pandoc
-    # that will escape the appearance of @ in generated man pages
-    # (e.g., in the "@VERSION@" that appears in the man page version
-    # field).  So rather than be clever in the regexp's above, do the
-    # simple/clear thing and repeat the same regexp's as above, but
-    # with double-escaped @'s.
-    $copy =~ s/\\\@VERSION\\\@/Libfabric v$version/g;
-    $copy =~ s/\\\@DATE\\\@/$today/g;
+    $copy =~ s/#VERSION#/Libfabric v$version/g;
+    $copy =~ s/#DATE#/$today/g;
 
     if ($copy ne $orig) {
         print "*** VERSION/DATE-ifying $file...\n";
diff --git a/config/fi_provider.m4 b/config/fi_provider.m4
index e01e337..eae2d8a 100644
--- a/config/fi_provider.m4
+++ b/config/fi_provider.m4
@@ -68,7 +68,13 @@ dnl
 					and use $1 installed under PATH)])
 			     ],
 			     [],
-			     [enable_$1=auto])])
+			     [AS_IF([test x"$enable_only" != x"no"],
+			            [AC_MSG_NOTICE([*** Skipping $1 because $enable_only set])
+			             enable_$1=no],
+			            [enable_$1=auto])
+			    ])
+	      ])
+
 
 	# Save CPPFLAGS and LDFLAGS before they are modified by FI_CHECK_PREFIX_DIR.
 	# Provider's local macros could use the value if needed.
diff --git a/config/fi_strip_optflags.m4 b/config/fi_strip_optflags.m4
new file mode 100644
index 0000000..3911b07
--- /dev/null
+++ b/config/fi_strip_optflags.m4
@@ -0,0 +1,62 @@
+dnl -*- shell-script -*-
+dnl
+dnl Copyright (c) 2004-2005 The Trustees of Indiana University and Indiana
+dnl                         University Research and Technology
+dnl                         Corporation.  All rights reserved.
+dnl Copyright (c) 2004-2005 The University of Tennessee and The University
+dnl                         of Tennessee Research Foundation.  All rights
+dnl                         reserved.
+dnl Copyright (c) 2004-2005 High Performance Computing Center Stuttgart,
+dnl                         University of Stuttgart.  All rights reserved.
+dnl Copyright (c) 2004-2005 The Regents of the University of California.
+dnl                         All rights reserved.
+dnl Copyright (c) 2008      Cisco Systems, Inc.  All rights reserved.
+dnl Copyright (c) 2008-2009 Sun Microsystems, Inc.  All rights reserved.
+dnl Copyright (c) 2014-2021 Intel, Inc. All rights reserved.
+dnl $COPYRIGHT$
+dnl
+dnl Additional copyrights may follow
+dnl
+dnl $HEADER$
+dnl
+
+dnl
+dnl This file derived from config/opal_strip_optflags.m4 in Open MPI.
+dnl
+dnl Example Usage:
+dnl      FI_STRIP_OPTFLAGS($CFLAGS)
+dnl      CFLAGS_WITHOUT_OPTFLAGS="$s_result"
+
+AC_DEFUN([FI_STRIP_OPTFLAGS],[
+
+# Process a set of flags and remove all debugging and optimization
+# flags
+
+s_arg="$1"
+s_result=
+for s_word in $s_arg; do
+    # See http://www.gnu.org/software/autoconf/manual/html_node/Quadrigraphs.html#Quadrigraphs
+    # for an explanation of @<:@ and @:>@ -- they m4 expand to [ and ]
+    case $s_word in
+    -g)                 ;;
+    -g@<:@1-3@:>@)      ;;
+    +K@<:@0-5@:>@)      ;;
+    -O)                 ;;
+    -O@<:@0-9@:>@)      ;;
+    -xO)                ;;
+    -xO@<:@0-9@:>@)     ;;
+    -fast)              ;;
+    -finline-functions) ;;
+
+    # The below Sun Studio flags require or
+    # trigger -xO optimization
+    -xvector*)          ;;
+    -xdepend=yes)       ;;
+
+    *)     s_result="$s_result $s_word"
+    esac
+done
+
+# Clean up
+
+unset s_word s_arg])
diff --git a/config/md2nroff.pl b/config/md2nroff.pl
index 18ce667..b0fef78 100755
--- a/config/md2nroff.pl
+++ b/config/md2nroff.pl
@@ -99,9 +99,9 @@ while ($pandoc_input =~ m/\[(.+?)\]\(.+?\)/) {
 }
 
 # Add the pandoc header
-$pandoc_input = "% $shortfile($section) Libfabric Programmer's Manual | \@VERSION\@
+$pandoc_input = "% $shortfile($section) Libfabric Programmer's Manual | #VERSION#
 % OpenFabrics
-% \@DATE\@\n\n$pandoc_input";
+% #DATE#\n\n$pandoc_input";
 
 # Generate the nroff output
 my ($fh, $temp_filename) = tempfile();
@@ -132,8 +132,8 @@ if (-r $target) {
     # compare and ignore if the date has changed.  Note that some
     # versions of pandoc render dates as xxxx\-xx\-xx, and others
     # render it as xxxx-xx-xx.  Handle both.
-    $target_nroff =~ s/\"\d\d\d\d\\\-\d\d\\\-\d\d\"/\"\\\@DATE\\\@\"/;
-    $target_nroff =~ s/\"\d\d\d\d\-\d\d\-\d\d\"/\"\\\@DATE\\\@\"/;
+    $target_nroff =~ s/\"\d\d\d\d\\\-\d\d\\\-\d\d\"/\"#DATE#\"/;
+    $target_nroff =~ s/\"\d\d\d\d\-\d\d\-\d\d\"/\"#DATE#\"/;
 
     $write_nroff = 0
         if ($pandoc_nroff eq $target_nroff);
@@ -144,7 +144,7 @@ if ($write_nroff) {
 
     # What's the date right now?
     my $now_string = strftime "%Y\\-%m\\-%d", localtime;
-    $pandoc_nroff =~ s/\\\@DATE\\\@/$now_string/g;
+    $pandoc_nroff =~ s/#DATE#/$now_string/g;
 
     # Make sure the target directory exists
     my $dirname = dirname($target);
diff --git a/configure.ac b/configure.ac
index b24b5ca..5f95b29 100644
--- a/configure.ac
+++ b/configure.ac
@@ -7,7 +7,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ([2.60])
-AC_INIT([libfabric], [1.12.0], [ofiwg@lists.openfabrics.org])
+AC_INIT([libfabric], [1.14.0a1], [ofiwg@lists.openfabrics.org])
 AC_CONFIG_SRCDIR([src/fabric.c])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
@@ -15,6 +15,7 @@ AC_CONFIG_HEADERS(config.h)
 AM_INIT_AUTOMAKE([1.11 dist-bzip2 foreign -Wall -Werror subdir-objects parallel-tests tar-pax])
 m4_ifdef([AM_SILENT_RULES], [AM_SILENT_RULES([yes])])
 m4_include(config/fi_check_package.m4)
+m4_include(config/fi_strip_optflags.m4)
 
 AC_CANONICAL_HOST
 
@@ -85,6 +86,12 @@ AC_ARG_ENABLE([direct],
 	[],
 	[enable_direct=no])
 
+AC_ARG_ENABLE([only],
+	[AS_HELP_STRING([--enable-only],
+		[Only build explicitly specified fabric providers])
+	],
+	[],
+	[enable_only=no])
 
 AC_ARG_ENABLE([atomics],
 	[AS_HELP_STRING([--enable-atomics],
@@ -157,7 +164,7 @@ AC_ARG_WITH([dlopen],
 		       [dl-loadable provider support @<:@default=yes@:>@]),
 	)
 
-if test "$freebsd" == "0"; then
+if test "$freebsd" = "0"; then
 AS_IF([test x"$with_dlopen" != x"no"], [
 AC_CHECK_LIB(dl, dlopen, [],
     AC_MSG_ERROR([dlopen not found.  libfabric requires libdl.]))
@@ -520,7 +527,7 @@ AC_ARG_ENABLE([cuda-dlopen],
         [Enable dlopen of CUDA libraries @<:@default=no@:>@])
     ],
     [
-        AS_IF([test "$freebsd" == "0"], [
+        AS_IF([test "$freebsd" = "0"], [
             AC_CHECK_LIB(dl, dlopen, [],
                 [AC_MSG_ERROR([dlopen not found.  libfabric requires libdl.])])
         ])
@@ -533,6 +540,7 @@ AC_ARG_WITH([ze],
 					 libraries and headers are installed.]),
 	[], [])
 
+have_ze=0
 AS_IF([test x"$with_ze" != x"no"],
       [FI_CHECK_PACKAGE([ze],
 			[level_zero/ze_api.h],
@@ -541,13 +549,40 @@ AS_IF([test x"$with_ze" != x"no"],
 			[],
 			[$with_ze],
 			[],
-			[AC_DEFINE([HAVE_LIBZE], [1],[ZE support])],
-			[], [])
-       CPPFLAGS="$CPPFLAGS $ze_CPPFLAGS"
-       LDFLAGS="$LDFLAGS $ze_LDFLAGS"
-       LIBS="$LIBS $ze_LIBS"],
+			[have_ze=1],
+			[], [])],
+      [])
+
+have_drm=0
+AS_IF([test "$have_ze" = "1"],
+      [AC_CHECK_HEADER(drm/i915_drm.h, [have_drm=1], [])]
       [])
 
+AS_IF([test x"$with_ze" != x"no" && test -n "$with_ze" && test "$have_ze" = "0" ],
+	[AC_MSG_ERROR([ZE support requested but ZE runtime not available.])],
+	[])
+
+AC_DEFINE_UNQUOTED([HAVE_LIBZE], [$have_ze], [ZE support])
+AC_DEFINE_UNQUOTED([HAVE_DRM], [$have_drm], [i915 DRM header])
+
+AC_ARG_ENABLE([ze-dlopen],
+    [AS_HELP_STRING([--enable-ze-dlopen],
+        [Enable dlopen of ZE libraries @<:@default=no@:>@])
+    ],
+    [
+        AS_IF([test "$freebsd" = "0"], [
+            AC_CHECK_LIB(dl, dlopen, [],
+                [AC_MSG_ERROR([dlopen not found.  libfabric requires libdl.])])
+        ])
+        AC_DEFINE([ENABLE_ZE_DLOPEN], [1], [dlopen ZE libraries])
+    ],
+    [enable_ze_dlopen=no])
+
+AS_IF([test x"$enable_ze_dlopen" != x"yes"], [LIBS="$LIBS $ze_LIBS"])
+AS_IF([test "$have_ze" = "1" && test x"$with_ze" != x"yes"],
+      [CPPFLAGS="$CPPFLAGS $ze_CPPFLAGS"
+       LDFLAGS="$LDFLAGS $ze_LDFLAGS"])
+
 enable_memhooks=1
 AC_ARG_ENABLE([memhooks-monitor],
               [AC_HELP_STRING([--disable-memhooks-monitor],
@@ -558,7 +593,7 @@ AC_ARG_ENABLE([memhooks-monitor],
 AC_DEFINE_UNQUOTED(ENABLE_MEMHOOKS_MONITOR, [$enable_memhooks],
 	[Define to 1 to enable memhooks memory monitor])
 
-AS_IF([test "$enable_memhooks" == "1"], [
+AS_IF([test "$enable_memhooks" = "1"], [
 	AC_CHECK_FUNCS([__curbrk __clear_cache])
 	AC_CHECK_HEADERS([linux/mman.h sys/syscall.h])
 	AC_CHECK_DECLS([__syscall], [], [], [#include <sys/syscall.h>])
@@ -602,13 +637,13 @@ AC_ARG_WITH([gdrcopy],
 			    and runtime libraries are installed.])],
 	    [], [])
 
-AS_IF([test -n "$with_gdrcopy" && test x"$with_gdrcopy" != x"no" && test "$have_libcuda" == "0"],
+AS_IF([test -n "$with_gdrcopy" && test x"$with_gdrcopy" != x"no" && test "$have_libcuda" = "0"],
 	[AC_MSG_ERROR([gdrcopy is requested but cuda is not requested or cuda runtime is not available.])],
 	[])
 
 have_gdrcopy=0
-AS_IF([test "$have_libcuda" == "1" && test x"$with_gdrcopy" != x"no"],
-	[AS_IF([test x"$with_gdrcopy" == x"yes"],[gdrcopy_dir=""],[gdrcopy_dir=$with_gdrcopy])
+AS_IF([test "$have_libcuda" = "1" && test x"$with_gdrcopy" != x"no"],
+	[AS_IF([test x"$with_gdrcopy" = x"yes"],[gdrcopy_dir=""],[gdrcopy_dir=$with_gdrcopy])
 	 FI_CHECK_PACKAGE([gdrcopy],
 			  [gdrapi.h],
 			  [gdrapi],
@@ -632,7 +667,7 @@ AC_ARG_ENABLE([gdrcopy-dlopen],
         [Enable dlopen of gdrcopy libraries @<:@default=no@:>@])
     ],
     [
-        AS_IF([test "$freebsd" == "0"], [
+        AS_IF([test "$freebsd" = "0"], [
             AC_CHECK_LIB(dl, dlopen, [],
                 [AC_MSG_ERROR([dlopen not found.  libfabric requires libdl.])])
         ])
@@ -657,7 +692,7 @@ AC_ARG_ENABLE([rocr-dlopen],
         [Enable dlopen of ROCR libraries @<:@default=no@:>@])
     ],
     [
-        AS_IF([test "$freebsd" == "0"], [
+        AS_IF([test "$freebsd" = "0"], [
             AC_CHECK_LIB(dl, dlopen, [],
                 [AC_MSG_ERROR([dlopen not found.  libfabric requires libdl.])])
         ])
@@ -739,7 +774,7 @@ fi
 
 for i in $PROVIDERS_TO_BUILD; do
 	v=${i}_dl
-	if test `eval echo \\$${v}` == "1"; then
+	if test `eval echo \\$${v}` = "1"; then
 		dso="$i ${dso}"
 	else
 		builtin="$i ${builtin}"
diff --git a/contrib/buildrpm/README b/contrib/buildrpm/README
index 98fc95a..f25a544 100644
--- a/contrib/buildrpm/README
+++ b/contrib/buildrpm/README
@@ -47,6 +47,10 @@ Provider parameters:
 
 
 General parameters:
+-b
+    Build binary packages only
+    By default, build binary and source packages
+
 -n
     Do nothing, useful with -v option. If used with -v option,
     it will just print what would have been done.
diff --git a/contrib/buildrpm/buildrpmLibfabric.sh b/contrib/buildrpm/buildrpmLibfabric.sh
index dd3c3ba..9202932 100755
--- a/contrib/buildrpm/buildrpmLibfabric.sh
+++ b/contrib/buildrpm/buildrpmLibfabric.sh
@@ -51,6 +51,7 @@ create_modulefile=""
 unpack_spec=""
 verbose=""
 verboseoption=""
+build_binary_only=""
 st=""
 version=""
 modulepath=""
@@ -116,6 +117,9 @@ usage="Usage: $0 [-i provider_name] [-e provider_name]
              exclude 'provider_name' provider support from the build
 
  General options:
+  -b         build binary packages only
+               {default: build binary and source packages}
+
   -n         no op, do nothing (useful with -v option)
 
   -o         install under /opt/libfabric/_VERSION_
@@ -163,8 +167,10 @@ usage="Usage: $0 [-i provider_name] [-e provider_name]
 # parse args
 ############
 export arguments="$@"
-while getopts DP:M:V:nolmi:e:dc:r:svh flag; do
+while getopts DP:M:V:nolmi:e:dc:r:svhb flag; do
     case "$flag" in
+      b) build_binary_only="true"
+         ;;
       n) noop="true"
          ;;
       o) install_in_opt="true"
@@ -335,7 +341,14 @@ if [[ -z "$verbose" ]]; then
 else
   build_opt="-v"
 fi
-cmd="rpmbuild $build_opt -bb $specfile $rpmbuild_options \
+
+if [[ -n "$build_binary_only" ]] ; then
+    rpmbuild_flag="-bb"
+else
+    rpmbuild_flag="-ba"
+fi
+
+cmd="rpmbuild $build_opt $rpmbuild_flag $specfile $rpmbuild_options \
   --define '_topdir $rpmbuilddir' \
   --define '_sourcedir $rpmbuilddir/SOURCES' \
   --define '_rpmdir $rpmbuilddir/RPMS' \
diff --git a/contrib/intel/jenkins/run.py b/contrib/intel/jenkins/run.py
index 7873f0f..8dd0d8f 100755
--- a/contrib/intel/jenkins/run.py
+++ b/contrib/intel/jenkins/run.py
@@ -19,17 +19,17 @@ bno = os.environ['BUILD_NUMBER']#args.buildno
 
 #run fi_info test
 def fi_info_test(core, hosts, mode,util=None):
-    
+
     fi_info_test = tests.FiInfoTest(jobname=jbname,buildno=bno,\
                     testname="fi_info", core_prov=core, fabric=fab,\
                          hosts=hosts, ofi_build_mode=mode, util_prov=util)
     print("running fi_info test for {}-{}-{}".format(core, util, fab))
     fi_info_test.execute_cmd()
-        
+
 
 #runfabtests
 def fabtests(core, hosts, mode, util=None):
-       
+
     runfabtest = tests.Fabtest(jobname=jbname,buildno=bno,\
                  testname="runfabtests", core_prov=core, fabric=fab,\
                  hosts=hosts, ofi_build_mode=mode, util_prov=util)
@@ -41,7 +41,7 @@ def fabtests(core, hosts, mode, util=None):
         print("skipping {} as execute condition fails"\
               .format(runfabtest.testname))
     print("----------------------------------------------------------------------------------------\n")
-    
+
 def shmemtest(core, hosts, mode, util=None):
     runshmemtest = tests.ShmemTest(jobname=jbname,buildno=bno,\
                  testname="shmem test", core_prov=core, fabric=fab,\
@@ -59,7 +59,7 @@ def shmemtest(core, hosts, mode, util=None):
         print("skipping {} as execute condition fails"\
               .format(runshmemtest.testname))
     print("----------------------------------------------------------------------------------------\n")
-    
+
 
 #imb-tests
 def intel_mpi_benchmark(core, hosts, mpi, mode, util=None):
@@ -67,7 +67,7 @@ def intel_mpi_benchmark(core, hosts, mpi, mode, util=None):
     imb_test = tests.MpiTestIMB(jobname=jbname,buildno=bno,\
                testname="IntelMPIbenchmark",core_prov=core, fabric=fab,\
                hosts=hosts, mpitype=mpi, ofi_build_mode=mode, util_prov=util)
-    
+
     if (imb_test.execute_condn == True  and imb_test.mpi_gen_execute_condn == True):
         print("running imb-tests for {}-{}-{}-{}".format(core, util, fab, mpi))
         imb_test.execute_cmd()
@@ -87,29 +87,15 @@ def mpich_test_suite(core, hosts, mpi, mode, util=None):
         print("Running mpich test suite: Spawn coll, comm, dt Tests for {}-{}-{}-{}".format(core, util, fab, mpi))
         os.environ["MPITEST_RETURN_WITH_CODE"] = "1"
         mpich_tests.execute_cmd("spawn")
- 
-#mpi_stress benchmark tests
-def mpistress_benchmark(core, hosts, mpi, mode, util=None):
-
-    stress_test = tests.MpiTestStress(jobname=jbname,buildno=bno,\
-                  testname="stress",core_prov=core, fabric=fab, mpitype=mpi,\
-                  hosts=hosts, ofi_build_mode=mode, util_prov=util)
- 
-    if (stress_test.execute_condn == True and stress_test.mpi_gen_execute_condn == True):
-        print("running mpistress-test for {}-{}-{}-{}".format(core, util, fab, mpi))
-        stress_test.execute_cmd()
-    else:
-        print("skipping {} as execute condition fails" \
-                    .format(stress_test.testname))
     print("----------------------------------------------------------------------------------------\n")
 
-#osu benchmark tests    
+#osu benchmark tests
 def osu_benchmark(core, hosts, mpi, mode, util=None):
 
     osu_test = tests.MpiTestOSU(jobname=jbname, buildno=bno, \
                testname="osu-benchmarks",core_prov=core, fabric=fab, mpitype=mpi, \
                hosts=hosts, ofi_build_mode=mode, util_prov=util)
-    
+
     if (osu_test.execute_condn == True and osu_test.mpi_gen_execute_condn == True):
         print("running osu-test for {}-{}-{}-{}".format(core, util, fab, mpi))
         osu_test.execute_cmd()
diff --git a/contrib/intel/jenkins/runtests.py b/contrib/intel/jenkins/runtests.py
index c3cfe28..c950706 100755
--- a/contrib/intel/jenkins/runtests.py
+++ b/contrib/intel/jenkins/runtests.py
@@ -28,7 +28,7 @@ node = (os.environ['NODE_NAME']).split('-')[0]
 hosts = [node]
 # Note: Temporarily disabling all mpich testing
 # due to mpich options issues which is causing
-# multiple tests to fail. 
+# multiple tests to fail.
 #mpilist = ['impi', 'mpich', 'ompi']
 mpilist = ['impi', 'ompi']
 
@@ -36,7 +36,7 @@ mpilist = ['impi', 'ompi']
 #this is done since some mpi tests
 #look for a valid location before running
 # the test on the secondary host(client)
-# but jenkins only creates a valid path on 
+# but jenkins only creates a valid path on
 # the primary host (server/test node)
 
 os.chdir('/tmp/')
@@ -51,9 +51,8 @@ if(args_core):
         run.shmemtest(args_core, hosts, ofi_build_mode)
         for mpi in mpilist:
             run.mpich_test_suite(args_core, hosts, mpi, ofi_build_mode)
-            run.intel_mpi_benchmark(args_core, hosts, mpi, ofi_build_mode)   
-            run.mpistress_benchmark(args_core, hosts, mpi, ofi_build_mode)
-            run.osu_benchmark(args_core, hosts, mpi, ofi_build_mode)  
+            run.intel_mpi_benchmark(args_core, hosts, mpi, ofi_build_mode)
+            run.osu_benchmark(args_core, hosts, mpi, ofi_build_mode)
     else:
         run.fi_info_test(args_core, hosts, ofi_build_mode, util=args_util)
         run.fabtests(args_core, hosts, ofi_build_mode, util=args_util)
@@ -64,10 +63,8 @@ if(args_core):
 
             run.intel_mpi_benchmark(args_core, hosts, mpi, ofi_build_mode, \
                                     util=args_util)
-            run.mpistress_benchmark(args_core, hosts, mpi, ofi_build_mode, \
-                                    util=args_util)
             run.osu_benchmark(args_core, hosts, mpi, ofi_build_mode, \
                                              util=args_util)
 else:
     print("Error : Specify a core provider to run tests")
-    
+
diff --git a/contrib/intel/jenkins/tests.py b/contrib/intel/jenkins/tests.py
index a3e670b..10a5b05 100755
--- a/contrib/intel/jenkins/tests.py
+++ b/contrib/intel/jenkins/tests.py
@@ -22,7 +22,7 @@ class Test:
         self.buildno = buildno
         self.testname = testname
         self.core_prov = core_prov
-        self.util_prov = "ofi_{}".format(util_prov) if util_prov != None else "" 
+        self.util_prov = "ofi_{}".format(util_prov) if util_prov != None else ""
         self.fabric = fabric
         self.hosts = hosts
         self.ofi_build_mode = ofi_build_mode
@@ -30,11 +30,11 @@ class Test:
         if (len(hosts) == 2):
             self.server = hosts[0]
             self.client = hosts[1]
-       
+
         self.nw_interface = ci_site_config.interface_map[self.fabric]
         self.libfab_installpath = "{}/{}/{}/{}".format(ci_site_config.install_dir,
                                   self.jobname, self.buildno, self.ofi_build_mode)
- 
+
         self.env = [("FI_VERBS_MR_CACHE_ENABLE", "1"),\
                     ("FI_VERBS_INLINE_SIZE", "256")] \
                     if self.core_prov == "verbs" else []
@@ -44,106 +44,106 @@ class FiInfoTest(Test):
 
         super().__init__(jobname, buildno, testname, core_prov, fabric,
                      hosts, ofi_build_mode, util_prov)
-     
-        self.fi_info_testpath =  "{}/bin".format(self.libfab_installpath) 
-     
+
+        self.fi_info_testpath =  "{}/bin".format(self.libfab_installpath)
+
     @property
     def cmd(self):
         return "{}/fi_info ".format(self.fi_info_testpath)
 
     @property
-    def options(self):       
+    def options(self):
         if (self.util_prov):
             opts  = "-f -p {};{}".format(self.core_prov, self.util_prov)
         else:
             opts = "-f -p {}".format(self.core_prov)
-        
-        return opts 
-    
+
+        return opts
+
     def execute_cmd(self):
         command = self.cmd + self.options
         outputcmd = shlex.split(command)
-        common.run_command(outputcmd)         
-     
+        common.run_command(outputcmd)
+
 
 class Fabtest(Test):
-    
+
     def __init__(self, jobname, buildno, testname, core_prov, fabric,
                  hosts, ofi_build_mode, util_prov=None):
-        
+
         super().__init__(jobname, buildno, testname, core_prov, fabric,
                          hosts, ofi_build_mode, util_prov)
-        self.fabtestpath = "{}/bin".format(self.libfab_installpath) 
+        self.fabtestpath = "{}/bin".format(self.libfab_installpath)
         self.fabtestconfigpath = "{}/share/fabtests".format(self.libfab_installpath)
     def get_exclude_file(self):
         path = self.libfab_installpath
         efile_path = "{}/share/fabtests/test_configs".format(path)
 
         prov = self.util_prov if self.util_prov else self.core_prov
-        efile_old = "{path}/{prov}/{prov}.exclude".format(path=efile_path, 
+        efile_old = "{path}/{prov}/{prov}.exclude".format(path=efile_path,
                       prov=prov)
-        
+
         if self.util_prov:
             efile = "{path}/{util_prov}/{core_prov}/exclude".format(path=efile_path,
                       util_prov=self.util_prov, core_prov=self.core_prov)
         else:
             efile = "{path}/{prov}/exclude".format(path=efile_path,
                       prov=self.core_prov)
-           
+
         if os.path.isfile(efile):
             return efile
         elif os.path.isfile(efile_old):
             return efile_old
         else:
             print("Exclude file: {} not found!".format(efile))
-            return None  
+            return None
 
-    @property    
-    def cmd(self):    
+    @property
+    def cmd(self):
         return "{}/runfabtests.sh ".format(self.fabtestpath)
-     
+
     @property
     def options(self):
         opts = "-T 300 -vvv -p {} -S ".format(self.fabtestpath)
         if (self.core_prov == "verbs" and self.nw_interface):
-            opts = "{} -s {} ".format(opts, common.get_node_name(self.server, 
+            opts = "{} -s {} ".format(opts, common.get_node_name(self.server,
                     self.nw_interface)) # include common.py
-            opts = "{} -c {} ".format(opts, common.get_node_name(self.client, 
+            opts = "{} -c {} ".format(opts, common.get_node_name(self.client,
                     self.nw_interface)) # from common.py
-       
+
         if (self.core_prov == "shm"):
             opts = "{} -s {} ".format(opts, self.server)
             opts = "{} -c {} ".format(opts, self.client)
             opts += "-N "
-            
-        if not re.match(".*sockets|udp|tcp.*", self.core_prov):
+
+        if not re.match(".*sockets|udp.*", self.core_prov):
             opts = "{} -t all ".format(opts)
 
         efile = self.get_exclude_file()
         if efile:
             opts = "{} -R ".format(opts)
-            opts = "{} -f {} ".format(opts, efile)  
-        
+            opts = "{} -f {} ".format(opts, efile)
+
         for key,val in self.env:
-            opts = "{options} -E {key}={value} ".format(options = opts, 
+            opts = "{options} -E {key}={value} ".format(options = opts,
                     key=key, value=val)
-    
+
         if self.util_prov:
-            opts = "{options} {core};{util} ".format(options=opts, 
+            opts = "{options} {core};{util} ".format(options=opts,
                     core=self.core_prov, util=self.util_prov)
         else:
             opts = "{options} {core} ".format(options=opts,
                     core=self.core_prov)
-        
+
         if (self.core_prov == "shm"):
             opts += "{} {} ".format(self.server, self.server)
         else:
             opts += "{} {} ".format(self.server, self.client)
-             
+
         return opts
-   
+
     @property
-    def execute_condn(self):     
+    def execute_condn(self):
         return True if (self.core_prov != 'shm' or \
                         self.ofi_build_mode == 'dbg') else False
 
@@ -158,13 +158,13 @@ class Fabtest(Test):
 class ShmemTest(Test):
     def __init__(self, jobname, buildno, testname, core_prov, fabric,
                  hosts, ofi_build_mode, util_prov=None):
-        
+
         super().__init__(jobname, buildno, testname, core_prov, fabric,
                          hosts, ofi_build_mode, util_prov)
-     
+
         #self.n - number of hosts * number of processes per host
-        self.n = 4 
-        # self.ppn - number of processes per node. 
+        self.n = 4
+        # self.ppn - number of processes per node.
         self.ppn = 2
         self.shmem_dir = "{}/shmem".format(self.libfab_installpath)
 
@@ -174,13 +174,13 @@ class ShmemTest(Test):
         return "{}/run_shmem.sh ".format(ci_site_config.mpi_testpath)
 
     def options(self, shmem_testname):
-       
+
         if self.util_prov:
-            prov = "{core};{util} ".format(core=self.core_prov, 
+            prov = "{core};{util} ".format(core=self.core_prov,
                     util=self.util_prov)
         else:
             prov = self.core_prov
- 
+
         opts = "-n {n} -hosts {server},{client} -shmem_dir={shmemdir} \
                 -libfabric_path={path}/lib -prov '{provider}' -test {test} \
                 -server {server} -inf {inf}" \
@@ -196,70 +196,70 @@ class ShmemTest(Test):
                         (self.core_prov == "psm2" or \
                         self.core_prov == "sockets")) \
                     else False
-            
+
     def execute_cmd(self, shmem_testname):
-        command = self.cmd + self.options(shmem_testname) 
+        command = self.cmd + self.options(shmem_testname)
         outputcmd = shlex.split(command)
-        common.run_command(outputcmd)        
-    
+        common.run_command(outputcmd)
+
 
 class MpiTests(Test):
     def __init__(self, jobname, buildno, testname, core_prov, fabric,
                  mpitype, hosts, ofi_build_mode, util_prov=None):
-       
-        super().__init__(jobname, buildno, testname, core_prov, 
+
+        super().__init__(jobname, buildno, testname, core_prov,
                          fabric, hosts, ofi_build_mode, util_prov)
         self.mpi = mpitype
 
     @property
     def cmd(self):
         if (self.mpi == "impi" or self.mpi == "mpich"):
-            self.testpath = ci_site_config.mpi_testpath 
+            self.testpath = ci_site_config.mpi_testpath
             return "{}/run_{}.sh ".format(self.testpath,self.mpi)
         elif(self.mpi =="ompi"):
             self.testpath = "{}/ompi/bin".format(self.libfab_installpath)
-            return "{}/mpirun ".format(self.testpath)      
-    
+            return "{}/mpirun ".format(self.testpath)
+
     @property
     def options(self):
-        opts = [] 
+        opts = []
         if (self.mpi == "impi" or self.mpi == "mpich"):
             opts = "-n {} -ppn {} -hosts {},{} ".format(self.n,self.ppn,
                     self.server,self.client)
-                
+
             if (self.mpi == "impi"):
-                opts = "{} -mpi_root={} ".format(opts, 
+                opts = "{} -mpi_root={} ".format(opts,
                         ci_site_config.impi_root)
             else:
-                opts = "{} -mpi_root={}/mpich".format(opts, 
+                opts = "{} -mpi_root={}/mpich".format(opts,
                         self.libfab_installpath)
-            
-            opts = "{} -libfabric_path={}/lib ".format(opts, 
+
+            opts = "{} -libfabric_path={}/lib ".format(opts,
                     self.libfab_installpath)
-            
+
             if self.util_prov:
-                opts = "{options} -prov {core};{util} ".format(options=opts, 
+                opts = "{options} -prov {core};{util} ".format(options=opts,
                         core=self.core_prov, util=self.util_prov)
             else:
                 opts = "{} -prov {} ".format(opts, self.core_prov)
 
             for key, val in self.env:
                 opts = "{} -genv {} {} ".format(opts, key, val)
-            
+
         elif (self.mpi == "ompi"):
             opts = "-np {} ".format(self.n)
             hosts = ",".join([":".join([host,str(self.ppn)]) \
                     for host in self.hosts])
-            
+
             opts = "{} --host {} ".format(opts, hosts)
-            
+
             if self.util_prov:
-                opts = "{} --mca mtl_ofi_provider_include {};{} ".format(opts, 
+                opts = "{} --mca mtl_ofi_provider_include {};{} ".format(opts,
                         self.core_prov,self.util_prov)
             else:
-                opts = "{} --mca mtl_ofi_provider_include {} ".format(opts, 
+                opts = "{} --mca mtl_ofi_provider_include {} ".format(opts,
                         self.core_prov)
- 
+
             opts += "--mca orte_base_help_aggregate 0 "
             opts += "--mca mtl ofi --mca pml cm -tag-output "
             for key,val in self.env:
@@ -269,7 +269,7 @@ class MpiTests(Test):
     @property
     def mpi_gen_execute_condn(self):
         #Skip MPI tests for udp, verbs(core) providers.
-        # we would still have MPI tests runnning for 
+        # we would still have MPI tests runnning for
         # verbs-rxd and verbs-rxm providers
         return True if (self.core_prov != "udp" and \
                         self.core_prov != "shm" and \
@@ -279,15 +279,15 @@ class MpiTests(Test):
 
 # IMBtests serves as an abstract class for different
 # types of intel MPI benchmarks. Currently we have
-# the mpi1 and rma tests enabled which are encapsulated 
-# in the IMB_mpi1 and IMB_rma classes below. 
+# the mpi1 and rma tests enabled which are encapsulated
+# in the IMB_mpi1 and IMB_rma classes below.
 
 class IMBtests(ABC):
     """
-    This is an abstract class for IMB tests. 
-    currently IMB-MPI1 and IMB-RMA tests are 
+    This is an abstract class for IMB tests.
+    currently IMB-MPI1 and IMB-RMA tests are
     supported. In future there could be more.
-    All abstract  methods must be implemented. 
+    All abstract  methods must be implemented.
     """
 
     @property
@@ -301,9 +301,9 @@ class IMBtests(ABC):
         pass
 
 class IMBmpi1(IMBtests):
-    
+
     def __init__(self):
-        self.additional_tests = [ 
+        self.additional_tests = [
                                    "Biband",
                                    "Uniband",
                                    "PingPongAnySource",
@@ -332,9 +332,9 @@ class IMBrma(IMBtests):
     @property
     def execute_condn(self):
         return True if (self.core_prov != "verbs") else False
- 
+
 # MpiTestIMB class inherits from the MPITests class.
-# It uses the same options method and class variables as all MPI tests. 
+# It uses the same options method and class variables as all MPI tests.
 # It creates IMB_xxx test objects for each kind of IMB test.
 class MpiTestIMB(MpiTests):
 
@@ -342,18 +342,18 @@ class MpiTestIMB(MpiTests):
                  mpitype, hosts, ofi_build_mode, util_prov=None):
         super().__init__(jobname, buildno, testname, core_prov, fabric,
                          mpitype, hosts, ofi_build_mode, util_prov)
-       
+
         self.n = 4
         self.ppn = 1
         self.mpi1 = IMBmpi1()
-        self.rma = IMBrma(self.core_prov) 
+        self.rma = IMBrma(self.core_prov)
 
     @property
     def execute_condn(self):
         return True if (self.mpi == "impi") else False
-       
+
     def execute_cmd(self):
-        command = self.cmd + self.options 
+        command = self.cmd + self.options
         if(self.mpi1.execute_condn):
             outputcmd = shlex.split(command +  self.mpi1.imb_cmd)
             common.run_command(outputcmd)
@@ -362,8 +362,8 @@ class MpiTestIMB(MpiTests):
             common.run_command(outputcmd)
 
 class MpichTestSuite(MpiTests):
-    
-    def __init__(self, jobname, buildno, testname, core_prov, fabric, 
+
+    def __init__(self, jobname, buildno, testname, core_prov, fabric,
 		     mpitype, hosts, ofi_build_mode, util_prov=None):
             super().__init__(jobname, buildno, testname, core_prov, fabric,
 			     mpitype,  hosts, ofi_build_mode, util_prov)
@@ -372,14 +372,14 @@ class MpichTestSuite(MpiTests):
             self.pwd = os.getcwd()
 
     def testgroup(self, testgroupname):
-        
+
         testpath = "{}/{}".format(self.mpichsuitepath, testgroupname)
         tests = []
         with open("{}/testlist".format(testpath)) as file:
             for line in file:
                 if(line[0] != '#' and  line[0] != '\n'):
                     tests.append((line.rstrip('\n')).split(' '))
-	
+
         return tests
 
     def options(self, nprocs, timeout=None):
@@ -411,7 +411,7 @@ class MpichTestSuite(MpiTests):
     def execute_condn(self):
         return True if (self.mpi == 'impi' and  self.core_prov != 'psm2' \
                         and self.core_prov != 'sockets') else False
- 
+
     def execute_cmd(self, testgroupname):
         print("Running Tests: " + testgroupname)
         tests = []
@@ -432,52 +432,15 @@ class MpichTestSuite(MpiTests):
             common.run_command(outputcmd)
         os.chdir(self.pwd)
 
-class MpiTestStress(MpiTests):
-     
-    def __init__(self, jobname, buildno, testname, core_prov, fabric, 
-                 mpitype, hosts, ofi_build_mode, util_prov=None):
-        super().__init__(jobname, buildno, testname, core_prov, fabric, 
-                         mpitype,  hosts, ofi_build_mode, util_prov)
-        
-         
-        if((self.core_prov == "verbs" or self.core_prov =="psm2")):
-            self.n = 16
-            self.ppn = 8
-        else:
-            self.n = 4
-            self.ppn = 2
-      
-    @property
-    def stress_cmd(self):
-        return "{}/{}/stress/mpi_stress -dcr".format(self.libfab_installpath, self.mpi)
-
-    @property
-    def execute_condn(self):
-        # Todo : run stress test for ompi with libfabirc-dbg builds if it works
-        # in Jenkins for buildbot these ompi did not build with libfabric-dbg 
-
-        # Due to an mpich issue when the correct mpich options are enabled during
-        # mpich builds, sttress test is failing. disabling mpich + stress tests
-        # untill the mpich team fixes the issue. 
-        return True if ((self.job_cadence  == 'daily') and \
-                       (self.mpi != 'ompi' or \
-                        self.ofi_build_mode != 'dbg')) else  False
-    
-    def execute_cmd(self):
-        command = self.cmd + self.options + self.stress_cmd
-        outputcmd = shlex.split(command)
-        common.run_command(outputcmd) 
 
-         
-      
 class MpiTestOSU(MpiTests):
 
     def __init__(self, jobname, buildno, testname, core_prov, fabric,
                  mpitype, hosts, ofi_build_mode, util_prov=None):
         super().__init__(jobname, buildno, testname, core_prov, fabric,
                          mpitype, hosts, ofi_build_mode, util_prov)
-        
-        self.n = 4 
+
+        self.n = 4
         self.ppn = 2
         self.two_proc_tests = {'osu_latency',
                                'osu_bibw',
@@ -494,10 +457,10 @@ class MpiTestOSU(MpiTests):
                               }
 
         self.osu_mpi_path = "{}/{}/osu/libexec/osu-micro-benchmarks/mpi/". \
-                            format(self.libfab_installpath,mpitype) 
-    
+                            format(self.libfab_installpath,mpitype)
+
     @property
-    def execute_condn(self): 
+    def execute_condn(self):
         # sockets and psm2 have some issues with OSU benchmark testing.
         return True if ((self.job_cadence  == 'daily') and \
                         (self.mpi != "ompi" or \
@@ -505,7 +468,7 @@ class MpiTestOSU(MpiTests):
                          self.core_prov != "psm2" and \
                          self.ofi_build_mode!="dbg"))) \
                     else False
-    
+
     def execute_cmd(self):
         assert(self.osu_mpi_path)
         p = re.compile('osu_put*')
@@ -523,6 +486,6 @@ class MpiTestOSU(MpiTests):
                     osu_cmd = os.path.join(root, test)
                     command = launcher + osu_cmd
                     outputcmd = shlex.split(command)
-                    common.run_command(outputcmd) 
+                    common.run_command(outputcmd)
 
 
diff --git a/fabtests/Makefile.am b/fabtests/Makefile.am
index 95e7b63..bdd71f5 100644
--- a/fabtests/Makefile.am
+++ b/fabtests/Makefile.am
@@ -43,6 +43,8 @@ bin_PROGRAMS = \
 	functional/fi_rdm_atomic \
 	functional/fi_multi_recv \
 	functional/fi_bw \
+	functional/fi_rdm_multi_client \
+	functional/fi_loopback \
 	benchmarks/fi_msg_pingpong \
 	benchmarks/fi_msg_bw \
 	benchmarks/fi_rma_bw \
@@ -84,6 +86,7 @@ nobase_dist_config_DATA = \
         test_configs/udp/functional.test \
         test_configs/udp/udp.exclude \
         test_configs/tcp/tcp.exclude \
+        test_configs/tcp/all.test \
         test_configs/verbs/all.test \
         test_configs/verbs/quick.test \
 	test_configs/verbs/verbs.exclude \
@@ -112,7 +115,6 @@ noinst_LTLIBRARIES = libfabtests.la
 
 libfabtests_la_SOURCES = \
 	common/shared.c \
-	common/jsmn.c \
 	common/hmem.c \
 	common/hmem_cuda.c \
 	common/hmem_rocr.c \
@@ -257,6 +259,14 @@ functional_fi_bw_SOURCES = \
 	functional/bw.c
 functional_fi_bw_LDADD = libfabtests.la
 
+functional_fi_rdm_multi_client_SOURCES = \
+	functional/rdm_multi_client.c
+functional_fi_rdm_multi_client_LDADD = libfabtests.la
+
+functional_fi_loopback_SOURCES = \
+	functional/loopback.c
+functional_fi_loopback_LDADD = libfabtests.la
+
 benchmarks_fi_msg_pingpong_SOURCES = \
 	benchmarks/msg_pingpong.c \
 	$(benchmarks_srcs)
@@ -426,6 +436,7 @@ dummy_man_pages = \
 	man/man1/fi_getinfo_test.1 \
 	man/man1/fi_mr_test.1 \
 	man/man1/fi_bw.1 \
+	man/man1/fi_rdm_multi_client.1 \
 	man/man1/fi_ubertest.1
 
 nroff:
diff --git a/fabtests/Makefile.win b/fabtests/Makefile.win
index 2c416de..0ec7f4b 100644
--- a/fabtests/Makefile.win
+++ b/fabtests/Makefile.win
@@ -35,7 +35,7 @@ outdir = $(output_root)$(arch)\release-v142
 CFLAGS = $(CFLAGS) /O2 /MT
 !endif
 
-basedeps = common\hmem.c common\shared.c common\jsmn.c \
+basedeps = common\hmem.c common\shared.c \
 	common\windows\getopt.c common\windows\osd.c \
 	common\hmem_cuda.c common\hmem_rocr.c common\hmem_ze.c
 
diff --git a/fabtests/benchmarks/dgram_pingpong.c b/fabtests/benchmarks/dgram_pingpong.c
index 74920a7..191f0da 100644
--- a/fabtests/benchmarks/dgram_pingpong.c
+++ b/fabtests/benchmarks/dgram_pingpong.c
@@ -119,6 +119,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_cntr_pingpong.c b/fabtests/benchmarks/rdm_cntr_pingpong.c
index 61c64a6..a4a46fa 100644
--- a/fabtests/benchmarks/rdm_cntr_pingpong.c
+++ b/fabtests/benchmarks/rdm_cntr_pingpong.c
@@ -101,6 +101,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_pingpong.c b/fabtests/benchmarks/rdm_pingpong.c
index bd2dc40..397d564 100644
--- a/fabtests/benchmarks/rdm_pingpong.c
+++ b/fabtests/benchmarks/rdm_pingpong.c
@@ -98,10 +98,11 @@ int main(int argc, char **argv)
 
 	hints->ep_attr->type = FI_EP_RDM;
 	hints->caps = FI_MSG;
-	hints->mode = FI_CONTEXT;
+	hints->mode |= FI_CONTEXT;
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_tagged_bw.c b/fabtests/benchmarks/rdm_tagged_bw.c
index 323a66d..932910d 100644
--- a/fabtests/benchmarks/rdm_tagged_bw.c
+++ b/fabtests/benchmarks/rdm_tagged_bw.c
@@ -101,10 +101,11 @@ int main(int argc, char **argv)
 	hints->ep_attr->type = FI_EP_RDM;
 	hints->domain_attr->resource_mgmt = FI_RM_ENABLED;
 	hints->caps = FI_TAGGED;
-	hints->mode = FI_CONTEXT;
+	hints->mode |= FI_CONTEXT;
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_BULK_DATA;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_tagged_pingpong.c b/fabtests/benchmarks/rdm_tagged_pingpong.c
index e0ca5f2..af94698 100644
--- a/fabtests/benchmarks/rdm_tagged_pingpong.c
+++ b/fabtests/benchmarks/rdm_tagged_pingpong.c
@@ -99,10 +99,11 @@ int main(int argc, char **argv)
 
 	hints->ep_attr->type = FI_EP_RDM;
 	hints->caps = FI_TAGGED;
-	hints->mode = FI_CONTEXT;
+	hints->mode |= FI_CONTEXT;
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/common/hmem_ze.c b/fabtests/common/hmem_ze.c
index 759faa3..149dced 100644
--- a/fabtests/common/hmem_ze.c
+++ b/fabtests/common/hmem_ze.c
@@ -34,10 +34,13 @@
 #include <config.h>
 #endif
 
+#include <stdio.h>
 #include "hmem.h"
+#include "shared.h"
 
-#ifdef HAVE_LIBZE
+#if HAVE_LIBZE
 
+#include <dlfcn.h>
 #include <level_zero/ze_api.h>
 
 #define ZE_MAX_DEVICES 4
@@ -71,6 +74,181 @@ static const ze_device_mem_alloc_desc_t device_desc = {
 	.ordinal	= 0,
 };
 
+static void *libze_handle;
+static struct libze_ops {
+	ze_result_t (*zeInit)(ze_init_flags_t flags);
+	ze_result_t (*zeDriverGet)(uint32_t *pCount,
+				   ze_driver_handle_t *phDrivers);
+	ze_result_t (*zeDeviceGet)(ze_driver_handle_t hDriver,
+				   uint32_t *pCount,
+				   ze_device_handle_t *phDevices);
+	ze_result_t (*zeDeviceCanAccessPeer)(ze_device_handle_t hDevice,
+					     ze_device_handle_t hPeerDevice,
+					     ze_bool_t *value);
+	ze_result_t (*zeContextCreate)(ze_driver_handle_t hDriver,
+				       const ze_context_desc_t *desc,
+				       ze_context_handle_t *phContext);
+	ze_result_t (*zeContextDestroy)(ze_context_handle_t hContext);
+	ze_result_t (*zeCommandQueueCreate)(ze_context_handle_t hContext,
+					    ze_device_handle_t hDevice,
+					    const ze_command_queue_desc_t *desc,
+					    ze_command_queue_handle_t *phCommandQueue);
+	ze_result_t (*zeCommandQueueDestroy)(ze_command_queue_handle_t hCommandQueue);
+	ze_result_t (*zeCommandQueueExecuteCommandLists)(
+					ze_command_queue_handle_t hCommandQueue,
+					uint32_t numCommandLists,
+					ze_command_list_handle_t *phCommandLists,
+					ze_fence_handle_t hFence);
+	ze_result_t (*zeCommandListCreate)(ze_context_handle_t hContext,
+					   ze_device_handle_t hDevice,
+					   const ze_command_list_desc_t *desc,
+					   ze_command_list_handle_t *phCommandList);
+	ze_result_t (*zeCommandListDestroy)(ze_command_list_handle_t hCommandList);
+	ze_result_t (*zeCommandListClose)(ze_command_list_handle_t hCommandList);
+	ze_result_t (*zeCommandListAppendMemoryCopy)(
+				ze_command_list_handle_t hCommandList,
+				void *dstptr, const void *srcptr, size_t size,
+				ze_event_handle_t hSignalEvent,
+				uint32_t numWaitEvents,
+				ze_event_handle_t *phWaitEvents);
+	ze_result_t (*zeCommandListAppendMemoryFill)(
+				ze_command_list_handle_t hCommandList,
+				void *ptr, const void *pattern,
+				size_t pattern_size, size_t size,
+				ze_event_handle_t hSignalEvent,
+				uint32_t numWaitEvents,
+				ze_event_handle_t *phWaitEvents);
+	ze_result_t (*zeMemAllocDevice)(
+				ze_context_handle_t hContext,
+				const ze_device_mem_alloc_desc_t *device_desc,
+				size_t size, size_t alignment, ze_device_handle_t hDevice,
+				void *pptr);
+	ze_result_t (*zeMemFree)(ze_context_handle_t hContext, void *ptr);
+} libze_ops;
+
+static int init_libze_ops(void)
+{
+	libze_handle = dlopen("libze_loader.so", RTLD_NOW);
+	if (!libze_handle) {
+		FT_ERR("Failed to dlopen libze_loader.so\n");
+		goto err_out;
+	}
+
+	libze_ops.zeInit = dlsym(libze_handle, "zeInit");
+	if (!libze_ops.zeInit) {
+		FT_ERR("Failed to find zeInit\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDriverGet = dlsym(libze_handle, "zeDriverGet");
+	if (!libze_ops.zeDriverGet) {
+		FT_ERR("Failed to find zeDriverGet\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceGet = dlsym(libze_handle, "zeDeviceGet");
+	if (!libze_ops.zeDeviceGet) {
+		FT_ERR("Failed to find zeDeviceGet\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceCanAccessPeer = dlsym(libze_handle, "zeDeviceCanAccessPeer");
+	if (!libze_ops.zeDeviceCanAccessPeer) {
+		FT_ERR("Failed to find zeDeviceCanAccessPeer\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextCreate = dlsym(libze_handle, "zeContextCreate");
+	if (!libze_ops.zeContextCreate) {
+		FT_ERR("Failed to find zeContextCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextDestroy = dlsym(libze_handle, "zeContextDestroy");
+	if (!libze_ops.zeContextDestroy) {
+		FT_ERR("Failed to find zeContextDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextDestroy = dlsym(libze_handle, "zeContextDestroy");
+	if (!libze_ops.zeContextDestroy) {
+		FT_ERR("Failed to find zeContextDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueCreate = dlsym(libze_handle, "zeCommandQueueCreate");
+	if (!libze_ops.zeCommandQueueCreate) {
+		FT_ERR("Failed to find zeCommandQueueCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueDestroy = dlsym(libze_handle, "zeCommandQueueDestroy");
+	if (!libze_ops.zeCommandQueueDestroy) {
+		FT_ERR("Failed to find zeCommandQueueDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueExecuteCommandLists = dlsym(libze_handle, "zeCommandQueueExecuteCommandLists");
+	if (!libze_ops.zeCommandQueueExecuteCommandLists) {
+		FT_ERR("Failed to find zeCommandQueueExecuteCommandLists\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListCreate = dlsym(libze_handle, "zeCommandListCreate");
+	if (!libze_ops.zeCommandListCreate) {
+		FT_ERR("Failed to find zeCommandListCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListDestroy = dlsym(libze_handle, "zeCommandListDestroy");
+	if (!libze_ops.zeCommandListDestroy) {
+		FT_ERR("Failed to find zeCommandListDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListClose = dlsym(libze_handle, "zeCommandListClose");
+	if (!libze_ops.zeCommandListClose) {
+		FT_ERR("Failed to find zeCommandListClose\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListAppendMemoryCopy = dlsym(libze_handle, "zeCommandListAppendMemoryCopy");
+	if (!libze_ops.zeCommandListAppendMemoryCopy) {
+		FT_ERR("Failed to find zeCommandListAppendMemoryCopy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListAppendMemoryFill = dlsym(libze_handle, "zeCommandListAppendMemoryFill");
+	if (!libze_ops.zeCommandListAppendMemoryFill) {
+		FT_ERR("Failed to find zeCommandListAppendMemoryFill\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemAllocDevice = dlsym(libze_handle, "zeMemAllocDevice");
+	if (!libze_ops.zeMemAllocDevice) {
+		FT_ERR("Failed to find zeMemAllocDevice\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemFree = dlsym(libze_handle, "zeMemFree");
+	if (!libze_ops.zeMemFree) {
+		FT_ERR("Failed to find zeMemFree\n");
+		goto err_dlclose;
+	}
+	return FI_SUCCESS;
+
+err_dlclose:
+	dlclose(libze_handle);
+
+err_out:
+	return -FI_ENODATA;
+}
+
+static void cleanup_libze_ops(void)
+{
+	dlclose(libze_handle);
+}
+
 int ft_ze_init(void)
 {
 	ze_driver_handle_t driver;
@@ -78,31 +256,35 @@ int ft_ze_init(void)
 	ze_result_t ze_ret;
 	uint32_t count;
 
-	ze_ret = zeInit(ZE_INIT_FLAG_GPU_ONLY);
+	if (init_libze_ops())
+		return -FI_EIO;
+
+	ze_ret = (*libze_ops.zeInit)(ZE_INIT_FLAG_GPU_ONLY);
 	if (ze_ret)
 		return -FI_EIO;
 
 	count = 1;
-	ze_ret = zeDriverGet(&count, &driver);
+	ze_ret = (*libze_ops.zeDriverGet)(&count, &driver);
 	if (ze_ret)
 		return -FI_EIO;
 
-	ze_ret = zeContextCreate(driver, &context_desc, &context);
+	ze_ret = (*libze_ops.zeContextCreate)(driver, &context_desc, &context);
 	if (ze_ret)
 		return -FI_EIO;
 
 	count = 0;
-	ze_ret = zeDeviceGet(driver, &count, NULL);
+	ze_ret = (*libze_ops.zeDeviceGet)(driver, &count, NULL);
 	if (ze_ret || count > ZE_MAX_DEVICES)
 		goto err;;
 
-	ze_ret = zeDeviceGet(driver, &count, devices);
+	ze_ret = (*libze_ops.zeDeviceGet)(driver, &count, devices);
 	if (ze_ret)
 		goto err;
 
 	for (num_devices = 0; num_devices < count; num_devices++) {
-		ze_ret = zeCommandQueueCreate(context, devices[num_devices], &cq_desc,
-					      &cmd_queue[num_devices]);
+		ze_ret = (*libze_ops.zeCommandQueueCreate)(
+					context, devices[num_devices], &cq_desc,
+					&cmd_queue[num_devices]);
 		if (ze_ret)
 			goto err;
 	}
@@ -119,25 +301,28 @@ int ft_ze_cleanup(void)
 	int i, ret = FI_SUCCESS;
 
 	for (i = 0; i < num_devices; i++) {
-		if (cmd_queue[i] && zeCommandQueueDestroy(cmd_queue[i]))
+		if (cmd_queue[i] &&
+		    (*libze_ops.zeCommandQueueDestroy)(cmd_queue[i]))
 			ret = -FI_EINVAL;
 	}
 
-	if (zeContextDestroy(context))
+	if ((*libze_ops.zeContextDestroy)(context))
 		return -FI_EINVAL;
 
+	cleanup_libze_ops();
 	return ret;
 }
 
 int ft_ze_alloc(uint64_t device, void **buf, size_t size)
 {
-	return zeMemAllocDevice(context, &device_desc, size, 16,
-				devices[device], buf) ? -FI_EINVAL : 0;
+	return (*libze_ops.zeMemAllocDevice)(context, &device_desc, size, 16,
+					     devices[device], buf) ?
+			-FI_EINVAL : 0;
 }
 
 int ft_ze_free(void *buf)
 {
-	return zeMemFree(context, buf) ? -FI_EINVAL : FI_SUCCESS;
+	return (*libze_ops.zeMemFree)(context, buf) ? -FI_EINVAL : FI_SUCCESS;
 }
 
 int ft_ze_memset(uint64_t device, void *buf, int value, size_t size)
@@ -145,24 +330,26 @@ int ft_ze_memset(uint64_t device, void *buf, int value, size_t size)
 	ze_command_list_handle_t cmd_list;
 	ze_result_t ze_ret;
 
-	ze_ret = zeCommandListCreate(context, devices[device], &cl_desc, &cmd_list);
+	ze_ret = (*libze_ops.zeCommandListCreate)(context, devices[device],
+						  &cl_desc, &cmd_list);
 	if (ze_ret)
 		return -FI_EIO;
 
-	ze_ret = zeCommandListAppendMemoryFill(cmd_list, buf, &value,
-					       sizeof(value), size, NULL, 0, NULL);
+	ze_ret = (*libze_ops.zeCommandListAppendMemoryFill)(
+					cmd_list, buf, &value, sizeof(value),
+					size, NULL, 0, NULL);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandListClose(cmd_list);
+	ze_ret = (*libze_ops.zeCommandListClose)(cmd_list);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandQueueExecuteCommandLists(cmd_queue[device], 1,
-						   &cmd_list, NULL);
+	ze_ret = (*libze_ops.zeCommandQueueExecuteCommandLists)(
+					cmd_queue[device], 1, &cmd_list, NULL);
 
 free:
-	if (!zeCommandListDestroy(cmd_list) && !ze_ret)
+	if (!(*libze_ops.zeCommandListDestroy)(cmd_list) && !ze_ret)
 		return FI_SUCCESS;
 
 	return -FI_EINVAL;
@@ -173,23 +360,25 @@ int ft_ze_copy(uint64_t device, void *dst, const void *src, size_t size)
 	ze_command_list_handle_t cmd_list;
 	ze_result_t ze_ret;
 
-	ze_ret = zeCommandListCreate(context, devices[device], &cl_desc, &cmd_list);
+	ze_ret = (*libze_ops.zeCommandListCreate)(context, devices[device],
+						  &cl_desc, &cmd_list);
 	if (ze_ret)
 		return -FI_EIO;
 
-	ze_ret = zeCommandListAppendMemoryCopy(cmd_list, dst, src, size, NULL, 0, NULL);
+	ze_ret = (*libze_ops.zeCommandListAppendMemoryCopy)(
+					cmd_list, dst, src, size, NULL, 0, NULL);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandListClose(cmd_list);
+	ze_ret = (*libze_ops.zeCommandListClose)(cmd_list);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandQueueExecuteCommandLists(cmd_queue[device], 1,
-						   &cmd_list, NULL);
+	ze_ret = (*libze_ops.zeCommandQueueExecuteCommandLists)(
+					cmd_queue[device], 1, &cmd_list, NULL);
 
 free:
-	if (!zeCommandListDestroy(cmd_list) && !ze_ret)
+	if (!(*libze_ops.zeCommandListDestroy)(cmd_list) && !ze_ret)
 		return FI_SUCCESS;
 
 	return -FI_EINVAL;
diff --git a/fabtests/common/jsmn.c b/fabtests/common/jsmn.c
deleted file mode 100644
index 9fe0fb4..0000000
--- a/fabtests/common/jsmn.c
+++ /dev/null
@@ -1,333 +0,0 @@
-/*
- * Copyright (c) 2010 Serge A. Zaitsev
- *
- * Permission is hereby granted, free of charge, to any person obtaining a copy
- * of this software and associated documentation files (the "Software"), to deal
- * in the Software without restriction, including without limitation the rights
- * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
- * copies of the Software, and to permit persons to whom the Software is
- * furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
- * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
- * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
- * THE SOFTWARE.
- */
-
-#include <stdlib.h>
-
-#include "jsmn.h"
-
-/**
- * Allocates a fresh unused token from the token pull.
- */
-static jsmntok_t *jsmn_alloc_token(jsmn_parser *parser,
-		jsmntok_t *tokens, size_t num_tokens) {
-	jsmntok_t *tok;
-	if (parser->toknext >= num_tokens) {
-		return NULL;
-	}
-	tok = &tokens[parser->toknext++];
-	tok->start = tok->end = -1;
-	tok->size = 0;
-#ifdef JSMN_PARENT_LINKS
-	tok->parent = -1;
-#endif
-	return tok;
-}
-
-/**
- * Fills token type and boundaries.
- */
-static void jsmn_fill_token(jsmntok_t *token, jsmntype_t type,
-                            int start, int end) {
-	token->type = type;
-	token->start = start;
-	token->end = end;
-	token->size = 0;
-}
-
-/**
- * Fills next available token with JSON primitive.
- */
-static jsmnerr_t jsmn_parse_primitive(jsmn_parser *parser, const char *js,
-		size_t len, jsmntok_t *tokens, size_t num_tokens) {
-	jsmntok_t *token;
-	int start;
-
-	start = parser->pos;
-
-	for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
-		switch (js[parser->pos]) {
-#ifndef JSMN_STRICT
-			/* In strict mode primitive must be followed by "," or "}" or "]" */
-			case ':':
-#endif
-			case '\t' : case '\r' : case '\n' : case ' ' :
-			case ','  : case ']'  : case '}' :
-				goto found;
-		}
-		if (js[parser->pos] < 32 || js[parser->pos] >= 127) {
-			parser->pos = start;
-			return JSMN_ERROR_INVAL;
-		}
-	}
-#ifdef JSMN_STRICT
-	/* In strict mode primitive must be followed by a comma/object/array */
-	parser->pos = start;
-	return JSMN_ERROR_PART;
-#endif
-
-found:
-	if (tokens == NULL) {
-		parser->pos--;
-		return 0;
-	}
-	token = jsmn_alloc_token(parser, tokens, num_tokens);
-	if (token == NULL) {
-		parser->pos = start;
-		return JSMN_ERROR_NOMEM;
-	}
-	jsmn_fill_token(token, JSMN_PRIMITIVE, start, parser->pos);
-#ifdef JSMN_PARENT_LINKS
-	token->parent = parser->toksuper;
-#endif
-	parser->pos--;
-	return 0;
-}
-
-/**
- * Filsl next token with JSON string.
- */
-static jsmnerr_t jsmn_parse_string(jsmn_parser *parser, const char *js,
-		size_t len, jsmntok_t *tokens, size_t num_tokens) {
-	jsmntok_t *token;
-
-	int start = parser->pos;
-
-	parser->pos++;
-
-	/* Skip starting quote */
-	for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
-		char c = js[parser->pos];
-
-		/* Quote: end of string */
-		if (c == '\"') {
-			if (tokens == NULL) {
-				return 0;
-			}
-			token = jsmn_alloc_token(parser, tokens, num_tokens);
-			if (token == NULL) {
-				parser->pos = start;
-				return JSMN_ERROR_NOMEM;
-			}
-			jsmn_fill_token(token, JSMN_STRING, start+1, parser->pos);
-#ifdef JSMN_PARENT_LINKS
-			token->parent = parser->toksuper;
-#endif
-			return 0;
-		}
-
-		/* Backslash: Quoted symbol expected */
-		if (c == '\\' && parser->pos + 1 < len) {
-			int i;
-			parser->pos++;
-			switch (js[parser->pos]) {
-				/* Allowed escaped symbols */
-				case '\"': case '/' : case '\\' : case 'b' :
-				case 'f' : case 'r' : case 'n'  : case 't' :
-					break;
-				/* Allows escaped symbol \uXXXX */
-				case 'u':
-					parser->pos++;
-					for(i = 0; i < 4 && parser->pos < len && js[parser->pos] != '\0'; i++) {
-						/* If it isn't a hex character we have an error */
-						if(!((js[parser->pos] >= 48 && js[parser->pos] <= 57) || /* 0-9 */
-									(js[parser->pos] >= 65 && js[parser->pos] <= 70) || /* A-F */
-									(js[parser->pos] >= 97 && js[parser->pos] <= 102))) { /* a-f */
-							parser->pos = start;
-							return JSMN_ERROR_INVAL;
-						}
-						parser->pos++;
-					}
-					parser->pos--;
-					break;
-				/* Unexpected symbol */
-				default:
-					parser->pos = start;
-					return JSMN_ERROR_INVAL;
-			}
-		}
-	}
-	parser->pos = start;
-	return JSMN_ERROR_PART;
-}
-
-/**
- * Parse JSON string and fill tokens.
- */
-jsmnerr_t jsmn_parse(jsmn_parser *parser, const char *js, size_t len,
-		jsmntok_t *tokens, unsigned int num_tokens) {
-	jsmnerr_t r;
-	int i;
-	jsmntok_t *token;
-	int count = 0;
-
-	for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
-		char c;
-		jsmntype_t type;
-
-		c = js[parser->pos];
-		switch (c) {
-			case '{': case '[':
-				count++;
-				if (tokens == NULL) {
-					break;
-				}
-				token = jsmn_alloc_token(parser, tokens, num_tokens);
-				if (token == NULL)
-					return JSMN_ERROR_NOMEM;
-				if (parser->toksuper != -1) {
-					tokens[parser->toksuper].size++;
-#ifdef JSMN_PARENT_LINKS
-					token->parent = parser->toksuper;
-#endif
-				}
-				token->type = (c == '{' ? JSMN_OBJECT : JSMN_ARRAY);
-				token->start = parser->pos;
-				parser->toksuper = parser->toknext - 1;
-				break;
-			case '}': case ']':
-				if (tokens == NULL)
-					break;
-				type = (c == '}' ? JSMN_OBJECT : JSMN_ARRAY);
-#ifdef JSMN_PARENT_LINKS
-				if (parser->toknext < 1) {
-					return JSMN_ERROR_INVAL;
-				}
-				token = &tokens[parser->toknext - 1];
-				for (;;) {
-					if (token->start != -1 && token->end == -1) {
-						if (token->type != type) {
-							return JSMN_ERROR_INVAL;
-						}
-						token->end = parser->pos + 1;
-						parser->toksuper = token->parent;
-						break;
-					}
-					if (token->parent == -1) {
-						break;
-					}
-					token = &tokens[token->parent];
-				}
-#else
-				for (i = parser->toknext - 1; i >= 0; i--) {
-					token = &tokens[i];
-					if (token->start != -1 && token->end == -1) {
-						if (token->type != type) {
-							return JSMN_ERROR_INVAL;
-						}
-						parser->toksuper = -1;
-						token->end = parser->pos + 1;
-						break;
-					}
-				}
-				/* Error if unmatched closing bracket */
-				if (i == -1) return JSMN_ERROR_INVAL;
-				for (; i >= 0; i--) {
-					token = &tokens[i];
-					if (token->start != -1 && token->end == -1) {
-						parser->toksuper = i;
-						break;
-					}
-				}
-#endif
-				break;
-			case '\"':
-				r = jsmn_parse_string(parser, js, len, tokens, num_tokens);
-				if (r < 0) return r;
-				count++;
-				if (parser->toksuper != -1 && tokens != NULL)
-					tokens[parser->toksuper].size++;
-				break;
-			case '\t' : case '\r' : case '\n' : case ' ':
-				break;
-			case ':':
-				parser->toksuper = parser->toknext - 1;
-				break;
-			case ',':
-				if (tokens != NULL &&
-						tokens[parser->toksuper].type != JSMN_ARRAY &&
-						tokens[parser->toksuper].type != JSMN_OBJECT) {
-#ifdef JSMN_PARENT_LINKS
-					parser->toksuper = tokens[parser->toksuper].parent;
-#else
-					for (i = parser->toknext - 1; i >= 0; i--) {
-						if (tokens[i].type == JSMN_ARRAY || tokens[i].type == JSMN_OBJECT) {
-							if (tokens[i].start != -1 && tokens[i].end == -1) {
-								parser->toksuper = i;
-								break;
-							}
-						}
-					}
-#endif
-				}
-				break;
-#ifdef JSMN_STRICT
-			/* In strict mode primitives are: numbers and booleans */
-			case '-': case '0': case '1' : case '2': case '3' : case '4':
-			case '5': case '6': case '7' : case '8': case '9':
-			case 't': case 'f': case 'n' :
-				/* And they must not be keys of the object */
-				if (tokens != NULL) {
-					jsmntok_t *t = &tokens[parser->toksuper];
-					if (t->type == JSMN_OBJECT ||
-							(t->type == JSMN_STRING && t->size != 0)) {
-						return JSMN_ERROR_INVAL;
-					}
-				}
-#else
-			/* In non-strict mode every unquoted value is a primitive */
-			default:
-#endif
-				r = jsmn_parse_primitive(parser, js, len, tokens, num_tokens);
-				if (r < 0) return r;
-				count++;
-				if (parser->toksuper != -1 && tokens != NULL)
-					tokens[parser->toksuper].size++;
-				break;
-
-#ifdef JSMN_STRICT
-			/* Unexpected char in strict mode */
-			default:
-				return JSMN_ERROR_INVAL;
-#endif
-		}
-	}
-
-	for (i = parser->toknext - 1; i >= 0; i--) {
-		/* Unmatched opened object or array */
-		if (tokens && tokens[i].start != -1 && tokens[i].end == -1) {
-			return JSMN_ERROR_PART;
-		}
-	}
-
-	return count;
-}
-
-/**
- * Creates a new parser based over a given  buffer with an array of tokens
- * available.
- */
-void jsmn_init(jsmn_parser *parser) {
-	parser->pos = 0;
-	parser->toknext = 0;
-	parser->toksuper = -1;
-}
-
diff --git a/fabtests/common/shared.c b/fabtests/common/shared.c
index 96b90a3..d319a17 100644
--- a/fabtests/common/shared.c
+++ b/fabtests/common/shared.c
@@ -2,6 +2,7 @@
  * Copyright (c) 2013-2018 Intel Corporation.  All rights reserved.
  * Copyright (c) 2016 Cray Inc.  All rights reserved.
  * Copyright (c) 2014-2017, Cisco Systems, Inc. All rights reserved.
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under the BSD license below:
  *
@@ -29,6 +30,7 @@
  */
 
 #include <assert.h>
+#include <fcntl.h>
 #include <netdb.h>
 #include <poll.h>
 #include <stdlib.h>
@@ -144,7 +146,6 @@ struct test_size_param test_size[] = {
 
 const unsigned int test_cnt = (sizeof test_size / sizeof test_size[0]);
 
-#define INTEG_SEED 7
 static const char integ_alphabet[] = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";
 static const int integ_alphabet_length = (sizeof(integ_alphabet)/sizeof(*integ_alphabet)) - 1;
 
@@ -684,7 +685,7 @@ int ft_alloc_active_res(struct fi_info *fi)
 	return 0;
 }
 
-static int ft_init(void)
+int ft_init(void)
 {
 	tx_seq = 0;
 	rx_seq = 0;
@@ -700,10 +701,27 @@ static int ft_init(void)
 	return ft_hmem_init(opts.iface);
 }
 
+int ft_sock_setup(int sock)
+{
+	int ret, op;
+
+	op = 1;
+	ret = setsockopt(sock, IPPROTO_TCP, TCP_NODELAY,
+			  (void *) &op, sizeof(op));
+	if (ret)
+		return ret;
+
+	ret = ft_fd_nonblock(sock);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
 int ft_init_oob(void)
 {
-	int ret, op, err;
 	struct addrinfo *ai = NULL;
+	int ret;
 
 	if (!(opts.options & FT_OPT_OOB_CTRL) || oob_sock != -1)
 		return 0;
@@ -725,7 +743,6 @@ int ft_init_oob(void)
 
 		close(listen_sock);
 	} else {
-
 		ret = getaddrinfo(opts.dst_addr, opts.oob_port, NULL, &ai);
 		if (ret) {
 			perror("getaddrinfo");
@@ -748,11 +765,7 @@ int ft_init_oob(void)
 		sleep(1);
 	}
 
-	op = 1;
-	err = setsockopt(oob_sock, IPPROTO_TCP, TCP_NODELAY,
-			 (void *) &op, sizeof(op));
-	if (err)
-		perror("setsockopt"); /* non-fatal error */
+	ret = ft_sock_setup(oob_sock);
 
 free:
 	if (ai)
@@ -760,6 +773,10 @@ free:
 	return ret;
 }
 
+/*
+ * Handles a persistent server communicating with multiple clients,
+ * one at a time, in sequence.
+ */
 int ft_accept_next_client() {
 	int ret;
 
@@ -770,9 +787,48 @@ int ft_accept_next_client() {
 			return ret;
 	}
 
+	/* Clients may be separate processes, so re-initialize any OOB setup. */
+	if (opts.options & FT_OPT_OOB_ADDR_EXCH) {
+		ret = ft_reset_oob();
+		if (ret)
+			return ret;
+	}
 	return ft_init_av();
 }
 
+/*
+ * Re-initialize the OOB setup.
+ */
+int ft_reset_oob()
+{
+	int ret;
+	ret = ft_close_oob();
+	if (ret) {
+		FT_PRINTERR("ft_close_oob", ret);
+		return ret;
+	}
+	ret = ft_init_oob();
+	if (ret) {
+		FT_PRINTERR("ft_init_oob", ret);
+		return ret;
+	}
+	return 0;
+}
+
+int ft_close_oob()
+{
+	int ret;
+	if (oob_sock == -1)
+		return 0;
+	ret = close(oob_sock);
+	if (ret) {
+		FT_PRINTERR("close", errno);
+		return ret;
+	}
+	oob_sock = -1;
+	return 0;
+}
+
 int ft_getinfo(struct fi_info *hints, struct fi_info **info)
 {
 	char *node, *service;
@@ -966,6 +1022,9 @@ int ft_server_connect(void)
 	if (ret)
 		goto err;
 
+	if (ft_check_opts(FT_OPT_FORK_CHILD))
+		ft_fork_child();
+
 	return 0;
 
 err:
@@ -1024,6 +1083,9 @@ int ft_client_connect(void)
 	if (ret)
 		return ret;
 
+	if (ft_check_opts(FT_OPT_FORK_CHILD))
+		ft_fork_child();
+
 	return 0;
 }
 
@@ -1059,6 +1121,9 @@ int ft_init_fabric(void)
 	if (ret)
 		return ret;
 
+	if (ft_check_opts(FT_OPT_FORK_CHILD))
+		ft_fork_child();
+
 	return 0;
 }
 
@@ -1283,7 +1348,7 @@ int ft_init_av_dst_addr(struct fid_av *av_ptr, struct fid_ep *ep_ptr,
 		if (ret)
 			return ret;
 	} else {
-		ret = (int) ft_rx(ep, FT_MAX_CTRL_MSG);
+		ret = ft_get_rx_comp(rx_seq);
 		if (ret)
 			return ret;
 
@@ -1295,6 +1360,10 @@ int ft_init_av_dst_addr(struct fid_av *av_ptr, struct fid_ep *ep_ptr,
 		if (ret)
 			return ret;
 
+		ret = ft_post_rx(ep, rx_size, &rx_ctx);
+		if (ret)
+			return ret;
+
 		if (fi->domain_attr->av_type == FI_AV_TABLE)
 			*remote_addr = 0;
 
@@ -1769,6 +1838,14 @@ void init_test(struct ft_opts *opts, char *test_name, size_t test_name_len)
 		opts->iterations = size_to_count(opts->transfer_size);
 }
 
+static void ft_force_progress(void)
+{
+	if (txcq)
+		fi_cq_read(txcq, NULL, 0);
+	if (rxcq)
+		fi_cq_read(rxcq, NULL, 0);
+}
+
 static int ft_progress(struct fid_cq *cq, uint64_t total, uint64_t *cq_cntr)
 {
 	struct fi_cq_err_entry comp;
@@ -1859,8 +1936,11 @@ ssize_t ft_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size, void *ctx)
 {
 	ssize_t ret;
 
-	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
-		ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE)) {
+		ret = ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+		if (ret)
+			return ret;
+	}
 
 	ret = ft_post_tx(ep, fi_addr, size, NO_CQ_DATA, ctx);
 	if (ret)
@@ -1890,8 +1970,11 @@ ssize_t ft_inject(struct fid_ep *ep, fi_addr_t fi_addr, size_t size)
 {
 	ssize_t ret;
 
-	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
-		ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE)) {
+		ret = ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+		if (ret)
+			return ret;
+	}
 
 	ret = ft_post_inject(ep, fi_addr, size);
 	if (ret)
@@ -2087,8 +2170,8 @@ ssize_t ft_post_rx_buf(struct fid_ep *ep, size_t size, void *ctx,
 	if (hints->caps & FI_TAGGED) {
 		op_tag = op_tag ? op_tag : rx_seq;
 		FT_POST(fi_trecv, ft_progress, rxcq, rx_seq, &rx_cq_cntr,
-			"receive", ep, op_buf, size, op_mr_desc, 0, op_tag,
-			0, ctx);
+			"receive", ep, op_buf, size, op_mr_desc,
+			remote_fi_addr, op_tag, 0, ctx);
 	} else {
 		FT_POST(fi_recv, ft_progress, rxcq, rx_seq, &rx_cq_cntr,
 			"receive", ep, op_buf, size, op_mr_desc, 0, ctx);
@@ -2607,6 +2690,21 @@ int ft_fork_and_pair(void)
 	return 0;
 }
 
+int ft_fork_child(void)
+{
+	ft_child_pid = fork();
+	if (ft_child_pid < 0) {
+		FT_PRINTERR("fork", ft_child_pid);
+		return -errno;
+	}
+
+	if (ft_child_pid == 0) {
+		exit(0);
+	}
+
+	return 0;
+}
+
 int ft_wait_child(void)
 {
 	int ret;
@@ -2827,6 +2925,7 @@ void ft_usage(char *name, char *desc)
 	FT_PRINT_OPTS_USAGE("", "fi_rdm_tagged_pingpong");
 	FT_PRINT_OPTS_USAGE("", "fi_rma_bw");
 	FT_PRINT_OPTS_USAGE("-M <mode>", "Disable mode bit from test");
+	FT_PRINT_OPTS_USAGE("-K", "fork a child process after initializing endpoint");
 	FT_PRINT_OPTS_USAGE("", "mr_local");
 	FT_PRINT_OPTS_USAGE("-a <address vector name>", "name of address vector");
 	FT_PRINT_OPTS_USAGE("-h", "display this help output");
@@ -2938,6 +3037,9 @@ void ft_parseinfo(int op, char *optarg, struct fi_info *hints,
 	case 'H':
 		opts->options |= FT_OPT_ENABLE_HMEM;
 		break;
+	case 'K':
+		opts->options |= FT_OPT_FORK_CHILD;
+		break;
 	default:
 		/* let getopt handle unknown opts*/
 		break;
@@ -2968,10 +3070,10 @@ void ft_parse_addr_opts(int op, char *optarg, struct ft_opts *opts)
 			opts->oob_port = default_oob_port;
 		break;
 	case 'F':
-		if (!strncasecmp("fi_sockaddr_in", optarg, 14))
-			opts->address_format = FI_SOCKADDR_IN;
-		else if (!strncasecmp("fi_sockaddr_in6", optarg, 15))
+		if (!strncasecmp("fi_sockaddr_in6", optarg, 15))
 			opts->address_format = FI_SOCKADDR_IN6;
+		else if (!strncasecmp("fi_sockaddr_in", optarg, 14))
+			opts->address_format = FI_SOCKADDR_IN;
 		else if (!strncasecmp("fi_sockaddr_ib", optarg, 14))
 			opts->address_format = FI_SOCKADDR_IB;
 		else if (!strncasecmp("fi_sockaddr", optarg, 11)) /* keep me last */
@@ -3069,47 +3171,76 @@ int ft_parse_rma_opts(int op, char *optarg, struct fi_info *hints,
 	return 0;
 }
 
-void ft_fill_buf(void *buf, size_t size)
+int ft_fill_buf(void *buf, size_t size)
 {
 	char *msg_buf;
-	int msg_index;
-	static unsigned int iter = 0;
+	int msg_index = 0;
 	size_t i;
+	int ret = 0;
+
+	if (opts.iface != FI_HMEM_SYSTEM) {
+		msg_buf = malloc(size);
+		if (!msg_buf)
+			return -FI_ENOMEM;
+	} else {
+		msg_buf = (char *) buf;
+	}
 
-	msg_index = ((iter++)*INTEG_SEED) % integ_alphabet_length;
-	msg_buf = (char *)buf;
 	for (i = 0; i < size; i++) {
-		msg_buf[i] = integ_alphabet[msg_index++];
-		if (msg_index >= integ_alphabet_length)
+		msg_buf[i] = integ_alphabet[msg_index];
+		if (++msg_index >= integ_alphabet_length)
 			msg_index = 0;
 	}
+
+	if (opts.iface != FI_HMEM_SYSTEM) {
+		ret = ft_hmem_copy_to(opts.iface, opts.device, buf, msg_buf, size);
+		if (ret)
+			goto out;
+	}
+out:
+	if (opts.iface != FI_HMEM_SYSTEM)
+		free(msg_buf);
+	return ret;
 }
 
 int ft_check_buf(void *buf, size_t size)
 {
 	char *recv_data;
 	char c;
-	static unsigned int iter = 0;
-	int msg_index;
+	int msg_index = 0;
 	size_t i;
+	int ret = 0;
 
-	msg_index = ((iter++)*INTEG_SEED) % integ_alphabet_length;
-	recv_data = (char *)buf;
+	if (opts.iface != FI_HMEM_SYSTEM) {
+		recv_data = malloc(size);
+		if (!recv_data)
+			return -FI_ENOMEM;
+	
+		ret = ft_hmem_copy_from(opts.iface, opts.device,
+					recv_data, buf, size);
+		if (ret)
+			goto out;
+	} else {
+		recv_data = (char *)buf;
+	}
 
 	for (i = 0; i < size; i++) {
-		c = integ_alphabet[msg_index++];
-		if (msg_index >= integ_alphabet_length)
+		c = integ_alphabet[msg_index];
+		if (++msg_index >= integ_alphabet_length)
 			msg_index = 0;
 		if (c != recv_data[i])
 			break;
 	}
 	if (i != size) {
-		printf("Error at iteration=%d size=%zu byte=%zu\n",
-			iter, size, i);
-		return 1;
+		printf("Data check error (%c!=%c) at byte %zu for "
+		       "buffer size %zu\n", c, recv_data[i], i, size);
+		ret = -FI_EIO;
 	}
 
-	return 0;
+out:
+	if (opts.iface != FI_HMEM_SYSTEM)
+		free(recv_data);
+	return ret;
 }
 
 uint64_t ft_init_cq_data(struct fi_info *info)
@@ -3247,17 +3378,14 @@ int ft_sock_connect(char *node, char *service)
 		goto free;
 	}
 
-	ret = 1;
-	ret = setsockopt(sock, IPPROTO_TCP, TCP_NODELAY, (void *) &ret, sizeof(ret));
-	if (ret)
-		perror("setsockopt");
-
 	ret = connect(sock, ai->ai_addr, ai->ai_addrlen);
 	if (ret) {
 		perror("connect");
 		close(sock);
+		goto free;
 	}
 
+	ret = ft_sock_setup(sock);
 free:
 	freeaddrinfo(ai);
 	return ret;
@@ -3265,7 +3393,7 @@ free:
 
 int ft_sock_accept()
 {
-	int ret, op;
+	int ret;
 
 	sock = accept(listen_sock, NULL, 0);
         if (sock < 0) {
@@ -3274,48 +3402,51 @@ int ft_sock_accept()
 		return ret;
 	}
 
-	op = 1;
-	ret = setsockopt(sock, IPPROTO_TCP, TCP_NODELAY,
-			  (void *) &op, sizeof(op));
-	if (ret)
-		perror("setsockopt");
-
-	return 0;
+	ret = ft_sock_setup(sock);
+	return ret;
 }
 
 int ft_sock_send(int fd, void *msg, size_t len)
 {
-	int ret;
+	size_t sent;
+	ssize_t ret, err = 0;
 
-	ret = send(fd, msg, len, 0);
-	if (ret == len) {
-		return 0;
-	} else if (ret < 0) {
-		perror("send");
-		return -errno;
-	} else {
-		perror("send aborted");
-		return -FI_ECONNABORTED;
+	for (sent = 0; sent < len; ) {
+		ret = send(fd, ((char *) msg) + sent, len - sent, 0);
+		if (ret > 0) {
+			sent += ret;
+		} else if (errno == EAGAIN || errno == EWOULDBLOCK) {
+			ft_force_progress();
+		} else {
+			err = -errno;
+			break;
+		}
 	}
+
+	return err ? err: 0;
 }
 
 int ft_sock_recv(int fd, void *msg, size_t len)
 {
-	int ret;
+	size_t rcvd;
+	ssize_t ret, err = 0;
 
-	ret = recv(fd, msg, len, MSG_WAITALL);
-	if (ret == len) {
-		return 0;
-	} else if (ret == 0) {
-		return -FI_ENOTCONN;
-	} else if (ret < 0) {
-		FT_PRINTERR("ft_sock_recv", -errno);
-		perror("recv");
-		return -errno;
-	} else {
-		perror("recv aborted");
-		return -FI_ECONNABORTED;
+	for (rcvd = 0; rcvd < len; ) {
+		ret = recv(fd, ((char *) msg) + rcvd, len - rcvd, 0);
+		if (ret > 0) {
+			rcvd += ret;
+		} else if (ret == 0) {
+			err = -FI_ENOTCONN;
+			break;
+		} else if (errno == EAGAIN || errno == EWOULDBLOCK) {
+			ft_force_progress();
+		} else {
+			err = -errno;
+			break;
+		}
 	}
+
+	return err ? err: 0;
 }
 
 int ft_sock_sync(int value)
diff --git a/fabtests/configure.ac b/fabtests/configure.ac
index 3ddc499..4583f85 100644
--- a/fabtests/configure.ac
+++ b/fabtests/configure.ac
@@ -5,7 +5,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ(2.57)
-AC_INIT([fabtests], [1.12.0], [ofiwg@lists.openfabrics.org])
+AC_INIT([fabtests], [1.14.0a1], [ofiwg@lists.openfabrics.org])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
 AC_CONFIG_HEADERS(config.h)
@@ -147,17 +147,20 @@ AC_CHECK_HEADER([rdma/fabric.h], [],
 
 AC_ARG_WITH([ze],
             AC_HELP_STRING([--with-ze], [Use non-default ZE location - default NO]),
-            [CPPFLAGS="-I$withval/include $CPPFLAGS"
-             LDFLAGS="-L$withval/$lib $LDFLAGS"],
-            [])
+            AS_IF([test x"$withval" != x"no"],
+		  [CPPFLAGS="-I$withval/include $CPPFLAGS"
+		   LDFLAGS="-L$withval/$lib $LDFLAGS"]))
 
-dnl Checks for ZE libraries
+dnl Checks for ZE support. Require fabtests to dlopen ZE libraries
+have_ze=0
 AS_IF([test x"$with_ze" != x"no"],
-      [AC_CHECK_LIB([ze_loader], zeInit,
-       AC_CHECK_HEADER([level_zero/ze_api.h],
-			AC_DEFINE([HAVE_LIBZE], 1, [ZE support])),
-			[])]
-      [])
+      [AC_CHECK_HEADER([level_zero/ze_api.h], [have_ze=1])])
+
+AS_IF([test x"$with_ze" != x"no" && test -n "$with_ze" && test "$have_ze" = "0" ],
+	[AC_MSG_ERROR([ZE support requested but ZE runtime not available.])],
+	[])
+
+AC_DEFINE_UNQUOTED([HAVE_LIBZE], [$have_ze], [ZE support])
 
 AC_MSG_CHECKING([for fi_trywait support])
 AC_LINK_IFELSE([AC_LANG_PROGRAM([[#include <rdma/fi_eq.h>]],
diff --git a/fabtests/fabtests.vcxproj b/fabtests/fabtests.vcxproj
index 5242eb8..d7bd0fe 100644
--- a/fabtests/fabtests.vcxproj
+++ b/fabtests/fabtests.vcxproj
@@ -1,4 +1,4 @@
-﻿<?xml version="1.0" encoding="utf-8"?>
+<?xml version="1.0" encoding="utf-8"?>
 <Project DefaultTargets="Build" ToolsVersion="14.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
   <ItemGroup Label="ProjectConfigurations">
     <ProjectConfiguration Include="Debug-v140|x64">
@@ -149,7 +149,6 @@
     <ClCompile Include="benchmarks\rdm_tagged_bw.c" />
     <ClCompile Include="benchmarks\rdm_tagged_pingpong.c" />
     <ClCompile Include="benchmarks\rma_bw.c" />
-    <ClCompile Include="common\jsmn.c" />
     <ClCompile Include="common\hmem.c" />
     <ClCompile Include="common\hmem_cuda.c" />
     <ClCompile Include="common\hmem_rocr.c" />
diff --git a/fabtests/functional/bw.c b/fabtests/functional/bw.c
index ff48be6..7be849e 100644
--- a/fabtests/functional/bw.c
+++ b/fabtests/functional/bw.c
@@ -38,9 +38,14 @@ int sleep_time = 0;
 
 static ssize_t post_one_tx(struct ft_context *msg)
 {
-	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
-		ft_fill_buf(msg->buf + ft_tx_prefix_size(),
-			    opts.transfer_size);
+	ssize_t ret;
+
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE)) {
+		ret = ft_fill_buf(msg->buf + ft_tx_prefix_size(),
+				  opts.transfer_size);
+		if (ret)
+			return ret;
+	}
 
 	return ft_post_tx_buf(ep, remote_fi_addr, opts.transfer_size,
 			      NO_CQ_DATA, &msg->context, msg->buf,
diff --git a/fabtests/functional/cm_data.c b/fabtests/functional/cm_data.c
index 74fec72..68b91a5 100644
--- a/fabtests/functional/cm_data.c
+++ b/fabtests/functional/cm_data.c
@@ -150,7 +150,10 @@ static int server_reject(size_t paramlen)
 		return ret;
 
 	/* Data will appear in error event generated on remote end. */
-	ft_fill_buf(cm_data, paramlen);
+	ret = ft_fill_buf(cm_data, paramlen);
+	if (ret)
+		return ret;
+
 	ret = fi_reject(pep, fi->handle, cm_data, paramlen);
 	if (ret)
 		FT_PRINTERR("fi_reject", ret);
@@ -187,7 +190,9 @@ static int server_accept(size_t paramlen)
 		goto err;
 	}
 	/* Data will appear on accept event on remote end. */
-	ft_fill_buf(cm_data, paramlen);
+	ret = ft_fill_buf(cm_data, paramlen);
+	if (ret)
+		return ret;
 
 	/* Accept the incoming connection. Also transitions endpoint to active
 	 * state.
@@ -254,7 +259,11 @@ static int server(size_t paramlen)
 
 static int client_connect(size_t paramlen)
 {
-	ft_fill_buf(cm_data, paramlen);
+	int ret;
+
+	ret = ft_fill_buf(cm_data, paramlen);
+	if (ret)
+		return ret;
 
 	/* Connect to server */
 	return fi_connect(ep, fi->dest_addr, cm_data, paramlen);
diff --git a/fabtests/functional/inj_complete.c b/fabtests/functional/inj_complete.c
index 0980508..ce27ec4 100644
--- a/fabtests/functional/inj_complete.c
+++ b/fabtests/functional/inj_complete.c
@@ -44,8 +44,11 @@ static int send_msg(int sendmsg, size_t size)
 	int ret;
 	ft_tag = 0xabcd;
 
-	if (ft_check_opts(FT_OPT_VERIFY_DATA))
-		ft_fill_buf(tx_buf, size);
+	if (ft_check_opts(FT_OPT_VERIFY_DATA)) {
+		ret = ft_fill_buf(tx_buf, size);
+		if (ret)
+			return ret;
+	}
 
 	if (sendmsg) {
 		ret = ft_sendmsg(ep, remote_fi_addr, size,
diff --git a/fabtests/functional/loopback.c b/fabtests/functional/loopback.c
new file mode 100644
index 0000000..e15e4e3
--- /dev/null
+++ b/fabtests/functional/loopback.c
@@ -0,0 +1,113 @@
+/*
+ * Copyright (c) 2021 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under the BSD license
+ * below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <getopt.h>
+#include <unistd.h>
+
+#include <shared.h>
+
+
+static int run(void)
+{
+	int ret;
+
+	ret = ft_getinfo(hints, &fi);
+	if (ret)
+		return ret;
+
+	ret = ft_open_fabric_res();
+	if (ret)
+		return ret;
+
+	ret = ft_alloc_active_res(fi);
+	if (ret)
+		return ret;
+
+	ret = ft_enable_ep_recv();
+	if (ret)
+		return ret;
+
+	opts.dst_addr = fi->src_addr;
+	fi->dest_addr = fi->src_addr;
+	fi->dest_addrlen = fi->src_addrlen;
+
+	ret = ft_init_av();
+	if (ret)
+		goto out;
+
+	ret = ft_send_greeting(ep);
+	if (ret)
+		goto out;
+
+	ret = ft_recv_greeting(ep);
+	if (ret)
+		goto out;
+
+out:
+	fi->dest_addr = NULL;
+	fi->dest_addrlen = 0;
+	return ret;
+}
+
+int main(int argc, char **argv)
+{
+	int op, ret;
+
+	opts = INIT_OPTS;
+
+	hints = fi_allocinfo();
+	if (!hints)
+		return EXIT_FAILURE;
+
+	hints->caps |= FI_LOCAL_COMM;
+	hints->ep_attr->type = FI_EP_RDM;
+
+	while ((op = getopt(argc, argv, "h" INFO_OPTS)) != -1) {
+		switch (op) {
+		default:
+			ft_parseinfo(op, optarg, hints, &opts);
+			break;
+		case '?':
+		case 'h':
+			ft_usage(argv[0], "A loopback communication test.");
+			return EXIT_FAILURE;
+		}
+	}
+
+	hints->caps = FI_MSG;
+	hints->mode = FI_CONTEXT;
+	hints->domain_attr->mr_mode = opts.mr_mode;
+
+	ret = run();
+
+	ft_free_res();
+	return ft_exit_code(ret);
+}
diff --git a/fabtests/functional/multi_ep.c b/fabtests/functional/multi_ep.c
index 3e122ca..c233d14 100644
--- a/fabtests/functional/multi_ep.c
+++ b/fabtests/functional/multi_ep.c
@@ -111,8 +111,11 @@ static int do_transfers(void)
 	}
 
 	for (i = 0; i < num_eps; i++) {
-		if (ft_check_opts(FT_OPT_VERIFY_DATA))
-			ft_fill_buf(send_bufs[i], opts.transfer_size);
+		if (ft_check_opts(FT_OPT_VERIFY_DATA)) {
+			ret = ft_fill_buf(send_bufs[i], opts.transfer_size);
+			if (ret)
+				return ret;
+		}
 
 		tx_buf = send_bufs[i];
 		ret = ft_post_tx(eps[i], remote_addr[i], opts.transfer_size, NO_CQ_DATA, &send_ctx[i]);
diff --git a/fabtests/functional/multi_mr.c b/fabtests/functional/multi_mr.c
index 6a814fc..500e25f 100644
--- a/fabtests/functional/multi_mr.c
+++ b/fabtests/functional/multi_mr.c
@@ -184,8 +184,10 @@ static int mr_key_test()
 		tx_buf = (char *)mr_res_array[i].buf;
 
 		if (opts.dst_addr) {
-			ft_fill_buf(mr_res_array[i].buf,
-					opts.transfer_size);
+			ret = ft_fill_buf(mr_res_array[i].buf,
+					  opts.transfer_size);
+			if (ret)
+				return ret;
 
 			if (verbose)
 				printf("write to host's key %lx\n",
@@ -231,8 +233,10 @@ static int mr_key_test()
 					return ret;
 			}
 
-			ft_fill_buf(mr_res_array[i].buf,
-					opts.transfer_size);
+			ret = ft_fill_buf(mr_res_array[i].buf,
+					  opts.transfer_size);
+			if (ret)
+				return ret;
 
 			if (verbose)
 				printf("write to client's key %lx\n",
diff --git a/fabtests/functional/rdm_atomic.c b/fabtests/functional/rdm_atomic.c
index 369b324..3cd9611 100644
--- a/fabtests/functional/rdm_atomic.c
+++ b/fabtests/functional/rdm_atomic.c
@@ -435,6 +435,10 @@ static int init_fabric(void)
 {
 	int ret;
 
+	ret = ft_init();
+	if (ret)
+		return ret;
+
 	ret  = ft_init_oob();
 	if (ret)
 		return ret;
diff --git a/fabtests/functional/rdm_multi_client.c b/fabtests/functional/rdm_multi_client.c
new file mode 100644
index 0000000..a19b794
--- /dev/null
+++ b/fabtests/functional/rdm_multi_client.c
@@ -0,0 +1,242 @@
+/*
+ * Copyright (c) 2021, Amazon.com, Inc.  All rights reserved.
+ *
+ * This software is available to you under the BSD license
+ * below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * This program tests the functionality of RDM endpoint in the case
+ * that a persistent server does ping-pong with multiple clients that
+ * come and leave in sequence. The client connects to a server, sends
+ * ping-pong, disconnects with the server by cleaning all fabric
+ * resources, and repeats.
+ * If the `-R` option is specified, it will re-use the first client's 
+ * address for the subsequent clients by setting the src_addr for
+ * endpoints 2..n to the output of fi_getname() of the first client.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <getopt.h>
+
+#include <shared.h>
+#include <rdma/fi_cm.h>
+
+static int run_pingpong(void)
+{
+	int ret, i;
+
+	fprintf(stdout, "Start ping-pong.\n");
+	for (i = 0; i < opts.iterations; i++) {
+		if (opts.dst_addr) {
+			ret = ft_tx(ep, remote_fi_addr, opts.transfer_size, &tx_ctx);
+			if (ret) {
+				FT_PRINTERR("ft_tx", -ret);
+				return ret;
+			}
+			ret = ft_rx(ep, opts.transfer_size);
+			if (ret) {
+				FT_PRINTERR("ft_rx", -ret);
+				return ret;
+			}
+		} else {
+			ret = ft_rx(ep, opts.transfer_size);
+			if (ret) {
+				FT_PRINTERR("ft_rx", -ret);
+				return ret;
+			}
+			ret = ft_tx(ep, remote_fi_addr, opts.transfer_size, &tx_ctx);
+			if (ret) {
+				FT_PRINTERR("ft_tx", -ret);
+				return ret;
+			}
+		}
+	}
+
+	fprintf(stdout, "Ping-pong succeeds.\n");
+	return 0;
+}
+
+static int run_server(void)
+{
+	int nconn, ret;
+
+	ret = ft_init_fabric();
+	if (ret) {
+		FT_PRINTERR("ft_init_fabric", -ret);
+		return ret;
+	}
+
+	nconn = opts.num_connections;
+
+	while (nconn) {
+		ret = run_pingpong();
+		if (ret) {
+			FT_PRINTERR("run_pingpong", -ret);
+			return ret;
+		}
+		if (--nconn) {
+			ret = ft_init_av();
+			if (ret) {
+				FT_PRINTERR("ft_init_av", -ret);
+				return ret;
+			}
+		}
+	}
+	return 0;
+}
+
+static int run_client(int client_id, bool address_reuse)
+{
+	static char name[256];
+	static size_t size = sizeof(name);
+	int ret;
+
+	ret = ft_init();
+	if (ret) {
+		FT_PRINTERR("ft_init", -ret);
+		return ret;
+	}
+
+	ret = ft_init_oob();
+	if (ret) {
+		FT_PRINTERR("ft_init_oob", -ret);
+		return ret;
+	}
+
+	ret = ft_getinfo(hints, &fi);
+	if (ret) {
+		FT_PRINTERR("ft_getinfo", -ret);
+		return ret;
+	}
+
+	ret = ft_open_fabric_res();
+	if (ret) {
+		FT_PRINTERR("ft_open_fabric_res", -ret);
+		return ret;
+	}
+
+	if (client_id > 0 && address_reuse) {
+		memcpy(fi->src_addr, name, size);
+		fi->src_addrlen = size;
+	}
+
+	ret = ft_alloc_active_res(fi);
+	if (ret) {
+		FT_PRINTERR("ft_alloc_active_res", -ret);
+		return ret;
+	}
+
+	ret = ft_enable_ep_recv();
+	if (ret) {
+		FT_PRINTERR("ft_enable_ep_recv", -ret);
+		return ret;
+	}
+
+	ret = ft_init_av();
+	if (ret) {
+		FT_PRINTERR("ft_init_av", -ret);
+		return ret;
+	}
+
+	if (client_id == 0) {
+		ret = fi_getname(&ep->fid, name, &size);
+		if (ret) {
+			FT_PRINTERR("fi_getname", -ret);
+			return ret;
+		}
+	}
+
+	return run_pingpong();
+}
+
+static void print_opts_usage(char *name, char *desc)
+{
+	ft_usage(name, desc);
+	/* rdm_multi_client test op type */
+	FT_PRINT_OPTS_USAGE("-R", "Reuse the address of the first client for subsequent clients");
+}
+
+int main(int argc, char **argv)
+{
+	int op, ret, i;
+	struct fi_info *save;
+	bool address_reuse = false;
+
+	opts = INIT_OPTS;
+	opts.options |= FT_OPT_SIZE;
+
+	hints = fi_allocinfo();
+	if (!hints)
+		return EXIT_FAILURE;
+
+	while ((op = getopt(argc, argv, "URh" ADDR_OPTS INFO_OPTS CS_OPTS)) != -1) {
+		switch (op) {
+		default:
+			ft_parse_addr_opts(op, optarg, &opts);
+			ft_parseinfo(op, optarg, hints, &opts);
+			ft_parsecsopts(op, optarg, &opts);
+			break;
+		case 'U':
+			hints->tx_attr->op_flags |= FI_DELIVERY_COMPLETE;
+			break;
+		case 'R':
+			address_reuse = true;
+			break;
+		case '?':
+		case 'h':
+			print_opts_usage(argv[0], "RDM multi-client test");
+			return EXIT_FAILURE;
+		}
+	}
+
+	if (optind < argc)
+		opts.dst_addr = argv[optind];
+
+	hints->ep_attr->type = FI_EP_RDM;
+	hints->caps = FI_MSG;
+	hints->mode = FI_CONTEXT;
+	hints->domain_attr->mr_mode = opts.mr_mode;
+
+	if (opts.dst_addr) {
+		for (i = 0; i < opts.num_connections; i++) {
+			save = fi_dupinfo(hints);
+			printf("Starting client: %d\n", i);
+			ret = run_client(i, address_reuse);
+			if (ret) {
+				FT_PRINTERR("run_client", -ret);
+				goto out;
+			}
+			ft_free_res();
+			hints = save;
+		}
+	} else {
+		ret = run_server();
+		if (ret)
+			FT_PRINTERR("run_server", -ret);
+	}
+out:
+	ft_free_res();
+	return ft_exit_code(ret);
+}
diff --git a/fabtests/functional/unexpected_msg.c b/fabtests/functional/unexpected_msg.c
index 1c9ef71..c180fc0 100644
--- a/fabtests/functional/unexpected_msg.c
+++ b/fabtests/functional/unexpected_msg.c
@@ -132,9 +132,12 @@ static int run_test_loop(void)
 	for (i = 0; i < num_iters; i++) {
 		for (j = 0; j < concurrent_msgs; j++) {
 			op_buf = get_tx_buf(j);
-			if (ft_check_opts(FT_OPT_VERIFY_DATA))
-				ft_fill_buf(op_buf + ft_tx_prefix_size(),
-					    opts.transfer_size);
+			if (ft_check_opts(FT_OPT_VERIFY_DATA)) {
+				ret = ft_fill_buf(op_buf + ft_tx_prefix_size(),
+						  opts.transfer_size);
+				if (ret)
+					return ret;
+			}
 
 			ret = ft_post_tx_buf(ep, remote_fi_addr,
 					     opts.transfer_size,
diff --git a/fabtests/include/jsmn.h b/fabtests/include/jsmn.h
index 48a07c1..b95368a 100644
--- a/fabtests/include/jsmn.h
+++ b/fabtests/include/jsmn.h
@@ -1,5 +1,7 @@
 /*
- * Copyright (c) 2010 Serge A. Zaitsev
+ * MIT License
+ *
+ * Copyright (c) 2010 Serge Zaitsev
  *
  * Permission is hereby granted, free of charge, to any person obtaining a copy
  * of this software and associated documentation files (the "Software"), to deal
@@ -16,12 +18,11 @@
  * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
- * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
- * THE SOFTWARE.
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
  */
-
-#ifndef __JSMN_H_
-#define __JSMN_H_
+#ifndef JSMN_H
+#define JSMN_H
 
 #include <stddef.h>
 
@@ -29,6 +30,12 @@
 extern "C" {
 #endif
 
+#ifdef JSMN_STATIC
+#define JSMN_API static
+#else
+#define JSMN_API extern
+#endif
+
 /**
  * JSON type identifier. Basic types are:
  * 	o Object
@@ -37,61 +44,425 @@ extern "C" {
  * 	o Other primitive: number, boolean (true/false) or null
  */
 typedef enum {
-	JSMN_PRIMITIVE = 0,
-	JSMN_OBJECT = 1,
-	JSMN_ARRAY = 2,
-	JSMN_STRING = 3
+  JSMN_UNDEFINED = 0,
+  JSMN_OBJECT = 1,
+  JSMN_ARRAY = 2,
+  JSMN_STRING = 3,
+  JSMN_PRIMITIVE = 4
 } jsmntype_t;
 
-typedef enum {
-	/* Not enough tokens were provided */
-	JSMN_ERROR_NOMEM = -1,
-	/* Invalid character inside JSON string */
-	JSMN_ERROR_INVAL = -2,
-	/* The string is not a full JSON packet, more bytes expected */
-	JSMN_ERROR_PART = -3
-} jsmnerr_t;
+enum jsmnerr {
+  /* Not enough tokens were provided */
+  JSMN_ERROR_NOMEM = -1,
+  /* Invalid character inside JSON string */
+  JSMN_ERROR_INVAL = -2,
+  /* The string is not a full JSON packet, more bytes expected */
+  JSMN_ERROR_PART = -3
+};
 
 /**
  * JSON token description.
- * @param		type	type (object, array, string etc.)
- * @param		start	start position in JSON data string
- * @param		end		end position in JSON data string
+ * type		type (object, array, string etc.)
+ * start	start position in JSON data string
+ * end		end position in JSON data string
  */
 typedef struct {
-	jsmntype_t type;
-	int start;
-	int end;
-	int size;
+  jsmntype_t type;
+  int start;
+  int end;
+  int size;
 #ifdef JSMN_PARENT_LINKS
-	int parent;
+  int parent;
 #endif
 } jsmntok_t;
 
 /**
  * JSON parser. Contains an array of token blocks available. Also stores
- * the string being parsed now and current position in that string
+ * the string being parsed now and current position in that string.
  */
 typedef struct {
-	unsigned int pos; /* offset in the JSON string */
-	unsigned int toknext; /* next token to allocate */
-	int toksuper; /* superior token node, e.g parent object or array */
+  unsigned int pos;     /* offset in the JSON string */
+  unsigned int toknext; /* next token to allocate */
+  int toksuper;         /* superior token node, e.g. parent object or array */
 } jsmn_parser;
 
 /**
  * Create JSON parser over an array of tokens
  */
-void jsmn_init(jsmn_parser *parser);
+JSMN_API void jsmn_init(jsmn_parser *parser);
 
 /**
- * Run JSON parser. It parses a JSON data string into and array of tokens, each describing
+ * Run JSON parser. It parses a JSON data string into and array of tokens, each
+ * describing
  * a single JSON object.
  */
-jsmnerr_t jsmn_parse(jsmn_parser *parser, const char *js, size_t len,
-		jsmntok_t *tokens, unsigned int num_tokens);
+JSMN_API int jsmn_parse(jsmn_parser *parser, const char *js, const size_t len,
+                        jsmntok_t *tokens, const unsigned int num_tokens);
+
+#ifndef JSMN_HEADER
+/**
+ * Allocates a fresh unused token from the token pool.
+ */
+static jsmntok_t *jsmn_alloc_token(jsmn_parser *parser, jsmntok_t *tokens,
+                                   const size_t num_tokens) {
+  jsmntok_t *tok;
+  if (parser->toknext >= num_tokens) {
+    return NULL;
+  }
+  tok = &tokens[parser->toknext++];
+  tok->start = tok->end = -1;
+  tok->size = 0;
+#ifdef JSMN_PARENT_LINKS
+  tok->parent = -1;
+#endif
+  return tok;
+}
+
+/**
+ * Fills token type and boundaries.
+ */
+static void jsmn_fill_token(jsmntok_t *token, const jsmntype_t type,
+                            const int start, const int end) {
+  token->type = type;
+  token->start = start;
+  token->end = end;
+  token->size = 0;
+}
+
+/**
+ * Fills next available token with JSON primitive.
+ */
+static int jsmn_parse_primitive(jsmn_parser *parser, const char *js,
+                                const size_t len, jsmntok_t *tokens,
+                                const size_t num_tokens) {
+  jsmntok_t *token;
+  int start;
+
+  start = parser->pos;
+
+  for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
+    switch (js[parser->pos]) {
+#ifndef JSMN_STRICT
+    /* In strict mode primitive must be followed by "," or "}" or "]" */
+    case ':':
+#endif
+    case '\t':
+    case '\r':
+    case '\n':
+    case ' ':
+    case ',':
+    case ']':
+    case '}':
+      goto found;
+    }
+    if (js[parser->pos] < 32 || js[parser->pos] >= 127) {
+      parser->pos = start;
+      return JSMN_ERROR_INVAL;
+    }
+  }
+#ifdef JSMN_STRICT
+  /* In strict mode primitive must be followed by a comma/object/array */
+  parser->pos = start;
+  return JSMN_ERROR_PART;
+#endif
+
+found:
+  if (tokens == NULL) {
+    parser->pos--;
+    return 0;
+  }
+  token = jsmn_alloc_token(parser, tokens, num_tokens);
+  if (token == NULL) {
+    parser->pos = start;
+    return JSMN_ERROR_NOMEM;
+  }
+  jsmn_fill_token(token, JSMN_PRIMITIVE, start, parser->pos);
+#ifdef JSMN_PARENT_LINKS
+  token->parent = parser->toksuper;
+#endif
+  parser->pos--;
+  return 0;
+}
+
+/**
+ * Fills next token with JSON string.
+ */
+static int jsmn_parse_string(jsmn_parser *parser, const char *js,
+                             const size_t len, jsmntok_t *tokens,
+                             const size_t num_tokens) {
+  jsmntok_t *token;
+
+  int start = parser->pos;
+
+  parser->pos++;
+
+  /* Skip starting quote */
+  for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
+    char c = js[parser->pos];
+
+    /* Quote: end of string */
+    if (c == '\"') {
+      if (tokens == NULL) {
+        return 0;
+      }
+      token = jsmn_alloc_token(parser, tokens, num_tokens);
+      if (token == NULL) {
+        parser->pos = start;
+        return JSMN_ERROR_NOMEM;
+      }
+      jsmn_fill_token(token, JSMN_STRING, start + 1, parser->pos);
+#ifdef JSMN_PARENT_LINKS
+      token->parent = parser->toksuper;
+#endif
+      return 0;
+    }
+
+    /* Backslash: Quoted symbol expected */
+    if (c == '\\' && parser->pos + 1 < len) {
+      int i;
+      parser->pos++;
+      switch (js[parser->pos]) {
+      /* Allowed escaped symbols */
+      case '\"':
+      case '/':
+      case '\\':
+      case 'b':
+      case 'f':
+      case 'r':
+      case 'n':
+      case 't':
+        break;
+      /* Allows escaped symbol \uXXXX */
+      case 'u':
+        parser->pos++;
+        for (i = 0; i < 4 && parser->pos < len && js[parser->pos] != '\0';
+             i++) {
+          /* If it isn't a hex character we have an error */
+          if (!((js[parser->pos] >= 48 && js[parser->pos] <= 57) ||   /* 0-9 */
+                (js[parser->pos] >= 65 && js[parser->pos] <= 70) ||   /* A-F */
+                (js[parser->pos] >= 97 && js[parser->pos] <= 102))) { /* a-f */
+            parser->pos = start;
+            return JSMN_ERROR_INVAL;
+          }
+          parser->pos++;
+        }
+        parser->pos--;
+        break;
+      /* Unexpected symbol */
+      default:
+        parser->pos = start;
+        return JSMN_ERROR_INVAL;
+      }
+    }
+  }
+  parser->pos = start;
+  return JSMN_ERROR_PART;
+}
+
+/**
+ * Parse JSON string and fill tokens.
+ */
+JSMN_API int jsmn_parse(jsmn_parser *parser, const char *js, const size_t len,
+                        jsmntok_t *tokens, const unsigned int num_tokens) {
+  int r;
+  int i;
+  jsmntok_t *token;
+  int count = parser->toknext;
+
+  for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
+    char c;
+    jsmntype_t type;
+
+    c = js[parser->pos];
+    switch (c) {
+    case '{':
+    case '[':
+      count++;
+      if (tokens == NULL) {
+        break;
+      }
+      token = jsmn_alloc_token(parser, tokens, num_tokens);
+      if (token == NULL) {
+        return JSMN_ERROR_NOMEM;
+      }
+      if (parser->toksuper != -1) {
+        jsmntok_t *t = &tokens[parser->toksuper];
+#ifdef JSMN_STRICT
+        /* In strict mode an object or array can't become a key */
+        if (t->type == JSMN_OBJECT) {
+          return JSMN_ERROR_INVAL;
+        }
+#endif
+        t->size++;
+#ifdef JSMN_PARENT_LINKS
+        token->parent = parser->toksuper;
+#endif
+      }
+      token->type = (c == '{' ? JSMN_OBJECT : JSMN_ARRAY);
+      token->start = parser->pos;
+      parser->toksuper = parser->toknext - 1;
+      break;
+    case '}':
+    case ']':
+      if (tokens == NULL) {
+        break;
+      }
+      type = (c == '}' ? JSMN_OBJECT : JSMN_ARRAY);
+#ifdef JSMN_PARENT_LINKS
+      if (parser->toknext < 1) {
+        return JSMN_ERROR_INVAL;
+      }
+      token = &tokens[parser->toknext - 1];
+      for (;;) {
+        if (token->start != -1 && token->end == -1) {
+          if (token->type != type) {
+            return JSMN_ERROR_INVAL;
+          }
+          token->end = parser->pos + 1;
+          parser->toksuper = token->parent;
+          break;
+        }
+        if (token->parent == -1) {
+          if (token->type != type || parser->toksuper == -1) {
+            return JSMN_ERROR_INVAL;
+          }
+          break;
+        }
+        token = &tokens[token->parent];
+      }
+#else
+      for (i = parser->toknext - 1; i >= 0; i--) {
+        token = &tokens[i];
+        if (token->start != -1 && token->end == -1) {
+          if (token->type != type) {
+            return JSMN_ERROR_INVAL;
+          }
+          parser->toksuper = -1;
+          token->end = parser->pos + 1;
+          break;
+        }
+      }
+      /* Error if unmatched closing bracket */
+      if (i == -1) {
+        return JSMN_ERROR_INVAL;
+      }
+      for (; i >= 0; i--) {
+        token = &tokens[i];
+        if (token->start != -1 && token->end == -1) {
+          parser->toksuper = i;
+          break;
+        }
+      }
+#endif
+      break;
+    case '\"':
+      r = jsmn_parse_string(parser, js, len, tokens, num_tokens);
+      if (r < 0) {
+        return r;
+      }
+      count++;
+      if (parser->toksuper != -1 && tokens != NULL) {
+        tokens[parser->toksuper].size++;
+      }
+      break;
+    case '\t':
+    case '\r':
+    case '\n':
+    case ' ':
+      break;
+    case ':':
+      parser->toksuper = parser->toknext - 1;
+      break;
+    case ',':
+      if (tokens != NULL && parser->toksuper != -1 &&
+          tokens[parser->toksuper].type != JSMN_ARRAY &&
+          tokens[parser->toksuper].type != JSMN_OBJECT) {
+#ifdef JSMN_PARENT_LINKS
+        parser->toksuper = tokens[parser->toksuper].parent;
+#else
+        for (i = parser->toknext - 1; i >= 0; i--) {
+          if (tokens[i].type == JSMN_ARRAY || tokens[i].type == JSMN_OBJECT) {
+            if (tokens[i].start != -1 && tokens[i].end == -1) {
+              parser->toksuper = i;
+              break;
+            }
+          }
+        }
+#endif
+      }
+      break;
+#ifdef JSMN_STRICT
+    /* In strict mode primitives are: numbers and booleans */
+    case '-':
+    case '0':
+    case '1':
+    case '2':
+    case '3':
+    case '4':
+    case '5':
+    case '6':
+    case '7':
+    case '8':
+    case '9':
+    case 't':
+    case 'f':
+    case 'n':
+      /* And they must not be keys of the object */
+      if (tokens != NULL && parser->toksuper != -1) {
+        const jsmntok_t *t = &tokens[parser->toksuper];
+        if (t->type == JSMN_OBJECT ||
+            (t->type == JSMN_STRING && t->size != 0)) {
+          return JSMN_ERROR_INVAL;
+        }
+      }
+#else
+    /* In non-strict mode every unquoted value is a primitive */
+    default:
+#endif
+      r = jsmn_parse_primitive(parser, js, len, tokens, num_tokens);
+      if (r < 0) {
+        return r;
+      }
+      count++;
+      if (parser->toksuper != -1 && tokens != NULL) {
+        tokens[parser->toksuper].size++;
+      }
+      break;
+
+#ifdef JSMN_STRICT
+    /* Unexpected char in strict mode */
+    default:
+      return JSMN_ERROR_INVAL;
+#endif
+    }
+  }
+
+  if (tokens != NULL) {
+    for (i = parser->toknext - 1; i >= 0; i--) {
+      /* Unmatched opened object or array */
+      if (tokens[i].start != -1 && tokens[i].end == -1) {
+        return JSMN_ERROR_PART;
+      }
+    }
+  }
+
+  return count;
+}
+
+/**
+ * Creates a new parser based over a given buffer with an array of tokens
+ * available.
+ */
+JSMN_API void jsmn_init(jsmn_parser *parser) {
+  parser->pos = 0;
+  parser->toknext = 0;
+  parser->toksuper = -1;
+}
+
+#endif /* JSMN_HEADER */
 
 #ifdef __cplusplus
 }
 #endif
 
-#endif /* __JSMN_H_ */
+#endif /* JSMN_H */
diff --git a/fabtests/include/shared.h b/fabtests/include/shared.h
index 46bb751..10f0bb4 100644
--- a/fabtests/include/shared.h
+++ b/fabtests/include/shared.h
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2013-2017 Intel Corporation.  All rights reserved.
  * Copyright (c) 2014-2017, Cisco Systems, Inc. All rights reserved.
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under the BSD license below:
  *
@@ -118,6 +119,7 @@ enum {
 	FT_OPT_ENABLE_HMEM	= 1 << 17,
 	FT_OPT_USE_DEVICE	= 1 << 18,
 	FT_OPT_DOMAIN_EQ	= 1 << 19,
+	FT_OPT_FORK_CHILD	= 1 << 20,
 	FT_OPT_OOB_CTRL		= FT_OPT_OOB_SYNC | FT_OPT_OOB_ADDR_EXCH,
 };
 
@@ -230,7 +232,7 @@ void ft_usage(char *name, char *desc);
 void ft_mcusage(char *name, char *desc);
 void ft_csusage(char *name, char *desc);
 
-void ft_fill_buf(void *buf, size_t size);
+int ft_fill_buf(void *buf, size_t size);
 int ft_check_buf(void *buf, size_t size);
 int ft_check_opts(uint64_t flags);
 uint64_t ft_init_cq_data(struct fi_info *info);
@@ -248,7 +250,7 @@ extern int ft_socket_pair[2];
 extern int sock;
 extern int listen_sock;
 #define ADDR_OPTS "B:P:s:a:b::E::C:F:"
-#define FAB_OPTS "f:d:p:D:i:H"
+#define FAB_OPTS "f:d:p:D:i:HK"
 #define INFO_OPTS FAB_OPTS "e:M:"
 #define CS_OPTS ADDR_OPTS "I:QS:mc:t:w:l"
 #define NO_CQ_DATA 0
@@ -259,6 +261,7 @@ extern char default_port[8];
 	{	.options = FT_OPT_RX_CQ | FT_OPT_TX_CQ, \
 		.iterations = 1000, \
 		.warmup_iterations = 10, \
+		.num_connections = 1, \
 		.transfer_size = 1024, \
 		.window_size = 64, \
 		.av_size = 1, \
@@ -363,11 +366,14 @@ static inline int ft_use_size(int index, int enable_flags)
 		}							\
 	} while (0)
 
+int ft_init();
 int ft_alloc_bufs();
 int ft_open_fabric_res();
 int ft_getinfo(struct fi_info *hints, struct fi_info **info);
 int ft_init_fabric();
 int ft_init_oob();
+int ft_close_oob();
+int ft_reset_oob();
 int ft_start_server();
 int ft_server_connect();
 int ft_client_connect();
@@ -435,6 +441,7 @@ static inline bool ft_check_prefix_forced(struct fi_info *info,
 int ft_sync(void);
 int ft_sync_pair(int status);
 int ft_fork_and_pair(void);
+int ft_fork_child(void);
 int ft_wait_child(void);
 int ft_finalize(void);
 int ft_finalize_ep(struct fid_ep *ep);
diff --git a/fabtests/include/unix/osd.h b/fabtests/include/unix/osd.h
index 3d7b415..4e85ca9 100644
--- a/fabtests/include/unix/osd.h
+++ b/fabtests/include/unix/osd.h
@@ -34,12 +34,28 @@
 #define _FABTESTS_UNIX_OSD_H_
 
 #include <complex.h>
+#include <unistd.h>
+#include <fcntl.h>
 
 static inline int ft_startup(void)
 {
 	return 0;
 }
 
+static inline int ft_fd_nonblock(int fd)
+{
+	long flags;
+
+	flags = fcntl(fd, F_GETFL);
+	if (flags < 0)
+		return -errno;
+
+	if (fcntl(fd, F_SETFL, flags | O_NONBLOCK))
+		return -errno;
+
+	return 0;
+}
+
 /* complex operations implementation */
 #define OFI_COMPLEX(name) ofi_##name##_complex
 #define OFI_COMPLEX_OP(name, op) ofi_complex_##name##_##op
diff --git a/fabtests/include/windows/osd.h b/fabtests/include/windows/osd.h
index bf7a114..955f54f 100644
--- a/fabtests/include/windows/osd.h
+++ b/fabtests/include/windows/osd.h
@@ -131,6 +131,12 @@ static long int sysconf(int name)
 
 int socketpair(int af, int type, int protocol, int socks[2]);
 
+static inline int ft_fd_nonblock(int fd)
+{
+	u_long argp = 1;
+	return ioctlsocket(fd, FIONBIO, &argp) ? -WSAGetLastError() : 0;
+}
+
 /* Bits in the fourth argument to `waitid'.  */
 #define WSTOPPED	2	/* Report stopped child (same as WUNTRACED). */
 #define WEXITED		4	/* Report dead child. */
diff --git a/fabtests/man/fabtests.7.md b/fabtests/man/fabtests.7.md
index e127121..725dea4 100644
--- a/fabtests/man/fabtests.7.md
+++ b/fabtests/man/fabtests.7.md
@@ -148,6 +148,10 @@ features of libfabric.
   A sleep time on the receiving side can be enabled in order to allow
   the sender to get ahead of the receiver.
 
+*fi_rdm_multi_client*
+: Tests a persistent server communicating with multiple clients, one at a
+  time, in sequence.
+
 # Benchmarks
 
 The client and the server exchange messages in either a ping-pong manner,
@@ -261,7 +265,7 @@ The following keys and respective key values may be used in the config file.
   FT_FUNC_INJECT, FT_FUNC_INJECTDATA, FT_FUNC_SENDDATA
 
   For FT_CAP_RMA: FT_FUNC_WRITE, FT_FUNC_WRITEV, FT_FUNC_WRITEMSG,
-  FT_FUNC_WRITEDATA, FT_FUNC_INJECT_WRITE, FT_FUNC_INJECT_WRITEDATA
+  FT_FUNC_WRITEDATA, FT_FUNC_INJECT_WRITE, FT_FUNC_INJECT_WRITEDATA,
   FT_FUNC_READ, FT_FUNC_READV, FT_FUNC_READMSG
 
   For FT_CAP_ATOMIC: FT_FUNC_ATOMIC, FT_FUNC_ATOMICV, FT_FUNC_ATOMICMSG,
@@ -387,6 +391,9 @@ the list available for that test.
 *-F <address_format>
 : Specifies the address format.
 
+*-K
+: Fork a child process after initializing endpoint.
+
 *-b[=oob_port]*
 : Enables out-of-band (via sockets) address exchange and test
   synchronization.  A port for the out-of-band connection may be specified
diff --git a/fabtests/man/man1/fi_rdm_multi_client.1 b/fabtests/man/man1/fi_rdm_multi_client.1
new file mode 100644
index 0000000..3f6ccf9
--- /dev/null
+++ b/fabtests/man/man1/fi_rdm_multi_client.1
@@ -0,0 +1 @@
+.so man7/fabtests.7
diff --git a/fabtests/multinode/src/core.c b/fabtests/multinode/src/core.c
index f39efc2..eb1dd77 100644
--- a/fabtests/multinode/src/core.c
+++ b/fabtests/multinode/src/core.c
@@ -82,7 +82,7 @@ static int multi_setup_fabric(int argc, char **argv)
 	char my_name[FT_MAX_CTRL_MSG];
 	size_t len;
 	int i, ret;
-	struct fi_rma_iov *remote = malloc(sizeof(*remote));
+	struct fi_rma_iov remote;
 
 	hints->ep_attr->type = FI_EP_RDM;
 	hints->mode = FI_CONTEXT;
@@ -176,14 +176,14 @@ static int multi_setup_fabric(int argc, char **argv)
 	}
 
 	if (fi->domain_attr->mr_mode & FI_MR_VIRT_ADDR)
-		remote->addr = (uintptr_t) rx_buf;
+		remote.addr = (uintptr_t) rx_buf;
 	else
-		remote->addr = 0;
+		remote.addr = 0;
 
-	remote->key = fi_mr_key(mr);
-	remote->len = rx_size;
+	remote.key = fi_mr_key(mr);
+	remote.len = rx_size;
 
-	ret = pm_allgather(remote, pm_job.multi_iovs, sizeof(*remote));
+	ret = pm_allgather(&remote, pm_job.multi_iovs, sizeof(remote));
 	if (ret) {
 		FT_ERR("error exchanging rma_iovs\n");
 		goto err;
diff --git a/fabtests/scripts/runfabtests.sh b/fabtests/scripts/runfabtests.sh
index 7e9dbf9..6be927b 100755
--- a/fabtests/scripts/runfabtests.sh
+++ b/fabtests/scripts/runfabtests.sh
@@ -138,6 +138,8 @@ functional_tests=(
 	"fi_bw -e rdm -v -T 1"
 	"fi_bw -e rdm -v -T 1 -U"
 	"fi_bw -e msg -v -T 1"
+	"fi_rdm_multi_client -C 10 -I 5"
+	"fi_rdm_multi_client -C 10 -I 5 -U"
 )
 
 short_tests=(
@@ -476,7 +478,12 @@ function cs_test {
 	wait $c_pid
 	c_ret=$?
 
-	[[ c_ret -ne 0 ]] && kill -9 $s_pid 2> /dev/null
+	if [[ $c_ret -ne 0 ]] && ps -p $s_pid > /dev/null; then
+	    if [[ $STRICT_MODE -eq 0 ]]; then
+	        sleep 2
+	    fi
+	    kill -9 $s_pid 2> /dev/null
+	fi
 
 	wait $s_pid
 	s_ret=$?
@@ -505,6 +512,10 @@ function set_cfg_file {
 	local parent=$UTIL
 	local name=$CORE
 
+	if [[ ! -z "$COMPLEX_CFG" ]]; then
+		return
+	fi
+
 	if [ -z $UTIL ]; then
 		parent=$CORE
 		name=$1
@@ -532,9 +543,9 @@ function complex_test {
 	local end_time
 	local test_time
 
-	is_excluded "$test" && return
+	set_cfg_file $config
 	if [[ -z "$COMPLEX_CFG" ]]; then
-		set_cfg_file $config
+		is_excluded "$test" && return
 	fi
 
 	start_time=$(date '+%s')
@@ -549,12 +560,12 @@ function complex_test {
 		opts+=" -E"
 	fi
 
-	s_cmd="${BIN_PATH}${test_exe} -x $opts"
+	s_cmd="${BIN_PATH}${test_exe} ${S_ARGS} -x $opts"
 	FI_LOG_LEVEL=error ${SERVER_CMD} "${EXPORT_ENV} $s_cmd" &> $s_outp &
 	s_pid=$!
 	sleep 1
 
-	c_cmd="${BIN_PATH}${test_exe} -u "${COMPLEX_CFG}" $S_INTERFACE $opts"
+	c_cmd="${BIN_PATH}${test_exe} ${C_ARGS} -u "${COMPLEX_CFG}" $S_INTERFACE $opts"
 	FI_LOG_LEVEL=error ${CLIENT_CMD} "${EXPORT_ENV} $c_cmd" &> $c_outp &
 	c_pid=$!
 
@@ -632,7 +643,12 @@ function multinode_test {
 		c_ret=($?)||$c_ret
 	done
 
-	[[ c_ret -ne 0 ]] && kill -9 $s_pid 2> /dev/null
+	if [[ $c_ret -ne 0 ]] && ps -p $s_pid > /dev/null; then
+	    if [[ $STRICT_MODE -eq 0 ]]; then
+	        sleep 2
+	    fi
+	    kill -9 $s_pid 2> /dev/null
+	fi
 
 	wait $s_pid
 	s_ret=$?
diff --git a/fabtests/test_configs/efa/efa.exclude b/fabtests/test_configs/efa/efa.exclude
index 8ec42b4..ddf05a6 100644
--- a/fabtests/test_configs/efa/efa.exclude
+++ b/fabtests/test_configs/efa/efa.exclude
@@ -38,9 +38,6 @@
 # Exclude cq_data test until fi_pool/fi_wait is supported
 cq_data
 
-# Exclude all rdm prefix tests
-rdm.* -k
-
 multi_mr
 rdm_rma_trigger
 
@@ -61,9 +58,6 @@ trigger
 #rdm_cntr_pingpong
 
 
-# This test requires ENA IPs for the OOB sync
-av_xfer
-
 # Connection manager isn't supported
 cm_data
 
@@ -96,3 +90,4 @@ dgram_bw
 
 # Multinode tests failing with an unsupported address format
 multinode
+
diff --git a/fabtests/test_configs/ofi_rxm/tcp.test b/fabtests/test_configs/ofi_rxm/tcp.test
index baec15f..6087f7e 100644
--- a/fabtests/test_configs/ofi_rxm/tcp.test
+++ b/fabtests/test_configs/ofi_rxm/tcp.test
@@ -1,26 +1,190 @@
 {
 	prov_name: tcp;ofi_rxm,
 	test_type: [
+		FT_TEST_UNIT,
 		FT_TEST_LATENCY,
 		FT_TEST_BANDWIDTH,
 	],
+	test_class: [
+		FT_CAP_MSG,
+		FT_CAP_TAGGED,
+	],
 	class_function: [
 		FT_FUNC_SEND,
 		FT_FUNC_SENDV,
-		FT_FUNC_SENDDATA,
+		FT_FUNC_SENDMSG,
 		FT_FUNC_INJECT,
 		FT_FUNC_INJECTDATA,
+		FT_FUNC_SENDDATA,
 	],
 	ep_type: [
 		FI_EP_RDM,
 	],
 	comp_type: [
 		FT_COMP_QUEUE,
+		FT_COMP_CNTR,
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL,
+		FI_PROGRESS_AUTO
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST
+	],
+},
+{
+	prov_name: tcp;ofi_rxm,
+	test_type: [
+		FT_TEST_UNIT,
+		FT_TEST_LATENCY,
+		FT_TEST_BANDWIDTH
+	],
+	test_class: [
+		FT_CAP_RMA,
+	],
+	class_function: [
+		FT_FUNC_WRITE,
+		FT_FUNC_WRITEV,
+		FT_FUNC_WRITEMSG,
+		FT_FUNC_WRITEDATA,
+		FT_FUNC_INJECT_WRITE,
+		FT_FUNC_INJECT_WRITEDATA,
+		FT_FUNC_READ,
+		FT_FUNC_READV,
+		FT_FUNC_READMSG,
+	],
+	ep_type: [
+		FI_EP_RDM,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+		FT_COMP_CNTR,
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL,
+		FI_PROGRESS_AUTO
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST
+	],
+},
+{
+	prov_name: tcp;ofi_rxm,
+	test_type: [
+		FT_TEST_UNIT,
+		FT_TEST_LATENCY,
+		FT_TEST_BANDWIDTH
+	],
+	test_class: [
+		FT_CAP_ATOMIC,
+	],
+	class_function: [
+		FT_FUNC_ATOMIC,
+		FT_FUNC_ATOMICV,
+		FT_FUNC_ATOMICMSG,
+		FT_FUNC_INJECT_ATOMIC,
+		FT_FUNC_FETCH_ATOMIC,
+		FT_FUNC_FETCH_ATOMICV,
+		FT_FUNC_FETCH_ATOMICMSG,
+		FT_FUNC_COMPARE_ATOMIC,
+		FT_FUNC_COMPARE_ATOMICV,
+		FT_FUNC_COMPARE_ATOMICMSG
+	],
+	ep_type: [
+		FI_EP_RDM,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL,
+		FI_PROGRESS_AUTO
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST
+	],
+},
+{
+	prov_name: tcp;ofi_rxm,
+	test_type: [
+		FT_TEST_LATENCY,
+	],
+	test_class: [
+		FT_CAP_TAGGED,
+	],
+	ep_type: [
+		FI_EP_RDM,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	eq_wait_obj: [
+		FI_WAIT_NONE,
+		FI_WAIT_UNSPEC,
+		FI_WAIT_FD
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST
+	],
+},
+{
+	prov_name: tcp;ofi_rxm,
+	test_type: [
+		FT_TEST_LATENCY,
 	],
 	test_class: [
-		FT_CAP_MSG,
 		FT_CAP_TAGGED,
 	],
-	mr_mode: [FI_MR_LOCAL, FI_MR_VIRT_ADDR, FI_MR_ALLOCATED, FI_MR_PROV_KEY],
-	progress: [FI_PROGRESS_MANUAL, FI_PROGRESS_AUTO],
+	ep_type: [
+		FI_EP_RDM,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	cq_wait_obj: [
+		FI_WAIT_NONE,
+		FI_WAIT_UNSPEC,
+		FI_WAIT_FD
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST
+	],
+},
+{
+	prov_name: tcp;ofi_rxm,
+	test_type: [
+		FT_TEST_LATENCY,
+	],
+	test_class: [
+		FT_CAP_TAGGED,
+	],
+	ep_type: [
+		FI_EP_RDM,
+	],
+	comp_type: [
+		FT_COMP_CNTR,
+	],
+	cntr_wait_obj: [
+		FI_WAIT_NONE,
+		FI_WAIT_UNSPEC,
+		FI_WAIT_FD
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST
+	],
 },
diff --git a/fabtests/test_configs/psm2/psm2.exclude b/fabtests/test_configs/psm2/psm2.exclude
index 30303e0..36ec149 100644
--- a/fabtests/test_configs/psm2/psm2.exclude
+++ b/fabtests/test_configs/psm2/psm2.exclude
@@ -15,3 +15,4 @@ scalable_ep
 shared_av
 rdm_cntr_pingpong
 multi_recv
+rdm_multi_client
diff --git a/fabtests/test_configs/tcp/all.test b/fabtests/test_configs/tcp/all.test
new file mode 100644
index 0000000..c2f44c9
--- /dev/null
+++ b/fabtests/test_configs/tcp/all.test
@@ -0,0 +1,122 @@
+#: "Suite of tests for the tcp provider"
+{
+	prov_name: tcp,
+	test_type: [
+		FT_TEST_LATENCY,
+		FT_TEST_BANDWIDTH,
+		FT_TEST_UNIT
+	],
+	test_class: [
+		FT_CAP_MSG,
+	],
+	class_function: [
+		FT_FUNC_SEND,
+		FT_FUNC_SENDV,
+		FT_FUNC_SENDMSG,
+		FT_FUNC_INJECT,
+		FT_FUNC_INJECTDATA,
+		FT_FUNC_SENDDATA,
+	],
+	ep_type: [
+		FI_EP_MSG,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	progress: [
+		FI_PROGRESS_MANUAL,
+		FI_PROGRESS_AUTO,
+	]
+	test_flags: [
+		FT_FLAG_QUICKTEST,
+	],
+},
+{
+	prov_name: tcp,
+	test_type: [
+		FT_TEST_LATENCY,
+		FT_TEST_BANDWIDTH,
+		FT_TEST_UNIT,
+	],
+	test_class: [
+		FT_CAP_RMA,
+	],
+	class_function: [
+		FT_FUNC_WRITE,
+		FT_FUNC_WRITEV,
+		FT_FUNC_WRITEMSG,
+		FT_FUNC_WRITEDATA,
+		FT_FUNC_INJECT_WRITE,
+		FT_FUNC_INJECT_WRITEDATA,
+		FT_FUNC_READ,
+		FT_FUNC_READV,
+		FT_FUNC_READMSG,
+	],
+	ep_type: [
+		FI_EP_MSG,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	progress: [
+		FI_PROGRESS_MANUAL,
+		FI_PROGRESS_AUTO,
+	]
+	test_flags: [
+		FT_FLAG_QUICKTEST,
+	],
+}
+{
+	prov_name: tcp,
+	test_type: [
+		FT_TEST_LATENCY,
+	],
+	test_class: [
+		FT_CAP_MSG,
+	],
+	ep_type: [
+		FI_EP_MSG,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	eq_wait_obj: [
+		FI_WAIT_NONE,
+		FI_WAIT_UNSPEC,
+		FI_WAIT_FD
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL,
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST,
+	],
+},
+{
+	prov_name: tcp,
+	test_type: [
+		FT_TEST_LATENCY,
+	],
+	test_class: [
+		FT_CAP_MSG,
+	],
+	ep_type: [
+		FI_EP_MSG,
+	],
+	comp_type: [
+		FT_COMP_QUEUE,
+	],
+	cq_wait_obj: [
+		FI_WAIT_NONE,
+		FI_WAIT_UNSPEC,
+		FI_WAIT_FD
+	],
+	mr_mode: [],
+	progress: [
+		FI_PROGRESS_MANUAL,
+	],
+	test_flags: [
+		FT_FLAG_QUICKTEST,
+	],
+},
diff --git a/fabtests/test_configs/tcp/quick.test b/fabtests/test_configs/tcp/quick.test
deleted file mode 100644
index 34b323e..0000000
--- a/fabtests/test_configs/tcp/quick.test
+++ /dev/null
@@ -1,54 +0,0 @@
-#: "Suite of tests for the tcp provider"
-{
-	prov_name: tcp,
-	test_type: [
-		FT_TEST_LATENCY,
-		FT_TEST_BANDWIDTH,
-		FT_TEST_UNIT
-	],
-	class_function: [
-		FT_FUNC_SEND,
-		FT_FUNC_SENDV,
-		FT_FUNC_SENDMSG,
-		FT_FUNC_INJECT,
-		FT_FUNC_INJECTDATA,
-		FT_FUNC_SENDDATA,
-	],
-	ep_type: [
-		FI_EP_MSG,
-	],
-	comp_type: [
-		FT_COMP_QUEUE
-	],
-	test_class: [
-		FT_CAP_MSG,
-	],
-	test_flags: FT_FLAG_QUICKTEST
-},
-{
-	prov_name: tcp,
-	test_type: [
-		FT_TEST_LATENCY,
-		FT_TEST_BANDWIDTH,
-		FT_TEST_UNIT
-	],
-	class_function: [
-		FT_FUNC_READ,
-		FT_FUNC_READV,
-		FT_FUNC_READMSG,
-		FT_FUNC_WRITE,
-		FT_FUNC_WRITEV,
-		FT_FUNC_WRITEMSG,
-		FT_FUNC_WRITEDATA
-	],
-	ep_type: [
-		FI_EP_MSG,
-	],
-	comp_type: [
-		FT_COMP_QUEUE
-	],
-	test_class: [
-		FT_CAP_RMA,
-	],
-	test_flags: FT_FLAG_QUICKTEST
-}
diff --git a/fabtests/test_configs/tcp/tcp.exclude b/fabtests/test_configs/tcp/tcp.exclude
index 9600858..be4eb24 100644
--- a/fabtests/test_configs/tcp/tcp.exclude
+++ b/fabtests/test_configs/tcp/tcp.exclude
@@ -13,6 +13,7 @@ atomic
 inj_complete -e msg
 unexpected_msg -e msg
 multi_recv
+-k
 
 # TODO. Following fails with macOS. will fix them later
 cq_data -e rdm
diff --git a/fabtests/ubertest/uber.c b/fabtests/ubertest/uber.c
index eb6ef3b..934ac50 100644
--- a/fabtests/ubertest/uber.c
+++ b/fabtests/ubertest/uber.c
@@ -68,7 +68,7 @@ enum {
 
 static int results[FT_MAX_RESULT];
 static char *filename = NULL;
-
+static char *domain_name = NULL;
 
 static int ft_nullstr(char *str)
 {
@@ -374,6 +374,8 @@ static int ft_server_setup(struct fi_info *hints, struct fi_info *info)
 	}
 
 	ft_fw_convert_info(hints, &test_info);
+	if (domain_name)
+		hints->domain_attr->name = domain_name;
 
 	ret = fi_getinfo(FT_FIVERSION, ft_strptr(test_info.node),
 			 ft_strptr(test_info.service), FI_SOURCE, hints, &info);
@@ -446,7 +448,7 @@ static int ft_server_child()
 	}
 
 	printf("Ending test %d, result: %s\n", test_info.test_index,
-		fi_strerror(-ret));
+		fi_strerror(-result));
 
 	return result;
 }
@@ -501,6 +503,8 @@ static int ft_client_setup(struct fi_info *hints, struct fi_info *info)
 		goto err;
 
 	ft_fw_convert_info(hints, &test_info);
+	if (domain_name)
+		hints->domain_attr->name = domain_name;
 
 	ft_show_test_info();
 
@@ -518,9 +522,9 @@ static int ft_client_setup(struct fi_info *hints, struct fi_info *info)
 	ft_fw_update_info(&test_info, fabric_info);
 
 	ret = ft_open_res();
-	
+
 	return 0;
-	
+
 err:
 	ft_send_result(ret, info);
 	return ret;
@@ -579,7 +583,7 @@ static int ft_client_child(void)
 	fi_freeinfo(hints);
 	ft_cleanup();
 
-	return 0;
+	return result;
 
 err:
 	ft_send_result(ret, info);
@@ -649,6 +653,7 @@ static void ft_fw_usage(char *program)
 	FT_PRINT_OPTS_USAGE("-B <src_port>", "non default source port number");
 	FT_PRINT_OPTS_USAGE("-P <dst_port>", "non default destination port number "
 		"(config file service parameter will override this)");
+	FT_PRINT_OPTS_USAGE("-d <domain>", "domain name");
 }
 
 void ft_free()
@@ -663,7 +668,7 @@ int main(int argc, char **argv)
 	opts = INIT_OPTS;
 	int ret, op;
 
-	while ((op = getopt(argc, argv, "u:q:xy:z:hf" ADDR_OPTS)) != -1) {
+	while ((op = getopt(argc, argv, "u:q:xy:z:hfd:" ADDR_OPTS)) != -1) {
 		switch (op) {
 		case 'u':
 			filename = strdup(optarg);
@@ -683,6 +688,9 @@ int main(int argc, char **argv)
 		case 'f':
 			do_fork = 1;
 			break;
+		case 'd':
+			domain_name = strdup(optarg);
+			break;
 		default:
 			ft_parse_addr_opts(op, optarg, &opts);
 			break;
@@ -745,5 +753,13 @@ out:
 	if (opts.dst_addr)
 		fts_close(series);
 	ft_free();
+
+	if (results[FT_EIO])
+		ret = -FI_EIO;
+	else if (results[FT_ENOSYS])
+		ret = -FI_ENOSYS;
+	else if (results[FT_ENODATA])
+		ret = -FI_ENODATA;
+
 	return ft_exit_code(ret);
 }
diff --git a/fabtests/ubertest/verify.c b/fabtests/ubertest/verify.c
index b58fa92..1569b31 100644
--- a/fabtests/ubertest/verify.c
+++ b/fabtests/ubertest/verify.c
@@ -35,7 +35,6 @@
 #include "ofi_atomic.h"
 #include "fabtest.h"
 
-static int alph_index = 0;
 static const char integ_alphabet[] = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";
 static const int integ_alphabet_length = (sizeof(integ_alphabet)/sizeof(*integ_alphabet)) - 1;
 
@@ -53,15 +52,15 @@ static const int integ_alphabet_length = (sizeof(integ_alphabet)/sizeof(*integ_a
 	} while (0)				\
 
 
-#define FT_FILL(dst,cnt,TYPE)				\
-	do {								\
-		int i;							\
-		TYPE *d = (dst);					\
-		for (i = 0; i < cnt; i++) {				\
-			d[i] = (TYPE) (integ_alphabet[alph_index++]);	\
-			if (alph_index >= integ_alphabet_length)	\
-				alph_index = 0;				\
-		}							\
+#define FT_FILL(dst,cnt,TYPE)					\
+	do {							\
+		int i, a = 0;					\
+		TYPE *d = (dst);				\
+		for (i = 0; i < cnt; i++) {			\
+			d[i] = (TYPE) (integ_alphabet[a]);	\
+			if (++a >= integ_alphabet_length)	\
+				a = 0;				\
+		}						\
 	} while (0)
 
 #define SWITCH_TYPES(type,FUNC,...)				\
@@ -85,6 +84,7 @@ static const int integ_alphabet_length = (sizeof(integ_alphabet)/sizeof(*integ_a
 
 int ft_sync_fill_bufs(size_t size)
 {
+	int ret;
 	ft_sock_sync(0);
 
 	if (test_info.caps & FI_ATOMIC) {
@@ -95,9 +95,14 @@ int ft_sync_fill_bufs(size_t size)
 		memcpy(ft_atom_ctrl.orig_buf, ft_mr_ctrl.buf, size);
 		memcpy(ft_tx_ctrl.cpy_buf, ft_tx_ctrl.buf, size);
 	} else if (is_read_func(test_info.class_function)) {
-		ft_fill_buf(ft_mr_ctrl.buf, size);
+		ret = ft_fill_buf(ft_mr_ctrl.buf, size);
+		if (ret)
+			return ret;
 	} else {
-		ft_fill_buf(ft_tx_ctrl.buf, size);
+		ret = ft_fill_buf(ft_tx_ctrl.buf, size);
+		if (ret)
+			return ret;
+
 		memcpy(ft_tx_ctrl.cpy_buf, ft_tx_ctrl.buf, size);
 	}
 
@@ -163,7 +168,7 @@ int ft_verify_bufs()
 		compare_buf = (char *) ft_rx_ctrl.buf;
 	}
 
-	return ft_check_buf(compare_buf, compare_size) ? -FI_EIO : 0;
+	return ft_check_buf(compare_buf, compare_size);
 }
 
 void ft_verify_comp(void *buf)
diff --git a/include/linux/osd.h b/include/linux/osd.h
index 66c0e65..1c61993 100644
--- a/include/linux/osd.h
+++ b/include/linux/osd.h
@@ -43,11 +43,15 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/syscall.h>
+#include <sys/types.h>
+#include <sys/socket.h>
 
+#include <linux/errqueue.h>
 #include <ifaddrs.h>
 #include "unix/osd.h"
 #include "rdma/fi_errno.h"
 
+
 static inline int ofi_shm_remap(struct util_shm *shm,
 				size_t newsize, void **mapped)
 {
diff --git a/include/ofi_abi.h b/include/ofi_abi.h
index f11b069..b9008f5 100644
--- a/include/ofi_abi.h
+++ b/include/ofi_abi.h
@@ -111,7 +111,7 @@ extern "C" {
  * name appended with the ABI version that it is compatible with.
  */
 
-#define CURRENT_ABI "FABRIC_1.4"
+#define CURRENT_ABI "FABRIC_1.6"
 
 #if  HAVE_ALIAS_ATTRIBUTE == 1
 #define DEFAULT_SYMVER_PRE(a) a##_
diff --git a/include/ofi_atom.h b/include/ofi_atom.h
index 46d6eb9..0e1af6a 100644
--- a/include/ofi_atom.h
+++ b/include/ofi_atom.h
@@ -159,6 +159,13 @@ typedef atomic_long	ofi_atomic_int64_t;
 							     &expected, desired,			\
 							     memory_order_acq_rel,			\
 							     memory_order_relaxed);			\
+	}												\
+	static inline											\
+	bool ofi_atomic_cas_bool##radix(ofi_atomic##radix##_t *atomic, 					\
+					int##radix##_t expected, 					\
+					int##radix##_t desired)						\
+	{												\
+		return ofi_atomic_cas_bool_strong##radix(atomic, expected, desired);			\
 	}
 
 #elif defined HAVE_BUILTIN_ATOMICS
diff --git a/include/ofi_enosys.h b/include/ofi_enosys.h
index 5f7148b..671de28 100644
--- a/include/ofi_enosys.h
+++ b/include/ofi_enosys.h
@@ -56,12 +56,17 @@ static struct fi_ops X = {
 	.bind = fi_no_bind,
 	.control = fi_no_control,
 	.ops_open = fi_no_ops_open,
+	.tostr = fi_no_ops_tostr,
+	.ops_set = fi_no_ops_set,
 };
  */
 int fi_no_bind(struct fid *fid, struct fid *bfid, uint64_t flags);
 int fi_no_control(struct fid *fid, int command, void *arg);
 int fi_no_ops_open(struct fid *fid, const char *name,
 		uint64_t flags, void **ops, void *context);
+int fi_no_tostr(const struct fid *fid, char *buf, size_t len);
+int fi_no_ops_set(struct fid *fid, const char *name, uint64_t flags,
+		  void *ops, void *context);
 
 /*
 static struct fi_ops_fabric X = {
diff --git a/include/ofi_epoll.h b/include/ofi_epoll.h
index 6ec5dfc..de50d84 100644
--- a/include/ofi_epoll.h
+++ b/include/ofi_epoll.h
@@ -44,6 +44,19 @@
 #include <ofi_list.h>
 #include <ofi_signal.h>
 
+
+#ifdef HAVE_EPOLL
+#include <sys/epoll.h>
+#define ofi_epollfds_event epoll_event
+#else
+struct ofi_epollfds_event {
+	uint32_t events;
+	union {
+		void *ptr;
+	} data;
+};
+#endif
+
 enum ofi_pollfds_ctl {
 	POLLFDS_CTL_ADD,
 	POLLFDS_CTL_DEL,
@@ -69,21 +82,32 @@ struct ofi_pollfds {
 };
 
 int ofi_pollfds_create(struct ofi_pollfds **pfds);
+int ofi_pollfds_grow(struct ofi_pollfds *pfds, int max_size);
 int ofi_pollfds_add(struct ofi_pollfds *pfds, int fd, uint32_t events,
 		    void *context);
 int ofi_pollfds_mod(struct ofi_pollfds *pfds, int fd, uint32_t events,
 		    void *context);
 int ofi_pollfds_del(struct ofi_pollfds *pfds, int fd);
-int ofi_pollfds_wait(struct ofi_pollfds *pfds, void **contexts,
-		     int max_contexts, int timeout);
+int ofi_pollfds_wait(struct ofi_pollfds *pfds,
+		     struct ofi_epollfds_event *events,
+		     int maxevents, int timeout);
 void ofi_pollfds_close(struct ofi_pollfds *pfds);
 
+/* OS specific */
+void ofi_pollfds_do_add(struct ofi_pollfds *pfds,
+			struct ofi_pollfds_work_item *item);
+int ofi_pollfds_do_mod(struct ofi_pollfds *pfds, int fd, uint32_t events,
+		       void *context);
+void ofi_pollfds_do_del(struct ofi_pollfds *pfds,
+			struct ofi_pollfds_work_item *item);
+
 
 #ifdef HAVE_EPOLL
 #include <sys/epoll.h>
 
 #define OFI_EPOLL_IN  EPOLLIN
 #define OFI_EPOLL_OUT EPOLLOUT
+#define OFI_EPOLL_ERR EPOLLERR
 
 typedef int ofi_epoll_t;
 #define OFI_EPOLL_INVALID -1
@@ -121,19 +145,17 @@ static inline int ofi_epoll_del(int ep, int fd)
 	return epoll_ctl(ep, EPOLL_CTL_DEL, fd, NULL) ? -ofi_syserr() : 0;
 }
 
-static inline int ofi_epoll_wait(int ep, void **contexts, int max_contexts,
-                                int timeout)
+static inline int
+ofi_epoll_wait(int ep, struct ofi_epollfds_event *events,
+	       int maxevents, int timeout)
 {
-	struct epoll_event events[max_contexts];
 	int ret;
-	int i;
 
-	ret = epoll_wait(ep, events, max_contexts, timeout);
+	ret = epoll_wait(ep, (struct epoll_event *) events, maxevents,
+			 timeout);
 	if (ret == -1)
 		return -ofi_syserr();
 
-	for (i = 0; i < ret; i++)
-		contexts[i] = events[i].data.ptr;
 	return ret;
 }
 
@@ -146,6 +168,7 @@ static inline void ofi_epoll_close(int ep)
 
 #define OFI_EPOLL_IN  POLLIN
 #define OFI_EPOLL_OUT POLLOUT
+#define OFI_EPOLL_ERR POLLERR
 
 typedef struct ofi_pollfds *ofi_epoll_t;
 #define OFI_EPOLL_INVALID NULL
diff --git a/include/ofi_hmem.h b/include/ofi_hmem.h
index cfd9c0a..bb150b0 100644
--- a/include/ofi_hmem.h
+++ b/include/ofi_hmem.h
@@ -88,6 +88,26 @@ hsa_status_t ofi_hsa_amd_memory_unlock(void *host_ptr);
 
 #endif /* HAVE_ROCR */
 
+struct ofi_hmem_ops {
+	bool initialized;
+	int (*init)(void);
+	int (*cleanup)(void);
+	int (*copy_to_hmem)(uint64_t device, void *dest, const void *src,
+			    size_t size);
+	int (*copy_from_hmem)(uint64_t device, void *dest, const void *src,
+			      size_t size);
+	bool (*is_addr_valid)(const void *addr);
+	int (*get_handle)(void *dev_buf, void **handle);
+	int (*open_handle)(void **handle, uint64_t device, void **ipc_ptr);
+	int (*close_handle)(void *ipc_ptr);
+	int (*host_register)(void *ptr, size_t size);
+	int (*host_unregister)(void *ptr);
+	int (*get_base_addr)(const void *ptr, void **base);
+	bool (*is_ipc_enabled)(void);
+};
+
+extern struct ofi_hmem_ops hmem_ops[];
+
 int rocr_copy_from_dev(uint64_t device, void *dest, const void *src,
 		       size_t size);
 int rocr_copy_to_dev(uint64_t device, void *dest, const void *src,
@@ -107,6 +127,10 @@ int cuda_host_register(void *ptr, size_t size);
 int cuda_host_unregister(void *ptr);
 int cuda_dev_register(struct fi_mr_attr *mr_attr, uint64_t *handle);
 int cuda_dev_unregister(uint64_t handle);
+int cuda_get_handle(void *dev_buf, void **handle);
+int cuda_open_handle(void **handle, uint64_t device, void **ipc_ptr);
+int cuda_close_handle(void *ipc_ptr);
+bool cuda_is_ipc_enabled(void);
 
 void cuda_gdrcopy_to_dev(uint64_t handle, void *dev,
 			 const void *host, size_t size);
@@ -131,6 +155,7 @@ int ze_hmem_open_shared_handle(int dev_fd, void **handle, int *ze_fd,
 int ze_hmem_close_handle(void *ipc_ptr);
 bool ze_hmem_p2p_enabled(void);
 int ze_hmem_get_base_addr(const void *ptr, void **base);
+int ze_hmem_get_id(const void *ptr, uint64_t *id);
 int *ze_hmem_get_dev_fds(int *nfds);
 
 static inline int ofi_memcpy(uint64_t device, void *dest, const void *src,
@@ -180,6 +205,11 @@ static inline int ofi_hmem_no_base_addr(const void *ptr, void **base)
 	return -FI_ENOSYS;
 }
 
+static inline bool ofi_hmem_no_is_ipc_enabled(void)
+{
+	return false;
+}
+
 ssize_t ofi_copy_from_hmem_iov(void *dest, size_t size,
 			       enum fi_hmem_iface hmem_iface, uint64_t device,
 			       const struct iovec *hmem_iov,
@@ -196,11 +226,13 @@ int ofi_hmem_open_handle(enum fi_hmem_iface iface, void **handle,
 int ofi_hmem_close_handle(enum fi_hmem_iface iface, void *ipc_ptr);
 int ofi_hmem_get_base_addr(enum fi_hmem_iface iface, const void *ptr,
 			   void **base);
+bool ofi_hmem_is_initialized(enum fi_hmem_iface iface);
 
 void ofi_hmem_init(void);
 void ofi_hmem_cleanup(void);
 enum fi_hmem_iface ofi_get_hmem_iface(const void *addr);
 int ofi_hmem_host_register(void *ptr, size_t size);
 int ofi_hmem_host_unregister(void *ptr);
+bool ofi_hmem_is_ipc_enabled(enum fi_hmem_iface iface);
 
 #endif /* _OFI_HMEM_H_ */
diff --git a/include/ofi_hook.h b/include/ofi_hook.h
index d7a33cb..652c913 100644
--- a/include/ofi_hook.h
+++ b/include/ofi_hook.h
@@ -65,6 +65,15 @@ enum ofi_hook_class {
 
 
 /*
+ * Default fi_ops members, can be used to construct custom fi_ops
+ */
+int hook_close(struct fid *fid);
+int hook_bind(struct fid *fid, struct fid *bfid, uint64_t flags);
+int hook_control(struct fid *fid, int command, void *arg);
+int hook_ops_open(struct fid *fid, const char *name,
+		  uint64_t flags, void **ops, void *context);
+
+/*
  * Define hook structs so we can cast from fid to parent using simple cast.
  * This lets us have a single close() call.
  */
@@ -146,10 +155,14 @@ static inline struct fi_provider *hook_to_hprov(const struct fid *fid)
 	return hook_fabric_to_hprov(hook_to_fabric(fid));
 }
 
+struct ofi_ops_flow_ctrl;
+
 struct hook_domain {
 	struct fid_domain domain;
 	struct fid_domain *hdomain;
 	struct hook_fabric *fabric;
+	struct ofi_ops_flow_ctrl *base_ops_flow_ctrl;
+	ssize_t (*base_credit_handler)(struct fid_ep *ep_fid, size_t credits);
 };
 
 int hook_domain(struct fid_fabric *fabric, struct fi_info *info,
@@ -257,6 +270,7 @@ int hook_query_atomic(struct fid_domain *domain, enum fi_datatype datatype,
 		  enum fi_op op, struct fi_atomic_attr *attr, uint64_t flags);
 
 extern struct fi_ops hook_fabric_fid_ops;
+extern struct fi_ops hook_domain_fid_ops;
 extern struct fi_ops_fabric hook_fabric_ops;
 extern struct fi_ops_domain hook_domain_ops;
 extern struct fi_ops_cq hook_cq_ops;
diff --git a/include/ofi_list.h b/include/ofi_list.h
index 1b79d0a..5112000 100644
--- a/include/ofi_list.h
+++ b/include/ofi_list.h
@@ -45,6 +45,12 @@
 #include <ofi_signal.h>
 #include <ofi_lock.h>
 
+
+enum ofi_list_end {
+	OFI_LIST_TAIL,
+	OFI_LIST_HEAD
+};
+
 /*
  * Double-linked list
  */
diff --git a/include/ofi_lock.h b/include/ofi_lock.h
index 05ed4a1..c11ad18 100644
--- a/include/ofi_lock.h
+++ b/include/ofi_lock.h
@@ -133,6 +133,11 @@ static inline void fastlock_release(fastlock_t *lock)
 	assert(!ret);
 }
 
+static inline int fastlock_held(fastlock_t *lock)
+{
+	return lock->in_use;
+}
+
 #else /* !ENABLE_DEBUG */
 
 #  define fastlock_t fastlock_t_
@@ -141,6 +146,7 @@ static inline void fastlock_release(fastlock_t *lock)
 #  define fastlock_acquire(lock) fastlock_acquire_(lock)
 #  define fastlock_tryacquire(lock) fastlock_tryacquire_(lock)
 #  define fastlock_release(lock) fastlock_release_(lock)
+#  define fastlock_held(lock) true
 
 #endif
 
diff --git a/include/ofi_mr.h b/include/ofi_mr.h
index 940d4d3..a9fb920 100644
--- a/include/ofi_mr.h
+++ b/include/ofi_mr.h
@@ -50,6 +50,10 @@
 #include <ofi_tree.h>
 #include <ofi_hmem.h>
 
+
+int ofi_open_mr_cache(uint32_t version, void *attr, size_t attr_len,
+		      uint64_t flags, struct fid **fid, void *context);
+
 struct ofi_mr_info {
 	struct iovec iov;
 	enum fi_hmem_iface iface;
@@ -120,6 +124,7 @@ struct ofi_mr_cache;
 
 union ofi_mr_hmem_info {
 	uint64_t cuda_id;
+	uint64_t ze_id;
 };
 
 struct ofi_mem_monitor {
@@ -151,6 +156,8 @@ void ofi_monitor_init(struct ofi_mem_monitor *monitor);
 void ofi_monitor_cleanup(struct ofi_mem_monitor *monitor);
 void ofi_monitors_init(void);
 void ofi_monitors_cleanup(void);
+int ofi_monitor_import(struct fid *fid);
+
 int ofi_monitors_add_cache(struct ofi_mem_monitor **monitors,
 			   struct ofi_mr_cache *cache);
 void ofi_monitors_del_cache(struct ofi_mr_cache *cache);
@@ -168,6 +175,7 @@ void ofi_monitor_unsubscribe(struct ofi_mem_monitor *monitor,
 extern struct ofi_mem_monitor *default_monitor;
 extern struct ofi_mem_monitor *default_cuda_monitor;
 extern struct ofi_mem_monitor *default_rocr_monitor;
+extern struct ofi_mem_monitor *default_ze_monitor;
 
 /*
  * Userfault fd memory monitor
@@ -191,8 +199,9 @@ struct ofi_memhooks {
 extern struct ofi_mem_monitor *memhooks_monitor;
 
 extern struct ofi_mem_monitor *cuda_monitor;
-
 extern struct ofi_mem_monitor *rocr_monitor;
+extern struct ofi_mem_monitor *ze_monitor;
+extern struct ofi_mem_monitor *import_monitor;
 
 /*
  * Used to store registered memory regions into a lookup map.  This
@@ -262,6 +271,7 @@ struct ofi_mr_cache_params {
 	char *				monitor;
 	int				cuda_monitor_enabled;
 	int				rocr_monitor_enabled;
+	int				ze_monitor_enabled;
 };
 
 extern struct ofi_mr_cache_params	cache_params;
@@ -285,7 +295,7 @@ struct ofi_mr_cache {
 
 	struct ofi_rbmap		tree;
 	struct dlist_entry		lru_list;
-	struct dlist_entry		flush_list;
+	struct dlist_entry		dead_region_list;
 	pthread_mutex_t 		lock;
 
 	size_t				cached_cnt;
@@ -311,10 +321,17 @@ void ofi_mr_cache_cleanup(struct ofi_mr_cache *cache);
 
 void ofi_mr_cache_notify(struct ofi_mr_cache *cache, const void *addr, size_t len);
 
+static inline bool ofi_mr_cache_full(struct ofi_mr_cache *cache)
+{
+	return (cache->cached_cnt >= cache_params.max_cnt) ||
+	       (cache->cached_size >= cache_params.max_size);
+}
+
 bool ofi_mr_cache_flush(struct ofi_mr_cache *cache, bool flush_lru);
 
 int ofi_mr_cache_search(struct ofi_mr_cache *cache, const struct fi_mr_attr *attr,
 			struct ofi_mr_entry **entry);
+
 /**
  * Given an attr (with an iov range), if the iov range is already registered,
  * return the corresponding ofi_mr_entry. Otherwise, return NULL.
diff --git a/include/ofi_net.h b/include/ofi_net.h
index ee0a6f8..df36b9e 100644
--- a/include/ofi_net.h
+++ b/include/ofi_net.h
@@ -92,6 +92,14 @@ static inline uint64_t ntohll(uint64_t x) { return x; }
 #endif
 #endif
 
+#ifdef MSG_ZEROCOPY
+#define OFI_ZEROCOPY MSG_ZEROCOPY
+#define OFI_ZEROCOPY_SIZE 9000 /* arbitrary based on documentation */
+#else
+#define OFI_ZEROCOPY 0
+#define OFI_ZEROCOPY_SIZE SIZE_MAX
+#endif
+
 
 static inline int ofi_recvall_socket(SOCKET sock, void *buf, size_t len)
 {
@@ -118,6 +126,170 @@ static inline int ofi_sendall_socket(SOCKET sock, const void *buf, size_t len)
 int ofi_discard_socket(SOCKET sock, size_t len);
 
 /*
+ * Byte queue - streaming socket staging buffer
+ */
+enum {
+	OFI_BYTEQ_SIZE = 9000, /* Hard-coded max, good for 6 1500B buffers */
+};
+
+struct ofi_byteq {
+	size_t size;
+	unsigned int head;
+	unsigned int tail;
+	uint8_t data[OFI_BYTEQ_SIZE];
+};
+
+static inline void ofi_byteq_init(struct ofi_byteq *byteq, ssize_t size)
+{
+	memset(byteq, 0, sizeof *byteq);
+	if (size > OFI_BYTEQ_SIZE)
+		byteq->size = OFI_BYTEQ_SIZE;
+	else if (size >= 0)
+		byteq->size = size;
+	else
+		byteq->size = 0;
+}
+
+static inline void ofi_byteq_discard(struct ofi_byteq *byteq)
+{
+	byteq->head = 0;
+	byteq->tail = 0;
+}
+
+static inline size_t ofi_byteq_readable(struct ofi_byteq *byteq)
+{
+	return byteq->tail - byteq->head;
+}
+
+static inline size_t ofi_byteq_writeable(struct ofi_byteq *byteq)
+{
+	return byteq->size - byteq->tail;
+}
+
+static inline size_t
+ofi_byteq_read(struct ofi_byteq *byteq, void *buf, size_t len)
+{
+	size_t avail;
+
+	avail = ofi_byteq_readable(byteq);
+	if (!avail)
+		return 0;
+
+	if (len < avail) {
+		memcpy(buf, &byteq->data[byteq->head], len);
+		byteq->head += len;
+		return len;
+	}
+
+	memcpy(buf, &byteq->data[byteq->head], avail);
+	byteq->head = 0;
+	byteq->tail = 0;
+	return avail;
+}
+
+static inline void
+ofi_byteq_write(struct ofi_byteq *byteq, const void *buf, size_t len)
+{
+	assert(len <= ofi_byteq_writeable(byteq));
+	memcpy(&byteq->data[byteq->tail], buf, len);
+	byteq->tail += len;
+}
+
+void ofi_byteq_writev(struct ofi_byteq *byteq, const struct iovec *iov,
+		      size_t cnt);
+
+static inline ssize_t ofi_byteq_recv(struct ofi_byteq *byteq, SOCKET sock)
+{
+	size_t avail;
+	ssize_t ret;
+
+	avail = ofi_byteq_writeable(byteq);
+	assert(avail);
+	ret = ofi_recv_socket(sock, &byteq->data[byteq->tail], avail,
+			      MSG_NOSIGNAL);
+	if (ret > 0)
+		byteq->tail += ret;
+	return ret;
+}
+
+size_t ofi_byteq_readv(struct ofi_byteq *byteq, struct iovec *iov,
+		       size_t cnt, size_t offset);
+
+static inline ssize_t ofi_byteq_send(struct ofi_byteq *byteq, SOCKET sock)
+{
+	size_t avail;
+	ssize_t ret;
+
+	avail = ofi_byteq_readable(byteq);
+	assert(avail);
+	ret = ofi_send_socket(sock, &byteq->data[byteq->head], avail,
+			      MSG_NOSIGNAL);
+	if (ret == avail) {
+		byteq->head = 0;
+		byteq->tail = 0;
+	} else if (ret > 0) {
+		byteq->head += ret;
+	}
+	return ret;
+}
+
+
+/*
+ * Buffered socket - socket with send/receive staging buffers.
+ */
+struct ofi_bsock {
+	SOCKET sock;
+	struct ofi_byteq sq;
+	struct ofi_byteq rq;
+	size_t zerocopy_size;
+	uint32_t async_index;
+	uint32_t done_index;
+};
+
+static inline void
+ofi_bsock_init(struct ofi_bsock *bsock, ssize_t sbuf_size, ssize_t rbuf_size)
+{
+	bsock->sock = INVALID_SOCKET;
+	ofi_byteq_init(&bsock->sq, sbuf_size);
+	ofi_byteq_init(&bsock->rq, rbuf_size);
+	bsock->zerocopy_size = SIZE_MAX;
+
+	/* first async op will wrap back to 0 as the starting index */
+	bsock->async_index = UINT32_MAX;
+	bsock->done_index = UINT32_MAX;
+}
+
+static inline void ofi_bsock_discard(struct ofi_bsock *bsock)
+{
+	ofi_byteq_discard(&bsock->rq);
+	ofi_byteq_discard(&bsock->sq);
+}
+
+static inline size_t ofi_bsock_readable(struct ofi_bsock *bsock)
+{
+	return ofi_byteq_readable(&bsock->rq);
+}
+
+static inline size_t ofi_bsock_tosend(struct ofi_bsock *bsock)
+{
+	return ofi_byteq_readable(&bsock->sq);
+}
+
+ssize_t ofi_bsock_flush(struct ofi_bsock *bsock);
+/* For sends started asynchronously, the return value will be -EINPROGRESS,
+ * and len will be set to the number of bytes that were queued.
+ */
+ssize_t ofi_bsock_send(struct ofi_bsock *bsock, const void *buf, size_t *len);
+ssize_t ofi_bsock_sendv(struct ofi_bsock *bsock, const struct iovec *iov,
+			size_t cnt, size_t *len);
+ssize_t ofi_bsock_recv(struct ofi_bsock *bsock, void *buf, size_t len);
+ssize_t ofi_bsock_recvv(struct ofi_bsock *bsock, struct iovec *iov,
+			size_t cnt);
+uint32_t ofi_bsock_async_done(const struct fi_provider *prov,
+			      struct ofi_bsock *bsock);
+
+
+/*
  * Address utility functions
  */
 
@@ -229,6 +401,21 @@ static inline int ofi_translate_addr_format(int family)
 	}
 }
 
+static inline size_t ofi_sizeof_addr_format(int format)
+{
+	switch (format) {
+	case FI_SOCKADDR_IN:
+		return sizeof(struct sockaddr_in);
+	case FI_SOCKADDR_IN6:
+		return sizeof(struct sockaddr_in6);
+	case FI_SOCKADDR_IB:
+		return sizeof(struct ofi_sockaddr_ib);
+	default:
+		FI_WARN(&core_prov, FI_LOG_CORE, "Unsupported address format\n");
+		return 0;
+	}
+}
+
 uint16_t ofi_get_sa_family(const struct fi_info *info);
 
 static inline bool ofi_sin_is_any_addr(const struct sockaddr *sa)
diff --git a/include/ofi_recvwin.h b/include/ofi_recvwin.h
index 468c657..2d1073a 100644
--- a/include/ofi_recvwin.h
+++ b/include/ofi_recvwin.h
@@ -80,6 +80,12 @@ ofi_recvwin_id_valid(struct name *recvq, id_type id)			\
 }									\
 									\
 static inline int							\
+ofi_recvwin_id_processed(struct name *recvq, id_type id)		\
+{									\
+	return ofi_recvwin_id_processed_ ## id_type (recvq, id);	\
+}									\
+									\
+static inline int							\
 ofi_recvwin_queue_msg(struct name *recvq, entrytype * msg, id_type id)	\
 {									\
 	size_t write_idx;						\
@@ -139,4 +145,9 @@ ofi_recvwin_slide(struct name *recvq)					\
 #define ofi_recvwin_id_valid_uint64_t(rq, id) \
 	ofi_val64_inrange(rq->exp_msg_id, rq->win_size, id)
 
+#define ofi_recvwin_id_processed_uint32_t(rq, id) \
+	ofi_val32_gt(rq->exp_msg_id, id)
+#define ofi_recvwin_id_processed_uint64_t(rq, id) \
+	ofi_val64_gt(rq->exp_msg_id, id)
+
 #endif /* FI_RECVWIN_H */
diff --git a/include/ofi_shm.h b/include/ofi_shm.h
index 8bbb2ec..bba9601 100644
--- a/include/ofi_shm.h
+++ b/include/ofi_shm.h
@@ -210,6 +210,13 @@ struct smr_ep_name {
 	struct dlist_entry entry;
 };
 
+static inline const char *smr_no_prefix(const char *addr)
+{
+	char *start;
+
+	return (start = strstr(addr, "://")) ? start + 3 : addr;
+}
+
 struct smr_peer {
 	struct smr_addr		peer;
 	fi_addr_t		fiaddr;
@@ -236,6 +243,8 @@ struct smr_region {
 	fastlock_t	lock; /* lock for shm access
 				 Must hold smr->lock before tx/rx cq locks
 				 in order to progress or post recv */
+	ofi_atomic32_t	signal;
+
 	struct smr_map	*map;
 
 	size_t		total_size;
@@ -361,6 +370,11 @@ int	smr_create(const struct fi_provider *prov, struct smr_map *map,
 		   const struct smr_attr *attr, struct smr_region *volatile *smr);
 void	smr_free(struct smr_region *smr);
 
+static inline void smr_signal(struct smr_region *smr)
+{
+	ofi_atomic_set32(&smr->signal, 1);
+}
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/include/ofi_util.h b/include/ofi_util.h
index a8e7fb9..6f1add8 100644
--- a/include/ofi_util.h
+++ b/include/ofi_util.h
@@ -153,9 +153,14 @@ struct ofi_common_locks {
 /*
  * Provider details
  */
+typedef int (*ofi_alter_info_t)(uint32_t version, const struct fi_info *src_info,
+				const struct fi_info *base_info,
+				struct fi_info *dest_info);
+
 struct util_prov {
 	const struct fi_provider	*prov;
 	const struct fi_info		*info;
+	ofi_alter_info_t		alter_defaults;
 	const int			flags;
 };
 
@@ -330,6 +335,12 @@ static inline void ofi_ep_lock_release(struct util_ep *ep)
 	ep->lock_release(&ep->lock);
 }
 
+static inline bool ofi_ep_lock_held(struct util_ep *ep)
+{
+	return (ep->lock_acquire == ofi_fastlock_acquire_noop) ||
+		fastlock_held(&ep->lock);
+}
+
 static inline void ofi_ep_tx_cntr_inc(struct util_ep *ep)
 {
 	ep->tx_cntr_inc(ep->tx_cntr);
@@ -755,6 +766,8 @@ int ofi_ip_av_create_flags(struct fid_domain *domain_fid, struct fi_av_attr *att
 
 void *ofi_av_get_addr(struct util_av *av, fi_addr_t fi_addr);
 #define ofi_ip_av_get_addr ofi_av_get_addr
+void *ofi_av_addr_context(struct util_av *av, fi_addr_t fi_addr);
+
 fi_addr_t ofi_ip_av_get_fi_addr(struct util_av *av, const void *addr);
 
 int ofi_get_addr(uint32_t *addr_format, uint64_t flags,
@@ -949,10 +962,6 @@ static inline int ofi_has_util_prefix(const char *str)
 	return !strncasecmp(str, OFI_UTIL_PREFIX, strlen(OFI_UTIL_PREFIX));
 }
 
-typedef int (*ofi_alter_info_t)(uint32_t version, const struct fi_info *src_info,
-				const struct fi_info *base_info,
-				struct fi_info *dest_info);
-
 int ofi_get_core_info(uint32_t version, const char *node, const char *service,
 		      uint64_t flags, const struct util_prov *util_prov,
 		      const struct fi_info *util_hints,
@@ -1036,11 +1045,21 @@ struct ofi_ops_flow_ctrl {
 
 
 /* Dynamic receive buffering support. */
-#define OFI_OPS_DYNAMIC_RBUF "ofix_dynamic_rbuf"
+#define OFI_OPS_DYNAMIC_RBUF "ofix_dynamic_rbuf_v2"
+
+struct ofi_cq_rbuf_entry {
+	void			*op_context;
+	uint64_t		flags;
+	size_t			len;
+	void			*buf;
+	uint64_t		data;
+	uint64_t		tag;
+	void			*ep_context;
+};
 
 struct ofi_ops_dynamic_rbuf {
 	size_t	size;
-	ssize_t	(*get_rbuf)(struct fi_cq_data_entry *entry, struct iovec *iov,
+	ssize_t	(*get_rbuf)(struct ofi_cq_rbuf_entry *entry, struct iovec *iov,
 			    size_t *count);
 };
 
diff --git a/include/rdma/fabric.h b/include/rdma/fabric.h
index 9ddf5d1..cdfa11e 100644
--- a/include/rdma/fabric.h
+++ b/include/rdma/fabric.h
@@ -79,7 +79,7 @@ extern "C" {
 #endif
 
 #define FI_MAJOR_VERSION 1
-#define FI_MINOR_VERSION 12
+#define FI_MINOR_VERSION 13
 #define FI_REVISION_VERSION 0
 
 enum {
@@ -166,6 +166,7 @@ typedef struct fid *fid_t;
 #define FI_COMMIT_COMPLETE	(1ULL << 30)
 #define FI_MATCH_COMPLETE	(1ULL << 31)
 
+#define FI_HMEM_DEVICE_ONLY	(1ULL << 46)
 #define FI_HMEM			(1ULL << 47)
 #define FI_VARIABLE_MSG		(1ULL << 48)
 #define FI_RMA_PMEM		(1ULL << 49)
@@ -519,6 +520,8 @@ enum {
 	FI_CLASS_MC,
 	FI_CLASS_NIC,
 	FI_CLASS_AV_SET,
+	FI_CLASS_MR_CACHE,
+	FI_CLASS_MEM_MONITOR,
 };
 
 struct fi_eq_attr;
@@ -577,7 +580,10 @@ struct fid_fabric {
 	uint32_t		api_version;
 };
 
-int fi_fabric(struct fi_fabric_attr *attr, struct fid_fabric **fabric, void *context);
+int fi_fabric(struct fi_fabric_attr *attr, struct fid_fabric **fabric,
+	      void *context);
+int fi_open(uint32_t version, const char *name, void *attr, size_t attr_len,
+	    uint64_t flags, struct fid **fid, void *context);
 
 struct fid_nic {
 	struct fid		fid;
@@ -641,6 +647,7 @@ enum {
 	FI_GETWAITOBJ,		/*enum fi_wait_obj * */
 	FI_GET_VAL,		/* struct fi_fid_var */
 	FI_SET_VAL,		/* struct fi_fid_var */
+	FI_EXPORT_FID,		/* struct fi_fid_export */
 };
 
 static inline int fi_control(struct fid *fid, int command, void *arg)
diff --git a/include/rdma/fi_domain.h b/include/rdma/fi_domain.h
index 27d6dd3..b5399ba 100644
--- a/include/rdma/fi_domain.h
+++ b/include/rdma/fi_domain.h
@@ -185,6 +185,12 @@ enum fi_datatype {
 	FI_LONG_DOUBLE_COMPLEX,
 	/* End of point to point atomic datatypes */
 	FI_DATATYPE_LAST,
+	/*
+	 * enums for 128-bit integer atomics, existing ordering and
+	 * FI_DATATYPE_LAST preserved for compatabilty.
+	 */
+	FI_INT128 = FI_DATATYPE_LAST,
+	FI_UINT128,
 
 	/* Collective datatypes */
 	FI_VOID = FI_COLLECTIVE_OFFSET,
diff --git a/include/rdma/fi_errno.h b/include/rdma/fi_errno.h
index 63a6acb..fee1046 100644
--- a/include/rdma/fi_errno.h
+++ b/include/rdma/fi_errno.h
@@ -193,6 +193,7 @@ enum {
 	FI_ENOKEY        = 266, /* Required key not available */
 	FI_ENOAV	 = 267, /* Missing or unavailable address vector */
 	FI_EOVERRUN	 = 268, /* Queue has been overrun */
+	FI_ENORX	 = 269, /* Receiver not ready, no receive buffers available */
 	FI_ERRNO_MAX
 };
 
diff --git a/include/rdma/fi_ext.h b/include/rdma/fi_ext.h
new file mode 100644
index 0000000..f676cad
--- /dev/null
+++ b/include/rdma/fi_ext.h
@@ -0,0 +1,125 @@
+/*
+ * Copyright (c) 2021 Intel Corporation. All rights reserved.
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef FI_EXT_H
+#define FI_EXT_H
+
+#include <stdbool.h>
+#include <rdma/fabric.h>
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*
+ * Each provider can define provider-specific values by
+ * choosing an unique 11-bit (since the most significant
+ * bit must be 1) provider-specific code to avoid
+ * overlapping with other providers. E.g.,
+ *
+ * #define FI_PROV_SPECIFIC_XXX    (FI_PROV_SPECIFIC | 0xabc << 20)
+ * enum {
+ *        FI_PROV_XXX_VALUE        (1U | FI_PROV_SPECIFIC_XXX)
+ * }
+ */
+
+#define FI_PROV_SPECIFIC_EFA   (FI_PROV_SPECIFIC | 0xefa << 20)
+
+/* EFA options */
+enum {
+       FI_OPT_EFA_RNR_RETRY = (1U | FI_PROV_SPECIFIC_EFA),
+};
+
+struct fi_fid_export {
+	struct fid **fid;
+	uint64_t flags;
+	void *context;
+};
+
+static inline int
+fi_export_fid(struct fid *fid, uint64_t flags,
+	      struct fid **expfid, void *context)
+{
+	struct fi_fid_export exp;
+
+	exp.fid = expfid;
+	exp.flags = flags;
+	exp.context = context;
+	return fi_control(fid, FI_EXPORT_FID, &exp);
+}
+
+static inline int
+fi_import_fid(struct fid *fid, struct fid *expfid, uint64_t flags)
+{
+	return fid->ops->bind(fid, expfid, flags);
+}
+
+
+/*
+ * System memory monitor import extension:
+ * To use, open mr_cache fid and import.
+ */
+
+struct fid_mem_monitor;
+
+struct fi_ops_mem_monitor {
+	size_t	size;
+	int	(*start)(struct fid_mem_monitor *monitor);
+	void	(*stop)(struct fid_mem_monitor *monitor);
+	int	(*subscribe)(struct fid_mem_monitor *monitor,
+			const void *addr, size_t len);
+	void	(*unsubscribe)(struct fid_mem_monitor *monitor,
+			const void *addr, size_t len);
+	bool	(*valid)(struct fid_mem_monitor *monitor,
+			const void *addr, size_t len);
+};
+
+struct fi_ops_mem_notify {
+	size_t	size;
+	void	(*notify)(struct fid_mem_monitor *monitor, const void *addr,
+			size_t len);
+};
+
+struct fid_mem_monitor {
+	struct fid fid;
+	struct fi_ops_mem_monitor *export_ops;
+	struct fi_ops_mem_notify *import_ops;
+};
+
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* FI_EXT_H */
diff --git a/include/rdma/providers/fi_log.h b/include/rdma/providers/fi_log.h
index acf6f24..fca6910 100644
--- a/include/rdma/providers/fi_log.h
+++ b/include/rdma/providers/fi_log.h
@@ -68,6 +68,8 @@ enum fi_log_level {
 
 int fi_log_enabled(const struct fi_provider *prov, enum fi_log_level level,
 		   enum fi_log_subsys subsys);
+int fi_log_ready(const struct fi_provider *prov, enum fi_log_level level,
+		 enum fi_log_subsys subsys, uint64_t *showtime);
 void fi_log(const struct fi_provider *prov, enum fi_log_level level,
 	    enum fi_log_subsys subsys, const char *func, int line,
 	    const char *fmt, ...) __attribute__ ((__format__ (__printf__, 6, 7)));
@@ -82,8 +84,21 @@ void fi_log(const struct fi_provider *prov, enum fi_log_level level,
 		}							\
 	} while (0)
 
+#define FI_LOG_SPARSE(prov, level, subsystem, ...)			\
+	do {								\
+		static uint64_t showtime;				\
+		if (fi_log_ready(prov, level, subsystem, &showtime)) {	\
+			int saved_errno = errno;			\
+			fi_log(prov, level, subsystem,			\
+				__func__, __LINE__, __VA_ARGS__);	\
+			errno = saved_errno;				\
+		}							\
+	} while (0)
+
 #define FI_WARN(prov, subsystem, ...)					\
 	FI_LOG(prov, FI_LOG_WARN, subsystem, __VA_ARGS__)
+#define FI_WARN_SPARSE(prov, subsystem, ...)				\
+	FI_LOG_SPARSE(prov, FI_LOG_WARN, subsystem, __VA_ARGS__)
 
 #define FI_TRACE(prov, subsystem, ...)					\
 	FI_LOG(prov, FI_LOG_TRACE, subsystem, __VA_ARGS__)
diff --git a/include/rdma/providers/fi_prov.h b/include/rdma/providers/fi_prov.h
index 104bf78..ab8858d 100644
--- a/include/rdma/providers/fi_prov.h
+++ b/include/rdma/providers/fi_prov.h
@@ -76,14 +76,6 @@ struct fi_provider {
 
 /*
  * Defines a configuration parameter for use with libfabric.
- *
- * This registers the configuration variable "foo" in the specified
- * provider.
- *
- * The help string cannot be NULL or empty.
- *
- * The param_name and help_string parameters will be copied internally;
- * they can be freed upon return from fi_param_define().
  */
 int fi_param_define(const struct fi_provider *provider, const char *param_name,
 		    enum fi_param_type type, const char *help_string_fmt, ...);
@@ -92,22 +84,8 @@ int fi_param_define(const struct fi_provider *provider, const char *param_name,
  * Get the value of a configuration variable.
  *
  * Currently, configuration parameter will only be read from the
- * environment.  The environment variable names will be of the form
- * upper_case(FI_<provider_name>_<param_name>).
- *
- * Someday this call could be expanded to also check config files.
- *
- * If the parameter was previously defined and the user set a value,
- * FI_SUCCESS is returned and (*value) points to the retrieved
- * value.
- *
- * If the parameter name was previously defined, but the user did
- * not set a value, -FI_ENODATA is returned and the value of (*value)
- * is unchanged.
- *
- * If the variable name was not previously defined via
- * fi_param_define(), -FI_ENOENT will be returned and the value of
- * (*value) is unchanged.
+ * environment. Someday this call could be expanded to also check
+ * config files.
  */
 int fi_param_get(struct fi_provider *provider, const char *param_name,
 		 void *value);
diff --git a/include/windows/config.h b/include/windows/config.h
index fa0c281..8a986e5 100644
--- a/include/windows/config.h
+++ b/include/windows/config.h
@@ -165,7 +165,7 @@
 #define PACKAGE_TARNAME PACKAGE
 
 /* Define to the version of this package. */
-#define PACKAGE_VERSION "1.12.0"
+#define PACKAGE_VERSION "1.14.0a1"
 
 /* Define to the full name and version of this package. */
 #define PACKAGE_STRING PACKAGE_NAME " " PACKAGE_VERSION
diff --git a/libfabric.map.in b/libfabric.map.in
index 2bcc349..342216c 100644
--- a/libfabric.map.in
+++ b/libfabric.map.in
@@ -42,4 +42,14 @@ FABRIC_1.3 {
 FABRIC_1.4 {
 	global:
 		fi_tostr_r;
-} FABRIC_1.3;
\ No newline at end of file
+} FABRIC_1.3;
+
+FABRIC_1.5 {
+	global:
+		fi_open;
+} FABRIC_1.4;
+
+FABRIC_1.6 {
+	global:
+		fi_log_ready;
+} FABRIC_1.5;
\ No newline at end of file
diff --git a/libfabric.vcxproj b/libfabric.vcxproj
index fd77659..c1e1792 100644
--- a/libfabric.vcxproj
+++ b/libfabric.vcxproj
@@ -639,7 +639,6 @@
       <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Release-ICC|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
     </ClCompile>
     <ClCompile Include="prov\tcp\src\tcpx_attr.c" />
-    <ClCompile Include="prov\tcp\src\tcpx_comm.c" />
     <ClCompile Include="prov\tcp\src\tcpx_conn_mgr.c" />
     <ClCompile Include="prov\tcp\src\tcpx_shared_ctx.c" />
     <ClCompile Include="prov\tcp\src\tcpx_cq.c" />
@@ -688,6 +687,7 @@
     <ClCompile Include="prov\util\src\util_mr_cache.c" />
     <ClCompile Include="prov\util\src\cuda_mem_monitor.c" />
     <ClCompile Include="prov\util\src\rocr_mem_monitor.c" />
+    <ClCompile Include="prov\util\src\ze_mem_monitor.c" />
     <ClCompile Include="src\common.c" />
     <ClCompile Include="src\enosys.c">
       <DisableSpecificWarnings Condition="'$(Configuration)|$(Platform)'=='Debug-ICC|x64'">4127;869</DisableSpecificWarnings>
@@ -746,6 +746,7 @@
     <ClInclude Include="include\rdma\fi_endpoint.h" />
     <ClInclude Include="include\rdma\fi_eq.h" />
     <ClInclude Include="include\rdma\fi_errno.h" />
+    <ClInclude Include="include\rdma\fi_ext.h" />
     <ClInclude Include="include\rdma\fi_rma.h" />
     <ClInclude Include="include\rdma\fi_tagged.h" />
     <ClInclude Include="include\rdma\fi_trigger.h" />
diff --git a/libfabric.vcxproj.filters b/libfabric.vcxproj.filters
index 55f0e04..385816e 100644
--- a/libfabric.vcxproj.filters
+++ b/libfabric.vcxproj.filters
@@ -201,6 +201,9 @@
     <ClCompile Include="prov\util\src\rocr_mem_monitor.c">
       <Filter>Source Files\prov\util</Filter>
     </ClCompile>
+    <ClCompile Include="prov\util\src\ze_mem_monitor.c">
+      <Filter>Source Files\prov\util</Filter>
+    </ClCompile>
     <ClCompile Include="src\windows\osd.c">
       <Filter>Source Files\src\windows</Filter>
     </ClCompile>
@@ -429,9 +432,6 @@
     <ClCompile Include="prov\tcp\src\tcpx_attr.c">
       <Filter>Source Files\prov\tcp\src</Filter>
     </ClCompile>
-    <ClCompile Include="prov\tcp\src\tcpx_comm.c">
-      <Filter>Source Files\prov\tcp\src</Filter>
-    </ClCompile>
     <ClCompile Include="prov\tcp\src\tcpx_conn_mgr.c">
       <Filter>Source Files\prov\tcp\src</Filter>
     </ClCompile>
@@ -566,6 +566,9 @@
     <ClInclude Include="include\rdma\fi_errno.h">
       <Filter>Header Files\rdma</Filter>
     </ClInclude>
+    <ClInclude Include="include\rdma\fi_ext.h">
+      <Filter>Header Files\rdma</Filter>
+    </ClInclude>
     <ClInclude Include="include\rdma\fi_rma.h">
       <Filter>Header Files\rdma</Filter>
     </ClInclude>
diff --git a/man/fabric.7.md b/man/fabric.7.md
index 29f76f1..6fef65d 100644
--- a/man/fabric.7.md
+++ b/man/fabric.7.md
@@ -281,6 +281,7 @@ application, with the -e or --env command line option.
 
 # NOTES
 
+## System Calls
 Because libfabric is designed to provide applications direct access to
 fabric hardware, there are limits on how libfabric resources may be used
 in conjunction with system calls.  These limitations are notable for
@@ -297,6 +298,25 @@ portability across providers.
   fabric domain may not be available in a child process because of copy
   on write restrictions.
 
+## CUDA deadlock
+In some cases, calls to `cudaMemcpy` within libfabric may result in a
+deadlock. This typically occurs when a CUDA kernel blocks until a
+`cudaMemcpy` on the host completes.  To avoid this deadlock,
+`cudaMemcpy` may be disabled by setting
+`FI_HMEM_CUDA_ENABLE_XFER=0`. If this environment variable is set and
+there is a call to `cudaMemcpy` with libfabric, a warning will be
+emitted and no copy will occur. Note that not all providers support
+this option.
+
+Another mechanism which can be used to avoid deadlock is Nvidia's
+gdrcopy. Using gdrcopy requires an external library and kernel module
+available at https://github.com/NVIDIA/gdrcopy. Libfabric must be
+configured with gdrcopy support using the `--with-gdrcopy` option, and
+be run with `FI_HMEM_CUDA_USE_GDRCOPY=1`. This may be used in
+conjunction with the above option to provide a method for copying
+to/from CUDA device memory when `cudaMemcpy` cannot be used. Again,
+this may not be supported by all providers.
+
 # ABI CHANGES
 
 libfabric releases maintain compatibility with older releases, so that
@@ -350,8 +370,8 @@ expanded the following structure.
 
 ## ABI 1.3
 
-The 1.3 ABI is also the current ABI version.  All libfabric releases
-starting at 1.9 export this ABI.
+The 1.3 ABI version was exported by libfabric versions 1.9, 1.10, and
+1.11.  Added new fields to the following attributes:
 
 *fi_domain_attr*
 : Added tclass
@@ -359,6 +379,20 @@ starting at 1.9 export this ABI.
 *fi_tx_attr*
 : Added tclass
 
+## ABI 1.4
+
+The 1.4 ABI version was exported by libfabric 1.12.  Added fi_tostr_r, a
+thread-safe (re-entrant) version of fi_tostr.
+
+## ABI 1.5
+
+ABI version starting with libfabric 1.13.  Added new fi_open API
+call.
+
+## ABI 1.6
+
+ABI version starting with libfabric 1.14.  Added fi_log_ready for providers.
+
 # SEE ALSO
 
 [`fi_info`(1)](fi_info.1.html),
diff --git a/man/fi_atomic.3.md b/man/fi_atomic.3.md
index 3375856..b7d5d72 100644
--- a/man/fi_atomic.3.md
+++ b/man/fi_atomic.3.md
@@ -190,6 +190,12 @@ provider implementation constraints.
 *FI_UINT64*
 : Unsigned 64-bit integer.
 
+*FI_INT128*
+: Signed 128-bit integer.
+
+*FI_UINT128*
+: Unsigned 128-bit integer.
+
 *FI_FLOAT*
 : A single-precision floating point value (IEEE 754).
 
diff --git a/man/fi_cq.3.md b/man/fi_cq.3.md
index e160095..f29f0ff 100644
--- a/man/fi_cq.3.md
+++ b/man/fi_cq.3.md
@@ -463,7 +463,7 @@ of these fields are the same for all CQ entry structure formats.
   was provided with an asynchronous operation.  The op_context field is
   valid for all completions that are associated with an asynchronous
   operation.
-  
+
   For completion events that are not associated with a posted operation,
   this field will be set to NULL.  This includes completions generated
   at the target in response to RMA write operations that carry CQ data
@@ -624,7 +624,7 @@ operation.  The following completion flags are defined.
   have FI_BUFFERED_RECV mode enabled.  When set to one, it indicates that
   the buffer referenced by the completion is limited by the
   FI_OPT_BUFFERED_LIMIT threshold, and additional message data must be
-  retrieved by the application using an FI_CLAIM operation.  
+  retrieved by the application using an FI_CLAIM operation.
 
 *FI_CLAIM*
 : See the 'Buffered Receives' section in `fi_msg`(3) for more details.
@@ -762,7 +762,209 @@ The operational flags for the described completion levels are defined below.
   that was originally requested has been met.  It is the completion
   of the fenced operation that guarantees that the additional
   semantics have been met.
- 
+
+The above completion semantics are defined with respect to the initiator
+of the operation.  The different semantics are useful for describing
+when the initiator may re-use a data buffer, and guarantees what state
+a transfer must reach prior to a completion being generated.  This
+allows applications to determine appropriate error handling in case
+of communication failures.
+
+# TARGET COMPLETION SEMANTICS
+
+The completion semantic at the target is used to determine when data
+at the target is visible to the peer application.  Visibility
+indicates that a memory read to the same address that was
+the target of a data transfer will return the results of the transfer.
+The target of a transfer can be identified by the initiator,
+as may be the case for RMA and atomic operations, or determined by
+the target, for example by providing a matching receive buffer.
+Global visibility indicates that the results are available regardless
+of where the memory read originates.  For example, the read could come
+from a process running on a host CPU, it may be accessed by subsequent
+data transfer over the fabric, or read from a peer device such as a GPU.
+
+In terms of completion semantics, visibility usually indicates that the
+transfer meets the FI_DELIVERY_COMPLETE requirements from the
+perspective of the target.  The target completion semantic may be, but
+is not necessarily, linked with the completion semantic specified by the
+initiator of the transfer.
+
+Often, target processes do not explicitly state a desired completion
+semantic and instead rely on the default semantic.  The
+default behavior is based on several factors, including:
+
+- whether a completion even is generated at the target
+- the type of transfer involved (e.g. msg vs RMA)
+- endpoint data and message ordering guarantees
+- properties of the targeted memory buffer
+- the initiator's specified completion semantic
+
+Broadly, target completion semantics are grouped
+based on whether or not the transfer generates a completion event
+at the target.  This includes writing a CQ entry or updating a completion
+counter.  In common use cases, transfers that use a message
+interface (FI_MSG or FI_TAGGED) typically generate target events, while
+transfers involving an RMA interface (FI_RMA or FI_ATOMIC) often do not.
+There are exceptions to both these cases, depending on endpoint to CQ
+and counter bindings and operational flags.  For example, RMA writes that
+carry remote CQ data will generate a completion event at the target,
+and are frequently used to convey visibility to the target application.
+The general guidelines for target side semantics are described below,
+followed by exceptions that modify that behavior.
+
+By default, completions generated at the target indicate that the
+transferred data is immediately available to be read from the target buffer.
+That is, the target sees FI_DELIVERY_COMPLETE (or better) semantics,
+even if the initiator requested lower semantics.
+For applications using only data buffers allocated from
+host memory, this is often sufficient.
+
+For operations that do not generate a completion event at the target,
+the visibility of the data at the target may need to be inferred
+based on subsequent operations that do generate target completions.
+Absent a target completion, when a completion of an
+operation is written at the initiator, the visibility semantic
+of the operation at the target aligns with the initiator completion
+semantic.  For instance, if an RMA operation completes at the initiator
+as either FI_INJECT_COMPLETE or FI_TRANSMIT_COMPLETE, the data visibility
+at the target is not guaranteed.
+
+One or more of the following mechanisms can be used by the target process to
+guarantee that the results of a data transfer that did not generate a
+completion at the target is now visible.  This list is not inclusive of
+all options, but defines common uses.  In the descriptions below, the first
+transfer does not result in a completion event at the target, but is
+eventually followed by a transfer which does.
+
+- If the endpoint guarantees message ordering between two transfers, the
+  target completion of a second transfer will indicate that the data from
+  the first transfer is available.  For example, if the endpoint supports
+  send after write ordering (FI_ORDER_SAW), then a receive completion
+  corresponding to the send will indicate that the write data is available.
+  This holds independent of the initiator's completion semantic for either
+  the write or send.  When ordering is guaranteed, the second transfer
+  can be queued with the provider immediately after queuing the first.
+
+- If the endpoint does not guarantee message ordering, the initiator must take
+  additional steps to ensure visibility.  If initiator requests
+  FI_DELIVERY_COMPLETE semantics for the first operation, the initiator can wait
+  for the operation to complete locally.  Once the completion has been
+  read, the target completion of a second transfer will indicate that the
+  first transfer's data is visible.
+
+- Alternatively, if message ordering is not guaranteed by the endpoint, the
+  initiator can use the FI_FENCE and FI_DELIVERY_COMPLETE flags on the second
+  data transfer to force the first transfers to meet the
+  FI_DELIVERY_COMPLETE semantics.  If the second transfer generates a
+  completion at the target, that will indicate that the data is visible.
+  Otherwise, a target completion for any transfer after the
+  fenced operation will indicate that the data is visible.
+
+The above semantics apply for transfers targeting traditional host memory
+buffers.  However, the behavior may differ when device memory and/or
+persistent memory is involved (FI_HMEM and FI_PMEM capability bits).  When
+heterogenous memory is involved, the concept of memory domains come into
+play.  Memory domains identify the physical separation of memory, which
+may or may not be accessible through the same virtual address space.  See
+the [`fi_mr`(3)](fi_mr.3.html) man page for further details on memory domains.
+
+Completion ordering and data visibility are only well-defined for transfers
+that target the same memory domain.  Applications need to be aware of
+ordering and visibility differences when transfers target different memory
+domains.  Additionally, applications also need to be concerned with the
+memory domain that completions themselves are written and if it differs
+from the memory domain targeted by a transfer.  In some situations,
+either the provider or application may need to call device specific APIs
+to synchronize or flush device memory caches in order to achieve the
+desired data visibility.
+
+When heterogenous memory is in use, the default target completion semantic
+for transfers that generate a completion at the target is still
+FI_DELIVERY_COMPLETE, however, applications should be aware that there
+may be a negative impact on overall performance for providers to meet
+this requirement.
+
+For example, a target process may be using a GPU to accelerate computations.
+A memory region mapping to memory on the GPU may be exposed to peers as
+either an RMA target or posted locally as a receive buffer.  In this case,
+the application is concerned with two memory domains -- system and GPU
+memory.  Completions are written to system memory.
+
+Continuing the example, a peer process sends a tagged message.  That message
+is matched with the receive buffer located in GPU memory.  The NIC copies
+the data from the network into the receive buffer and writes an entry into
+the completion queue.  Note that both memory domains were accessed as part
+of this transfer.  The message data was directed to the GPU memory, but the
+completion went to host memory.   Because separate memory domains may not be
+synchronized with each other, it is possible for the host CPU to see and process
+the completion entry before the transfer to the GPU memory is visible to either
+the host GPU or even software running on the GPU.  From the perspective
+of the *provider*, visibility of the completion does not imply visibility of
+data written to the GPU's memory domain.
+
+The default completion semantic at the target *application* for message
+operations is FI_DELIVERY_COMPLETE.  An anticipated provider implementation
+in this situation is for the provider software running on the host CPU to
+intercept the CQ entry, detect that the data landed in heterogenous memory,
+and perform the necessary device synchronization or flush operation
+before reporting the completion up to the application.  This ensures that
+the data is visible to CPU _and_ GPU software prior to the application
+processing the completion.
+
+In addition to the cost of provider software intercepting completions
+and checking if a transfer targeted heterogenous memory, device
+synchronization itself may impact performance.  As a result, applications
+can request a lower completion semantic when posting receives.  That
+indicates to the provider that the application will be responsible for
+handling any device specific flush operations that might be needed.
+See [`fi_msg`(3)](fi_msg.3.html) FLAGS.
+
+For data transfers that do not generate a completion at the target,
+such as RMA or atomics, it is the responsibility of the application
+to ensure that all target buffers meet the necessary visibility
+requirements of the application.  The previously mentioned bulleted
+methods for notifying the target that the data is visible may not
+be sufficient, as the provider software at the target could lack
+the context needed to ensure visibility.  This implies that the
+application may need to call device synchronization/flush APIs
+directly.
+
+For example, a peer application could perform several RMA writes
+that target GPU memory buffers.  If the provider offloads RMA
+operations into the NIC, the provider software at the target will
+be unaware that the RMA operations have occurred.  If the peer
+sends a message to the target application that indicates that the
+RMA operations are done, the application must ensure that the RMA data
+is visible to the host CPU or GPU prior to executing code that accesses
+the data.  The target completion of having received the sent message
+is not sufficient, even if send-after-write ordering is supported.
+
+Most target heterogenous memory completion semantics map to
+FI_TRANSMIT_COMPLETE or FI_DELIVERY_COMPLETE.  Persistent memory
+(FI_PMEM capability), however, is often used with FI_COMMIT_COMPLETE
+semantics.  Heterogenous completion concepts still apply.
+
+For transfers flagged by the initiator with FI_COMMIT_COMPLETE,
+a completion at the target indicates that the results are visible
+and durable.  For transfers targeting persistent memory, but using
+a different completion semantic at the initiator, the visibility
+at the target is similar to that described above.  Durability is
+only associated with transfers marked with FI_COMMIT_COMPLETE.
+
+For transfers targeting persistent memory that request
+FI_DELIVERY_COMPLETE, then a completion, at either the initiator or
+target, indicates that the data is visible.  Visibility at the
+target can be conveyed using one of the above describe mechanism --
+generating a target completion, sending a message from the initiator,
+etc.  Similarly, if the initiator requested FI_TRANSMIT_COMPLETE,
+then additional steps are needed to ensure visibility at the target.
+For example, the transfer can generate a completion at the target,
+which would indicate visibility, but not durability.  The initiator
+can also follow the transfer with another operation that forces
+visibility, such as using FI_FENCE in conjunction with
+FI_DELIVERY_COMPLETE.
+
 # NOTES
 
 A completion queue must be bound to at least one enabled endpoint before any
diff --git a/man/fi_domain.3.md b/man/fi_domain.3.md
index 7acda47..ce7c085 100644
--- a/man/fi_domain.3.md
+++ b/man/fi_domain.3.md
@@ -446,8 +446,10 @@ transfer operation.
   seen by the initiator of a request.  For FI_EP_DGRAM endpoints, if the target EP
   queues are unable to accept incoming messages, received messages will
   be dropped.  For reliable endpoints, if RM is disabled, the transmit
-  operation will complete in error.  If RM is enabled, the provider will
-  internally retry the operation.
+  operation will complete in error. A provider may choose to return an error
+  completion with the error code FI_ENORX for that transmit operation so that
+  it can be retried. If RM is enabled, the provider will internally retry the
+  operation.
 
 *Rx Buffer Overrun*
 : This refers to buffers posted to receive incoming tagged or untagged messages,
diff --git a/man/fi_efa.7.md b/man/fi_efa.7.md
index 89a3dfc..186799a 100644
--- a/man/fi_efa.7.md
+++ b/man/fi_efa.7.md
@@ -82,6 +82,14 @@ No support for counters for the DGRAM endpoint.
 
 No support for inject.
 
+# PROVIDER SPECIFIC ENDPOINT LEVEL OPTION
+
+*FI_OPT_EFA_RNR_RETRY*
+: Defines the number of RNR retry. The application can use it to reset RNR retry
+  counter via the call to fi_setopt. Note that this option must be set before
+  the endpoint is enabled. Otherwise, the call will fail. Also note that this
+  option only applies to RDM endpoint.
+
 # RUNTIME PARAMETERS
 
 *FI_EFA_TX_SIZE*
@@ -165,10 +173,14 @@ These OFI runtime parameters apply only to the RDM endpoint.
 *FI_EFA_SHM_MAX_MEDIUM_SIZE*
 : Defines the switch point between small/medium message and large message. The message
   larger than this switch point will be transferred with large message protocol.
+  NOTE: This parameter is now deprecated.
 
 *FI_EFA_INTER_MAX_MEDIUM_MESSAGE_SIZE*
 : The maximum size for inter EFA messages to be sent by using medium message protocol. Messages which can fit in one packet will be sent as eager message. Messages whose sizes are smaller than this value will be sent using medium message protocol. Other messages will be sent using CTS based long message protocol.
 
+*FI_EFA_FORK_SAFE*
+: Enable fork() support. This may have a small performance impact and should only be set when required. Applications that require to register regions backed by huge pages and also require fork support are not supported.
+
 # SEE ALSO
 
 [`fabric`(7)](fabric.7.html),
diff --git a/man/fi_endpoint.3.md b/man/fi_endpoint.3.md
index 7797e30..c7ffb6d 100644
--- a/man/fi_endpoint.3.md
+++ b/man/fi_endpoint.3.md
@@ -718,16 +718,26 @@ data into target memory for RMA and atomic operations.  Data ordering
 is separate, but dependent on message ordering (defined below).  Data
 ordering is unspecified where message order is not defined.
 
-Data ordering refers to the access of target memory by subsequent
+Data ordering refers to the access of the same target memory by subsequent
 operations.  When back to back RMA read or write operations access the
 same registered memory location, data ordering indicates whether the
 second operation reads or writes the target memory after the first
-operation has completed.  Because RMA ordering applies between two
-operations, and not within a single data transfer, ordering is defined
-per byte-addressable memory location.  I.e.  ordering specifies
+operation has completed.  For example, will an RMA read that follows
+an RMA write read back the data that was written?  Similarly, will an
+RMA write that follows an RMA read update the target buffer after the
+read has transferred the original data?  Data ordering answers these
+questions, even in the presence of errors, such as the need to resend
+data because of lost or corrupted network traffic.
+
+RMA ordering applies between two operations, and not within a single
+data transfer.  Therefore, ordering is defined
+per byte-addressable memory location.  I.e. ordering specifies
 whether location X is accessed by the second operation after the first
 operation.  Nothing is implied about the completion of the first
-operation before the second operation is initiated.
+operation before the second operation is initiated.  For example, if
+the first operation updates locations X and Y, but the second operation
+only accesses location X, there are no guarantees defined relative to
+location Y and the second operation.
 
 In order to support large data transfers being broken into multiple packets
 and sent using multiple paths through the fabric, data ordering may be
diff --git a/man/fi_fabric.3.md b/man/fi_fabric.3.md
index 4e71633..9b7bf19 100644
--- a/man/fi_fabric.3.md
+++ b/man/fi_fabric.3.md
@@ -7,10 +7,10 @@ tagline: Libfabric Programmer's Manual
 
 # NAME
 
-fi_fabric \- Fabric domain operations
+fi_fabric \- Fabric network operations
 
 fi_fabric / fi_close
-: Open / close a fabric domain
+: Open / close a fabric network
 
 fi_tostr / fi_tostr_r
 : Convert fabric attributes, flags, and capabilities to printable string
@@ -37,7 +37,7 @@ char * fi_tostr(char *buf, size_t len, const void *data,
 : Attributes of fabric to open.
 
 *fabric*
-: Fabric domain
+: Fabric network
 
 *context*
 : User specified context associated with the opened object.  This
@@ -58,22 +58,27 @@ char * fi_tostr(char *buf, size_t len, const void *data,
 
 # DESCRIPTION
 
-A fabric domain represents a collection of hardware and software
+A fabric identifier is used to reference opened fabric resources
+and library related objects.
+
+The fabric network represents a collection of hardware and software
 resources that access a single physical or virtual network.  All
 network ports on a system that can communicate with each other through
-their attached networks belong to the same fabric domain.  A fabric
-domain shares network addresses and can span multiple providers.
+their attached networks belong to the same fabric.  A fabric
+network shares network addresses and can span multiple providers.  An
+application must open a fabric network prior to allocating other network
+resources, such as communication endpoints.
 
 ## fi_fabric
 
-Opens a fabric provider.  The attributes of the fabric provider are
+Opens a fabric network provider.  The attributes of the fabric provider are
 specified through the open call, and may be obtained by calling
 fi_getinfo.
 
 ## fi_close
 
 The fi_close call is used to release all resources associated with a
-fabric domain or interface.  All items associated with the opened
+fabric object.  All items associated with the opened
 fabric must be released prior to calling fi_close.
 
 ## fi_tostr / fi_tostr_r
diff --git a/man/fi_mr.3.md b/man/fi_mr.3.md
index d460c67..decda08 100644
--- a/man/fi_mr.3.md
+++ b/man/fi_mr.3.md
@@ -72,8 +72,8 @@ int fi_mr_unmap_key(struct fid_domain *domain, uint64_t key);
 
 int fi_mr_bind(struct fid_mr *mr, struct fid *bfid, uint64_t flags);
 
-int fi_mr_refresh(struct fid_mr *mr, const struct iovec *iov, size, count,
-    uint64_t flags)
+int fi_mr_refresh(struct fid_mr *mr, const struct iovec *iov,
+    size_t count, uint64_t flags);
 
 int fi_mr_enable(struct fid_mr *mr);
 ```
@@ -667,6 +667,52 @@ The follow flag may be specified to any memory registration call.
   specified if persistent completion semantics or persistent data transfers
   are required when accessing the registered region.
 
+*FI_HMEM_DEVICE_ONLY*
+: This flag indicates that the memory is only accessible by a device. Which
+  device is specified by the fi_mr_attr fields iface and device. This refers
+  to memory regions that were allocated using a device API AllocDevice call
+  (as opposed to using the host allocation or unified/shared memory allocation).
+
+# MEMORY DOMAINS
+
+Memory domains identify the physical separation of memory which
+may or may not be accessible through the same virtual address space.
+Traditionally, applications only dealt with a single memory domain,
+that of host memory tightly coupled with the system CPUs.  With
+the introduction of device and non-uniform memory subsystems,
+applications often need to be aware of which memory domain a particular
+virtual address maps to.
+
+As a general rule, separate physical devices can be considered to have
+their own memory domains.  For example, a NIC may have user accessible
+memory, and would be considered a separate memory domain from memory
+on a GPU.  Both the NIC and GPU memory domains are separate from host
+system memory.  Individual GPUs or computation accelerators may have
+distinct memory domains, or may be connected in such a way (e.g. a GPU
+specific fabric) that all GPUs would belong to the same memory domain.
+Unfortunately, identifying memory domains is specific to each
+system and its physical and/or virtual configuration.
+
+Understanding memory domains in heterogenous memory environments is
+important as it can impact data ordering and visibility as viewed
+by an application.  It is also important to understand which memory
+domain an application is most tightly coupled to.  In most cases,
+applications are tightly coupled to host memory.  However, an
+application running directly on a GPU or NIC may be more tightly
+coupled to memory associated with those devices.
+
+Memory regions are often associated with a single memory domain.
+The domain is often indicated by the fi_mr_attr iface and device
+fields.  Though it is possible for physical pages backing a virtual
+memory region to migrate between memory domains based on access patterns.
+For example, the physical pages referenced by a virtual address range
+could migrate between host memory and GPU memory, depending on which
+computational unit is actively using it.
+
+See the [`fi_endpoint`(3)](fi_endpoint.3.html) and [`fi_cq`(3)](fi_cq.3.html)
+man pages for addition discussion on message, data, and completion ordering
+semantics, including the impact of memory domains.
+
 # RETURN VALUES
 
 Returns 0 on success.  On error, a negative value corresponding to
@@ -754,6 +800,19 @@ configure registration caches.
   are: 0 or 1. Note that the ROCR memory monitor requires a ROCR version with
   unified virtual addressing enabled.
 
+*FI_MR_ZE_CACHE_MONITOR_ENABLED*
+: The ZE cache monitor is responsible for detecting ZE device memory
+  (FI_HMEM_ZE) changes made between the device virtual addresses used by an
+  application and the underlying device physical pages. Valid monitor options
+  are: 0 or 1.
+
+More direct access to the internal registration cache is possible through the
+fi_open() call, using the "mr_cache" service name.  Once opened, custom
+memory monitors may be installed.  A memory monitor is a component of the cache
+responsible for detecting changes in virtual to physical address mappings.
+Some level of control over the cache is possible through the above mentioned
+environment variables.
+
 # SEE ALSO
 
 [`fi_getinfo`(3)](fi_getinfo.3.html),
diff --git a/man/fi_msg.3.md b/man/fi_msg.3.md
index 26d806c..02fc15d 100644
--- a/man/fi_msg.3.md
+++ b/man/fi_msg.3.md
@@ -274,9 +274,14 @@ fi_sendmsg.
   generated when the source buffer(s) may be reused.
 
 *FI_TRANSMIT_COMPLETE*
-: Applies to fi_sendmsg.  Indicates that a completion should not be
-  generated until the operation has been successfully transmitted and
-  is no longer being tracked by the provider.
+: Applies to fi_sendmsg and fi_recvmsg.  For sends, indicates that a
+  completion should not be generated until the operation has been
+  successfully transmitted and is no longer being tracked by the provider.
+  For receive operations, indicates that a completion may be generated
+  as soon as the message has been processed by the local provider,
+  even if the message data may not be visible to all processing
+  elements.  See [`fi_cq`(3)](fi_cq.3.html) for target side completion
+  semantics.
 
 *FI_DELIVERY_COMPLETE*
 : Applies to fi_sendmsg.  Indicates that a completion should be
diff --git a/man/fi_provider.3.md b/man/fi_provider.3.md
new file mode 100644
index 0000000..f5b94b8
--- /dev/null
+++ b/man/fi_provider.3.md
@@ -0,0 +1,255 @@
+---
+layout: page
+title: fi_provider(3)
+tagline: Libfabric Programmer's Manual
+---
+{% include JB/setup %}
+
+# NAME
+
+fi_prov_ini \- External provider entry point
+
+fi_param_define / fi_param_get
+: Register and retrieve environment variables with the libfabric core
+
+fi_log_enabled / fi_log_ready / fi_log
+: Control and output debug logging information.
+
+fi_open / fi_close
+: Open a named library object
+
+fi_export_fid / fi_import_fid
+: Share a fabric object between different providers or resources
+
+# SYNOPSIS
+
+```c
+#include <rdma/fabric.h>
+#include <rdma/prov/fi_prov.h>
+
+struct fi_provider* fi_prov_ini(void);
+
+int fi_param_define(const struct fi_provider *provider, const char *param_name,
+	enum fi_param_type type, const char *help_string_fmt, ...);
+
+int fi_param_get_str(struct fi_provider *provider, const char *param_name,
+	char **value);
+
+int fi_param_get_int(struct fi_provider *provider, const char *param_name,
+	int *value);
+
+int fi_param_get_bool(struct fi_provider *provider, const char *param_name,
+	int *value);
+
+int fi_param_get_size_t(struct fi_provider *provider, const char *param_name,
+	size_t *value);
+```
+
+```c
+#include <rdma/fabric.h>
+#include <rdma/prov/fi_prov.h>
+#include <rdma/prov/fi_log.h>
+
+int fi_log_enabled(const struct fi_provider *prov, enum fi_log_level level,
+	enum fi_log_subsys subsys);
+
+int fi_log_ready(const struct fi_provider *prov, enum fi_log_level level,
+	enum fi_log_subsys subsys, uint64_t *showtime);
+
+void fi_log(const struct fi_provider *prov, enum fi_log_level level,
+	enum fi_log_subsys subsys, const char *func, int line,
+	const char *fmt, ...);
+```
+
+```c
+#include <rdma/fabric.h>
+
+int fi_open(uint32_t version, const char *name, void *attr,
+	size_t attr_len, uint64_t flags, struct fid **fid, void *context);
+
+int fi_close(struct fid *fid);
+```
+
+```c
+#include <rdma/fabric.h>
+#include <rdma/fi_ext.h>
+
+int fi_export_fid(struct fid *fid, uint64_t flags,
+	struct fid **expfid, void *context);
+
+int fi_import_fid(struct fid *fid, struct fid *expfid, uint64_t flags);
+```
+
+# ARGUMENTS
+
+*provider*
+: Reference to the provider.
+
+*version*
+: API version requested by application.
+
+*name*
+: Well-known name of the library object to open.
+
+*attr*
+: Optional attributes of object to open.
+
+*attr_len*
+: Size of any attribute structure passed to fi_open.  Should be 0
+  if no attributes are give.
+
+*fid*
+: Returned fabric identifier for opened object.
+
+# DESCRIPTION
+
+A fabric provider implements the application facing software
+interfaces needed to access network specific protocols,
+drivers, and hardware.  The interfaces and structures defined by
+this man page are exported by the libfabric library, but are
+targeted for provider implementations, rather than for direct
+use by most applications.
+
+Integrated providers are those built directly into the libfabric
+library itself.  External providers are loaded dynamically by
+libfabric at initialization time.  External providers must be in
+a standard library path or in the libfabric library search path
+as specified by environment variable.  Additionally, external
+providers must be named with the suffix "-fi.so" at the end of
+the name.
+
+Named objects are special purpose resources which are accessible directly
+to applications.  They may be used to enhance or modify the behavior of
+library core.  For details, see the fi_open call below.
+
+## fi_prov_ini
+
+This entry point must be defined by external providers.  On loading,
+libfabric will invoke fi_prov_ini() to retrieve the provider's
+fi_provider structure.  Additional interactions between the libfabric
+core and the provider will be through the interfaces defined by that
+struct.
+
+## fi_param_define
+
+Defines a configuration parameter for use by a specified provider. The
+help_string and param_name arguments must be non-NULL, help_string
+must additionally be non-empty. They are copied internally and may be
+freed after calling fi_param_define.
+
+## fi_param_get
+
+Gets the value of a configuration parameter previously defined using
+fi_param_define(). The value comes from the environment variable name of
+the form FI_<provider_name>_<param_name>, all converted to upper case.
+
+If the parameter was previously defined and the user set a value,
+FI_SUCCESS is returned and (*value) points to the retrieved
+value.
+
+If the parameter name was previously defined, but the user did
+not set a value, -FI_ENODATA is returned and the value of (*value)
+is unchanged.
+
+If the parameter name was not previously defined via
+fi_param_define(), -FI_ENOENT will be returned and the value of
+(*value) is unchanged.
+
+If the value in the environment is not valid for the parameter type,
+-FI_EINVAL will be returned and the value of (*value) is unchanged.
+
+## fi_log_enabled / fi_log_ready / fi_log
+
+These functions control debug and informational logging output.
+Providers typically access these functions through the FI_LOG and
+related macros in fi_log.h and do not call these function directly.
+
+## fi_open
+
+Open a library resource using a well-known name.  This feature allows
+applications and providers a mechanism which can be used to modify or
+enhance core library services and behavior.  The details are specific
+based on the requested object name.  Most applications will not need
+this level of control.
+
+The library API version known to the application should be provided
+through the version parameter.  The use of attributes is object dependent.
+If required, attributes should be provided through the attr parameter,
+with attr_len set to the size of the referenced attribute structure.
+The following is a list of published names, along with descriptions
+of the service or resource to which they correspond.
+
+*mr_cache*
+: The mr_cache object references the internal memory registration cache
+  used by the different providers.  Additional information on the cache
+  is available in the `fi_mr(3)` man page.
+
+## fi_export_fid / fi_import_fid
+
+Generally, fabric objects are allocated and managed entirely by a single
+provider.  Typically only the application facing software interfaces of
+a fabric object are defined, for example, the message or tagged operations
+of an endpoint.  The fi_export_fid and fi_import_fid calls provide a
+a mechanism by which provider facing APIs may be accessed.  This allows
+the creation of fid objects that are shareable between providers, or
+for library plug-in services.  The ability to export a shareable object
+is object and provider implementation dependent.
+
+Shareable fids typically contain at least 3 main components: a
+base fid, a set of exporter defined ops, and a set of importer defined
+ops.
+
+# NOTES
+
+TODO
+
+# PROVIDER INTERFACE
+
+The fi_provider structure defines entry points for the libfabric core
+to use to access the provider.  All other calls into a provider are
+through function pointers associated with allocated objects.
+
+```c
+struct fi_provider {
+	uint32_t version;
+	uint32_t fi_version;
+	struct fi_context context;
+	const char *name;
+	int	(*getinfo)(uint32_t version, const char *node, const char *service,
+			uint64_t flags, const struct fi_info *hints,
+			struct fi_info **info);
+	int	(*fabric)(struct fi_fabric_attr *attr, struct fid_fabric **fabric,
+			void *context);
+	void	(*cleanup)(void);
+};
+```
+
+## version
+
+The provider version.  For providers integrated with the library, this is
+often the same as the library version.
+
+## fi_version
+
+The library interface version that the provider was implemented against.
+The provider's fi_version must be greater than or equal to an application's
+requested api version for the application to use the provider.  It is a
+provider's responsibility to support older versions of the api if it
+wishes to supports legacy applications.  For integrated providers
+
+## TODO
+
+# RETURN VALUE
+
+Returns FI_SUCCESS on success. On error, a negative value corresponding to
+fabric errno is returned. Fabric errno values are defined in
+`rdma/fi_errno.h`.
+
+# ERRORS
+
+
+# SEE ALSO
+
+[`fabric`(7)](fabric.7.html),
+[`fi_getinfo`(3)](fi_getinfo.3.html)
+[`fi_mr`(3)](fi_mr.3.html),
diff --git a/man/fi_shm.7.md b/man/fi_shm.7.md
index 8e00a59..b27a57c 100644
--- a/man/fi_shm.7.md
+++ b/man/fi_shm.7.md
@@ -74,7 +74,7 @@ of operations.
   provided (and in the case of setting the src address without FI_SOURCE and
   no hints), the process ID will be used as a default address.
   On endpoint creation, if the src_addr has the "fi_shm://" prefix, the provider
-  will append ":[uid]:[dom_idx]:[ep_idx]" as a unique endpoint name (essentially,
+  will append ":[uid]:[ep_idx]" as a unique endpoint name (essentially,
   in place of a service).  In the case of the "fi_ns://" prefix (or any other
   prefix if one was provided by the application), no supplemental information
   is required to make it unique and it will remain with only the
@@ -122,6 +122,9 @@ The *shm* provider checks for the following environment variables:
 *FI_SHM_RX_SIZE*
 : Maximum number of outstanding rx operations. Default 1024
 
+*FI_SHM_DISABLE_CMA*
+: Manually disables CMA. Default false
+
 # SEE ALSO
 
 [`fabric`(7)](fabric.7.html),
diff --git a/man/fi_tcp.7.md b/man/fi_tcp.7.md
index 4eb3e44..82efd96 100644
--- a/man/fi_tcp.7.md
+++ b/man/fi_tcp.7.md
@@ -53,6 +53,12 @@ The tcp provider check for the following enviroment variables -
   tcp provider for its passive endpoint creation. This is useful where
   only a range of ports are allowed by firewall for tcp connections.
 
+*FI_TCP_TX_SIZE*
+: Default tx context size (default: 256)
+
+*FI_TCP_RX_SIZE*
+: Default rx context size (default: 256)
+
 # LIMITATIONS
 
 The tcp provider is implemented over TCP sockets to emulate libfabric API.
diff --git a/man/man1/fi_info.1 b/man/man1/fi_info.1
index 90c75c1..2b413e5 100644
--- a/man/man1/fi_info.1
+++ b/man/man1/fi_info.1
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_info" "1" "2020\-01\-30" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_info" "1" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,8 +9,8 @@ fi_info \- Simple utility to query for fabric interfaces
 .IP
 .nf
 \f[C]
-\ fi_info\ [OPTIONS]
-\f[]
+ fi_info [OPTIONS]
+\f[R]
 .fi
 .SH DESCRIPTION
 .PP
@@ -25,101 +25,73 @@ all providers and endpoint types will be returned.
 .SH OPTIONS
 .SS Filtering
 .TP
-.B \f[I]\-n, \-\-node=<NAME>\f[]
+.B \f[I]\-n, \[en]node=<NAME>\f[R]
 Node name or address used to filter interfaces.
 Only interfaces which can reach the given node or address will respond.
-.RS
-.RE
 .TP
-.B \f[I]\-P, \-\-port=<PORT>\f[]
+.B \f[I]\-P, \[en]port=<PORT>\f[R]
 Port number used to filter interfaces.
-.RS
-.RE
 .TP
-.B \f[I]\-c, \-\-caps=<CAP1|CAP2>..\f[]
+.B \f[I]\-c, \[en]caps=<CAP1|CAP2>..\f[R]
 Pipe separated list of capabilities used to filter interfaces.
 Only interfaces supporting all of the given capabilities will respond.
 For more information on capabilities, see fi_getinfo(3).
-.RS
-.RE
 .TP
-.B \f[I]\-m, \-\-mode=<MOD1|MOD2>..\f[]
+.B \f[I]\-m, \[en]mode=<MOD1|MOD2>..\f[R]
 Pipe separated list of modes used to filter interfaces.
 Only interfaces supporting all of the given modes will respond.
 For more information on, modes see fi_getinfo(3).
-.RS
-.RE
 .TP
-.B \f[I]\-t, \-\-ep_type=<EPTYPE>\f[]
+.B \f[I]\-t, \[en]ep_type=<EPTYPE>\f[R]
 Specifies the type of fabric interface communication desired.
 For example, specifying FI_EP_DGRAM would return only interfaces which
 support unreliable datagram.
 For more information on endpoint types, see fi_endpoint(3).
-.RS
-.RE
 .TP
-.B \f[I]\-a, \-\-addr_format=<FMT>\f[]
+.B \f[I]\-a, \[en]addr_format=<FMT>\f[R]
 Filter fabric interfaces by their address format.
 For example, specifying FI_SOCKADDR_IN would return only interfaces
 which use sockaddr_in structures for addressing.
 For more information on address formats, see fi_getinfo(3).
-.RS
-.RE
 .TP
-.B \f[I]\-p, \-\-provider=<PROV>\f[]
+.B \f[I]\-p, \[en]provider=<PROV>\f[R]
 Filter fabric interfaces by the provider implementation.
-For a list of providers, see the \f[C]\-\-list\f[] option.
-.RS
-.RE
+For a list of providers, see the \f[C]\-\-list\f[R] option.
 .TP
-.B \f[I]\-d, \-\-domain=<DOMAIN>\f[]
+.B \f[I]\-d, \[en]domain=<DOMAIN>\f[R]
 Filter interfaces to only those with the given domain name.
-.RS
-.RE
 .TP
-.B \f[I]\-f, \-\-fabric=<FABRIC>\f[]
+.B \f[I]\-f, \[en]fabric=<FABRIC>\f[R]
 Filter interfaces to only those with the given fabric name.
-.RS
-.RE
 .SS Discovery
 .TP
-.B \f[I]\-e, \-\-env\f[]
+.B \f[I]\-e, \[en]env\f[R]
 List libfabric related environment variables which can be used to enable
 extra configuration or tuning.
-.RS
-.RE
 .TP
 .B *\-g [filter]
 Same as \-e option, with output limited to environment variables
 containing filter as a substring.
-.RS
-.RE
 .TP
-.B \f[I]\-l, \-\-list\f[]
+.B \f[I]\-l, \[en]list\f[R]
 List available libfabric providers.
-.RS
-.RE
 .TP
-.B \f[I]\-v, \-\-verbose\f[]
+.B \f[I]\-v, \[en]verbose\f[R]
 By default, fi_info will display a summary of each of the interfaces
 discovered.
 If the verbose option is enabled, then all of the contents of the
 fi_info structure are displayed.
 For more information on the data contained in the fi_info structure, see
 fi_getinfo(3).
-.RS
-.RE
 .TP
-.B \f[I]\-\-version\f[]
+.B \f[I]\[en]version\f[R]
 Display versioning information.
-.RS
-.RE
 .SH USAGE EXAMPLES
 .IP
 .nf
 \f[C]
-$\ fi_info\ \-n\ 30.0.11.1\ \-p\ usnic\ \-t\ FI_EP_DGRAM
-\f[]
+$ fi_info \-n 30.0.11.1 \-p usnic \-t FI_EP_DGRAM
+\f[R]
 .fi
 .PP
 This will respond with all fabric interfaces that can reach address
@@ -131,19 +103,19 @@ discovered:
 .IP
 .nf
 \f[C]
-$\ ./fi_info\ \-n\ 30.0.11.1\ \-p\ usnic\ \-t\ FI_EP_DGRAM
-provider:\ usnic
-\ \ \ \ fabric:\ 30.0.11.0/24
-\ \ \ \ domain:\ usnic_2
-\ \ \ \ version:\ 1.0
-\ \ \ \ type:\ FI_EP_DGRAM
-\ \ \ \ protocol:\ FI_PROTO_UDP
-\f[]
+$ ./fi_info \-n 30.0.11.1 \-p usnic \-t FI_EP_DGRAM
+provider: usnic
+    fabric: 30.0.11.0/24
+    domain: usnic_2
+    version: 1.0
+    type: FI_EP_DGRAM
+    protocol: FI_PROTO_UDP
+\f[R]
 .fi
 .PP
-To see the full fi_info structure, specify the \f[C]\-v\f[] option.
+To see the full fi_info structure, specify the \f[C]\-v\f[R] option.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo(3)\f[], \f[C]fi_endpoint(3)\f[]
+\f[C]fi_getinfo(3)\f[R], \f[C]fi_endpoint(3)\f[R]
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man1/fi_pingpong.1 b/man/man1/fi_pingpong.1
index c9b859c..49c9ffc 100644
--- a/man/man1/fi_pingpong.1
+++ b/man/man1/fi_pingpong.1
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_pingpong" "1" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_pingpong" "1" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,9 +9,9 @@ fi_pingpong \- Quick and simple pingpong test for libfabric
 .IP
 .nf
 \f[C]
-\ fi_pingpong\ [OPTIONS]\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ start\ server
-\ fi_pingpong\ [OPTIONS]\ <server\ address>\ \ \ \ \ connect\ to\ server
-\f[]
+ fi_pingpong [OPTIONS]                      start server
+ fi_pingpong [OPTIONS] <server address>     connect to server
+\f[R]
 .fi
 .SH DESCRIPTION
 .PP
@@ -21,7 +21,7 @@ fi_pingpong also displays aggregated statistics after each test run, and
 can additionally verify data integrity upon receipt.
 .PP
 By default, the datagram (FI_EP_DGRAM) endpoint is used for the test,
-unless otherwise specified via \f[C]\-e\f[].
+unless otherwise specified via \f[C]\-e\f[R].
 .SH HOW TO RUN TESTS
 .PP
 Two copies of the program must be launched: first, one copy must be
@@ -33,112 +33,92 @@ As a client\-server test, each have the following usage model:
 .IP
 .nf
 \f[C]
-server$\ fi_pingpong
-\f[]
+server$ fi_pingpong
+\f[R]
 .fi
 .SS Start the client
 .IP
 .nf
 \f[C]
-client$\ fi_pingpong\ <server\ address>
-\f[]
+client$ fi_pingpong <server address>
+\f[R]
 .fi
 .SH OPTIONS
 .PP
 The server and client must be able to communicate properly for the
 fi_pingpong utility to function.
-If any of the \f[C]\-e\f[], \f[C]\-I\f[], \f[C]\-S\f[], or \f[C]\-p\f[]
-options are used, then they must be specified on the invocation for both
-the server and the client process.
-If the \f[C]\-d\f[] option is specified on the server, then the client
+If any of the \f[C]\-e\f[R], \f[C]\-I\f[R], \f[C]\-S\f[R], or
+\f[C]\-p\f[R] options are used, then they must be specified on the
+invocation for both the server and the client process.
+If the \f[C]\-d\f[R] option is specified on the server, then the client
 will select the appropriate domain if no hint is provided on the client
 side.
-If the \f[C]\-d\f[] option is specified on the client, then it must also
-be specified on the server.
-If both the server and client specify the \f[C]\-d\f[] option and the
+If the \f[C]\-d\f[R] option is specified on the client, then it must
+also be specified on the server.
+If both the server and client specify the \f[C]\-d\f[R] option and the
 given domains cannot communicate, then the application will fail.
 .SS Control Messaging
 .TP
-.B \f[I]\-B <src_port>\f[]
+.B \f[I]\-B <src_port>\f[R]
 The non\-default source port number of the control socket.
 If this is not provided then the server will bind to port 47592 by
 default and the client will allow the port to be selected automatically.
-.RS
-.RE
 .TP
-.B \f[I]\-P <dest_port>\f[]
+.B \f[I]\-P <dest_port>\f[R]
 The non\-default destination port number of the control socket.
 If this is not provided then the client will connect to 47592 by
 default.
 The server ignores this option.
-.RS
-.RE
 .SS Fabric Filtering
 .TP
-.B \f[I]\-p <provider_name>\f[]
+.B \f[I]\-p <provider_name>\f[R]
 The name of the underlying fabric provider (e.g., sockets, psm, usnic,
 etc.).
 If a provider is not specified via the \-p switch, the test will pick
 one from the list of available providers (as returned by fi_getinfo(3)).
-.RS
-.RE
 .TP
-.B \f[I]\-e <endpoint>\f[]
+.B \f[I]\-e <endpoint>\f[R]
 The type of endpoint to be used for data messaging between the two
 processes.
 Supported values are dgram, rdm, and msg.
 For more information on endpoint types, see fi_endpoint(3).
-.RS
-.RE
 .TP
-.B \f[I]\-d <domain>\f[]
+.B \f[I]\-d <domain>\f[R]
 The name of the specific domain to be used.
-.RS
-.RE
 .SS Test Options
 .TP
-.B \f[I]\-I <iter>\f[]
+.B \f[I]\-I <iter>\f[R]
 The number of iterations of the test will run.
-.RS
-.RE
 .TP
-.B \f[I]\-S <msg_size>\f[]
-The specific size of the message in bytes the test will use or
-\[aq]all\[aq] to run all the default sizes.
-.RS
-.RE
+.B \f[I]\-S <msg_size>\f[R]
+The specific size of the message in bytes the test will use or `all' to
+run all the default sizes.
 .TP
-.B \f[I]\-c\f[]
+.B \f[I]\-c\f[R]
 Activate data integrity checks at the receiver (note: this will degrade
 performance).
-.RS
-.RE
 .SS Utility
 .TP
-.B \f[I]\-v\f[]
+.B \f[I]\-v\f[R]
 Activate output debugging (warning: highly verbose)
-.RS
-.RE
 .TP
-.B \f[I]\-h\f[]
+.B \f[I]\-h\f[R]
 Displays help output for the pingpong test.
-.RS
-.RE
 .SH USAGE EXAMPLES
 .SS A simple example
-.SS Server: \f[C]fi_pingpong\ \-p\ <provider_name>\f[]
+.SS Server: \f[C]fi_pingpong \-p <provider_name>\f[R]
 .PP
-\f[C]server$\ fi_pingpong\ \-p\ sockets\f[]
-.SS Client: \f[C]fi_pingpong\ \-p\ <provider_name>\ <server_addr>\f[]
+\f[C]server$ fi_pingpong \-p sockets\f[R]
+.SS Client: \f[C]fi_pingpong \-p <provider_name> <server_addr>\f[R]
 .PP
-\f[C]client$\ fi_pingpong\ \-p\ sockets\ 192.168.0.123\f[]
+\f[C]client$ fi_pingpong \-p sockets 192.168.0.123\f[R]
 .SS An example with various options
 .SS Server:
 .PP
-\f[C]server$\ fi_pingpong\ \-p\ usnic\ \-I\ 1000\ \-S\ 1024\f[]
+\f[C]server$ fi_pingpong \-p usnic \-I 1000 \-S 1024\f[R]
 .SS Client:
 .PP
-\f[C]client$\ fi_pingpong\ \-p\ usnic\ \-I\ 1000\ \-S\ 1024\ 192.168.0.123\f[]
+\f[C]client$ fi_pingpong \-p usnic \-I 1000 \-S 1024 192.168.0.123\f[R]
 .PP
 Specifically, this will run a pingpong test with:
 .IP \[bu] 2
@@ -152,17 +132,17 @@ server node as 192.168.0.123
 .SS A longer test
 .SS Server:
 .PP
-\f[C]server$\ fi_pingpong\ \-p\ usnic\ \-I\ 10000\ \-S\ all\f[]
+\f[C]server$ fi_pingpong \-p usnic \-I 10000 \-S all\f[R]
 .SS Client:
 .PP
-\f[C]client$\ fi_pingpong\ \-p\ usnic\ \-I\ 10000\ \-S\ all\ 192.168.0.123\f[]
+\f[C]client$ fi_pingpong \-p usnic \-I 10000 \-S all 192.168.0.123\f[R]
 .SH DEFAULTS
 .PP
 There is no default provider; if a provider is not specified via the
-\f[C]\-p\f[] switch, the test will pick one from the list of available
+\f[C]\-p\f[R] switch, the test will pick one from the list of available
 providers (as returned by fi_getinfo(3)).
 .PP
-If no endpoint type is specified, \[aq]dgram\[aq] is used.
+If no endpoint type is specified, `dgram' is used.
 .PP
 The default tested sizes are: 64, 256, 1024, 4096, 65536, and 1048576.
 The test will only test sizes that are within the selected endpoints
@@ -172,28 +152,28 @@ maximum message size boundary.
 Each test generates data messages which are accounted for.
 Specifically, the displayed statistics at the end are :
 .IP \[bu] 2
-\f[I]bytes\f[] : number of bytes per message sent
+\f[I]bytes\f[R] : number of bytes per message sent
 .IP \[bu] 2
-\f[I]#sent\f[] : number of messages (ping) sent from the client to the
+\f[I]#sent\f[R] : number of messages (ping) sent from the client to the
 server
 .IP \[bu] 2
-\f[I]#ack\f[] : number of replies (pong) of the server received by the
+\f[I]#ack\f[R] : number of replies (pong) of the server received by the
 client
 .IP \[bu] 2
-\f[I]total\f[] : amount of memory exchanged between the processes
+\f[I]total\f[R] : amount of memory exchanged between the processes
 .IP \[bu] 2
-\f[I]time\f[] : duration of this single test
+\f[I]time\f[R] : duration of this single test
 .IP \[bu] 2
-\f[I]MB/sec\f[] : throughput computed from \f[I]total\f[] and
-\f[I]time\f[]
+\f[I]MB/sec\f[R] : throughput computed from \f[I]total\f[R] and
+\f[I]time\f[R]
 .IP \[bu] 2
-\f[I]usec/xfer\f[] : average time for transferring a message outbound
+\f[I]usec/xfer\f[R] : average time for transferring a message outbound
 (ping or pong) in microseconds
 .IP \[bu] 2
-\f[I]Mxfers/sec\f[] : average amount of transfers of message outbound
+\f[I]Mxfers/sec\f[R] : average amount of transfers of message outbound
 per second
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3) \f[C]fabric\f[](7),
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3) \f[C]fabric\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man1/fi_strerror.1 b/man/man1/fi_strerror.1
index f2f8676..0a9be34 100644
--- a/man/man1/fi_strerror.1
+++ b/man/man1/fi_strerror.1
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_strerror" "1" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_strerror" "1" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,20 +9,21 @@ fi_strerror \- display libfabric error strings
 .IP
 .nf
 \f[C]
-fi_strerror\ FI_ERROR_CODE
-\f[]
+fi_strerror FI_ERROR_CODE
+\f[R]
 .fi
 .SH DESCRIPTION
 .PP
-Display the error string for the given numeric \f[C]FI_ERROR_CODE\f[].
-\f[C]FI_ERROR_CODE\f[] may be a hexadecimal, octal, or decimal constant.
-Although the \f[C]fi_strerror\f[](3) library function only accepts
+Display the error string for the given numeric \f[C]FI_ERROR_CODE\f[R].
+\f[C]FI_ERROR_CODE\f[R] may be a hexadecimal, octal, or decimal
+constant.
+Although the \f[C]fi_strerror\f[R](3) library function only accepts
 positive error values, for convenience this utility accepts both
 positive and negative error values.
 .PP
 This is primarily a convenience tool for developers.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7) \f[C]fi_errno\f[](3)
+\f[C]fabric\f[R](7) \f[C]fi_errno\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_atomic.3 b/man/man3/fi_atomic.3
index 3570a18..50aaaed 100644
--- a/man/man3/fi_atomic.3
+++ b/man/man3/fi_atomic.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_atomic" "3" "2020\-10\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_atomic" "3" "2021\-06\-10" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,20 +8,14 @@ fi_atomic \- Remote atomic functions
 .TP
 .B fi_atomic / fi_atomicv / fi_atomicmsg / fi_inject_atomic
 Initiates an atomic operation to remote memory
-.RS
-.RE
 .TP
 .B fi_fetch_atomic / fi_fetch_atomicv / fi_fetch_atomicmsg
 Initiates an atomic operation to remote memory, retrieving the initial
 value.
-.RS
-.RE
 .TP
 .B fi_compare_atomic / fi_compare_atomicv / fi_compare_atomicmsg
 Initiates an atomic compare\-operation to remote memory, retrieving the
 initial value.
-.RS
-.RE
 .PP
 fi_atomicvalid / fi_fetch_atomicvalid / fi_compare_atomicvalid /
 fi_query_atomic : Indicates if a provider supports a specific atomic
@@ -30,165 +24,134 @@ operation
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_atomic.h>
+#include <rdma/fi_atomic.h>
 
-ssize_t\ fi_atomic(struct\ fid_ep\ *ep,\ const\ void\ *buf,
-\ \ \ \ size_t\ count,\ void\ *desc,\ fi_addr_t\ dest_addr,
-\ \ \ \ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,\ void\ *context);
+ssize_t fi_atomic(struct fid_ep *ep, const void *buf,
+    size_t count, void *desc, fi_addr_t dest_addr,
+    uint64_t addr, uint64_t key,
+    enum fi_datatype datatype, enum fi_op op, void *context);
 
-ssize_t\ fi_atomicv(struct\ fid_ep\ *ep,\ const\ struct\ fi_ioc\ *iov,
-\ \ \ \ void\ **desc,\ size_t\ count,\ fi_addr_t\ dest_addr,
-\ \ \ \ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,\ void\ *context);
+ssize_t fi_atomicv(struct fid_ep *ep, const struct fi_ioc *iov,
+    void **desc, size_t count, fi_addr_t dest_addr,
+    uint64_t addr, uint64_t key,
+    enum fi_datatype datatype, enum fi_op op, void *context);
 
-ssize_t\ fi_atomicmsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg_atomic\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_atomicmsg(struct fid_ep *ep, const struct fi_msg_atomic *msg,
+    uint64_t flags);
 
-ssize_t\ fi_inject_atomic(struct\ fid_ep\ *ep,\ const\ void\ *buf,
-\ \ \ \ size_t\ count,\ fi_addr_t\ dest_addr,
-\ \ \ \ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op);
+ssize_t fi_inject_atomic(struct fid_ep *ep, const void *buf,
+    size_t count, fi_addr_t dest_addr,
+    uint64_t addr, uint64_t key,
+    enum fi_datatype datatype, enum fi_op op);
 
-ssize_t\ fi_fetch_atomic(struct\ fid_ep\ *ep,\ const\ void\ *buf,
-\ \ \ \ size_t\ count,\ void\ *desc,\ void\ *result,\ void\ *result_desc,
-\ \ \ \ fi_addr_t\ dest_addr,\ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,\ void\ *context);
+ssize_t fi_fetch_atomic(struct fid_ep *ep, const void *buf,
+    size_t count, void *desc, void *result, void *result_desc,
+    fi_addr_t dest_addr, uint64_t addr, uint64_t key,
+    enum fi_datatype datatype, enum fi_op op, void *context);
 
-ssize_t\ fi_fetch_atomicv(struct\ fid_ep\ *ep,\ const\ struct\ fi_ioc\ *iov,
-\ \ \ \ void\ **desc,\ size_t\ count,\ struct\ fi_ioc\ *resultv,
-\ \ \ \ void\ **result_desc,\ size_t\ result_count,\ fi_addr_t\ dest_addr,
-\ \ \ \ uint64_t\ addr,\ uint64_t\ key,\ enum\ fi_datatype\ datatype,
-\ \ \ \ enum\ fi_op\ op,\ void\ *context);
+ssize_t fi_fetch_atomicv(struct fid_ep *ep, const struct fi_ioc *iov,
+    void **desc, size_t count, struct fi_ioc *resultv,
+    void **result_desc, size_t result_count, fi_addr_t dest_addr,
+    uint64_t addr, uint64_t key, enum fi_datatype datatype,
+    enum fi_op op, void *context);
 
-ssize_t\ fi_fetch_atomicmsg(struct\ fid_ep\ *ep,
-\ \ \ \ const\ struct\ fi_msg_atomic\ *msg,\ struct\ fi_ioc\ *resultv,
-\ \ \ \ void\ **result_desc,\ size_t\ result_count,\ uint64_t\ flags);
+ssize_t fi_fetch_atomicmsg(struct fid_ep *ep,
+    const struct fi_msg_atomic *msg, struct fi_ioc *resultv,
+    void **result_desc, size_t result_count, uint64_t flags);
 
-ssize_t\ fi_compare_atomic(struct\ fid_ep\ *ep,\ const\ void\ *buf,
-\ \ \ \ size_t\ count,\ void\ *desc,\ const\ void\ *compare,
-\ \ \ \ void\ *compare_desc,\ void\ *result,\ void\ *result_desc,
-\ \ \ \ fi_addr_t\ dest_addr,\ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,\ void\ *context);
+ssize_t fi_compare_atomic(struct fid_ep *ep, const void *buf,
+    size_t count, void *desc, const void *compare,
+    void *compare_desc, void *result, void *result_desc,
+    fi_addr_t dest_addr, uint64_t addr, uint64_t key,
+    enum fi_datatype datatype, enum fi_op op, void *context);
 
-size_t\ fi_compare_atomicv(struct\ fid_ep\ *ep,\ const\ struct\ fi_ioc\ *iov,
-\ \ \ \ \ \ \ void\ **desc,\ size_t\ count,\ const\ struct\ fi_ioc\ *comparev,
-\ \ \ \ \ \ \ void\ **compare_desc,\ size_t\ compare_count,\ struct\ fi_ioc\ *resultv,
-\ \ \ \ \ \ \ void\ **result_desc,\ size_t\ result_count,\ fi_addr_t\ dest_addr,
-\ \ \ \ \ \ \ uint64_t\ addr,\ uint64_t\ key,\ enum\ fi_datatype\ datatype,
-\ \ \ \ \ \ \ enum\ fi_op\ op,\ void\ *context);
+size_t fi_compare_atomicv(struct fid_ep *ep, const struct fi_ioc *iov,
+       void **desc, size_t count, const struct fi_ioc *comparev,
+       void **compare_desc, size_t compare_count, struct fi_ioc *resultv,
+       void **result_desc, size_t result_count, fi_addr_t dest_addr,
+       uint64_t addr, uint64_t key, enum fi_datatype datatype,
+       enum fi_op op, void *context);
 
-ssize_t\ fi_compare_atomicmsg(struct\ fid_ep\ *ep,
-\ \ \ \ const\ struct\ fi_msg_atomic\ *msg,\ const\ struct\ fi_ioc\ *comparev,
-\ \ \ \ void\ **compare_desc,\ size_t\ compare_count,
-\ \ \ \ struct\ fi_ioc\ *resultv,\ void\ **result_desc,\ size_t\ result_count,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_compare_atomicmsg(struct fid_ep *ep,
+    const struct fi_msg_atomic *msg, const struct fi_ioc *comparev,
+    void **compare_desc, size_t compare_count,
+    struct fi_ioc *resultv, void **result_desc, size_t result_count,
+    uint64_t flags);
 
-int\ fi_atomicvalid(struct\ fid_ep\ *ep,\ enum\ fi_datatype\ datatype,
-\ \ \ \ enum\ fi_op\ op,\ size_t\ *count);
+int fi_atomicvalid(struct fid_ep *ep, enum fi_datatype datatype,
+    enum fi_op op, size_t *count);
 
-int\ fi_fetch_atomicvalid(struct\ fid_ep\ *ep,\ enum\ fi_datatype\ datatype,
-\ \ \ \ enum\ fi_op\ op,\ size_t\ *count);
+int fi_fetch_atomicvalid(struct fid_ep *ep, enum fi_datatype datatype,
+    enum fi_op op, size_t *count);
 
-int\ fi_compare_atomicvalid(struct\ fid_ep\ *ep,\ enum\ fi_datatype\ datatype,
-\ \ \ \ enum\ fi_op\ op,\ size_t\ *count);
+int fi_compare_atomicvalid(struct fid_ep *ep, enum fi_datatype datatype,
+    enum fi_op op, size_t *count);
 
-int\ fi_query_atomic(struct\ fid_domain\ *domain,
-\ \ \ \ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
-\ \ \ \ struct\ fi_atomic_attr\ *attr,\ uint64_t\ flags);
-\f[]
+int fi_query_atomic(struct fid_domain *domain,
+    enum fi_datatype datatype, enum fi_op op,
+    struct fi_atomic_attr *attr, uint64_t flags);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]ep\f[]
+.B \f[I]ep\f[R]
 Fabric endpoint on which to initiate atomic operation.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Local data buffer that specifies first operand of atomic operation
-.RS
-.RE
 .TP
-.B \f[I]iov / comparev / resultv\f[]
+.B \f[I]iov / comparev / resultv\f[R]
 Vectored data buffer(s).
-.RS
-.RE
 .TP
-.B \f[I]count / compare_count / result_count\f[]
+.B \f[I]count / compare_count / result_count\f[R]
 Count of vectored data entries.
 The number of elements referenced, where each element is the indicated
 datatype.
-.RS
-.RE
 .TP
-.B \f[I]addr\f[]
+.B \f[I]addr\f[R]
 Address of remote memory to access.
-.RS
-.RE
 .TP
-.B \f[I]key\f[]
+.B \f[I]key\f[R]
 Protection key associated with the remote memory.
-.RS
-.RE
 .TP
-.B \f[I]datatype\f[]
+.B \f[I]datatype\f[R]
 Datatype associated with atomic operands
-.RS
-.RE
 .TP
-.B \f[I]op\f[]
+.B \f[I]op\f[R]
 Atomic operation to perform
-.RS
-.RE
 .TP
-.B \f[I]compare\f[]
+.B \f[I]compare\f[R]
 Local compare buffer, containing comparison data.
-.RS
-.RE
 .TP
-.B \f[I]result\f[]
+.B \f[I]result\f[R]
 Local data buffer to store initial value of remote buffer
-.RS
-.RE
 .TP
-.B \f[I]desc / compare_desc / result_desc\f[]
+.B \f[I]desc / compare_desc / result_desc\f[R]
 Data descriptor associated with the local data buffer, local compare
 buffer, and local result buffer, respectively.
-See \f[C]fi_mr\f[](3).
-.RS
-.RE
+See \f[C]fi_mr\f[R](3).
 .TP
-.B \f[I]dest_addr\f[]
+.B \f[I]dest_addr\f[R]
 Destination address for connectionless atomic operations.
 Ignored for connected endpoints.
-.RS
-.RE
 .TP
-.B \f[I]msg\f[]
+.B \f[I]msg\f[R]
 Message descriptor for atomic operations
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply for the atomic operation
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified pointer to associate with the operation.
 This parameter is ignored if the operation will not generate a
 successful completion, unless an op flag specifies the context parameter
 be used for required input.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Atomic transfers are used to read and update data located in remote
 memory regions in an atomic fashion.
 Conceptually, they are similar to local atomic operations of a similar
-nature (e.g.
-atomic increment, compare and swap, etc.).
+nature (e.g.\ atomic increment, compare and swap, etc.).
 Updates to remote data involve one of several operations on the data,
 and act on specific types of data, as listed below.
 As such, atomic transfers have knowledge of the format of the data being
@@ -203,71 +166,53 @@ types.
 A given atomic function may support any datatype, subject to provider
 implementation constraints.
 .TP
-.B \f[I]FI_INT8\f[]
+.B \f[I]FI_INT8\f[R]
 Signed 8\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_UINT8\f[]
+.B \f[I]FI_UINT8\f[R]
 Unsigned 8\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_INT16\f[]
+.B \f[I]FI_INT16\f[R]
 Signed 16\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_UINT16\f[]
+.B \f[I]FI_UINT16\f[R]
 Unsigned 16\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_INT32\f[]
+.B \f[I]FI_INT32\f[R]
 Signed 32\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_UINT32\f[]
+.B \f[I]FI_UINT32\f[R]
 Unsigned 32\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_INT64\f[]
+.B \f[I]FI_INT64\f[R]
 Signed 64\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_UINT64\f[]
+.B \f[I]FI_UINT64\f[R]
 Unsigned 64\-bit integer.
-.RS
-.RE
 .TP
-.B \f[I]FI_FLOAT\f[]
+.B \f[I]FI_INT128\f[R]
+Signed 128\-bit integer.
+.TP
+.B \f[I]FI_UINT128\f[R]
+Unsigned 128\-bit integer.
+.TP
+.B \f[I]FI_FLOAT\f[R]
 A single\-precision floating point value (IEEE 754).
-.RS
-.RE
 .TP
-.B \f[I]FI_DOUBLE\f[]
+.B \f[I]FI_DOUBLE\f[R]
 A double\-precision floating point value (IEEE 754).
-.RS
-.RE
 .TP
-.B \f[I]FI_FLOAT_COMPLEX\f[]
+.B \f[I]FI_FLOAT_COMPLEX\f[R]
 An ordered pair of single\-precision floating point values (IEEE 754),
 with the first value representing the real portion of a complex number
 and the second representing the imaginary portion.
-.RS
-.RE
 .TP
-.B \f[I]FI_DOUBLE_COMPLEX\f[]
+.B \f[I]FI_DOUBLE_COMPLEX\f[R]
 An ordered pair of double\-precision floating point values (IEEE 754),
 with the first value representing the real portion of a complex number
 and the second representing the imaginary portion.
-.RS
-.RE
 .TP
-.B \f[I]FI_LONG_DOUBLE\f[]
+.B \f[I]FI_LONG_DOUBLE\f[R]
 A double\-extended precision floating point value (IEEE 754).
 Note that the size of a long double and number of bits used for
 precision is compiler, platform, and/or provider specific.
@@ -276,15 +221,11 @@ using a long double format that is compatible with their application,
 and that format is supported by the provider.
 The mechanism used for this validation is currently beyond the scope of
 the libfabric API.
-.RS
-.RE
 .TP
-.B \f[I]FI_LONG_DOUBLE_COMPLEX\f[]
+.B \f[I]FI_LONG_DOUBLE_COMPLEX\f[R]
 An ordered pair of double\-extended precision floating point values
 (IEEE 754), with the first value representing the real portion of a
 complex number and the second representing the imaginary portion.
-.RS
-.RE
 .SS Atomic Operations
 .PP
 The following atomic operations are defined.
@@ -294,227 +235,189 @@ It may also carry source data to replace the target value in compare and
 swap operations.
 A conceptual description of each operation is provided.
 .TP
-.B \f[I]FI_MIN\f[]
+.B \f[I]FI_MIN\f[R]
 Minimum
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (buf[i]\ <\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (buf[i] < addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_MAX\f[]
+.B \f[I]FI_MAX\f[R]
 Maximum
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (buf[i]\ >\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (buf[i] > addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_SUM\f[]
+.B \f[I]FI_SUM\f[R]
 Sum
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ addr[i]\ +\ buf[i]
-\f[]
+addr[i] = addr[i] + buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_PROD\f[]
+.B \f[I]FI_PROD\f[R]
 Product
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ addr[i]\ *\ buf[i]
-\f[]
+addr[i] = addr[i] * buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_LOR\f[]
+.B \f[I]FI_LOR\f[R]
 Logical OR
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ (addr[i]\ ||\ buf[i])
-\f[]
+addr[i] = (addr[i] || buf[i])
+\f[R]
 .fi
 .TP
-.B \f[I]FI_LAND\f[]
+.B \f[I]FI_LAND\f[R]
 Logical AND
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ (addr[i]\ &&\ buf[i])
-\f[]
+addr[i] = (addr[i] && buf[i])
+\f[R]
 .fi
 .TP
-.B \f[I]FI_BOR\f[]
+.B \f[I]FI_BOR\f[R]
 Bitwise OR
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ addr[i]\ |\ buf[i]
-\f[]
+addr[i] = addr[i] | buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_BAND\f[]
+.B \f[I]FI_BAND\f[R]
 Bitwise AND
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ addr[i]\ &\ buf[i]
-\f[]
+addr[i] = addr[i] & buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_LXOR\f[]
+.B \f[I]FI_LXOR\f[R]
 Logical exclusive\-OR (XOR)
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ ((addr[i]\ &&\ !buf[i])\ ||\ (!addr[i]\ &&\ buf[i]))
-\f[]
+addr[i] = ((addr[i] && !buf[i]) || (!addr[i] && buf[i]))
+\f[R]
 .fi
 .TP
-.B \f[I]FI_BXOR\f[]
+.B \f[I]FI_BXOR\f[R]
 Bitwise exclusive\-OR (XOR)
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ addr[i]\ ^\ buf[i]
-\f[]
+addr[i] = addr[i] \[ha] buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_ATOMIC_READ\f[]
+.B \f[I]FI_ATOMIC_READ\f[R]
 Read data atomically
-.RS
-.RE
 .IP
 .nf
 \f[C]
-result[i]\ =\ addr[i]
-\f[]
+result[i] = addr[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_ATOMIC_WRITE\f[]
+.B \f[I]FI_ATOMIC_WRITE\f[R]
 Write data atomically
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ buf[i]
-\f[]
+addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_CSWAP\f[]
+.B \f[I]FI_CSWAP\f[R]
 Compare values and if equal swap with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (compare[i]\ ==\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (compare[i] == addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_CSWAP_NE\f[]
+.B \f[I]FI_CSWAP_NE\f[R]
 Compare values and if not equal swap with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (compare[i]\ !=\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (compare[i] != addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_CSWAP_LE\f[]
+.B \f[I]FI_CSWAP_LE\f[R]
 Compare values and if less than or equal swap with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (compare[i]\ <=\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (compare[i] <= addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_CSWAP_LT\f[]
+.B \f[I]FI_CSWAP_LT\f[R]
 Compare values and if less than swap with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (compare[i]\ <\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (compare[i] < addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_CSWAP_GE\f[]
+.B \f[I]FI_CSWAP_GE\f[R]
 Compare values and if greater than or equal swap with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (compare[i]\ >=\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (compare[i] >= addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_CSWAP_GT\f[]
+.B \f[I]FI_CSWAP_GT\f[R]
 Compare values and if greater than swap with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-if\ (compare[i]\ >\ addr[i])
-\ \ \ \ addr[i]\ =\ buf[i]
-\f[]
+if (compare[i] > addr[i])
+    addr[i] = buf[i]
+\f[R]
 .fi
 .TP
-.B \f[I]FI_MSWAP\f[]
+.B \f[I]FI_MSWAP\f[R]
 Swap masked bits with data
-.RS
-.RE
 .IP
 .nf
 \f[C]
-addr[i]\ =\ (buf[i]\ &\ compare[i])\ |\ (addr[i]\ &\ ~compare[i])
-\f[]
+addr[i] = (buf[i] & compare[i]) | (addr[i] & \[ti]compare[i])
+\f[R]
 .fi
 .SS Base Atomic Functions
 .PP
-The base atomic functions \-\- fi_atomic, fi_atomicv, fi_atomicmsg \-\-
-are used to transmit data to a remote node, where the specified atomic
-operation is performed against the target data.
+The base atomic functions \[en] fi_atomic, fi_atomicv, fi_atomicmsg
+\[en] are used to transmit data to a remote node, where the specified
+atomic operation is performed against the target data.
 The result of a base atomic function is stored at the remote memory
 region.
 The main difference between atomic functions are the number and type of
@@ -555,30 +458,30 @@ The fi_atomicmsg function takes a struct fi_msg_atomic as input.
 .IP
 .nf
 \f[C]
-struct\ fi_msg_atomic\ {
-\ \ \ \ const\ struct\ fi_ioc\ *msg_iov;\ /*\ local\ scatter\-gather\ array\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **desc;\ \ \ /*\ local\ access\ descriptors\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ iov_count;/*\ #\ elements\ in\ ioc\ */
-\ \ \ \ const\ void\ \ \ \ \ \ \ \ \ \ *addr;\ \ \ \ /*\ optional\ endpoint\ address\ */
-\ \ \ \ const\ struct\ fi_rma_ioc\ *rma_iov;\ /*\ remote\ SGL\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ rma_iov_count;/*\ #\ elements\ in\ remote\ SGL\ */
-\ \ \ \ enum\ fi_datatype\ \ \ \ datatype;\ /*\ operand\ datatype\ */
-\ \ \ \ enum\ fi_op\ \ \ \ \ \ \ \ \ \ op;\ \ \ \ \ \ \ /*\ atomic\ operation\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *context;\ /*\ user\-defined\ context\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ data;\ \ \ \ \ /*\ optional\ data\ */
+struct fi_msg_atomic {
+    const struct fi_ioc *msg_iov; /* local scatter\-gather array */
+    void                **desc;   /* local access descriptors */
+    size_t              iov_count;/* # elements in ioc */
+    const void          *addr;    /* optional endpoint address */
+    const struct fi_rma_ioc *rma_iov; /* remote SGL */
+    size_t              rma_iov_count;/* # elements in remote SGL */
+    enum fi_datatype    datatype; /* operand datatype */
+    enum fi_op          op;       /* atomic operation */
+    void                *context; /* user\-defined context */
+    uint64_t            data;     /* optional data */
 };
 
-struct\ fi_ioc\ {
-\ \ \ \ void\ \ \ \ \ \ \ \ *addr;\ \ \ \ /*\ local\ address\ */
-\ \ \ \ size_t\ \ \ \ \ \ count;\ \ \ \ /*\ #\ target\ operands\ */
+struct fi_ioc {
+    void        *addr;    /* local address */
+    size_t      count;    /* # target operands */
 };
 
-struct\ fi_rma_ioc\ {
-\ \ \ \ uint64_t\ \ \ \ addr;\ \ \ \ \ /*\ target\ address\ */
-\ \ \ \ size_t\ \ \ \ \ \ count;\ \ \ \ /*\ #\ target\ operands\ */
-\ \ \ \ uint64_t\ \ \ \ key;\ \ \ \ \ \ /*\ access\ key\ */
+struct fi_rma_ioc {
+    uint64_t    addr;     /* target address */
+    size_t      count;    /* # target operands */
+    uint64_t    key;      /* access key */
 };
-\f[]
+\f[R]
 .fi
 .PP
 The following list of atomic operations are usable with base atomic
@@ -586,8 +489,8 @@ operations: FI_MIN, FI_MAX, FI_SUM, FI_PROD, FI_LOR, FI_LAND, FI_BOR,
 FI_BAND, FI_LXOR, FI_BXOR, and FI_ATOMIC_WRITE.
 .SS Fetch\-Atomic Functions
 .PP
-The fetch atomic functions \-\- fi_fetch_atomic, fi_fetch_atomicv, and
-fi_fetch atomicmsg \-\- behave similar to the equivalent base atomic
+The fetch atomic functions \[en] fi_fetch_atomic, fi_fetch_atomicv, and
+fi_fetch atomicmsg \[en] behave similar to the equivalent base atomic
 function.
 The difference between the fetch and base atomic calls are the fetch
 atomic routines return the initial value that was stored at the target
@@ -605,16 +508,16 @@ fi_fetch_atomic buf parameter) is ignored and may be NULL.
 The results are written into the result buffer.
 .SS Compare\-Atomic Functions
 .PP
-The compare atomic functions \-\- fi_compare_atomic, fi_compare_atomicv,
-and fi_compare atomicmsg \-\- are used for operations that require
-comparing the target data against a value before performing a swap
-operation.
+The compare atomic functions \[en] fi_compare_atomic,
+fi_compare_atomicv, and fi_compare atomicmsg \[en] are used for
+operations that require comparing the target data against a value before
+performing a swap operation.
 The compare atomic functions support: FI_CSWAP, FI_CSWAP_NE,
 FI_CSWAP_LE, FI_CSWAP_LT, FI_CSWAP_GE, FI_CSWAP_GT, and FI_MSWAP.
 .SS Atomic Valid Functions
 .PP
-The atomic valid functions \-\- fi_atomicvalid, fi_fetch_atomicvalid,
-and fi_compare_atomicvalid \-\-indicate which operations the local
+The atomic valid functions \[en] fi_atomicvalid, fi_fetch_atomicvalid,
+and fi_compare_atomicvalid \[en]indicate which operations the local
 provider supports.
 Needed operations not supported by the provider must be emulated by the
 application.
@@ -657,11 +560,11 @@ The output of fi_query_atomic is struct fi_atomic_attr:
 .IP
 .nf
 \f[C]
-struct\ fi_atomic_attr\ {
-\ \ \ \ size_t\ count;
-\ \ \ \ size_t\ size;
+struct fi_atomic_attr {
+    size_t count;
+    size_t size;
 };
-\f[]
+\f[R]
 .fi
 .PP
 The count attribute field is as defined for the atomic valid calls.
@@ -693,7 +596,7 @@ prior to the completion notification.
 After processing a completion for the atomic, if the initiator submits a
 transfer between the same endpoints that generates a completion at the
 target, the results will be available prior to the subsequent
-transfer\[aq]s event.
+transfer\[cq]s event.
 Or, if a fenced data transfer from the initiator follows the atomic
 request, the results will be available prior to a completion at the
 target for the fenced transfer.
@@ -703,14 +606,14 @@ guaranteed only when performed by a single actor for a given window of
 time.
 An actor is defined as a single libfabric domain (identified by the
 domain name, and not an open instance of that domain), a coherent CPU
-complex, or other device (e.g.
-GPU) capable of performing atomic operations on the target memory.
+complex, or other device (e.g.\ GPU) capable of performing atomic
+operations on the target memory.
 The results of atomic operations performed by multiple actors
 simultaneously are undefined.
 For example, issuing CPU based atomic operations to a target region
 concurrently being updated by NIC based atomics may leave the
-region\[aq]s data in an unknown state.
-The results of a first actor\[aq]s atomic operations must be visible to
+region\[cq]s data in an unknown state.
+The results of a first actor\[cq]s atomic operations must be visible to
 a second actor prior to the second actor issuing its own atomics.
 .SH FLAGS
 .PP
@@ -722,38 +625,32 @@ previously configured with the endpoint, except where noted (see
 fi_control).
 The following list of flags are usable with atomic message calls.
 .TP
-.B \f[I]FI_COMPLETION\f[]
+.B \f[I]FI_COMPLETION\f[R]
 Indicates that a completion entry should be generated for the specified
 operation.
 The endpoint must be bound to a completion queue with
 FI_SELECTIVE_COMPLETION that corresponds to the specified operation, or
 this flag is ignored.
-.RS
-.RE
 .TP
-.B \f[I]FI_MORE\f[]
+.B \f[I]FI_MORE\f[R]
 Indicates that the user has additional requests that will immediately be
 posted after the current call returns.
 Use of this flag may improve performance by enabling the provider to
 optimize its access to the fabric hardware.
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT\f[]
+.B \f[I]FI_INJECT\f[R]
 Indicates that the control of constant data buffers should be returned
 to the user immediately after the call returns, even if the operation is
 handled asynchronously.
 This may require that the underlying provider implementation copy the
 data into a local buffer and transfer out of that buffer.
 Constant data buffers refers to any data buffer or iovec used by the
-atomic APIs that are marked as \[aq]const\[aq].
+atomic APIs that are marked as `const'.
 Non\-constant or output buffers are unaffected by this flag and may be
 accessed by the provider at anytime until the operation has completed.
 This flag can only be used with messages smaller than inject_size.
-.RS
-.RE
 .TP
-.B \f[I]FI_FENCE\f[]
+.B \f[I]FI_FENCE\f[R]
 Applies to transmits.
 Indicates that the requested operation, also known as the fenced
 operation, and any operation posted after the fenced operation will be
@@ -761,43 +658,34 @@ deferred until all previous operations targeting the same peer endpoint
 have completed.
 Operations posted after the fencing will see and/or replace the results
 of any operations initiated prior to the fenced operation.
-.RS
-.RE
 .PP
 The ordering of operations starting at the posting of the fenced
 operation (inclusive) to the posting of a subsequent fenced operation
-(exclusive) is controlled by the endpoint\[aq]s ordering semantics.
+(exclusive) is controlled by the endpoint\[cq]s ordering semantics.
 .TP
-.B \f[I]FI_TAGGED\f[]
+.B \f[I]FI_TAGGED\f[R]
 Specifies that the target of the atomic operation is a tagged receive
 buffer instead of an RMA buffer.
 When a tagged buffer is the target memory region, the addr parameter is
 used as a 0\-based byte offset into the tagged buffer, with the key
 parameter specifying the tag.
-.RS
-.RE
 .SH RETURN VALUE
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .TP
-.B \f[I]\-FI_EAGAIN\f[]
-See \f[C]fi_msg\f[](3) for a detailed description of handling FI_EAGAIN.
-.RS
-.RE
+.B \f[I]\-FI_EAGAIN\f[R]
+See \f[C]fi_msg\f[R](3) for a detailed description of handling
+FI_EAGAIN.
 .TP
-.B \f[I]\-FI_EOPNOTSUPP\f[]
+.B \f[I]\-FI_EOPNOTSUPP\f[R]
 The requested atomic operation is not supported on this endpoint.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_EMSGSIZE\f[]
+.B \f[I]\-FI_EMSGSIZE\f[R]
 The number of atomic operations in a single request exceeds that
 supported by the underlying provider.
-.RS
-.RE
 .SH NOTES
 .PP
 Atomic operations operate on an array of values of a specific data type.
@@ -810,14 +698,14 @@ bytes to an aligned memory location.
 .IP
 .nf
 \f[C]
-fi_atomic(ep,\ buf,\ count,\ NULL,\ dest_addr,\ addr,\ key,
-\ \ \ \ \ \ FI_UINT64,\ FI_ATOMIC_WRITE,\ context)
+fi_atomic(ep, buf, count, NULL, dest_addr, addr, key,
+      FI_UINT64, FI_ATOMIC_WRITE, context)
 {
-\ \ \ \ for\ (i\ =\ 1;\ i\ <\ count;\ i\ ++)
-\ \ \ \ \ \ \ \ ATOMIC_WRITE_U64(((uint64_t\ *)\ addr)[i],
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ((uint64_t\ *)\ buf)[i]);
+    for (i = 1; i < count; i ++)
+        ATOMIC_WRITE_U64(((uint64_t *) addr)[i],
+                 ((uint64_t *) buf)[i]);
 }
-\f[]
+\f[R]
 .fi
 .PP
 The number of array elements to operate on is specified through a count
@@ -833,11 +721,11 @@ assigned to the transmitting and receiving endpoints.
 Both message and data ordering are required if the results of two atomic
 operations to the same memory buffers are to reflect the second
 operation acting on the results of the first.
-See \f[C]fi_endpoint\f[](3) for further details and message size
+See \f[C]fi_endpoint\f[R](3) for further details and message size
 restrictions.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_cq\f[](3), \f[C]fi_rma\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_cq\f[R](3), \f[C]fi_rma\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_av.3 b/man/man3/fi_av.3
index 34cfbe9..5525e40 100644
--- a/man/man3/fi_av.3
+++ b/man/man3/fi_av.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_av" "3" "2019\-07\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_av" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,120 +8,90 @@ fi_av \- Address vector operations
 .TP
 .B fi_av_open / fi_close
 Open or close an address vector
-.RS
-.RE
 .TP
 .B fi_av_bind
 Associate an address vector with an event queue.
-.RS
-.RE
 .TP
 .B fi_av_insert / fi_av_insertsvc / fi_av_remove
 Insert/remove an address into/from the address vector.
-.RS
-.RE
 .TP
 .B fi_av_lookup
 Retrieve an address stored in the address vector.
-.RS
-.RE
 .TP
 .B fi_av_straddr
 Convert an address into a printable string.
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_av_open(struct\ fid_domain\ *domain,\ struct\ fi_av_attr\ *attr,
-\ \ \ \ struct\ fid_av\ **av,\ void\ *context);
+int fi_av_open(struct fid_domain *domain, struct fi_av_attr *attr,
+    struct fid_av **av, void *context);
 
-int\ fi_close(struct\ fid\ *av);
+int fi_close(struct fid *av);
 
-int\ fi_av_bind(struct\ fid_av\ *av,\ struct\ fid\ *eq,\ uint64_t\ flags);
+int fi_av_bind(struct fid_av *av, struct fid *eq, uint64_t flags);
 
-int\ fi_av_insert(struct\ fid_av\ *av,\ void\ *addr,\ size_t\ count,
-\ \ \ \ fi_addr_t\ *fi_addr,\ uint64_t\ flags,\ void\ *context);
+int fi_av_insert(struct fid_av *av, void *addr, size_t count,
+    fi_addr_t *fi_addr, uint64_t flags, void *context);
 
-int\ fi_av_insertsvc(struct\ fid_av\ *av,\ const\ char\ *node,
-\ \ \ \ const\ char\ *service,\ fi_addr_t\ *fi_addr,\ uint64_t\ flags,
-\ \ \ \ void\ *context);
+int fi_av_insertsvc(struct fid_av *av, const char *node,
+    const char *service, fi_addr_t *fi_addr, uint64_t flags,
+    void *context);
 
-int\ fi_av_insertsym(struct\ fid_av\ *av,\ const\ char\ *node,
-\ \ \ \ size_t\ nodecnt,\ const\ char\ *service,\ size_t\ svccnt,
-\ \ \ \ fi_addr_t\ *fi_addr,\ uint64_t\ flags,\ void\ *context);
+int fi_av_insertsym(struct fid_av *av, const char *node,
+    size_t nodecnt, const char *service, size_t svccnt,
+    fi_addr_t *fi_addr, uint64_t flags, void *context);
 
-int\ fi_av_remove(struct\ fid_av\ *av,\ fi_addr_t\ *fi_addr,\ size_t\ count,
-\ \ \ \ uint64_t\ flags);
+int fi_av_remove(struct fid_av *av, fi_addr_t *fi_addr, size_t count,
+    uint64_t flags);
 
-int\ fi_av_lookup(struct\ fid_av\ *av,\ fi_addr_t\ fi_addr,
-\ \ \ \ void\ *addr,\ size_t\ *addrlen);
+int fi_av_lookup(struct fid_av *av, fi_addr_t fi_addr,
+    void *addr, size_t *addrlen);
 
-fi_addr_t\ fi_rx_addr(fi_addr_t\ fi_addr,\ int\ rx_index,
-\ \ \ \ \ \ int\ rx_ctx_bits);
+fi_addr_t fi_rx_addr(fi_addr_t fi_addr, int rx_index,
+      int rx_ctx_bits);
 
-const\ char\ *\ fi_av_straddr(struct\ fid_av\ *av,\ const\ void\ *addr,
-\ \ \ \ \ \ char\ *buf,\ size_t\ *len);
-\f[]
+const char * fi_av_straddr(struct fid_av *av, const void *addr,
+      char *buf, size_t *len);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]domain\f[]
+.B \f[I]domain\f[R]
 Resource domain
-.RS
-.RE
 .TP
-.B \f[I]av\f[]
+.B \f[I]av\f[R]
 Address vector
-.RS
-.RE
 .TP
-.B \f[I]eq\f[]
+.B \f[I]eq\f[R]
 Event queue
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Address vector attributes
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the address vector or insert
 operation.
-.RS
-.RE
 .TP
-.B \f[I]addr\f[]
+.B \f[I]addr\f[R]
 Buffer containing one or more addresses to insert into address vector.
-.RS
-.RE
 .TP
-.B \f[I]addrlen\f[]
+.B \f[I]addrlen\f[R]
 On input, specifies size of addr buffer.
 On output, stores number of bytes written to addr buffer.
-.RS
-.RE
 .TP
-.B \f[I]fi_addr\f[]
+.B \f[I]fi_addr\f[R]
 For insert, a reference to an array where returned fabric addresses will
 be written.
 For remove, one or more fabric addresses to remove.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Number of addresses to insert/remove from an AV.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply to the operation.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Address vectors are used to map higher level addresses, which may be
@@ -129,39 +99,37 @@ more natural for an application to use, into fabric specific addresses.
 The mapping of addresses is fabric and provider specific, but may
 involve lengthy address resolution and fabric management protocols.
 AV operations are synchronous by default, but may be set to operate
-asynchronously by specifying the FI_EVENT flag to \f[C]fi_av_open\f[].
+asynchronously by specifying the FI_EVENT flag to \f[C]fi_av_open\f[R].
 When requesting asynchronous operation, the application must first bind
 an event queue to the AV before inserting addresses.
 .SS fi_av_open
 .PP
 fi_av_open allocates or opens an address vector.
 The properties and behavior of the address vector are defined by
-\f[C]struct\ fi_av_attr\f[].
+\f[C]struct fi_av_attr\f[R].
 .IP
 .nf
 \f[C]
-struct\ fi_av_attr\ {
-\ \ \ \ enum\ fi_av_type\ \ type;\ \ \ \ \ \ \ \ /*\ type\ of\ AV\ */
-\ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ rx_ctx_bits;\ /*\ address\ bits\ to\ identify\ rx\ ctx\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ count;\ \ \ \ \ \ \ /*\ #\ entries\ for\ AV\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ ep_per_node;\ /*\ #\ endpoints\ per\ fabric\ address\ */
-\ \ \ \ const\ char\ \ \ \ \ \ \ *name;\ \ \ \ \ \ \ /*\ system\ name\ of\ AV\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ *map_addr;\ \ \ /*\ base\ mmap\ address\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ flags;\ \ \ \ \ \ \ /*\ operation\ flags\ */
+struct fi_av_attr {
+    enum fi_av_type  type;        /* type of AV */
+    int              rx_ctx_bits; /* address bits to identify rx ctx */
+    size_t           count;       /* # entries for AV */
+    size_t           ep_per_node; /* # endpoints per fabric address */
+    const char       *name;       /* system name of AV */
+    void             *map_addr;   /* base mmap address */
+    uint64_t         flags;       /* operation flags */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]type\f[]
+.B \f[I]type\f[R]
 An AV type corresponds to a conceptual implementation of an address
 vector.
 The type specifies how an application views data stored in the AV,
 including how it may be accessed.
 Valid values are:
-.RS
-.RE
 .TP
-.B \- \f[I]FI_AV_MAP\f[]
+.B \- \f[I]FI_AV_MAP\f[R]
 Addresses which are inserted into an AV are mapped to a native fabric
 address for use by the application.
 The use of FI_AV_MAP requires that an application store the returned
@@ -176,10 +144,8 @@ store the returned addresses.
 Addresses are stored in the AV using a provider specific mechanism,
 including, but not limited to a tree, hash table, or maintained on the
 heap.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_AV_TABLE\f[]
+.B \- \f[I]FI_AV_TABLE\f[R]
 Addresses which are inserted into an AV of type FI_AV_TABLE are
 accessible using a simple index.
 Conceptually, the AV may be treated as an array of addresses, though the
@@ -191,34 +157,26 @@ The index of the first address inserted into an FI_AV_TABLE will be 0,
 and successive insertions will be given sequential indices.
 Sequential indices will be assigned across insertion calls on the same
 AV.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_AV_UNSPEC\f[]
+.B \- \f[I]FI_AV_UNSPEC\f[R]
 Provider will choose its preferred AV type.
 The AV type used will be returned through the type field in fi_av_attr.
-.RS
-.RE
 .TP
-.B \f[I]Receive Context Bits (rx_ctx_bits)\f[]
+.B \f[I]Receive Context Bits (rx_ctx_bits)\f[R]
 The receive context bits field is only for use with scalable endpoints.
 It indicates the number of bits reserved in a returned fi_addr_t, which
 will be used to identify a specific target receive context.
 See fi_rx_addr() and fi_endpoint(3) for additional details on receive
 contexts.
-The requested number of bits should be selected such that 2 ^
+The requested number of bits should be selected such that 2 \[ha]
 rx_ctx_bits >= rx_ctx_cnt for the endpoint.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Indicates the expected number of addresses that will be inserted into
 the AV.
 The provider uses this to optimize resource allocations.
-.RS
-.RE
 .TP
-.B \f[I]ep_per_node\f[]
+.B \f[I]ep_per_node\f[R]
 This field indicates the number of endpoints that will be associated
 with a specific fabric, or network, address.
 If the number of endpoints per node is unknown, this value should be set
@@ -227,28 +185,22 @@ The provider uses this value to optimize resource allocations.
 For example, distributed, parallel applications may set this to the
 number of processes allocated per node, times the number of endpoints
 each process will open.
-.RS
-.RE
 .TP
-.B \f[I]name\f[]
+.B \f[I]name\f[R]
 An optional system name associated with the address vector to create or
 open.
 Address vectors may be shared across multiple processes which access the
 same named domain on the same node.
 The name field allows the underlying provider to identify a shared AV.
-.RS
-.RE
 .PP
 If the name field is non\-NULL and the AV is not opened for read\-only
 access, a named AV will be created, if it does not already exist.
 .TP
-.B \f[I]map_addr\f[]
+.B \f[I]map_addr\f[R]
 The map_addr determines the base fi_addr_t address that a provider
 should use when sharing an AV of type FI_AV_MAP between processes.
 Processes that provide the same value for map_addr to a shared AV may
 use the same fi_addr_t values returned from an fi_av_insert call.
-.RS
-.RE
 .PP
 The map_addr may be used by the provider to mmap memory allocated for a
 shared AV between processes; however, the provider is not required to
@@ -263,12 +215,10 @@ If name is non\-NULL and map_addr is 0, then the map_addr used by the
 provider will be returned through the attribute structure.
 The map_addr field is ignored if name is NULL.
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 The following flags may be used when opening an AV.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_EVENT\f[]
+.B \- \f[I]FI_EVENT\f[R]
 When the flag FI_EVENT is specified, all insert operations on this AV
 will occur asynchronously.
 There will be one EQ error entry generated for each failed address
@@ -280,15 +230,13 @@ The context field in all completions will be the context specified to
 the insert call, and the data field in the final completion entry will
 report the number of addresses successfully inserted.
 If an error occurs during the asynchronous insertion, an error
-completion entry is returned (see \f[C]fi_eq\f[](3) for a discussion of
+completion entry is returned (see \f[C]fi_eq\f[R](3) for a discussion of
 the fi_eq_err_entry error completion struct).
 The context field of the error completion will be the context that was
 specified in the insert call; the data field will contain the index of
 the failed address.
 There will be one error completion returned for each address that fails
 to insert into the AV.
-.RS
-.RE
 .PP
 If an AV is opened with FI_EVENT, any insertions attempted before an EQ
 is bound to the AV will fail with \-FI_ENOEQ.
@@ -302,23 +250,19 @@ The only guarantee is that all error completions for a given call to
 fi_av_insert will precede the single associated non\-error completion.
 \[bu] .RS 2
 .TP
-.B \f[I]FI_READ\f[]
+.B \f[I]FI_READ\f[R]
 Opens an AV for read\-only access.
 An AV opened for read\-only access must be named (name attribute
 specified), and the AV must exist.
-.RS
-.RE
 .RE
 \[bu] .RS 2
 .TP
-.B \f[I]FI_SYMMETRIC\f[]
+.B \f[I]FI_SYMMETRIC\f[R]
 Indicates that each node will be associated with the same number of
 endpoints, the same transport addresses will be allocated on each node,
 and the transport addresses will be sequential.
 This feature targets distributed applications on large fabrics and
 allows for highly\-optimized storage of remote endpoint addressing.
-.RS
-.RE
 .RE
 .SS fi_close
 .PP
@@ -336,10 +280,10 @@ the call will return \-FI_EBUSY.
 .SS fi_av_bind
 .PP
 Associates an event queue with the AV.
-If an AV has been opened with \f[C]FI_EVENT\f[], then an event queue
+If an AV has been opened with \f[C]FI_EVENT\f[R], then an event queue
 must be bound to the AV before any insertion calls are attempted.
 Any calls to insert addresses before an event queue has been bound will
-fail with \f[C]\-FI_ENOEQ\f[].
+fail with \f[C]\-FI_ENOEQ\f[R].
 Flags are reserved for future use and must be 0.
 .SS fi_av_insert
 .PP
@@ -350,10 +294,10 @@ AV.
 Addresses inserted into an address vector must be in the same format as
 specified in the addr_format field of the fi_info struct provided when
 opening the corresponding domain.
-When using the \f[C]FI_ADDR_STR\f[] format, the \f[C]addr\f[] parameter
-should reference an array of strings (char **).
+When using the \f[C]FI_ADDR_STR\f[R] format, the \f[C]addr\f[R]
+parameter should reference an array of strings (char **).
 .PP
-For AV\[aq]s of type FI_AV_MAP, once inserted addresses have been
+For AV\[cq]s of type FI_AV_MAP, once inserted addresses have been
 mapped, the mapped values are written into the buffer referenced by
 fi_addr.
 The fi_addr buffer must remain valid until the AV insertion has
@@ -365,7 +309,7 @@ specific encoding of low\-level addressing data, for example.
 In the latter case, use of FI_AV_MAP may be able to avoid memory
 references during data transfer operations.
 .PP
-For AV\[aq]s of type FI_AV_TABLE, addresses are placed into the table in
+For AV\[cq]s of type FI_AV_TABLE, addresses are placed into the table in
 order.
 An address is inserted at the lowest index that corresponds to an unused
 table location, with indices starting at 0.
@@ -384,7 +328,7 @@ the buffer must remain valid until the insertion operation completes.
 Note that if fi_addr is NULL and synchronous operation is requested
 without using FI_SYNC_ERR flag, individual insertion failures cannot be
 reported and the application must use other calls, such as
-\f[C]fi_av_lookup\f[] to learn which specific addresses failed to
+\f[C]fi_av_lookup\f[R] to learn which specific addresses failed to
 insert.
 Since fi_av_remove is provider\-specific, it is recommended that calls
 to fi_av_insert following a call to fi_av_remove always reference a
@@ -392,13 +336,11 @@ valid buffer in the fi_addr parameter.
 Otherwise it may be difficult to determine what the next assigned index
 will be.
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 The following flag may be passed to AV insertion calls: fi_av_insert,
 fi_av_insertsvc, or fi_av_insertsym.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_MORE\f[]
+.B \- \f[I]FI_MORE\f[R]
 In order to allow optimized address insertion, the application may
 specify the FI_MORE flag to the insert call to give a hint to the
 provider that more insertion requests will follow, allowing the provider
@@ -407,10 +349,8 @@ An application may make any number of insertion calls with FI_MORE set,
 provided that they are followed by an insertion call without FI_MORE.
 This signifies to the provider that the insertion list is complete.
 Providers are free to ignore FI_MORE.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_SYNC_ERR\f[]
+.B \- \f[I]FI_SYNC_ERR\f[R]
 This flag applies to synchronous insertions only, and is used to
 retrieve error details of failed insertions.
 If set, the context parameter of insertion calls references an array of
@@ -419,8 +359,6 @@ The resulting status of attempting to insert each address will be
 written to the corresponding array location.
 Successful insertions will be updated to 0.
 Failures will contain a fabric errno code.
-.RS
-.RE
 .SS fi_av_insertsvc
 .PP
 The fi_av_insertsvc call behaves similar to fi_av_insert, but allows the
@@ -431,10 +369,10 @@ Node should be a string that corresponds to a hostname or network
 address.
 The service string corresponds to a textual representation of a
 transport address.
-Applications may also pass in an \f[C]FI_ADDR_STR\f[] formatted address
+Applications may also pass in an \f[C]FI_ADDR_STR\f[R] formatted address
 as the node parameter.
 In such cases, the service parameter must be NULL.
-See fi_getinfo.3 for details on using \f[C]FI_ADDR_STR\f[].
+See fi_getinfo.3 for details on using \f[C]FI_ADDR_STR\f[R].
 Supported flags are the same as for fi_av_insert.
 .SS fi_av_insertsym
 .PP
@@ -456,11 +394,12 @@ Inserted node addresses will be of the range {node, node + nodecnt \-
 If node is a non\-numeric string, such as a hostname, it must contain a
 numeric suffix if nodecnt > 1.
 .PP
-As an example, if node = "10.1.1.1", nodecnt = 2, service = "5000", and
-svccnt = 2, the following addresses will be inserted into the AV in the
-order shown: 10.1.1.1:5000, 10.1.1.1:5001, 10.1.1.2:5000, 10.1.1.2:5001.
-If node were replaced by the hostname "host10", the addresses would be:
-host10:5000, host10:5001, host11:5000, host11:5001.
+As an example, if node = \[lq]10.1.1.1\[rq], nodecnt = 2, service =
+\[lq]5000\[rq], and svccnt = 2, the following addresses will be inserted
+into the AV in the order shown: 10.1.1.1:5000, 10.1.1.1:5001,
+10.1.1.2:5000, 10.1.1.2:5001.
+If node were replaced by the hostname \[lq]host10\[rq], the addresses
+would be: host10:5000, host10:5001, host11:5000, host11:5001.
 .PP
 The total number of inserted addresses will be nodecnt x svccnt.
 .PP
@@ -522,7 +461,7 @@ If the provided buffer is too small, the results will be truncated.
 fi_av_straddr returns a pointer to buf.
 .SH NOTES
 .PP
-Providers may implement AV\[aq]s using a variety of mechanisms.
+Providers may implement AV\[cq]s using a variety of mechanisms.
 Specifically, a provider may begin resolving inserted addresses as soon
 as they have been added to an AV, even if asynchronous operation has
 been specified.
@@ -549,10 +488,10 @@ FI_ADDR_NOTAVAIL.
 .PP
 All other calls return 0 on success, or a negative value corresponding
 to fabric errno on error.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_eq\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_eq\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_av_set.3 b/man/man3/fi_av_set.3
index 51548a6..89dca55 100644
--- a/man/man3/fi_av_set.3
+++ b/man/man3/fi_av_set.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_av_set" "3" "2020\-03\-20" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_av_set" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,108 +8,76 @@ fi_av_set \- Address vector set operations
 .TP
 .B fi_av_set / fi_close
 Open or close an address vector set
-.RS
-.RE
 .TP
 .B fi_av_set_union
 Perform a set union operation on two AV sets
-.RS
-.RE
 .TP
 .B fi_av_set_intersect
 Perform a set intersect operation on two AV sets
-.RS
-.RE
 .TP
 .B fi_av_set_diff
 Perform a set difference operation on two AV sets
-.RS
-.RE
 .TP
 .B fi_av_set_insert
 Add an address to an AV set
-.RS
-.RE
 .TP
 .B fi_av_set_remove
 Remove an address from an AV set
-.RS
-.RE
 .TP
 .B fi_av_set_addr
 Obtain a collective address for current addresses in an AV set
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_collective.h>
+#include <rdma/fi_collective.h>
 
-int\ fi_av_set(struct\ fid_av\ *av,\ struct\ fi_av_set_attr\ *attr,
-\ \ \ \ \ \ struct\ fid_av_set\ **set,\ void\ *\ context);
+int fi_av_set(struct fid_av *av, struct fi_av_set_attr *attr,
+      struct fid_av_set **set, void * context);
 
-int\ fi_av_set_union(struct\ fid_av_set\ *dst,\ const\ struct\ fid_av_set\ *src);
+int fi_av_set_union(struct fid_av_set *dst, const struct fid_av_set *src);
 
-int\ fi_av_set_intersect(struct\ fid_av_set\ *dst,\ const\ struct\ fid_av_set\ *src);
+int fi_av_set_intersect(struct fid_av_set *dst, const struct fid_av_set *src);
 
-int\ fi_av_set_diff(struct\ fid_av_set\ *dst,\ const\ struct\ fid_av_set\ *src);
+int fi_av_set_diff(struct fid_av_set *dst, const struct fid_av_set *src);
 
-int\ fi_av_set_insert(struct\ fid_av_set\ *set,\ fi_addr_t\ addr);
+int fi_av_set_insert(struct fid_av_set *set, fi_addr_t addr);
 
-int\ fi_av_set_remove(struct\ fid_av_set\ *set,\ fi_addr_t\ addr);
+int fi_av_set_remove(struct fid_av_set *set, fi_addr_t addr);
 
-int\ fi_av_set_addr(struct\ fid_av_set\ *set,\ fi_addr_t\ *coll_addr);
+int fi_av_set_addr(struct fid_av_set *set, fi_addr_t *coll_addr);
 
-int\ fi_close(struct\ fid\ *av_set);
-\f[]
+int fi_close(struct fid *av_set);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]av\f[]
+.B \f[I]av\f[R]
 Address vector
-.RS
-.RE
 .TP
-.B \f[I]set\f[]
+.B \f[I]set\f[R]
 Address vector set
-.RS
-.RE
 .TP
-.B \f[I]dst\f[]
+.B \f[I]dst\f[R]
 Address vector set updated by set operation
-.RS
-.RE
 .TP
-.B \f[I]src\f[]
+.B \f[I]src\f[R]
 Address vector set providing input to a set operation
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Address vector set attributes
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the address vector set
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply to the operation.
-.RS
-.RE
 .TP
-.B \f[I]addr\f[]
+.B \f[I]addr\f[R]
 Destination address to insert to remove from AV set.
-.RS
-.RE
 .TP
-.B \f[I]coll_addr\f[]
+.B \f[I]coll_addr\f[R]
 Address identifying collective group.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 An address vector set (AV set) represents an ordered subset of addresses
@@ -124,7 +92,7 @@ The creation and manipulation of an AV set is a local operation.
 No fabric traffic is exchanged between peers.
 As a result, each peer is responsible for creating matching AV sets as
 part of their collective membership definition.
-See \f[C]fi_collective\f[](3) for a discussion of membership models.
+See \f[C]fi_collective\f[R](3) for a discussion of membership models.
 .SS fi_av_set
 .PP
 The fi_av_set call creates a new AV set.
@@ -138,28 +106,26 @@ interfaces defined below.
 .IP
 .nf
 \f[C]
-struct\ fi_av_set_attr\ {
-\ \ \ \ size_t\ count;
-\ \ \ \ fi_addr_t\ start_addr;
-\ \ \ \ fi_addr_t\ end_addr;
-\ \ \ \ uint64_t\ stride;
-\ \ \ \ size_t\ comm_key_size;
-\ \ \ \ uint8_t\ *comm_key;
-\ \ \ \ uint64_t\ flags;
+struct fi_av_set_attr {
+    size_t count;
+    fi_addr_t start_addr;
+    fi_addr_t end_addr;
+    uint64_t stride;
+    size_t comm_key_size;
+    uint8_t *comm_key;
+    uint64_t flags;
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Indicates the expected the number of members that will be a part of the
 AV set.
 The provider uses this to optimize resource allocations.
 If count is 0, the provider will select a size based on available system
 configuration data or underlying limitations.
-.RS
-.RE
 .TP
-.B \f[I]start_addr / end_addr\f[]
+.B \f[I]start_addr / end_addr\f[R]
 The starting and ending addresses, inclusive, to include as part of the
 AV set.
 The use of start and end address require that the associated AV have
@@ -170,13 +136,11 @@ members to the AV set.
 The start_addr and end_addr must be set to FI_ADDR_NOTAVAIL if creating
 an empty AV set, a communication key is being provided, or the AV is of
 type FI_AV_MAP.
-.RS
-.RE
 .PP
 The number of addresses between start_addr and end_addr must be less
 than or equal to the specified count value.
 .TP
-.B \f[I]stride\f[]
+.B \f[I]stride\f[R]
 The number of entries between successive addresses included in the AV
 set.
 The AV set will include all addresses from start_addr + stride x i, for
@@ -185,16 +149,12 @@ A stride of 1 indicates that all addresses between start_addr and
 end_addr should be added to the AV set.
 Stride should be set to 0 unless the start_addr and end_addr fields are
 valid.
-.RS
-.RE
 .TP
-.B \f[I]comm_key_size\f[]
+.B \f[I]comm_key_size\f[R]
 The length of the communication key in bytes.
 This field should be 0 if a communication key is not available.
-.RS
-.RE
 .TP
-.B \f[I]comm_key\f[]
+.B \f[I]comm_key\f[R]
 If supported by the fabric, this represents a key associated with the AV
 set.
 The communication key is used by applications that directly manage
@@ -203,14 +163,10 @@ manager.
 The key is used to convey that results of the membership setup to the
 underlying provider.
 The use and format of a communication key is fabric provider specific.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 If the flag FI_UNIVERSE is set, then the AV set will be created
 containing all addresses stored in the AV.
-.RS
-.RE
 .SS fi_av_set_union
 .PP
 The AV set union call adds all addresses in the source AV set that are
@@ -244,9 +200,8 @@ This is a local operation only that does not involve network
 communication.
 The returned address may be used as input into fi_join_collective.
 Note that attempting to use the address returned from fi_av_set_addr
-(e.g.
-passing it to fi_join_collective) while simultaneously modifying the
-addresses stored in an AV set results in undefined behavior.
+(e.g.\ passing it to fi_join_collective) while simultaneously modifying
+the addresses stored in an AV set results in undefined behavior.
 .SS fi_close
 .PP
 Closes an AV set and releases all resources associated with it.
@@ -261,9 +216,9 @@ situations.
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH SEE ALSO
 .PP
-\f[C]fi_av\f[](3), \f[C]fi_collective\f[](3)
+\f[C]fi_av\f[R](3), \f[C]fi_collective\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_cm.3 b/man/man3/fi_cm.3
index d48f467..bd34e2c 100644
--- a/man/man3/fi_cm.3
+++ b/man/man3/fi_cm.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_cm" "3" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_cm" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,60 +8,52 @@ fi_cm \- Connection management operations
 .TP
 .B fi_connect / fi_listen / fi_accept / fi_reject / fi_shutdown
 Manage endpoint connection state.
-.RS
-.RE
 .TP
 .B fi_setname / fi_getname / fi_getpeer
 Set local, or return local or peer endpoint address.
-.RS
-.RE
 .TP
 .B fi_join / fi_close / fi_mc_addr
 Join, leave, or retrieve a multicast address.
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_cm.h>
+#include <rdma/fi_cm.h>
 
-int\ fi_connect(struct\ fid_ep\ *ep,\ const\ void\ *addr,
-\ \ \ \ const\ void\ *param,\ size_t\ paramlen);
+int fi_connect(struct fid_ep *ep, const void *addr,
+    const void *param, size_t paramlen);
 
-int\ fi_listen(struct\ fid_pep\ *pep);
+int fi_listen(struct fid_pep *pep);
 
-int\ fi_accept(struct\ fid_ep\ *ep,\ const\ void\ *param,\ size_t\ paramlen);
+int fi_accept(struct fid_ep *ep, const void *param, size_t paramlen);
 
-int\ fi_reject(struct\ fid_pep\ *pep,\ fid_t\ handle,
-\ \ \ \ const\ void\ *param,\ size_t\ paramlen);
+int fi_reject(struct fid_pep *pep, fid_t handle,
+    const void *param, size_t paramlen);
 
-int\ fi_shutdown(struct\ fid_ep\ *ep,\ uint64_t\ flags);
+int fi_shutdown(struct fid_ep *ep, uint64_t flags);
 
-int\ fi_setname(fid_t\ fid,\ void\ *addr,\ size_t\ addrlen);
+int fi_setname(fid_t fid, void *addr, size_t addrlen);
 
-int\ fi_getname(fid_t\ fid,\ void\ *addr,\ size_t\ *addrlen);
+int fi_getname(fid_t fid, void *addr, size_t *addrlen);
 
-int\ fi_getpeer(struct\ fid_ep\ *ep,\ void\ *addr,\ size_t\ *addrlen);
+int fi_getpeer(struct fid_ep *ep, void *addr, size_t *addrlen);
 
-int\ fi_join(struct\ fid_ep\ *ep,\ const\ void\ *addr,\ uint64_t\ flags,
-\ \ \ \ struct\ fid_mc\ **mc,\ void\ *context);
+int fi_join(struct fid_ep *ep, const void *addr, uint64_t flags,
+    struct fid_mc **mc, void *context);
 
-int\ fi_close(struct\ fid\ *mc);
+int fi_close(struct fid *mc);
 
-fi_addr_t\ fi_mc_addr(struct\ fid_mc\ *mc);
-\f[]
+fi_addr_t fi_mc_addr(struct fid_mc *mc);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]ep / pep\f[]
+.B \f[I]ep / pep\f[R]
 Fabric endpoint on which to change connection state.
-.RS
-.RE
 .PP
-\f[I]fid\f[] Active or passive endpoint to get/set address.
+\f[I]fid\f[R] Active or passive endpoint to get/set address.
 .TP
-.B \f[I]addr\f[]
+.B \f[I]addr\f[R]
 Buffer to address.
 On a set call, the endpoint will be assigned the specified address.
 On a get, the local address will be copied into the buffer, up to the
@@ -69,44 +61,28 @@ space provided.
 For connect, this parameter indicates the peer address to connect to.
 The address must be in the same format as that specified using fi_info:
 addr_format when the endpoint was created.
-.RS
-.RE
 .TP
-.B \f[I]addrlen\f[]
+.B \f[I]addrlen\f[R]
 On input, specifies size of addr buffer.
 On output, stores number of bytes written to addr buffer.
-.RS
-.RE
 .TP
-.B \f[I]param\f[]
+.B \f[I]param\f[R]
 User\-specified data exchanged as part of the connection exchange.
-.RS
-.RE
 .TP
-.B \f[I]paramlen\f[]
+.B \f[I]paramlen\f[R]
 Size of param buffer.
-.RS
-.RE
 .TP
-.B \f[I]info\f[]
+.B \f[I]info\f[R]
 Fabric information associated with a connection request.
-.RS
-.RE
 .TP
-.B \f[I]mc\f[]
+.B \f[I]mc\f[R]
 Multicast group associated with an endpoint.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags for controlling connection operation.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User context associated with the request.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Connection management functions are used to connect an
@@ -120,7 +96,7 @@ Connection requests against a listening endpoint are reported
 asynchronously to the user through a bound CM event queue using the
 FI_CONNREQ event type.
 The number of outstanding connection requests that can be queued at an
-endpoint is limited by the listening endpoint\[aq]s backlog parameter.
+endpoint is limited by the listening endpoint\[cq]s backlog parameter.
 The backlog is initialized based on administrative configuration values,
 but may be adjusted through the fi_control call.
 .SS fi_connect
@@ -232,7 +208,7 @@ An endpoint may be enabled explicitly through fi_enable, or implicitly,
 such as through fi_connect or fi_listen.
 An address may be assigned using fi_setname.
 fi_getpeer is not guaranteed to return a valid peer address until an
-endpoint has been completely connected \-\- an FI_CONNECTED event has
+endpoint has been completely connected \[en] an FI_CONNECTED event has
 been generated.
 .SS fi_join
 .PP
@@ -268,28 +244,24 @@ and paired with the FI_MULTICAST operation flag.
 .PP
 Except in functions noted below, flags are reserved and must be 0.
 .TP
-.B \f[I]FI_SEND\f[]
+.B \f[I]FI_SEND\f[R]
 Applies to fi_join.
 This flag indicates that the endpoint should join the multicast group as
 a send only member.
 The endpoint must be configured for transmit operations to use this
 flag, or an error will occur.
-.RS
-.RE
 .TP
-.B \f[I]FI_RECV\f[]
+.B \f[I]FI_RECV\f[R]
 Applies to fi_join.
 This flag indicates that the endpoint should join the multicast group
 with receive permissions only.
 The endpoint must be configured for receive operations to use this flag,
 or an error will occur.
-.RS
-.RE
 .SH RETURN VALUE
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .SH NOTES
 .PP
@@ -307,7 +279,7 @@ events, or as additional err_data to fi_eq_err_entry, in the case of a
 rejected connection.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_eq\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_eq\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_cntr.3 b/man/man3/fi_cntr.3
index e958718..f7da7ef 100644
--- a/man/man3/fi_cntr.3
+++ b/man/man3/fi_cntr.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_cntr" "3" "2019\-12\-13" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_cntr" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,97 +8,71 @@ fi_cntr \- Completion and event counter operations
 .TP
 .B fi_cntr_open / fi_close
 Allocate/free a counter
-.RS
-.RE
 .TP
 .B fi_cntr_read
 Read the current value of a counter
-.RS
-.RE
 .TP
 .B fi_cntr_readerr
 Reads the number of operations which have completed in error.
-.RS
-.RE
 .TP
 .B fi_cntr_add
 Increment a counter by a specified value
-.RS
-.RE
 .TP
 .B fi_cntr_set
 Set a counter to a specified value
-.RS
-.RE
 .TP
 .B fi_cntr_wait
 Wait for a counter to be greater or equal to a threshold value
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_cntr_open(struct\ fid_domain\ *domain,\ struct\ fi_cntr_attr\ *attr,
-\ \ \ \ struct\ fid_cntr\ **cntr,\ void\ *context);
+int fi_cntr_open(struct fid_domain *domain, struct fi_cntr_attr *attr,
+    struct fid_cntr **cntr, void *context);
 
-int\ fi_close(struct\ fid\ *cntr);
+int fi_close(struct fid *cntr);
 
-uint64_t\ fi_cntr_read(struct\ fid_cntr\ *cntr);
+uint64_t fi_cntr_read(struct fid_cntr *cntr);
 
-uint64_t\ fi_cntr_readerr(struct\ fid_cntr\ *cntr);
+uint64_t fi_cntr_readerr(struct fid_cntr *cntr);
 
-int\ fi_cntr_add(struct\ fid_cntr\ *cntr,\ uint64_t\ value);
+int fi_cntr_add(struct fid_cntr *cntr, uint64_t value);
 
-int\ fi_cntr_adderr(struct\ fid_cntr\ *cntr,\ uint64_t\ value);
+int fi_cntr_adderr(struct fid_cntr *cntr, uint64_t value);
 
-int\ fi_cntr_set(struct\ fid_cntr\ *cntr,\ uint64_t\ value);
+int fi_cntr_set(struct fid_cntr *cntr, uint64_t value);
 
-int\ fi_cntr_seterr(struct\ fid_cntr\ *cntr,\ uint64_t\ value);
+int fi_cntr_seterr(struct fid_cntr *cntr, uint64_t value);
 
-int\ fi_cntr_wait(struct\ fid_cntr\ *cntr,\ uint64_t\ threshold,
-\ \ \ \ int\ timeout);
-\f[]
+int fi_cntr_wait(struct fid_cntr *cntr, uint64_t threshold,
+    int timeout);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]domain\f[]
+.B \f[I]domain\f[R]
 Fabric domain
-.RS
-.RE
 .TP
-.B \f[I]cntr\f[]
+.B \f[I]cntr\f[R]
 Fabric counter
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Counter attributes
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the counter
-.RS
-.RE
 .TP
-.B \f[I]value\f[]
+.B \f[I]value\f[R]
 Value to increment or set counter
-.RS
-.RE
 .TP
-.B \f[I]threshold\f[]
+.B \f[I]threshold\f[R]
 Value to compare counter against
-.RS
-.RE
 .TP
-.B \f[I]timeout\f[]
+.B \f[I]timeout\f[R]
 Time in milliseconds to wait.
 A negative value indicates infinite timeout.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Counters record the number of requested operations that have completed.
@@ -117,44 +91,40 @@ That is, a counter actually stores two distinct values, with error
 completions updating an error specific value.
 .PP
 Counters are updated following the completion event semantics defined in
-\f[C]fi_cq\f[](3).
+\f[C]fi_cq\f[R](3).
 The timing of the update is based on the type of transfer and any
 specified operation flags.
 .SS fi_cntr_open
 .PP
 fi_cntr_open allocates a new fabric counter.
 The properties and behavior of the counter are defined by
-\f[C]struct\ fi_cntr_attr\f[].
+\f[C]struct fi_cntr_attr\f[R].
 .IP
 .nf
 \f[C]
-struct\ fi_cntr_attr\ {
-\ \ \ \ enum\ fi_cntr_events\ \ events;\ \ \ \ /*\ type\ of\ events\ to\ count\ */
-\ \ \ \ enum\ fi_wait_obj\ \ \ \ \ wait_obj;\ \ /*\ requested\ wait\ object\ */
-\ \ \ \ struct\ fid_wait\ \ \ \ \ *wait_set;\ \ /*\ optional\ wait\ set\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ flags;\ \ \ \ \ /*\ operation\ flags\ */
+struct fi_cntr_attr {
+    enum fi_cntr_events  events;    /* type of events to count */
+    enum fi_wait_obj     wait_obj;  /* requested wait object */
+    struct fid_wait     *wait_set;  /* optional wait set */
+    uint64_t             flags;     /* operation flags */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]events\f[]
+.B \f[I]events\f[R]
 A counter captures different types of events.
 The specific type which is to counted are one of the following:
-.RS
-.RE
 .TP
-.B \- \f[I]FI_CNTR_EVENTS_COMP\f[]
+.B \- \f[I]FI_CNTR_EVENTS_COMP\f[R]
 The counter increments for every successful completion that occurs on an
 associated bound endpoint.
-The type of completions \-\- sends and/or receives \-\- which are
+The type of completions \[en] sends and/or receives \[en] which are
 counted may be restricted using control flags when binding the counter
 and the endpoint.
 Counters increment on all successful completions, separately from
 whether the operation generates an entry in an event queue.
-.RS
-.RE
 .TP
-.B \f[I]wait_obj\f[]
+.B \f[I]wait_obj\f[R]
 Counters may be associated with a specific wait object.
 Wait objects allow applications to block until the wait object is
 signaled, indicating that a counter has reached a specific threshold.
@@ -164,16 +134,12 @@ The following values may be used to specify the type of wait object
 associated with a counter: FI_WAIT_NONE, FI_WAIT_UNSPEC, FI_WAIT_SET,
 FI_WAIT_FD, FI_WAIT_MUTEX_COND, and FI_WAIT_YIELD.
 The default is FI_WAIT_NONE.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_NONE\f[]
+.B \- \f[I]FI_WAIT_NONE\f[R]
 Used to indicate that the user will not block (wait) for events on the
 counter.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_UNSPEC\f[]
+.B \- \f[I]FI_WAIT_UNSPEC\f[R]
 Specifies that the user will only wait on the counter using fabric
 interface calls, such as fi_cntr_wait.
 In this case, the underlying provider may select the most appropriate or
@@ -181,41 +147,31 @@ highest performing wait object available, including custom wait
 mechanisms.
 Applications that select FI_WAIT_UNSPEC are not guaranteed to retrieve
 the underlying wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_SET\f[]
+.B \- \f[I]FI_WAIT_SET\f[R]
 Indicates that the event counter should use a wait set object to wait
 for events.
 If specified, the wait_set field must reference an existing wait set
 object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_FD\f[]
+.B \- \f[I]FI_WAIT_FD\f[R]
 Indicates that the counter should use a file descriptor as its wait
 mechanism.
 A file descriptor wait object must be usable in select, poll, and epoll
 routines.
 However, a provider may signal an FD wait object by marking it as
 readable, writable, or with an error.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_MUTEX_COND\f[]
+.B \- \f[I]FI_WAIT_MUTEX_COND\f[R]
 Specifies that the counter should use a pthread mutex and cond variable
 as a wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_YIELD\f[]
+.B \- \f[I]FI_WAIT_YIELD\f[R]
 Indicates that the counter will wait without a wait object but instead
 yield on every wait.
 Allows usage of fi_cntr_wait through a spin.
-.RS
-.RE
 .TP
-.B \f[I]wait_set\f[]
+.B \f[I]wait_set\f[R]
 If wait_obj is FI_WAIT_SET, this field references a wait object to which
 the event counter should attach.
 When an event is added to the event counter, the corresponding wait set
@@ -223,13 +179,9 @@ will be signaled if all necessary conditions are met.
 The use of a wait_set enables an optimized method of waiting for events
 across multiple event counters.
 This field is ignored if wait_obj is not FI_WAIT_SET.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Flags are reserved for future use, and must be set to 0.
-.RS
-.RE
 .SS fi_close
 .PP
 The fi_close call releases all resources associated with a counter.
@@ -247,26 +199,20 @@ fi_cntr_control is invoked, as it may redirect the implementation of
 counter operations.
 The following control commands are usable with a counter:
 .TP
-.B \f[I]FI_GETOPSFLAG (uint64_t *)\f[]
+.B \f[I]FI_GETOPSFLAG (uint64_t *)\f[R]
 Returns the current default operational flags associated with the
 counter.
-.RS
-.RE
 .TP
-.B \f[I]FI_SETOPSFLAG (uint64_t *)\f[]
+.B \f[I]FI_SETOPSFLAG (uint64_t *)\f[R]
 Modifies the current default operational flags associated with the
 counter.
-.RS
-.RE
 .TP
-.B \f[I]FI_GETWAIT (void **)\f[]
+.B \f[I]FI_GETWAIT (void **)\f[R]
 This command allows the user to retrieve the low\-level wait object
 associated with the counter.
 The format of the wait\-object is specified during counter creation,
 through the counter attributes.
 See fi_eq.3 for addition details using control with FI_GETWAIT.
-.RS
-.RE
 .SS fi_cntr_read
 .PP
 The fi_cntr_read call returns the current value of the counter.
@@ -295,7 +241,7 @@ or equal to the input threshold value.
 .PP
 If an operation associated with the counter encounters an error, it will
 increment the error value associated with the counter.
-Any change in a counter\[aq]s error value will unblock any thread inside
+Any change in a counter\[cq]s error value will unblock any thread inside
 fi_cntr_wait.
 .PP
 If the call returns due to timeout, \-FI_ETIMEDOUT will be returned.
@@ -310,17 +256,14 @@ On error, a negative value corresponding to fabric errno is returned.
 .TP
 .B fi_cntr_read / fi_cntr_readerr
 Returns the current value of the counter.
-.RS
-.RE
 .PP
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH NOTES
 .PP
 In order to support a variety of counter implementations, updates made
-to counter values (e.g.
-fi_cntr_set or fi_cntr_add) may not be immediately visible to counter
-read operations (i.e.
-fi_cntr_read or fi_cntr_readerr).
+to counter values (e.g.\ fi_cntr_set or fi_cntr_add) may not be
+immediately visible to counter read operations (i.e.\ fi_cntr_read or
+fi_cntr_readerr).
 A small, but undefined, delay may occur between the counter changing and
 the reported value being updated.
 However, a final updated value will eventually be reflected in the read
@@ -336,7 +279,7 @@ fi_cntr_set / fi_cntr_seterr and results of related operations are
 reflected in the observed value of the counter.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_eq\f[](3), \f[C]fi_poll\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_eq\f[R](3), \f[C]fi_poll\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_collective.3 b/man/man3/fi_collective.3
index 3d000b0..852ef95 100644
--- a/man/man3/fi_collective.3
+++ b/man/man3/fi_collective.3
@@ -1,192 +1,146 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_collective" "3" "2020\-04\-13" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_collective" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .TP
 .B fi_join_collective
 Operation where a subset of peers join a new collective group.
-.RS
-.RE
 .TP
 .B fi_barrier
 Collective operation that does not complete until all peers have entered
 the barrier call.
-.RS
-.RE
 .TP
 .B fi_broadcast
 A single sender transmits data to all peers, including itself.
-.RS
-.RE
 .TP
 .B fi_alltoall
 Each peer distributes a slice of its local data to all peers.
-.RS
-.RE
 .TP
 .B fi_allreduce
 Collective operation where all peers broadcast an atomic operation to
 all other peers.
-.RS
-.RE
 .TP
 .B fi_allgather
 Each peer sends a complete copy of its local data to all peers.
-.RS
-.RE
 .TP
 .B fi_reduce_scatter
 Collective call where data is collected from all peers and merged
 (reduced).
 The results of the reduction is distributed back to the peers, with each
 peer receiving a slice of the results.
-.RS
-.RE
 .TP
 .B fi_reduce
 Collective call where data is collected from all peers to a root peer
 and merged (reduced).
-.RS
-.RE
 .TP
 .B fi_scatter
 A single sender distributes (scatters) a slice of its local data to all
 peers.
-.RS
-.RE
 .TP
 .B fi_gather
 All peers send their data to a root peer.
-.RS
-.RE
 .TP
 .B fi_query_collective
 Returns information about which collective operations are supported by a
 provider, and limitations on the collective.
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_collective.h>
+#include <rdma/fi_collective.h>
 
-int\ fi_join_collective(struct\ fid_ep\ *ep,\ fi_addr_t\ coll_addr,
-\ \ \ \ const\ struct\ fid_av_set\ *set,
-\ \ \ \ uint64_t\ flags,\ struct\ fid_mc\ **mc,\ void\ *context);
+int fi_join_collective(struct fid_ep *ep, fi_addr_t coll_addr,
+    const struct fid_av_set *set,
+    uint64_t flags, struct fid_mc **mc, void *context);
 
-ssize_t\ fi_barrier(struct\ fid_ep\ *ep,\ fi_addr_t\ coll_addr,
-\ \ \ \ void\ *context);
+ssize_t fi_barrier(struct fid_ep *ep, fi_addr_t coll_addr,
+    void *context);
 
-ssize_t\ fi_broadcast(struct\ fid_ep\ *ep,\ void\ *buf,\ size_t\ count,\ void\ *desc,
-\ \ \ \ fi_addr_t\ coll_addr,\ fi_addr_t\ root_addr,\ enum\ fi_datatype\ datatype,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_broadcast(struct fid_ep *ep, void *buf, size_t count, void *desc,
+    fi_addr_t coll_addr, fi_addr_t root_addr, enum fi_datatype datatype,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_alltoall(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
-\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_alltoall(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc,
+    fi_addr_t coll_addr, enum fi_datatype datatype,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_allreduce(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
-\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_allreduce(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc,
+    fi_addr_t coll_addr, enum fi_datatype datatype, enum fi_op op,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_allgather(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
-\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_allgather(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc,
+    fi_addr_t coll_addr, enum fi_datatype datatype,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_reduce_scatter(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,
-\ \ \ \ fi_addr_t\ coll_addr,\ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_reduce_scatter(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc,
+    fi_addr_t coll_addr, enum fi_datatype datatype, enum fi_op op,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_reduce(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,\ fi_addr_t\ coll_addr,
-\ \ \ \ fi_addr_t\ root_addr,\ enum\ fi_datatype\ datatype,\ enum\ fi_op\ op,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_reduce(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc, fi_addr_t coll_addr,
+    fi_addr_t root_addr, enum fi_datatype datatype, enum fi_op op,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_scatter(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,\ fi_addr_t\ coll_addr,
-\ \ \ \ fi_addr_t\ root_addr,\ enum\ fi_datatype\ datatype,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_scatter(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc, fi_addr_t coll_addr,
+    fi_addr_t root_addr, enum fi_datatype datatype,
+    uint64_t flags, void *context);
 
-ssize_t\ fi_gather(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ count,
-\ \ \ \ void\ *desc,\ void\ *result,\ void\ *result_desc,\ fi_addr_t\ coll_addr,
-\ \ \ \ fi_addr_t\ root_addr,\ enum\ fi_datatype\ datatype,
-\ \ \ \ uint64_t\ flags,\ void\ *context);
+ssize_t fi_gather(struct fid_ep *ep, const void *buf, size_t count,
+    void *desc, void *result, void *result_desc, fi_addr_t coll_addr,
+    fi_addr_t root_addr, enum fi_datatype datatype,
+    uint64_t flags, void *context);
 
-int\ fi_query_collective(struct\ fid_domain\ *domain,
-\ \ \ \ fi_collective_op\ coll,\ struct\ fi_collective_attr\ *attr,\ uint64_t\ flags);
-\f[]
+int fi_query_collective(struct fid_domain *domain,
+    fi_collective_op coll, struct fi_collective_attr *attr, uint64_t flags);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]ep\f[]
+.B \f[I]ep\f[R]
 Fabric endpoint on which to initiate collective operation.
-.RS
-.RE
 .TP
-.B \f[I]set\f[]
+.B \f[I]set\f[R]
 Address vector set defining the collective membership.
-.RS
-.RE
 .TP
-.B \f[I]mc\f[]
+.B \f[I]mc\f[R]
 Multicast group associated with the collective.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Local data buffer that specifies first operand of collective operation
-.RS
-.RE
 .TP
-.B \f[I]datatype\f[]
+.B \f[I]datatype\f[R]
 Datatype associated with atomic operands
-.RS
-.RE
 .TP
-.B \f[I]op\f[]
+.B \f[I]op\f[R]
 Atomic operation to perform
-.RS
-.RE
 .TP
-.B \f[I]result\f[]
+.B \f[I]result\f[R]
 Local data buffer to store the result of the collective operation.
-.RS
-.RE
 .TP
-.B \f[I]desc / result_desc\f[]
+.B \f[I]desc / result_desc\f[R]
 Data descriptor associated with the local data buffer and local result
 buffer, respectively.
-.RS
-.RE
 .TP
-.B \f[I]coll_addr\f[]
+.B \f[I]coll_addr\f[R]
 Address referring to the collective group of endpoints.
-.RS
-.RE
 .TP
-.B \f[I]root_addr\f[]
+.B \f[I]root_addr\f[R]
 Single endpoint that is the source or destination of collective data.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply for the atomic operation
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified pointer to associate with the operation.
 This parameter is ignored if the operation will not generate a
 successful completion, unless an op flag specifies the context parameter
 be used for required input.
-.RS
-.RE
 .SH DESCRIPTION (EXPERIMENTAL APIs)
 .PP
 The collective APIs are new to the 1.9 libfabric release.
@@ -197,7 +151,7 @@ versions of the library until the experimental tag has been removed.
 .PP
 In general collective operations can be thought of as coordinated atomic
 operations between a set of peer endpoints.
-Readers should refer to the \f[C]fi_atomic\f[](3) man page for details
+Readers should refer to the \f[C]fi_atomic\f[R](3) man page for details
 on the atomic operations and datatypes defined by libfabric.
 .PP
 A collective operation is a group communication exchange.
@@ -244,7 +198,7 @@ provider by creating and configuring an address vector set (AV set).
 An AV set represents an ordered subset of addresses in an address vector
 (AV).
 Details on creating and configuring an AV set are available in
-\f[C]fi_av_set\f[](3).
+\f[C]fi_av_set\f[R](3).
 .PP
 Once an AV set has been programmed with the collective membership
 information, an endpoint is joined to the set.
@@ -303,7 +257,7 @@ Applications must call fi_close on the collective group to disconnect
 the endpoint from the group.
 After a join operation has completed, the fi_mc_addr call may be used to
 retrieve the address associated with the multicast group.
-See \f[C]fi_cm\f[](3) for additional details on fi_mc_addr().
+See \f[C]fi_cm\f[R](3) for additional details on fi_mc_addr().
 .SS Barrier (fi_barrier)
 .PP
 The fi_barrier operation provides a mechanism to synchronize peers.
@@ -329,13 +283,13 @@ transfer an array of integers to a group of peers.
 .IP
 .nf
 \f[C]
-[1]\ \ [1]\ \ [1]
-[5]\ \ [5]\ \ [5]
-[9]\ \ [9]\ \ [9]
-\ |____^\ \ \ \ ^
-\ |_________|
-\ broadcast
-\f[]
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+ |____\[ha]    \[ha]
+ |_________|
+ broadcast
+\f[R]
 .fi
 .SS All to All (fi_alltoall)
 .PP
@@ -347,16 +301,16 @@ entries in an integer array.
 .IP
 .nf
 \f[C]
-[1]\ \ \ [2]\ \ \ [3]
-[5]\ \ \ [6]\ \ \ [7]
-[9]\ \ [10]\ \ [11]
-\ \ \ \\\ \ \ |\ \ \ /
-\ \ \ All\ to\ all
-\ \ \ /\ \ \ |\ \ \ \\
-[1]\ \ \ [5]\ \ \ [9]
-[2]\ \ \ [6]\ \ [10]
-[3]\ \ \ [7]\ \ [11]
-\f[]
+[1]   [2]   [3]
+[5]   [6]   [7]
+[9]  [10]  [11]
+   \[rs]   |   /
+   All to all
+   /   |   \[rs]
+[1]   [5]   [9]
+[2]   [6]  [10]
+[3]   [7]  [11]
+\f[R]
 .fi
 .PP
 Each peer sends a piece of its data to the other peers.
@@ -383,17 +337,17 @@ involving summing an array of integers between three peers.
 .IP
 .nf
 \f[C]
-\ [1]\ \ [1]\ \ [1]
-\ [5]\ \ [5]\ \ [5]
-\ [9]\ \ [9]\ \ [9]
-\ \ \ \\\ \ \ |\ \ \ /
-\ \ \ \ \ \ sum
-\ \ \ /\ \ \ |\ \ \ \\
-\ [3]\ \ [3]\ \ [3]
-[15]\ [15]\ [15]
-[27]\ [27]\ [27]
-\ \ All\ Reduce
-\f[]
+ [1]  [1]  [1]
+ [5]  [5]  [5]
+ [9]  [9]  [9]
+   \[rs]   |   /
+      sum
+   /   |   \[rs]
+ [3]  [3]  [3]
+[15] [15] [15]
+[27] [27] [27]
+  All Reduce
+\f[R]
 .fi
 .SS All Gather (fi_allgather)
 .PP
@@ -404,14 +358,14 @@ that array back to each peer.
 .IP
 .nf
 \f[C]
-[1]\ \ [5]\ \ [9]
-\ \ \\\ \ \ |\ \ \ /
-\ All\ gather
-\ \ /\ \ \ |\ \ \ \\
-[1]\ \ [1]\ \ [1]
-[5]\ \ [5]\ \ [5]
-[9]\ \ [9]\ \ [9]
-\f[]
+[1]  [5]  [9]
+  \[rs]   |   /
+ All gather
+  /   |   \[rs]
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+\f[R]
 .fi
 .PP
 All gather may be performed on any non\-void datatype.
@@ -430,20 +384,20 @@ This is shown by the following example:
 .IP
 .nf
 \f[C]
-[1]\ \ [1]\ \ [1]
-[5]\ \ [5]\ \ [5]
-[9]\ \ [9]\ \ [9]
-\ \ \\\ \ \ |\ \ \ /
-\ \ \ \ \ sum\ (reduce)
-\ \ \ \ \ \ |
-\ \ \ \ \ [3]
-\ \ \ \ [15]
-\ \ \ \ [27]
-\ \ \ \ \ \ |
-\ \ \ scatter
-\ \ /\ \ \ |\ \ \ \\
-[3]\ [15]\ [27]
-\f[]
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+  \[rs]   |   /
+     sum (reduce)
+      |
+     [3]
+    [15]
+    [27]
+      |
+   scatter
+  /   |   \[rs]
+[3] [15] [27]
+\f[R]
 .fi
 .PP
 The reduce scatter call supports the same datatype and atomic operation
@@ -452,23 +406,23 @@ as fi_allreduce.
 .PP
 The fi_reduce collective is the first half of an fi_allreduce operation.
 With reduce, all peers provide input into an atomic operation, with the
-the results collected by a single \[aq]root\[aq] endpoint.
+the results collected by a single `root' endpoint.
 .PP
 This is shown by the following example, with the leftmost peer
 identified as the root:
 .IP
 .nf
 \f[C]
-[1]\ \ [1]\ \ [1]
-[5]\ \ [5]\ \ [5]
-[9]\ \ [9]\ \ [9]
-\ \ \\\ \ \ |\ \ \ /
-\ \ \ \ \ sum\ (reduce)
-\ \ \ \ /
-\ [3]
+[1]  [1]  [1]
+[5]  [5]  [5]
+[9]  [9]  [9]
+  \[rs]   |   /
+     sum (reduce)
+    /
+ [3]
 [15]
 [27]
-\f[]
+\f[R]
 .fi
 .PP
 The reduce call supports the same datatype and atomic operation as
@@ -477,21 +431,21 @@ fi_allreduce.
 .PP
 The fi_scatter collective is the second half of an fi_reduce_scatter
 operation.
-The data from a single \[aq]root\[aq] endpoint is split and distributed
-to all peers.
+The data from a single `root' endpoint is split and distributed to all
+peers.
 .PP
 This is shown by the following example:
 .IP
 .nf
 \f[C]
-\ [3]
+ [3]
 [15]
 [27]
-\ \ \ \ \\
-\ \ \ scatter
-\ \ /\ \ \ |\ \ \ \\
-[3]\ [15]\ [27]
-\f[]
+    \[rs]
+   scatter
+  /   |   \[rs]
+[3] [15] [27]
+\f[R]
 .fi
 .PP
 The scatter operation is used to distribute results to the peers.
@@ -499,21 +453,21 @@ No atomic operation is performed on the data.
 .SS Gather (fi_gather)
 .PP
 The fi_gather operation is used to collect (gather) the results from all
-peers and store them at a \[aq]root\[aq] peer.
+peers and store them at a `root' peer.
 .PP
 This is shown by the following example, with the leftmost peer
 identified as the root.
 .IP
 .nf
 \f[C]
-[1]\ \ [5]\ \ [9]
-\ \ \\\ \ \ |\ \ \ /
-\ \ \ \ gather
-\ \ \ /
+[1]  [5]  [9]
+  \[rs]   |   /
+    gather
+   /
 [1]
 [5]
 [9]
-\f[]
+\f[R]
 .fi
 .PP
 The gather operation does not perform any operation on the data itself.
@@ -540,19 +494,19 @@ must be specified through the given attributes.
 .IP
 .nf
 \f[C]
-struct\ fi_collective_attr\ {
-\ \ \ \ enum\ fi_op\ op;
-\ \ \ \ enum\ fi_datatype\ datatype;
-\ \ \ \ struct\ fi_atomic_attr\ datatype_attr;
-\ \ \ \ size_t\ max_members;
-\ \ \ \ \ \ uint64_t\ mode;
+struct fi_collective_attr {
+    enum fi_op op;
+    enum fi_datatype datatype;
+    struct fi_atomic_attr datatype_attr;
+    size_t max_members;
+      uint64_t mode;
 };
-\f[]
+\f[R]
 .fi
 .PP
-For a description of struct fi_atomic_attr, see \f[C]fi_atomic\f[](3).
+For a description of struct fi_atomic_attr, see \f[C]fi_atomic\f[R](3).
 .TP
-.B \f[I]op\f[]
+.B \f[I]op\f[R]
 On input, this specifies the atomic operation involved with the
 collective call.
 This should be set to one of the following values: FI_MIN, FI_MAX,
@@ -560,10 +514,8 @@ FI_SUM, FI_PROD, FI_LOR, FI_LAND, FI_BOR, FI_BAND, FI_LXOR, FI_BXOR,
 FI_ATOMIC_READ, FI_ATOMIC_WRITE, of FI_NOOP.
 For collectives that do not exchange application data (fi_barrier), this
 should be set to FI_NOOP.
-.RS
-.RE
 .TP
-.B \f[I]datatype\f[]
+.B \f[I]datatype\f[R]
 On onput, specifies the datatype of the data being modified by the
 collective.
 This should be set to one of the following values: FI_INT8, FI_UINT8,
@@ -572,31 +524,21 @@ FI_DOUBLE, FI_FLOAT_COMPLEX, FI_DOUBLE_COMPLEX, FI_LONG_DOUBLE,
 FI_LONG_DOUBLE_COMPLEX, or FI_VOID.
 For collectives that do not exchange application data (fi_barrier), this
 should be set to FI_VOID.
-.RS
-.RE
 .TP
-.B \f[I]datatype_attr.count\f[]
+.B \f[I]datatype_attr.count\f[R]
 The maximum number of elements that may be used with the collective.
-.RS
-.RE
 .TP
-.B \f[I]datatype.size\f[]
+.B \f[I]datatype.size\f[R]
 The size of the datatype as supported by the provider.
 Applications should validate the size of datatypes that differ based on
 the platform, such as FI_LONG_DOUBLE.
-.RS
-.RE
 .TP
-.B \f[I]max_members\f[]
+.B \f[I]max_members\f[R]
 The maximum number of peers that may participate in a collective
 operation.
-.RS
-.RE
 .TP
-.B \f[I]mode\f[]
+.B \f[I]mode\f[R]
 This field is reserved and should be 0.
-.RS
-.RE
 .PP
 If a collective operation is supported, the query call will return
 FI_SUCCESS, along with attributes on the limits for using that
@@ -605,41 +547,34 @@ collective operation through the provider.
 .PP
 Collective operations map to underlying fi_atomic operations.
 For a discussion of atomic completion semantics, see
-\f[C]fi_atomic\f[](3).
+\f[C]fi_atomic\f[R](3).
 The completion, ordering, and atomicity of collective operations match
 those defined for point to point atomic operations.
 .SH FLAGS
 .PP
 The following flags are defined for the specified operations.
 .TP
-.B \f[I]FI_SCATTER\f[]
+.B \f[I]FI_SCATTER\f[R]
 Applies to fi_query_collective.
 When set, requests attribute information on the reduce\-scatter
 collective operation.
-.RS
-.RE
 .SH RETURN VALUE
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .TP
-.B \f[I]\-FI_EAGAIN\f[]
-See \f[C]fi_msg\f[](3) for a detailed description of handling FI_EAGAIN.
-.RS
-.RE
+.B \f[I]\-FI_EAGAIN\f[R]
+See \f[C]fi_msg\f[R](3) for a detailed description of handling
+FI_EAGAIN.
 .TP
-.B \f[I]\-FI_EOPNOTSUPP\f[]
+.B \f[I]\-FI_EOPNOTSUPP\f[R]
 The requested atomic operation is not supported on this endpoint.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_EMSGSIZE\f[]
+.B \f[I]\-FI_EMSGSIZE\f[R]
 The number of collective operations in a single request exceeds that
 supported by the underlying provider.
-.RS
-.RE
 .SH NOTES
 .PP
 Collective operations map to atomic operations.
@@ -647,11 +582,11 @@ As such, they follow most of the conventions and restrictions as peer to
 peer atomic operations.
 This includes data atomicity, data alignment, and message ordering
 semantics.
-See \f[C]fi_atomic\f[](3) for additional information on the datatypes
+See \f[C]fi_atomic\f[R](3) for additional information on the datatypes
 and operations defined for atomic and collective operations.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_av\f[](3), \f[C]fi_atomic\f[](3),
-\f[C]fi_cm\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_av\f[R](3), \f[C]fi_atomic\f[R](3),
+\f[C]fi_cm\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_control.3 b/man/man3/fi_control.3
index 0ae4b52..8292417 100644
--- a/man/man3/fi_control.3
+++ b/man/man3/fi_control.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_control" "3" "2020\-11\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_control" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,30 +9,24 @@ fi_control \- Perform an operation on a fabric resource.
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
+#include <rdma/fabric.h>
 
-int\ fi_control(struct\ fid\ *fid,\ int\ command,\ void\ *arg);
-int\ fi_alias(struct\ fid\ *fid,\ struct\ fid\ **alias_fid,\ uint64_t\ flags);
-int\ fi_get_val(struct\ fid\ *fid,\ int\ name,\ void\ *val);
-int\ fi_set_val(struct\ fid\ *fid,\ int\ name,\ void\ *val);
-\f[]
+int fi_control(struct fid *fid, int command, void *arg);
+int fi_alias(struct fid *fid, struct fid **alias_fid, uint64_t flags);
+int fi_get_val(struct fid *fid, int name, void *val);
+int fi_set_val(struct fid *fid, int name, void *val);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]fid\f[]
+.B \f[I]fid\f[R]
 Fabric resource
-.RS
-.RE
 .TP
-.B \f[I]command\f[]
+.B \f[I]command\f[R]
 Operation to perform
-.RS
-.RE
 .TP
-.B \f[I]arg\f[]
+.B \f[I]arg\f[R]
 Optional argument to the command
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 The fi_control operation is used to perform one or more operations on a
@@ -52,11 +46,11 @@ fabric resource, while fi_set_val updates that value.
 Available parameter names depend on the type of the fabric resource and
 the provider in use.
 Providers may define provider specific names in the provider extension
-header files (\[aq]rdma/fi_ext_*.h\[aq]).
+header files (\[cq]rdma/fi_ext_*.h\[cq]).
 Please refer to the provider man pages for details.
 .SH SEE ALSO
 .PP
-\f[C]fi_endpoint\f[](3), \f[C]fi_cm\f[](3), \f[C]fi_cntr\f[](3),
-\f[C]fi_cq\f[](3), \f[C]fi_eq\f[](3),
+\f[C]fi_endpoint\f[R](3), \f[C]fi_cm\f[R](3), \f[C]fi_cntr\f[R](3),
+\f[C]fi_cq\f[R](3), \f[C]fi_eq\f[R](3),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_cq.3 b/man/man3/fi_cq.3
index b84308d..5d7201d 100644
--- a/man/man3/fi_cq.3
+++ b/man/man3/fi_cq.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_cq" "3" "2019\-12\-13" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_cq" "3" "2021\-03\-23" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,147 +8,105 @@ fi_cq \- Completion queue operations
 .TP
 .B fi_cq_open / fi_close
 Open/close a completion queue
-.RS
-.RE
 .TP
 .B fi_control
 Control CQ operation or attributes.
-.RS
-.RE
 .TP
 .B fi_cq_read / fi_cq_readfrom / fi_cq_readerr
 Read a completion from a completion queue
-.RS
-.RE
 .TP
 .B fi_cq_sread / fi_cq_sreadfrom
 A synchronous (blocking) read that waits until a specified condition has
 been met before reading a completion from a completion queue.
-.RS
-.RE
 .TP
 .B fi_cq_signal
 Unblock any thread waiting in fi_cq_sread or fi_cq_sreadfrom.
-.RS
-.RE
 .TP
 .B fi_cq_strerror
 Converts provider specific error information into a printable string
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_cq_open(struct\ fid_domain\ *domain,\ struct\ fi_cq_attr\ *attr,
-\ \ \ \ struct\ fid_cq\ **cq,\ void\ *context);
+int fi_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
+    struct fid_cq **cq, void *context);
 
-int\ fi_close(struct\ fid\ *cq);
+int fi_close(struct fid *cq);
 
-int\ fi_control(struct\ fid\ *cq,\ int\ command,\ void\ *arg);
+int fi_control(struct fid *cq, int command, void *arg);
 
-ssize_t\ fi_cq_read(struct\ fid_cq\ *cq,\ void\ *buf,\ size_t\ count);
+ssize_t fi_cq_read(struct fid_cq *cq, void *buf, size_t count);
 
-ssize_t\ fi_cq_readfrom(struct\ fid_cq\ *cq,\ void\ *buf,\ size_t\ count,
-\ \ \ \ fi_addr_t\ *src_addr);
+ssize_t fi_cq_readfrom(struct fid_cq *cq, void *buf, size_t count,
+    fi_addr_t *src_addr);
 
-ssize_t\ fi_cq_readerr(struct\ fid_cq\ *cq,\ struct\ fi_cq_err_entry\ *buf,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_cq_readerr(struct fid_cq *cq, struct fi_cq_err_entry *buf,
+    uint64_t flags);
 
-ssize_t\ fi_cq_sread(struct\ fid_cq\ *cq,\ void\ *buf,\ size_t\ count,
-\ \ \ \ const\ void\ *cond,\ int\ timeout);
+ssize_t fi_cq_sread(struct fid_cq *cq, void *buf, size_t count,
+    const void *cond, int timeout);
 
-ssize_t\ fi_cq_sreadfrom(struct\ fid_cq\ *cq,\ void\ *buf,\ size_t\ count,
-\ \ \ \ fi_addr_t\ *src_addr,\ const\ void\ *cond,\ int\ timeout);
+ssize_t fi_cq_sreadfrom(struct fid_cq *cq, void *buf, size_t count,
+    fi_addr_t *src_addr, const void *cond, int timeout);
 
-int\ fi_cq_signal(struct\ fid_cq\ *cq);
+int fi_cq_signal(struct fid_cq *cq);
 
-const\ char\ *\ fi_cq_strerror(struct\ fid_cq\ *cq,\ int\ prov_errno,
-\ \ \ \ \ \ const\ void\ *err_data,\ char\ *buf,\ size_t\ len);
-\f[]
+const char * fi_cq_strerror(struct fid_cq *cq, int prov_errno,
+      const void *err_data, char *buf, size_t len);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]domain\f[]
+.B \f[I]domain\f[R]
 Open resource domain
-.RS
-.RE
 .TP
-.B \f[I]cq\f[]
+.B \f[I]cq\f[R]
 Completion queue
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Completion queue attributes
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the completion queue.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 For read calls, the data buffer to write completions into.
 For write calls, a completion to insert into the completion queue.
 For fi_cq_strerror, an optional buffer that receives printable error
 information.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Number of CQ entries.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Length of data buffer
-.RS
-.RE
 .TP
-.B \f[I]src_addr\f[]
+.B \f[I]src_addr\f[R]
 Source address of a completed receive operation
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply to the operation
-.RS
-.RE
 .TP
-.B \f[I]command\f[]
+.B \f[I]command\f[R]
 Command of control operation to perform on CQ.
-.RS
-.RE
 .TP
-.B \f[I]arg\f[]
+.B \f[I]arg\f[R]
 Optional control argument
-.RS
-.RE
 .TP
-.B \f[I]cond\f[]
+.B \f[I]cond\f[R]
 Condition that must be met before a completion is generated
-.RS
-.RE
 .TP
-.B \f[I]timeout\f[]
+.B \f[I]timeout\f[R]
 Time in milliseconds to wait.
 A negative value indicates infinite timeout.
-.RS
-.RE
 .TP
-.B \f[I]prov_errno\f[]
+.B \f[I]prov_errno\f[R]
 Provider specific error value
-.RS
-.RE
 .TP
-.B \f[I]err_data\f[]
+.B \f[I]err_data\f[R]
 Provider specific error data related to a completion
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Completion queues are used to report events associated with data
@@ -165,39 +123,33 @@ Unlike event queues, completion queues are associated with a resource
 domain and may be offloaded entirely in provider hardware.
 .PP
 The properties and behavior of a completion queue are defined by
-\f[C]struct\ fi_cq_attr\f[].
+\f[C]struct fi_cq_attr\f[R].
 .IP
 .nf
 \f[C]
-struct\ fi_cq_attr\ {
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ size;\ \ \ \ \ \ /*\ #\ entries\ for\ CQ\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ flags;\ \ \ \ \ /*\ operation\ flags\ */
-\ \ \ \ enum\ fi_cq_format\ \ \ \ format;\ \ \ \ /*\ completion\ format\ */
-\ \ \ \ enum\ fi_wait_obj\ \ \ \ \ wait_obj;\ \ /*\ requested\ wait\ object\ */
-\ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ signaling_vector;\ /*\ interrupt\ affinity\ */
-\ \ \ \ enum\ fi_cq_wait_cond\ wait_cond;\ /*\ wait\ condition\ format\ */
-\ \ \ \ struct\ fid_wait\ \ \ \ \ *wait_set;\ \ /*\ optional\ wait\ set\ */
+struct fi_cq_attr {
+    size_t               size;      /* # entries for CQ */
+    uint64_t             flags;     /* operation flags */
+    enum fi_cq_format    format;    /* completion format */
+    enum fi_wait_obj     wait_obj;  /* requested wait object */
+    int                  signaling_vector; /* interrupt affinity */
+    enum fi_cq_wait_cond wait_cond; /* wait condition format */
+    struct fid_wait     *wait_set;  /* optional wait set */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]size\f[]
+.B \f[I]size\f[R]
 Specifies the minimum size of a completion queue.
 A value of 0 indicates that the provider may choose a default value.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Flags that control the configuration of the CQ.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_AFFINITY\f[]
+.B \- \f[I]FI_AFFINITY\f[R]
 Indicates that the signaling_vector field (see below) is valid.
-.RS
-.RE
 .TP
-.B \f[I]format\f[]
+.B \f[I]format\f[R]
 Completion queues allow the application to select the amount of detail
 that it must store and report.
 The format attribute allows the application to select one of several
@@ -206,92 +158,80 @@ completion queue should return when read.
 Supported formats and the structures that correspond to each are listed
 below.
 The meaning of the CQ entry fields are defined in the \f[I]Completion
-Fields\f[] section.
-.RS
-.RE
+Fields\f[R] section.
 .TP
-.B \- \f[I]FI_CQ_FORMAT_UNSPEC\f[]
+.B \- \f[I]FI_CQ_FORMAT_UNSPEC\f[R]
 If an unspecified format is requested, then the CQ will use a provider
 selected default format.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_CQ_FORMAT_CONTEXT\f[]
+.B \- \f[I]FI_CQ_FORMAT_CONTEXT\f[R]
 Provides only user specified context that was associated with the
 completion.
-.RS
-.RE
 .IP
 .nf
 \f[C]
-struct\ fi_cq_entry\ {
-\ \ \ \ void\ \ \ \ \ *op_context;\ /*\ operation\ context\ */
+struct fi_cq_entry {
+    void     *op_context; /* operation context */
 };
-\f[]
+\f[R]
 .fi
 \[bu] .RS 2
 .TP
-.B \f[I]FI_CQ_FORMAT_MSG\f[]
+.B \f[I]FI_CQ_FORMAT_MSG\f[R]
 Provides minimal data for processing completions, with expanded support
 for reporting information about received messages.
-.RS
-.RE
 .RE
 .IP
 .nf
 \f[C]
-struct\ fi_cq_msg_entry\ {
-\ \ \ \ void\ \ \ \ \ *op_context;\ /*\ operation\ context\ */
-\ \ \ \ uint64_t\ flags;\ \ \ \ \ \ \ /*\ completion\ flags\ */
-\ \ \ \ size_t\ \ \ len;\ \ \ \ \ \ \ \ \ /*\ size\ of\ received\ data\ */
+struct fi_cq_msg_entry {
+    void     *op_context; /* operation context */
+    uint64_t flags;       /* completion flags */
+    size_t   len;         /* size of received data */
 };
-\f[]
+\f[R]
 .fi
 \[bu] .RS 2
 .TP
-.B \f[I]FI_CQ_FORMAT_DATA\f[]
+.B \f[I]FI_CQ_FORMAT_DATA\f[R]
 Provides data associated with a completion.
 Includes support for received message length, remote CQ data, and
 multi\-receive buffers.
-.RS
-.RE
 .RE
 .IP
 .nf
 \f[C]
-struct\ fi_cq_data_entry\ {
-\ \ \ \ void\ \ \ \ \ *op_context;\ /*\ operation\ context\ */
-\ \ \ \ uint64_t\ flags;\ \ \ \ \ \ \ /*\ completion\ flags\ */
-\ \ \ \ size_t\ \ \ len;\ \ \ \ \ \ \ \ \ /*\ size\ of\ received\ data\ */
-\ \ \ \ void\ \ \ \ \ *buf;\ \ \ \ \ \ \ \ /*\ receive\ data\ buffer\ */
-\ \ \ \ uint64_t\ data;\ \ \ \ \ \ \ \ /*\ completion\ data\ */
+struct fi_cq_data_entry {
+    void     *op_context; /* operation context */
+    uint64_t flags;       /* completion flags */
+    size_t   len;         /* size of received data */
+    void     *buf;        /* receive data buffer */
+    uint64_t data;        /* completion data */
 };
-\f[]
+\f[R]
 .fi
 \[bu] .RS 2
 .TP
-.B \f[I]FI_CQ_FORMAT_TAGGED\f[]
+.B \f[I]FI_CQ_FORMAT_TAGGED\f[R]
 Expands completion data to include support for the tagged message
 interfaces.
-.RS
-.RE
 .RE
 .IP
 .nf
 \f[C]
-struct\ fi_cq_tagged_entry\ {
-\ \ \ \ void\ \ \ \ \ *op_context;\ /*\ operation\ context\ */
-\ \ \ \ uint64_t\ flags;\ \ \ \ \ \ \ /*\ completion\ flags\ */
-\ \ \ \ size_t\ \ \ len;\ \ \ \ \ \ \ \ \ /*\ size\ of\ received\ data\ */
-\ \ \ \ void\ \ \ \ \ *buf;\ \ \ \ \ \ \ \ /*\ receive\ data\ buffer\ */
-\ \ \ \ uint64_t\ data;\ \ \ \ \ \ \ \ /*\ completion\ data\ */
-\ \ \ \ uint64_t\ tag;\ \ \ \ \ \ \ \ \ /*\ received\ tag\ */
+struct fi_cq_tagged_entry {
+    void     *op_context; /* operation context */
+    uint64_t flags;       /* completion flags */
+    size_t   len;         /* size of received data */
+    void     *buf;        /* receive data buffer */
+    uint64_t data;        /* completion data */
+    uint64_t tag;         /* received tag */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]wait_obj\f[]
-CQ\[aq]s may be associated with a specific wait object.
+.B \f[I]wait_obj\f[R]
+CQ\[cq]s may be associated with a specific wait object.
 Wait objects allow applications to block until the wait object is
 signaled, indicating that a completion is available to be read.
 Users may use fi_control to retrieve the underlying wait object
@@ -300,18 +240,14 @@ The following values may be used to specify the type of wait object
 associated with a CQ: FI_WAIT_NONE, FI_WAIT_UNSPEC, FI_WAIT_SET,
 FI_WAIT_FD, FI_WAIT_MUTEX_COND, and FI_WAIT_YIELD.
 The default is FI_WAIT_NONE.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_NONE\f[]
+.B \- \f[I]FI_WAIT_NONE\f[R]
 Used to indicate that the user will not block (wait) for completions on
 the CQ.
 When FI_WAIT_NONE is specified, the application may not call fi_cq_sread
 or fi_cq_sreadfrom.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_UNSPEC\f[]
+.B \- \f[I]FI_WAIT_UNSPEC\f[R]
 Specifies that the user will only wait on the CQ using fabric interface
 calls, such as fi_cq_sread or fi_cq_sreadfrom.
 In this case, the underlying provider may select the most appropriate or
@@ -319,49 +255,37 @@ highest performing wait object available, including custom wait
 mechanisms.
 Applications that select FI_WAIT_UNSPEC are not guaranteed to retrieve
 the underlying wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_SET\f[]
+.B \- \f[I]FI_WAIT_SET\f[R]
 Indicates that the completion queue should use a wait set object to wait
 for completions.
 If specified, the wait_set field must reference an existing wait set
 object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_FD\f[]
+.B \- \f[I]FI_WAIT_FD\f[R]
 Indicates that the CQ should use a file descriptor as its wait
 mechanism.
 A file descriptor wait object must be usable in select, poll, and epoll
 routines.
 However, a provider may signal an FD wait object by marking it as
 readable, writable, or with an error.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_MUTEX_COND\f[]
+.B \- \f[I]FI_WAIT_MUTEX_COND\f[R]
 Specifies that the CQ should use a pthread mutex and cond variable as a
 wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_YIELD\f[]
+.B \- \f[I]FI_WAIT_YIELD\f[R]
 Indicates that the CQ will wait without a wait object but instead yield
 on every wait.
 Allows usage of fi_cq_sread and fi_cq_sreadfrom through a spin.
-.RS
-.RE
 .TP
-.B \f[I]signaling_vector\f[]
+.B \f[I]signaling_vector\f[R]
 If the FI_AFFINITY flag is set, this indicates the logical cpu number
 (0..max cpu \- 1) that interrupts associated with the CQ should target.
 This field should be treated as a hint to the provider and may be
 ignored if the provider does not support interrupt affinity.
-.RS
-.RE
 .TP
-.B \f[I]wait_cond\f[]
+.B \f[I]wait_cond\f[R]
 By default, when a completion is inserted into a CQ that supports
 blocking reads (fi_cq_sread/fi_cq_sreadfrom), the corresponding wait
 object is signaled.
@@ -369,8 +293,6 @@ Users may specify a condition that must first be met before the wait is
 satisfied.
 This field indicates how the provider should interpret the cond field,
 which describes the condition needed to signal the wait object.
-.RS
-.RE
 .PP
 A wait condition should be treated as an optimization.
 Providers are not required to meet the requirements of the condition
@@ -388,7 +310,7 @@ before at the CQ before the wait is satisfied.
 .PP
 This field is ignored if wait_obj is set to FI_WAIT_NONE.
 .TP
-.B \f[I]wait_set\f[]
+.B \f[I]wait_set\f[R]
 If wait_obj is FI_WAIT_SET, this field references a wait object to which
 the completion queue should attach.
 When an event is inserted into the completion queue, the corresponding
@@ -396,8 +318,6 @@ wait set will be signaled if all necessary conditions are met.
 The use of a wait_set enables an optimized method of waiting for events
 across multiple event and completion queues.
 This field is ignored if wait_obj is not FI_WAIT_SET.
-.RS
-.RE
 .SS fi_close
 .PP
 The fi_close call releases all resources associated with a completion
@@ -416,7 +336,7 @@ Access to the CQ should be serialized across all calls when fi_control
 is invoked, as it may redirect the implementation of CQ operations.
 The following control commands are usable with a CQ.
 .TP
-.B \f[I]FI_GETWAIT (void **)\f[]
+.B \f[I]FI_GETWAIT (void **)\f[R]
 This command allows the user to retrieve the low\-level wait object
 associated with the CQ.
 The format of the wait\-object is specified during CQ creation, through
@@ -424,8 +344,6 @@ the CQ attributes.
 The fi_control arg parameter should be an address where a pointer to the
 returned wait object will be written.
 See fi_eq.3 for addition details using fi_control with FI_GETWAIT.
-.RS
-.RE
 .SS fi_cq_read
 .PP
 The fi_cq_read operation performs a non\-blocking read of completion
@@ -440,7 +358,7 @@ CQ returned by the call.
 .PP
 CQs are optimized to report operations which have completed
 successfully.
-Operations which fail are reported \[aq]out of band\[aq].
+Operations which fail are reported `out of band'.
 Such operations are retrieved using the fi_cq_readerr function.
 When an operation that has completed with an unexpected error is
 encountered, it is placed into a temporary error queue.
@@ -510,25 +428,25 @@ fi_cq_readerr is a non\-blocking call, returning immediately whether an
 error completion was found or not.
 .PP
 Error information is reported to the user through
-\f[C]struct\ fi_cq_err_entry\f[].
+\f[C]struct fi_cq_err_entry\f[R].
 The format of this structure is defined below.
 .IP
 .nf
 \f[C]
-struct\ fi_cq_err_entry\ {
-\ \ \ \ void\ \ \ \ \ *op_context;\ /*\ operation\ context\ */
-\ \ \ \ uint64_t\ flags;\ \ \ \ \ \ \ /*\ completion\ flags\ */
-\ \ \ \ size_t\ \ \ len;\ \ \ \ \ \ \ \ \ /*\ size\ of\ received\ data\ */
-\ \ \ \ void\ \ \ \ \ *buf;\ \ \ \ \ \ \ \ /*\ receive\ data\ buffer\ */
-\ \ \ \ uint64_t\ data;\ \ \ \ \ \ \ \ /*\ completion\ data\ */
-\ \ \ \ uint64_t\ tag;\ \ \ \ \ \ \ \ \ /*\ message\ tag\ */
-\ \ \ \ size_t\ \ \ olen;\ \ \ \ \ \ \ \ /*\ overflow\ length\ */
-\ \ \ \ int\ \ \ \ \ \ err;\ \ \ \ \ \ \ \ \ /*\ positive\ error\ code\ */
-\ \ \ \ int\ \ \ \ \ \ prov_errno;\ \ /*\ provider\ error\ code\ */
-\ \ \ \ void\ \ \ \ *err_data;\ \ \ \ /*\ \ error\ data\ */
-\ \ \ \ size_t\ \ \ err_data_size;\ /*\ size\ of\ err_data\ */
+struct fi_cq_err_entry {
+    void     *op_context; /* operation context */
+    uint64_t flags;       /* completion flags */
+    size_t   len;         /* size of received data */
+    void     *buf;        /* receive data buffer */
+    uint64_t data;        /* completion data */
+    uint64_t tag;         /* message tag */
+    size_t   olen;        /* overflow length */
+    int      err;         /* positive error code */
+    int      prov_errno;  /* provider error code */
+    void    *err_data;    /*  error data */
+    size_t   err_data_size; /* size of err_data */
 };
-\f[]
+\f[R]
 .fi
 .PP
 The general reason for the error is provided through the err field.
@@ -540,9 +458,9 @@ See field details below for more information on the use of err_data and
 err_data_size.
 .PP
 Note that error completions are generated for all operations, including
-those for which a completion was not requested (e.g.
-an endpoint is configured with FI_SELECTIVE_COMPLETION, but the request
-did not have the FI_COMPLETION flag set).
+those for which a completion was not requested (e.g.\ an endpoint is
+configured with FI_SELECTIVE_COMPLETION, but the request did not have
+the FI_COMPLETION flag set).
 In such cases, providers will return as much information as made
 available by the underlying software and hardware about the failure,
 other fields will be set to NULL or 0.
@@ -551,7 +469,7 @@ was ignored on input as part of the transfer.
 .PP
 Notable completion error codes are given below.
 .TP
-.B \f[I]FI_EADDRNOTAVAIL\f[]
+.B \f[I]FI_EADDRNOTAVAIL\f[R]
 This error code is used by CQs configured with FI_SOURCE_ERR to report
 completions for which a usable fi_addr_t source address could not be
 found.
@@ -563,8 +481,6 @@ The source address will be in the same format as specified through the
 fi_info addr_format field for the opened domain.
 This may be passed directly into an fi_av_insert call to add the source
 address to the address vector.
-.RS
-.RE
 .SS fi_cq_signal
 .PP
 The fi_cq_signal call will unblock any thread waiting in fi_cq_sread or
@@ -579,13 +495,11 @@ The CQ entry data structures share many of the same fields.
 The meanings of these fields are the same for all CQ entry structure
 formats.
 .TP
-.B \f[I]op_context\f[]
+.B \f[I]op_context\f[R]
 The operation context is the application specified context value that
 was provided with an asynchronous operation.
 The op_context field is valid for all completions that are associated
 with an asynchronous operation.
-.RS
-.RE
 .PP
 For completion events that are not associated with a posted operation,
 this field will be set to NULL.
@@ -593,76 +507,60 @@ This includes completions generated at the target in response to RMA
 write operations that carry CQ data (FI_REMOTE_WRITE | FI_REMOTE_CQ_DATA
 flags set), when the FI_RX_CQ_DATA mode bit is not required.
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 This specifies flags associated with the completed operation.
-The \f[I]Completion Flags\f[] section below lists valid flag values.
+The \f[I]Completion Flags\f[R] section below lists valid flag values.
 Flags are set for all relevant completions.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
-This len field only applies to completed receive operations (e.g.
-fi_recv, fi_trecv, etc.).
-It indicates the size of received \f[I]message\f[] data \-\- i.e.
-how many data bytes were placed into the associated receive buffer by a
+.B \f[I]len\f[R]
+This len field only applies to completed receive operations
+(e.g.\ fi_recv, fi_trecv, etc.).
+It indicates the size of received \f[I]message\f[R] data \[en] i.e.\ how
+many data bytes were placed into the associated receive buffer by a
 corresponding fi_send/fi_tsend/et al call.
 If an endpoint has been configured with the FI_MSG_PREFIX mode, the len
 also reflects the size of the prefix buffer.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 The buf field is only valid for completed receive operations, and only
 applies when the receive buffer was posted with the FI_MULTI_RECV flag.
 In this case, buf points to the starting location where the receive data
 was placed.
-.RS
-.RE
 .TP
-.B \f[I]data\f[]
+.B \f[I]data\f[R]
 The data field is only valid if the FI_REMOTE_CQ_DATA completion flag is
 set, and only applies to receive completions.
 If FI_REMOTE_CQ_DATA is set, this field will contain the completion data
 provided by the peer as part of their transmit request.
 The completion data will be given in host byte order.
-.RS
-.RE
 .TP
-.B \f[I]tag\f[]
+.B \f[I]tag\f[R]
 A tag applies only to received messages that occur using the tagged
 interfaces.
 This field contains the tag that was included with the received message.
 The tag will be in host byte order.
-.RS
-.RE
 .TP
-.B \f[I]olen\f[]
+.B \f[I]olen\f[R]
 The olen field applies to received messages.
 It is used to indicate that a received message has overrun the available
 buffer space and has been truncated.
 The olen specifies the amount of data that did not fit into the
 available receive buffer and was discarded.
-.RS
-.RE
 .TP
-.B \f[I]err\f[]
+.B \f[I]err\f[R]
 This err code is a positive fabric errno associated with a completion.
 The err value indicates the general reason for an error, if one
 occurred.
 See fi_errno.3 for a list of possible error codes.
-.RS
-.RE
 .TP
-.B \f[I]prov_errno\f[]
+.B \f[I]prov_errno\f[R]
 On an error, prov_errno may contain a provider specific error code.
 The use of this field and its meaning is provider specific.
 It is intended to be used as a debugging aid.
 See fi_cq_strerror for additional details on converting this error value
 into a human readable string.
-.RS
-.RE
 .TP
-.B \f[I]err_data\f[]
+.B \f[I]err_data\f[R]
 The err_data field is used to return provider specific information, if
 available, about the error.
 On input, err_data should reference a data buffer of size err_data_size.
@@ -674,18 +572,14 @@ See fi_cq_strerror for additional details on converting this error data
 into a human readable string.
 See the compatibility note below on how this field is used for older
 libfabric releases.
-.RS
-.RE
 .TP
-.B \f[I]err_data_size\f[]
+.B \f[I]err_data_size\f[R]
 On input, err_data_size indicates the size of the err_data buffer in
 bytes.
 On output, err_data_size will be set to the number of bytes copied to
 the err_data buffer.
 The err_data information is typically used with fi_cq_strerror to
 provide details about the type of error that occurred.
-.RS
-.RE
 .PP
 For compatibility purposes, the behavior of the err_data and
 err_data_size fields is may be modified from that listed above.
@@ -703,87 +597,63 @@ Completion flags provide additional details regarding the completed
 operation.
 The following completion flags are defined.
 .TP
-.B \f[I]FI_SEND\f[]
+.B \f[I]FI_SEND\f[R]
 Indicates that the completion was for a send operation.
 This flag may be combined with an FI_MSG or FI_TAGGED flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_RECV\f[]
+.B \f[I]FI_RECV\f[R]
 Indicates that the completion was for a receive operation.
 This flag may be combined with an FI_MSG or FI_TAGGED flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_RMA\f[]
+.B \f[I]FI_RMA\f[R]
 Indicates that an RMA operation completed.
 This flag may be combined with an FI_READ, FI_WRITE, FI_REMOTE_READ, or
 FI_REMOTE_WRITE flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_ATOMIC\f[]
+.B \f[I]FI_ATOMIC\f[R]
 Indicates that an atomic operation completed.
 This flag may be combined with an FI_READ, FI_WRITE, FI_REMOTE_READ, or
 FI_REMOTE_WRITE flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_MSG\f[]
+.B \f[I]FI_MSG\f[R]
 Indicates that a message\-based operation completed.
 This flag may be combined with an FI_SEND or FI_RECV flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_TAGGED\f[]
+.B \f[I]FI_TAGGED\f[R]
 Indicates that a tagged message operation completed.
 This flag may be combined with an FI_SEND or FI_RECV flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_MULTICAST\f[]
+.B \f[I]FI_MULTICAST\f[R]
 Indicates that a multicast operation completed.
 This flag may be combined with FI_MSG and relevant flags.
 This flag is only guaranteed to be valid for received messages if the
 endpoint has been configured with FI_SOURCE.
-.RS
-.RE
 .TP
-.B \f[I]FI_READ\f[]
+.B \f[I]FI_READ\f[R]
 Indicates that a locally initiated RMA or atomic read operation has
 completed.
 This flag may be combined with an FI_RMA or FI_ATOMIC flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_WRITE\f[]
+.B \f[I]FI_WRITE\f[R]
 Indicates that a locally initiated RMA or atomic write operation has
 completed.
 This flag may be combined with an FI_RMA or FI_ATOMIC flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_READ\f[]
+.B \f[I]FI_REMOTE_READ\f[R]
 Indicates that a remotely initiated RMA or atomic read operation has
 completed.
 This flag may be combined with an FI_RMA or FI_ATOMIC flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_WRITE\f[]
+.B \f[I]FI_REMOTE_WRITE\f[R]
 Indicates that a remotely initiated RMA or atomic write operation has
 completed.
 This flag may be combined with an FI_RMA or FI_ATOMIC flag.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_CQ_DATA\f[]
+.B \f[I]FI_REMOTE_CQ_DATA\f[R]
 This indicates that remote CQ data is available as part of the
 completion.
-.RS
-.RE
 .TP
-.B \f[I]FI_MULTI_RECV\f[]
+.B \f[I]FI_MULTI_RECV\f[R]
 This flag applies to receive buffers that were posted with the
 FI_MULTI_RECV flag set.
 This completion flag indicates that the original receive buffer
@@ -792,8 +662,6 @@ provider.
 Providers may set this flag on the last message that is received into
 the multi\- recv buffer, or may generate a separate completion that
 indicates that the buffer has been released.
-.RS
-.RE
 .PP
 Applications can distinguish between these two cases by examining the
 completion entry flags field.
@@ -807,30 +675,26 @@ If other flag bits are zero, the provider is reporting that the
 multi\-recv buffer has been released, and the completion entry is not
 associated with a received message.
 .TP
-.B \f[I]FI_MORE\f[]
-See the \[aq]Buffered Receives\[aq] section in \f[C]fi_msg\f[](3) for
-more details.
+.B \f[I]FI_MORE\f[R]
+See the `Buffered Receives' section in \f[C]fi_msg\f[R](3) for more
+details.
 This flag is associated with receive completions on endpoints that have
 FI_BUFFERED_RECV mode enabled.
 When set to one, it indicates that the buffer referenced by the
 completion is limited by the FI_OPT_BUFFERED_LIMIT threshold, and
 additional message data must be retrieved by the application using an
 FI_CLAIM operation.
-.RS
-.RE
 .TP
-.B \f[I]FI_CLAIM\f[]
-See the \[aq]Buffered Receives\[aq] section in \f[C]fi_msg\f[](3) for
-more details.
+.B \f[I]FI_CLAIM\f[R]
+See the `Buffered Receives' section in \f[C]fi_msg\f[R](3) for more
+details.
 This flag is set on completions associated with receive operations that
 claim buffered receive data.
 Note that this flag only applies to endpoints configured with the
 FI_BUFFERED_RECV mode bit.
-.RS
-.RE
 .SH COMPLETION EVENT SEMANTICS
 .PP
-Libfabric defines several completion \[aq]levels\[aq], identified using
+Libfabric defines several completion `levels', identified using
 operational flags.
 Each flag indicates the soonest that a completion event may be generated
 by a provider, and the assumptions that an application may make upon
@@ -846,32 +710,30 @@ is guaranteed.
 To help understand the conceptual differences in completion levels,
 consider mailing a letter.
 Placing the letter into the local mailbox for pick\-up is similar to
-\[aq]inject complete\[aq].
+`inject complete'.
 Having the letter picked up and dropped off at the destination mailbox
-is equivalent to \[aq]transmit complete\[aq].
-The \[aq]delivery complete\[aq] semantic is a stronger guarantee, with a
-person at the destination signing for the letter.
+is equivalent to `transmit complete'.
+The `delivery complete' semantic is a stronger guarantee, with a person
+at the destination signing for the letter.
 However, the person who signed for the letter is not necessarily the
 intended recipient.
-The \[aq]match complete\[aq] option is similar to delivery complete, but
+The `match complete' option is similar to delivery complete, but
 requires the intended recipient to sign for the letter.
 .PP
-The \[aq]commit complete\[aq] level has different semantics than the
-previously mentioned levels.
+The `commit complete' level has different semantics than the previously
+mentioned levels.
 Commit complete would be closer to the letter arriving at the
 destination and being placed into a fire proof safe.
 .PP
 The operational flags for the described completion levels are defined
 below.
 .TP
-.B \f[I]FI_INJECT_COMPLETE\f[]
+.B \f[I]FI_INJECT_COMPLETE\f[R]
 Indicates that a completion should be generated when the source
 buffer(s) may be reused.
 A completion guarantees that the buffers will not be read from again and
 the application may reclaim them.
 No other guarantees are made with respect to the state of the operation.
-.RS
-.RE
 .PP
 Example: A provider may generate this completion event after copying the
 source buffer into a network buffer, either in host memory or on the
@@ -891,12 +753,10 @@ It does not apply to operations that do not generate a completion queue
 entry, such as the fi_inject operation, and is not subject to the
 inject_size message limit restriction.
 .TP
-.B \f[I]FI_TRANSMIT_COMPLETE\f[]
+.B \f[I]FI_TRANSMIT_COMPLETE\f[R]
 Indicates that a completion should be generated when the transmit
 operation has completed relative to the local provider.
 The exact behavior is dependent on the endpoint type.
-.RS
-.RE
 .PP
 For reliable endpoints:
 .PP
@@ -919,7 +779,7 @@ A completion guarantees that the operation is no longer dependent on
 local resources.
 The state of the operation within the fabric is not defined.
 .TP
-.B \f[I]FI_DELIVERY_COMPLETE\f[]
+.B \f[I]FI_DELIVERY_COMPLETE\f[R]
 Indicates that a completion should not be generated until an operation
 has been processed by the destination endpoint(s).
 A completion guarantees that the result of the operation is available;
@@ -927,14 +787,12 @@ however, additional steps may need to be taken at the destination to
 retrieve the results.
 For example, an application may need to provide a receive buffers in
 order to retrieve messages that were buffered by the provider.
-.RS
-.RE
 .PP
 Delivery complete indicates that the message has been processed by the
 peer.
 If an application buffer was ready to receive the results of the message
 when it arrived, then delivery complete indicates that the data was
-placed into the application\[aq]s buffer.
+placed into the application\[cq]s buffer.
 .PP
 This completion mode applies only to reliable endpoints.
 For operations that return data to the initiator, such as RMA read or
@@ -942,7 +800,7 @@ atomic\-fetch, the source endpoint is also considered a destination
 endpoint.
 This is the default completion mode for such operations.
 .TP
-.B \f[I]FI_MATCH_COMPLETE\f[]
+.B \f[I]FI_MATCH_COMPLETE\f[R]
 Indicates that a completion should be generated only after the operation
 has been matched with an application specified buffer.
 Operations using this completion semantic are dependent on the
@@ -952,22 +810,18 @@ acknowledgements or lengthy delays.
 However, this completion model enables peer applications to synchronize
 their execution.
 Many providers may not support this semantic.
-.RS
-.RE
 .TP
-.B \f[I]FI_COMMIT_COMPLETE\f[]
+.B \f[I]FI_COMMIT_COMPLETE\f[R]
 Indicates that a completion should not be generated (locally or at the
 peer) until the result of an operation have been made persistent.
 A completion guarantees that the result is both available and durable,
 in the case of power failure.
-.RS
-.RE
 .PP
 This completion mode applies only to operations that target persistent
 memory regions over reliable endpoints.
 This completion mode is experimental.
 .TP
-.B \f[I]FI_FENCE\f[]
+.B \f[I]FI_FENCE\f[R]
 This is not a completion level, but plays a role in the completion
 ordering between operations that would not normally be ordered.
 An operation that is marked with the FI_FENCE flag and all operations
@@ -981,14 +835,242 @@ FI_FENCE, then its completion indicates prior operations have met the
 semantic required for FI_DELIVERY_COMPLETE.
 This is true even if the prior operation was posted with a lower
 completion level, such as FI_TRANSMIT_COMPLETE or FI_INJECT_COMPLETE.
-.RS
-.RE
 .PP
 Note that a completion generated for an operation posted prior to the
 fenced operation only guarantees that the completion level that was
 originally requested has been met.
 It is the completion of the fenced operation that guarantees that the
 additional semantics have been met.
+.PP
+The above completion semantics are defined with respect to the initiator
+of the operation.
+The different semantics are useful for describing when the initiator may
+re\-use a data buffer, and guarantees what state a transfer must reach
+prior to a completion being generated.
+This allows applications to determine appropriate error handling in case
+of communication failures.
+.SH TARGET COMPLETION SEMANTICS
+.PP
+The completion semantic at the target is used to determine when data at
+the target is visible to the peer application.
+Visibility indicates that a memory read to the same address that was the
+target of a data transfer will return the results of the transfer.
+The target of a transfer can be identified by the initiator, as may be
+the case for RMA and atomic operations, or determined by the target, for
+example by providing a matching receive buffer.
+Global visibility indicates that the results are available regardless of
+where the memory read originates.
+For example, the read could come from a process running on a host CPU,
+it may be accessed by subsequent data transfer over the fabric, or read
+from a peer device such as a GPU.
+.PP
+In terms of completion semantics, visibility usually indicates that the
+transfer meets the FI_DELIVERY_COMPLETE requirements from the
+perspective of the target.
+The target completion semantic may be, but is not necessarily, linked
+with the completion semantic specified by the initiator of the transfer.
+.PP
+Often, target processes do not explicitly state a desired completion
+semantic and instead rely on the default semantic.
+The default behavior is based on several factors, including:
+.IP \[bu] 2
+whether a completion even is generated at the target
+.IP \[bu] 2
+the type of transfer involved (e.g.\ msg vs RMA)
+.IP \[bu] 2
+endpoint data and message ordering guarantees
+.IP \[bu] 2
+properties of the targeted memory buffer
+.IP \[bu] 2
+the initiator\[cq]s specified completion semantic
+.PP
+Broadly, target completion semantics are grouped based on whether or not
+the transfer generates a completion event at the target.
+This includes writing a CQ entry or updating a completion counter.
+In common use cases, transfers that use a message interface (FI_MSG or
+FI_TAGGED) typically generate target events, while transfers involving
+an RMA interface (FI_RMA or FI_ATOMIC) often do not.
+There are exceptions to both these cases, depending on endpoint to CQ
+and counter bindings and operational flags.
+For example, RMA writes that carry remote CQ data will generate a
+completion event at the target, and are frequently used to convey
+visibility to the target application.
+The general guidelines for target side semantics are described below,
+followed by exceptions that modify that behavior.
+.PP
+By default, completions generated at the target indicate that the
+transferred data is immediately available to be read from the target
+buffer.
+That is, the target sees FI_DELIVERY_COMPLETE (or better) semantics,
+even if the initiator requested lower semantics.
+For applications using only data buffers allocated from host memory,
+this is often sufficient.
+.PP
+For operations that do not generate a completion event at the target,
+the visibility of the data at the target may need to be inferred based
+on subsequent operations that do generate target completions.
+Absent a target completion, when a completion of an operation is written
+at the initiator, the visibility semantic of the operation at the target
+aligns with the initiator completion semantic.
+For instance, if an RMA operation completes at the initiator as either
+FI_INJECT_COMPLETE or FI_TRANSMIT_COMPLETE, the data visibility at the
+target is not guaranteed.
+.PP
+One or more of the following mechanisms can be used by the target
+process to guarantee that the results of a data transfer that did not
+generate a completion at the target is now visible.
+This list is not inclusive of all options, but defines common uses.
+In the descriptions below, the first transfer does not result in a
+completion event at the target, but is eventually followed by a transfer
+which does.
+.IP \[bu] 2
+If the endpoint guarantees message ordering between two transfers, the
+target completion of a second transfer will indicate that the data from
+the first transfer is available.
+For example, if the endpoint supports send after write ordering
+(FI_ORDER_SAW), then a receive completion corresponding to the send will
+indicate that the write data is available.
+This holds independent of the initiator\[cq]s completion semantic for
+either the write or send.
+When ordering is guaranteed, the second transfer can be queued with the
+provider immediately after queuing the first.
+.IP \[bu] 2
+If the endpoint does not guarantee message ordering, the initiator must
+take additional steps to ensure visibility.
+If initiator requests FI_DELIVERY_COMPLETE semantics for the first
+operation, the initiator can wait for the operation to complete locally.
+Once the completion has been read, the target completion of a second
+transfer will indicate that the first transfer\[cq]s data is visible.
+.IP \[bu] 2
+Alternatively, if message ordering is not guaranteed by the endpoint,
+the initiator can use the FI_FENCE and FI_DELIVERY_COMPLETE flags on the
+second data transfer to force the first transfers to meet the
+FI_DELIVERY_COMPLETE semantics.
+If the second transfer generates a completion at the target, that will
+indicate that the data is visible.
+Otherwise, a target completion for any transfer after the fenced
+operation will indicate that the data is visible.
+.PP
+The above semantics apply for transfers targeting traditional host
+memory buffers.
+However, the behavior may differ when device memory and/or persistent
+memory is involved (FI_HMEM and FI_PMEM capability bits).
+When heterogenous memory is involved, the concept of memory domains come
+into play.
+Memory domains identify the physical separation of memory, which may or
+may not be accessible through the same virtual address space.
+See the \f[C]fi_mr\f[R](3) man page for further details on memory
+domains.
+.PP
+Completion ordering and data visibility are only well\-defined for
+transfers that target the same memory domain.
+Applications need to be aware of ordering and visibility differences
+when transfers target different memory domains.
+Additionally, applications also need to be concerned with the memory
+domain that completions themselves are written and if it differs from
+the memory domain targeted by a transfer.
+In some situations, either the provider or application may need to call
+device specific APIs to synchronize or flush device memory caches in
+order to achieve the desired data visibility.
+.PP
+When heterogenous memory is in use, the default target completion
+semantic for transfers that generate a completion at the target is still
+FI_DELIVERY_COMPLETE, however, applications should be aware that there
+may be a negative impact on overall performance for providers to meet
+this requirement.
+.PP
+For example, a target process may be using a GPU to accelerate
+computations.
+A memory region mapping to memory on the GPU may be exposed to peers as
+either an RMA target or posted locally as a receive buffer.
+In this case, the application is concerned with two memory domains \[en]
+system and GPU memory.
+Completions are written to system memory.
+.PP
+Continuing the example, a peer process sends a tagged message.
+That message is matched with the receive buffer located in GPU memory.
+The NIC copies the data from the network into the receive buffer and
+writes an entry into the completion queue.
+Note that both memory domains were accessed as part of this transfer.
+The message data was directed to the GPU memory, but the completion went
+to host memory.
+Because separate memory domains may not be synchronized with each other,
+it is possible for the host CPU to see and process the completion entry
+before the transfer to the GPU memory is visible to either the host GPU
+or even software running on the GPU.
+From the perspective of the \f[I]provider\f[R], visibility of the
+completion does not imply visibility of data written to the GPU\[cq]s
+memory domain.
+.PP
+The default completion semantic at the target \f[I]application\f[R] for
+message operations is FI_DELIVERY_COMPLETE.
+An anticipated provider implementation in this situation is for the
+provider software running on the host CPU to intercept the CQ entry,
+detect that the data landed in heterogenous memory, and perform the
+necessary device synchronization or flush operation before reporting the
+completion up to the application.
+This ensures that the data is visible to CPU \f[I]and\f[R] GPU software
+prior to the application processing the completion.
+.PP
+In addition to the cost of provider software intercepting completions
+and checking if a transfer targeted heterogenous memory, device
+synchronization itself may impact performance.
+As a result, applications can request a lower completion semantic when
+posting receives.
+That indicates to the provider that the application will be responsible
+for handling any device specific flush operations that might be needed.
+See \f[C]fi_msg\f[R](3) FLAGS.
+.PP
+For data transfers that do not generate a completion at the target, such
+as RMA or atomics, it is the responsibility of the application to ensure
+that all target buffers meet the necessary visibility requirements of
+the application.
+The previously mentioned bulleted methods for notifying the target that
+the data is visible may not be sufficient, as the provider software at
+the target could lack the context needed to ensure visibility.
+This implies that the application may need to call device
+synchronization/flush APIs directly.
+.PP
+For example, a peer application could perform several RMA writes that
+target GPU memory buffers.
+If the provider offloads RMA operations into the NIC, the provider
+software at the target will be unaware that the RMA operations have
+occurred.
+If the peer sends a message to the target application that indicates
+that the RMA operations are done, the application must ensure that the
+RMA data is visible to the host CPU or GPU prior to executing code that
+accesses the data.
+The target completion of having received the sent message is not
+sufficient, even if send\-after\-write ordering is supported.
+.PP
+Most target heterogenous memory completion semantics map to
+FI_TRANSMIT_COMPLETE or FI_DELIVERY_COMPLETE.
+Persistent memory (FI_PMEM capability), however, is often used with
+FI_COMMIT_COMPLETE semantics.
+Heterogenous completion concepts still apply.
+.PP
+For transfers flagged by the initiator with FI_COMMIT_COMPLETE, a
+completion at the target indicates that the results are visible and
+durable.
+For transfers targeting persistent memory, but using a different
+completion semantic at the initiator, the visibility at the target is
+similar to that described above.
+Durability is only associated with transfers marked with
+FI_COMMIT_COMPLETE.
+.PP
+For transfers targeting persistent memory that request
+FI_DELIVERY_COMPLETE, then a completion, at either the initiator or
+target, indicates that the data is visible.
+Visibility at the target can be conveyed using one of the above describe
+mechanism \[en] generating a target completion, sending a message from
+the initiator, etc.
+Similarly, if the initiator requested FI_TRANSMIT_COMPLETE, then
+additional steps are needed to ensure visibility at the target.
+For example, the transfer can generate a completion at the target, which
+would indicate visibility, but not durability.
+The initiator can also follow the transfer with another operation that
+forces visibility, such as using FI_FENCE in conjunction with
+FI_DELIVERY_COMPLETE.
 .SH NOTES
 .PP
 A completion queue must be bound to at least one enabled endpoint before
@@ -1004,7 +1086,7 @@ completion data when they are valid: FI_REMOTE_READ and FI_REMOTE_WRITE
 FI_MULTI_RECV.
 .PP
 If a completion queue has been overrun, it will be placed into an
-\[aq]overrun\[aq] state.
+`overrun' state.
 Read operations will continue to return any valid, non\-corrupted
 completions, if available.
 After all valid completions have been retrieved, any attempt to read the
@@ -1016,8 +1098,6 @@ report additional completions once the overrun occurs.
 .B fi_cq_open / fi_cq_signal
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-.RS
-.RE
 .PP
 fi_cq_read / fi_cq_readfrom / fi_cq_readerr fi_cq_sread /
 fi_cq_sreadfrom : On success, returns the number of completion events
@@ -1032,19 +1112,16 @@ completion queue.
 On error, a negative value corresponding to fabric errno is returned.
 If the timeout expires or the calling thread is signaled and no data is
 available to be read from the completion queue, \-FI_EAGAIN is returned.
-.RS
-.RE
 .TP
 .B fi_cq_strerror
 Returns a character string interpretation of the provider specific error
 returned with a completion.
-.RS
-.RE
 .PP
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_eq\f[](3), \f[C]fi_cntr\f[](3), \f[C]fi_poll\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_eq\f[R](3), \f[C]fi_cntr\f[R](3),
+\f[C]fi_poll\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_domain.3 b/man/man3/fi_domain.3
index bc42f00..1869053 100644
--- a/man/man3/fi_domain.3
+++ b/man/man3/fi_domain.3
@@ -1,7 +1,7 @@
 .\"t
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_domain" "3" "2020\-10\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_domain" "3" "2021\-08\-05" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -10,63 +10,49 @@ fi_domain \- Open a fabric access domain
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
+#include <rdma/fabric.h>
 
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_domain(struct\ fid_fabric\ *fabric,\ struct\ fi_info\ *info,
-\ \ \ \ struct\ fid_domain\ **domain,\ void\ *context);
+int fi_domain(struct fid_fabric *fabric, struct fi_info *info,
+    struct fid_domain **domain, void *context);
 
-int\ fi_close(struct\ fid\ *domain);
+int fi_close(struct fid *domain);
 
-int\ fi_domain_bind(struct\ fid_domain\ *domain,\ struct\ fid\ *eq,
-\ \ \ \ uint64_t\ flags);
+int fi_domain_bind(struct fid_domain *domain, struct fid *eq,
+    uint64_t flags);
 
-int\ fi_open_ops(struct\ fid\ *domain,\ const\ char\ *name,\ uint64_t\ flags,
-\ \ \ \ void\ **ops,\ void\ *context);
+int fi_open_ops(struct fid *domain, const char *name, uint64_t flags,
+    void **ops, void *context);
 
-int\ fi_set_ops(struct\ fid\ *domain,\ const\ char\ *name,\ uint64_t\ flags,
-\ \ \ \ void\ *ops,\ void\ *context);
-\f[]
+int fi_set_ops(struct fid *domain, const char *name, uint64_t flags,
+    void *ops, void *context);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]fabric\f[]
+.B \f[I]fabric\f[R]
 Fabric domain
-.RS
-.RE
 .TP
-.B \f[I]info\f[]
+.B \f[I]info\f[R]
 Fabric information, including domain capabilities and attributes.
-.RS
-.RE
 .TP
-.B \f[I]domain\f[]
+.B \f[I]domain\f[R]
 An opened access domain.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the domain.
 This context is returned as part of any asynchronous event associated
 with the domain.
-.RS
-.RE
 .TP
-.B \f[I]eq\f[]
+.B \f[I]eq\f[R]
 Event queue for asynchronous operations initiated on the domain.
-.RS
-.RE
 .TP
-.B \f[I]name\f[]
+.B \f[I]name\f[R]
 Name associated with an interface.
-.RS
-.RE
 .TP
-.B \f[I]ops\f[]
+.B \f[I]ops\f[R]
 Fabric interface operations.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 An access domain typically refers to a physical or virtual NIC or
@@ -92,7 +78,7 @@ documentation.
 .PP
 fi_set_ops assigns callbacks that a provider should invoke in place of
 performing selected tasks.
-This allows users to modify or control a provider\[aq]s default
+This allows users to modify or control a provider\[cq]s default
 behavior.
 Conceptually, it allows the user to hook specific functions used by a
 provider and replace it with their own.
@@ -101,7 +87,7 @@ The operations being modified are identified using a well\-known
 character string, passed as the name parameter.
 The format of the ops parameter is dependent upon the name value.
 The ops parameter will reference a structure containing the callbacks
-and other fields needed by the provider to invoke the user\[aq]s
+and other fields needed by the provider to invoke the user\[cq]s
 functions.
 .PP
 If a provider accepts the override, it will return FI_SUCCESS.
@@ -112,7 +98,7 @@ Overrides should be set prior to allocating resources on the domain.
 The following fi_set_ops operations and corresponding callback
 structures are defined.
 .PP
-\f[B]FI_SET_OPS_HMEM_OVERRIDE \-\- Heterogeneous Memory Overrides\f[]
+\f[B]FI_SET_OPS_HMEM_OVERRIDE \[en] Heterogeneous Memory Overrides\f[R]
 .PP
 HMEM override allows users to override HMEM related operations a
 provider may perform.
@@ -128,44 +114,38 @@ The following is the HMEM override operation name and structure.
 .IP
 .nf
 \f[C]
-#define\ FI_SET_OPS_HMEM_OVERRIDE\ "hmem_override_ops"
+#define FI_SET_OPS_HMEM_OVERRIDE \[dq]hmem_override_ops\[dq]
 
-struct\ fi_hmem_override_ops\ {
-\ \ \ \ size_t\ \ size;
+struct fi_hmem_override_ops {
+    size_t  size;
 
-\ \ \ \ ssize_t\ (*copy_from_hmem_iov)(void\ *dest,\ size_t\ size,
-\ \ \ \ \ \ \ \ enum\ fi_hmem_iface\ iface,\ uint64_t\ device,\ const\ struct\ iovec\ *hmem_iov,
-\ \ \ \ \ \ \ \ size_t\ hmem_iov_count,\ uint64_t\ hmem_iov_offset);
+    ssize_t (*copy_from_hmem_iov)(void *dest, size_t size,
+        enum fi_hmem_iface iface, uint64_t device, const struct iovec *hmem_iov,
+        size_t hmem_iov_count, uint64_t hmem_iov_offset);
 
-\ \ \ \ ssize_t\ (*copy_to_hmem_iov)(enum\ fi_hmem_iface\ iface,\ uint64_t\ device,
-\ \ \ \ const\ struct\ iovec\ *hmem_iov,\ size_t\ hmem_iov_count,
-\ \ \ \ \ \ \ \ uint64_t\ hmem_iov_offset,\ const\ void\ *src,\ size_t\ size);
+    ssize_t (*copy_to_hmem_iov)(enum fi_hmem_iface iface, uint64_t device,
+    const struct iovec *hmem_iov, size_t hmem_iov_count,
+        uint64_t hmem_iov_offset, const void *src, size_t size);
 };
-\f[]
+\f[R]
 .fi
 .PP
 All fields in struct fi_hmem_override_ops must be set (non\-null) to a
 valid value.
 .TP
-.B \f[I]size\f[]
+.B \f[I]size\f[R]
 This should be set to the sizeof(struct fi_hmem_override_ops).
 The size field is used for forward and backward compatibility purposes.
-.RS
-.RE
 .TP
-.B \f[I]copy_from_hmem_iov\f[]
+.B \f[I]copy_from_hmem_iov\f[R]
 Copy data from the device/hmem to host memory.
 This function should return a negative fi_errno on error, or the number
 of bytes copied on success.
-.RS
-.RE
 .TP
-.B \f[I]copy_to_hmem_iov\f[]
+.B \f[I]copy_to_hmem_iov\f[R]
 Copy data from host memory to the device/hmem.
 This function should return a negative fi_errno on error, or the number
 of bytes copied on success.
-.RS
-.RE
 .SS fi_domain_bind
 .PP
 Associates an event queue with the domain.
@@ -182,9 +162,9 @@ asynchronously, with the completion reported through the event queue.
 If an event queue is not bound to the domain with the FI_REG_MR flag,
 then memory registration requests complete synchronously.
 .PP
-See \f[C]fi_av_bind\f[](3), \f[C]fi_ep_bind\f[](3),
-\f[C]fi_mr_bind\f[](3), \f[C]fi_pep_bind\f[](3), and
-\f[C]fi_scalable_ep_bind\f[](3) for more information.
+See \f[C]fi_av_bind\f[R](3), \f[C]fi_ep_bind\f[R](3),
+\f[C]fi_mr_bind\f[R](3), \f[C]fi_pep_bind\f[R](3), and
+\f[C]fi_scalable_ep_bind\f[R](3) for more information.
 .SS fi_close
 .PP
 The fi_close call is used to release all resources associated with a
@@ -193,41 +173,41 @@ All objects associated with the opened domain must be released prior to
 calling fi_close, otherwise the call will return \-FI_EBUSY.
 .SH DOMAIN ATTRIBUTES
 .PP
-The \f[C]fi_domain_attr\f[] structure defines the set of attributes
+The \f[C]fi_domain_attr\f[R] structure defines the set of attributes
 associated with a domain.
 .IP
 .nf
 \f[C]
-struct\ fi_domain_attr\ {
-\ \ \ \ struct\ fid_domain\ \ \ \ \ *domain;
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *name;
-\ \ \ \ enum\ fi_threading\ \ \ \ \ threading;
-\ \ \ \ enum\ fi_progress\ \ \ \ \ \ control_progress;
-\ \ \ \ enum\ fi_progress\ \ \ \ \ \ data_progress;
-\ \ \ \ enum\ fi_resource_mgmt\ resource_mgmt;
-\ \ \ \ enum\ fi_av_type\ \ \ \ \ \ \ av_type;
-\ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ mr_mode;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ mr_key_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cq_data_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cq_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ep_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tx_ctx_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ rx_ctx_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max_ep_tx_ctx;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max_ep_rx_ctx;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max_ep_stx_ctx;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max_ep_srx_ctx;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cntr_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ mr_iov_limit;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ caps;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ mode;
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *auth_key;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ auth_key_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max_err_data;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ mr_cnt;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ tclass;
+struct fi_domain_attr {
+    struct fid_domain     *domain;
+    char                  *name;
+    enum fi_threading     threading;
+    enum fi_progress      control_progress;
+    enum fi_progress      data_progress;
+    enum fi_resource_mgmt resource_mgmt;
+    enum fi_av_type       av_type;
+    int                   mr_mode;
+    size_t                mr_key_size;
+    size_t                cq_data_size;
+    size_t                cq_cnt;
+    size_t                ep_cnt;
+    size_t                tx_ctx_cnt;
+    size_t                rx_ctx_cnt;
+    size_t                max_ep_tx_ctx;
+    size_t                max_ep_rx_ctx;
+    size_t                max_ep_stx_ctx;
+    size_t                max_ep_srx_ctx;
+    size_t                cntr_cnt;
+    size_t                mr_iov_limit;
+    uint64_t              caps;
+    uint64_t              mode;
+    uint8_t               *auth_key;
+    size_t                auth_key_size;
+    size_t                max_err_data;
+    size_t                mr_cnt;
+    uint32_t              tclass;
 };
-\f[]
+\f[R]
 .fi
 .SS domain
 .PP
@@ -254,15 +234,13 @@ Applications which can guarantee serialization in their access of
 provider allocated resources and interfaces enables a provider to
 eliminate lower\-level locks.
 .TP
-.B \f[I]FI_THREAD_COMPLETION\f[]
+.B \f[I]FI_THREAD_COMPLETION\f[R]
 The completion threading model is intended for providers that make use
 of manual progress.
 Applications must serialize access to all objects that are associated
 through the use of having a shared completion structure.
 This includes endpoint, transmit context, receive context, completion
 queue, counter, wait set, and poll set objects.
-.RS
-.RE
 .PP
 For example, threads must serialize access to an endpoint and its bound
 completion queue(s) and/or counters.
@@ -272,23 +250,19 @@ serialized.
 The use of FI_THREAD_COMPLETION can increase parallelism over
 FI_THREAD_SAFE, but requires the use of isolated resources.
 .TP
-.B \f[I]FI_THREAD_DOMAIN\f[]
+.B \f[I]FI_THREAD_DOMAIN\f[R]
 A domain serialization model requires applications to serialize access
 to all objects belonging to a domain.
-.RS
-.RE
 .TP
-.B \f[I]FI_THREAD_ENDPOINT\f[]
+.B \f[I]FI_THREAD_ENDPOINT\f[R]
 The endpoint threading model is similar to FI_THREAD_FID, but with the
 added restriction that serialization is required when accessing the same
 endpoint, even if multiple transmit and receive contexts are used.
 Conceptually, FI_THREAD_ENDPOINT maps well to providers that implement
 fabric services in hardware but use a single command queue to access
 different data flows.
-.RS
-.RE
 .TP
-.B \f[I]FI_THREAD_FID\f[]
+.B \f[I]FI_THREAD_FID\f[R]
 A fabric descriptor (FID) serialization model requires applications to
 serialize access to individual fabric resources associated with data
 transfer operations and completions.
@@ -296,8 +270,6 @@ Multiple threads must be serialized when accessing the same endpoint,
 transmit context, receive context, completion queue, counter, wait set,
 or poll set.
 Serialization is required only by threads accessing the same object.
-.RS
-.RE
 .PP
 For example, one thread may be initiating a data transfer on an
 endpoint, while another thread reads from a completion queue associated
@@ -316,21 +288,17 @@ Conceptually, FI_THREAD_FID maps well to providers that implement fabric
 services in hardware and provide separate command queues to different
 data flows.
 .TP
-.B \f[I]FI_THREAD_SAFE\f[]
+.B \f[I]FI_THREAD_SAFE\f[R]
 A thread safe serialization model allows a multi\-threaded application
 to access any allocated resources through any interface without
 restriction.
 All providers are required to support FI_THREAD_SAFE.
-.RS
-.RE
 .TP
-.B \f[I]FI_THREAD_UNSPEC\f[]
+.B \f[I]FI_THREAD_UNSPEC\f[R]
 This value indicates that no threading model has been defined.
 It may be used on input hints to the fi_getinfo call.
 When specified, providers will return a threading model that allows for
 the greatest level of parallelism.
-.RS
-.RE
 .SS Progress Models (control_progress / data_progress)
 .PP
 Progress is the ability of the underlying implementation to complete
@@ -363,7 +331,7 @@ and acknowledgement processing.
 To balance between performance and ease of use, two progress models are
 defined.
 .TP
-.B \f[I]FI_PROGRESS_AUTO\f[]
+.B \f[I]FI_PROGRESS_AUTO\f[R]
 This progress model indicates that the provider will make forward
 progress on an asynchronous operation without further intervention by
 the application.
@@ -371,15 +339,13 @@ When FI_PROGRESS_AUTO is provided as output to fi_getinfo in the absence
 of any progress hints, it often indicates that the desired functionality
 is implemented by the provider hardware or is a standard service of the
 operating system.
-.RS
-.RE
 .PP
 All providers are required to support FI_PROGRESS_AUTO.
 However, if a provider does not natively support automatic progress,
 forcing the use of FI_PROGRESS_AUTO may result in threads being
 allocated below the fabric interfaces.
 .TP
-.B \f[I]FI_PROGRESS_MANUAL\f[]
+.B \f[I]FI_PROGRESS_MANUAL\f[R]
 This progress model indicates that the provider requires the use of an
 application thread to complete an asynchronous request.
 When manual progress is set, the provider will attempt to advance an
@@ -388,8 +354,6 @@ or read an event queue, completion queue, or counter where the completed
 operation will be reported.
 Progress also occurs when the application processes a poll or wait set
 that has been associated with the event or completion queue.
-.RS
-.RE
 .PP
 Only wait operations defined by the fabric interface will result in an
 operation progressing.
@@ -405,11 +369,9 @@ For example, an endpoint that acts purely as the target of RMA or atomic
 operations that uses manual progress may still need application
 assistance to process received operations.
 .TP
-.B \f[I]FI_PROGRESS_UNSPEC\f[]
+.B \f[I]FI_PROGRESS_UNSPEC\f[R]
 This value indicates that no progress model has been defined.
 It may be used on input hints to the fi_getinfo call.
-.RS
-.RE
 .SS Resource Management (resource_mgmt)
 .PP
 Resource management (RM) is provider and protocol support to protect
@@ -433,23 +395,17 @@ protection against overruns.
 However, such protection is not guaranteed.
 The following values for resource management are defined.
 .TP
-.B \f[I]FI_RM_DISABLED\f[]
+.B \f[I]FI_RM_DISABLED\f[R]
 The provider is free to select an implementation and protocol that does
 not protect against resource overruns.
 The application is responsible for resource protection.
-.RS
-.RE
 .TP
-.B \f[I]FI_RM_ENABLED\f[]
+.B \f[I]FI_RM_ENABLED\f[R]
 Resource management is enabled for this provider domain.
-.RS
-.RE
 .TP
-.B \f[I]FI_RM_UNSPEC\f[]
+.B \f[I]FI_RM_UNSPEC\f[R]
 This value indicates that no resource management model has been defined.
 It may be used on input hints to the fi_getinfo call.
-.RS
-.RE
 .PP
 The behavior of the various resource management options depends on
 whether the endpoint is reliable or unreliable, as well as provider and
@@ -459,7 +415,7 @@ The table assumes that all peers enable or disable RM the same.
 .PP
 .TS
 tab(@);
-cw(8.0n) cw(16.0n) cw(16.0n) cw(15.3n) cw(14.6n).
+cw(7.7n) cw(16.2n) cw(16.2n) cw(15.4n) cw(14.6n).
 T{
 Resource
 T}@T{
@@ -576,7 +532,7 @@ T}
 The resource column indicates the resource being accessed by a data
 transfer operation.
 .TP
-.B \f[I]Tx Ctx / Rx Ctx\f[]
+.B \f[I]Tx Ctx / Rx Ctx\f[R]
 Refers to the transmit/receive contexts when a data transfer operation
 is submitted.
 When RM is enabled, attempting to submit a request will fail if the
@@ -584,10 +540,8 @@ context is full.
 If RM is disabled, an undefined error (provider specific) will occur.
 Such errors should be considered fatal to the context, and applications
 must take steps to avoid queue overruns.
-.RS
-.RE
 .TP
-.B \f[I]Tx CQ / Rx CQ\f[]
+.B \f[I]Tx CQ / Rx CQ\f[R]
 Refers to the completion queue associated with the Tx or Rx context when
 a local operation completes.
 When RM is disabled, applications must take care to ensure that
@@ -604,13 +558,11 @@ that could result in CQ overruns, or internally retrying requests (which
 will be hidden from the application).
 See notes at the end of this section regarding CQ resource management
 restrictions.
-.RS
-.RE
 .TP
-.B \f[I]Target EP / No Rx Buffer\f[]
+.B \f[I]Target EP / No Rx Buffer\f[R]
 Target EP refers to resources associated with the endpoint that is the
 target of a transmit operation.
-This includes the target endpoint\[aq]s receive queue, posted receive
+This includes the target endpoint\[cq]s receive queue, posted receive
 buffers (no Rx buffers), the receive side completion queue, and other
 related packet processing queues.
 The defined behavior is that seen by the initiator of a request.
@@ -618,11 +570,11 @@ For FI_EP_DGRAM endpoints, if the target EP queues are unable to accept
 incoming messages, received messages will be dropped.
 For reliable endpoints, if RM is disabled, the transmit operation will
 complete in error.
+A provider may choose to return an error completion with the error code
+FI_ENORX for that transmit operation so that it can be retried.
 If RM is enabled, the provider will internally retry the operation.
-.RS
-.RE
 .TP
-.B \f[I]Rx Buffer Overrun\f[]
+.B \f[I]Rx Buffer Overrun\f[R]
 This refers to buffers posted to receive incoming tagged or untagged
 messages, with the behavior defined from the viewpoint of the sender.
 The behavior for handling received messages that are larger than the
@@ -636,20 +588,16 @@ be truncated at the receive side.
 This can occur when the target side buffers received data until an
 application buffer is made available.
 The completion status may also be dependent upon the completion model
-selected byt the application (e.g.
-FI_DELIVERY_COMPLETE versus FI_TRANSMIT_COMPLETE).
-.RS
-.RE
+selected byt the application (e.g.\ FI_DELIVERY_COMPLETE versus
+FI_TRANSMIT_COMPLETE).
 .TP
-.B \f[I]Unmatched RMA / RMA Overrun\f[]
+.B \f[I]Unmatched RMA / RMA Overrun\f[R]
 Unmatched RMA and RMA overruns deal with the processing of RMA and
 atomic operations.
 Unlike send operations, RMA operations that attempt to access a memory
 address that is either not registered for such operations, or attempt to
 access outside of the target memory region will fail, resulting in a
 transmit error.
-.RS
-.RE
 .PP
 When a resource management error occurs on an endpoint, the endpoint is
 transitioned into a disabled state.
@@ -682,23 +630,17 @@ size as the endpoint queue(s) that are bound to it.
 .SS AV Type (av_type)
 .PP
 Specifies the type of address vectors that are usable with this domain.
-For additional details on AV type, see \f[C]fi_av\f[](3).
+For additional details on AV type, see \f[C]fi_av\f[R](3).
 The following values may be specified.
 .TP
-.B \f[I]FI_AV_MAP\f[]
+.B \f[I]FI_AV_MAP\f[R]
 Only address vectors of type AV map are requested or supported.
-.RS
-.RE
 .TP
-.B \f[I]FI_AV_TABLE\f[]
+.B \f[I]FI_AV_TABLE\f[R]
 Only address vectors of type AV index are requested or supported.
-.RS
-.RE
 .TP
-.B \f[I]FI_AV_UNSPEC\f[]
+.B \f[I]FI_AV_UNSPEC\f[R]
 Any address vector format is requested and supported.
-.RS
-.RE
 .PP
 Address vectors are only used by connectionless endpoints.
 Applications that require the use of a specific type of address vector
@@ -712,87 +654,65 @@ optimal AV type supported by this domain.
 .SS Memory Registration Mode (mr_mode)
 .PP
 Defines memory registration specific mode bits used with this domain.
-Full details on MR mode options are available in \f[C]fi_mr\f[](3).
+Full details on MR mode options are available in \f[C]fi_mr\f[R](3).
 The following values may be specified.
 .TP
-.B \f[I]FI_MR_ALLOCATED\f[]
+.B \f[I]FI_MR_ALLOCATED\f[R]
 Indicates that memory registration occurs on allocated data buffers, and
 physical pages must back all virtual addresses being registered.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_ENDPOINT\f[]
+.B \f[I]FI_MR_ENDPOINT\f[R]
 Memory registration occurs at the endpoint level, rather than domain.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_LOCAL\f[]
+.B \f[I]FI_MR_LOCAL\f[R]
 The provider is optimized around having applications register memory for
 locally accessed data buffers.
 Data buffers used in send and receive operations and as the source
 buffer for RMA and atomic operations must be registered by the
 application for access domains opened with this capability.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_MMU_NOTIFY\f[]
+.B \f[I]FI_MR_MMU_NOTIFY\f[R]
 Indicates that the application is responsible for notifying the provider
 when the page tables referencing a registered memory region may have
 been updated.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_PROV_KEY\f[]
+.B \f[I]FI_MR_PROV_KEY\f[R]
 Memory registration keys are selected and returned by the provider.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_RAW\f[]
+.B \f[I]FI_MR_RAW\f[R]
 The provider requires additional setup as part of their memory
 registration process.
 This mode is required by providers that use a memory key that is larger
 than 64\-bits.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_RMA_EVENT\f[]
+.B \f[I]FI_MR_RMA_EVENT\f[R]
 Indicates that the memory regions associated with completion counters
 must be explicitly enabled after being bound to any counter.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_UNSPEC\f[]
-Defined for compatibility \-\- library versions 1.4 and earlier.
+.B \f[I]FI_MR_UNSPEC\f[R]
+Defined for compatibility \[en] library versions 1.4 and earlier.
 Setting mr_mode to 0 indicates that FI_MR_BASIC or FI_MR_SCALABLE are
 requested and supported.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_VIRT_ADDR\f[]
+.B \f[I]FI_MR_VIRT_ADDR\f[R]
 Registered memory regions are referenced by peers using the virtual
 address of the registered memory region, rather than a 0\-based offset.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_BASIC\f[]
-Defined for compatibility \-\- library versions 1.4 and earlier.
+.B \f[I]FI_MR_BASIC\f[R]
+Defined for compatibility \[en] library versions 1.4 and earlier.
 Only basic memory registration operations are requested or supported.
 This mode is equivalent to the FI_MR_VIRT_ADDR, FI_MR_ALLOCATED, and
 FI_MR_PROV_KEY flags being set in later library versions.
 This flag may not be used in conjunction with other mr_mode bits.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_SCALABLE\f[]
-Defined for compatibility \-\- library versions 1.4 and earlier.
+.B \f[I]FI_MR_SCALABLE\f[R]
+Defined for compatibility \[en] library versions 1.4 and earlier.
 Only scalable memory registration operations are requested or supported.
 Scalable registration uses offset based addressing, with application
 selectable memory keys.
 For library versions 1.5 and later, this is the default if no mr_mode
 bits are set.
 This flag may not be used in conjunction with other mr_mode bits.
-.RS
-.RE
 .PP
 Buffers used in data transfer operations may require notifying the
 provider of their use before a data transfer can occur.
@@ -842,8 +762,8 @@ Providers return capability limits based on configured hardware maximum
 capabilities.
 Providers cannot predict all possible system limitations without
 posteriori knowledge acquired during runtime that will further limit
-these hardware maximums (e.g.
-application memory consumption, FD usage, etc.).
+these hardware maximums (e.g.\ application memory consumption, FD usage,
+etc.).
 .SS Transmit Context Count (tx_ctx_cnt)
 .PP
 The number of outbound command queues optimally supported by the
@@ -899,46 +819,38 @@ Domain level capabilities.
 Domain capabilities indicate domain level features that are supported by
 the provider.
 .TP
-.B \f[I]FI_LOCAL_COMM\f[]
+.B \f[I]FI_LOCAL_COMM\f[R]
 At a conceptual level, this field indicates that the underlying device
 supports loopback communication.
 More specifically, this field indicates that an endpoint may communicate
 with other endpoints that are allocated from the same underlying named
 domain.
 If this field is not set, an application may need to use an alternate
-domain or mechanism (e.g.
-shared memory) to communicate with peers that execute on the same node.
-.RS
-.RE
+domain or mechanism (e.g.\ shared memory) to communicate with peers that
+execute on the same node.
 .TP
-.B \f[I]FI_REMOTE_COMM\f[]
+.B \f[I]FI_REMOTE_COMM\f[R]
 This field indicates that the underlying provider supports communication
 with nodes that are reachable over the network.
 If this field is not set, then the provider only supports communication
-between processes that execute on the same node \-\- a shared memory
+between processes that execute on the same node \[en] a shared memory
 provider, for example.
-.RS
-.RE
 .TP
-.B \f[I]FI_SHARED_AV\f[]
+.B \f[I]FI_SHARED_AV\f[R]
 Indicates that the domain supports the ability to share address vectors
 among multiple processes using the named address vector feature.
-.RS
-.RE
 .PP
-See \f[C]fi_getinfo\f[](3) for a discussion on primary versus secondary
+See \f[C]fi_getinfo\f[R](3) for a discussion on primary versus secondary
 capabilities.
 All domain capabilities are considered secondary capabilities.
 .SS mode
 .PP
 The operational mode bit related to using the domain.
 .TP
-.B \f[I]FI_RESTRICTED_COMP\f[]
+.B \f[I]FI_RESTRICTED_COMP\f[R]
 This bit indicates that the domain limits completion queues and counters
 to only be used with endpoints, transmit contexts, and receive contexts
 that have the same set of capability flags.
-.RS
-.RE
 .SS Default authorization key (auth_key)
 .PP
 The default authorization key to associate with endpoint and memory
@@ -975,13 +887,13 @@ cache or lookup tables.
 .PP
 This specifies the default traffic class that will be associated any
 endpoints created within the domain.
-See [\f[C]fi_endpoint\f[](3)](fi_endpoint.3.html for additional
+See [\f[C]fi_endpoint\f[R](3)](fi_endpoint.3.html for additional
 information.
 .SH RETURN VALUE
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH NOTES
 .PP
 Users should call fi_close to release all resources allocated to the
@@ -1000,7 +912,7 @@ lightly loaded systems, without an administrator configuring system
 resources appropriately for the installed provider(s).
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_av\f[](3),
-\f[C]fi_ep\f[](3), \f[C]fi_eq\f[](3), \f[C]fi_mr\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3), \f[C]fi_av\f[R](3),
+\f[C]fi_ep\f[R](3), \f[C]fi_eq\f[R](3), \f[C]fi_mr\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_endpoint.3 b/man/man3/fi_endpoint.3
index 334743d..37ec4a2 100644
--- a/man/man3/fi_endpoint.3
+++ b/man/man3/fi_endpoint.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_endpoint" "3" "2021\-02\-10" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_endpoint" "3" "2021\-03\-23" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,212 +8,158 @@ fi_endpoint \- Fabric endpoint operations
 .TP
 .B fi_endpoint / fi_scalable_ep / fi_passive_ep / fi_close
 Allocate or close an endpoint.
-.RS
-.RE
 .TP
 .B fi_ep_bind
 Associate an endpoint with hardware resources, such as event queues,
 completion queues, counters, address vectors, or shared transmit/receive
 contexts.
-.RS
-.RE
 .TP
 .B fi_scalable_ep_bind
 Associate a scalable endpoint with an address vector
-.RS
-.RE
 .TP
 .B fi_pep_bind
 Associate a passive endpoint with an event queue
-.RS
-.RE
 .TP
 .B fi_enable
 Transitions an active endpoint into an enabled state.
-.RS
-.RE
 .TP
 .B fi_cancel
 Cancel a pending asynchronous data transfer
-.RS
-.RE
 .TP
 .B fi_ep_alias
 Create an alias to the endpoint
-.RS
-.RE
 .TP
 .B fi_control
 Control endpoint operation.
-.RS
-.RE
 .TP
 .B fi_getopt / fi_setopt
 Get or set endpoint options.
-.RS
-.RE
 .TP
 .B fi_rx_context / fi_tx_context / fi_srx_context / fi_stx_context
 Open a transmit or receive context.
-.RS
-.RE
 .TP
 .B fi_tc_dscp_set / fi_tc_dscp_get
 Convert between a DSCP value and a network traffic class
-.RS
-.RE
 .TP
 .B fi_rx_size_left / fi_tx_size_left (DEPRECATED)
 Query the lower bound on how many RX/TX operations may be posted without
 an operation returning \-FI_EAGAIN.
 This functions have been deprecated and will be removed in a future
 version of the library.
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
+#include <rdma/fabric.h>
 
-#include\ <rdma/fi_endpoint.h>
+#include <rdma/fi_endpoint.h>
 
-int\ fi_endpoint(struct\ fid_domain\ *domain,\ struct\ fi_info\ *info,
-\ \ \ \ struct\ fid_ep\ **ep,\ void\ *context);
+int fi_endpoint(struct fid_domain *domain, struct fi_info *info,
+    struct fid_ep **ep, void *context);
 
-int\ fi_scalable_ep(struct\ fid_domain\ *domain,\ struct\ fi_info\ *info,
-\ \ \ \ struct\ fid_ep\ **sep,\ void\ *context);
+int fi_scalable_ep(struct fid_domain *domain, struct fi_info *info,
+    struct fid_ep **sep, void *context);
 
-int\ fi_passive_ep(struct\ fi_fabric\ *fabric,\ struct\ fi_info\ *info,
-\ \ \ \ struct\ fid_pep\ **pep,\ void\ *context);
+int fi_passive_ep(struct fi_fabric *fabric, struct fi_info *info,
+    struct fid_pep **pep, void *context);
 
-int\ fi_tx_context(struct\ fid_ep\ *sep,\ int\ index,
-\ \ \ \ struct\ fi_tx_attr\ *attr,\ struct\ fid_ep\ **tx_ep,
-\ \ \ \ void\ *context);
+int fi_tx_context(struct fid_ep *sep, int index,
+    struct fi_tx_attr *attr, struct fid_ep **tx_ep,
+    void *context);
 
-int\ fi_rx_context(struct\ fid_ep\ *sep,\ int\ index,
-\ \ \ \ struct\ fi_rx_attr\ *attr,\ struct\ fid_ep\ **rx_ep,
-\ \ \ \ void\ *context);
+int fi_rx_context(struct fid_ep *sep, int index,
+    struct fi_rx_attr *attr, struct fid_ep **rx_ep,
+    void *context);
 
-int\ fi_stx_context(struct\ fid_domain\ *domain,
-\ \ \ \ struct\ fi_tx_attr\ *attr,\ struct\ fid_stx\ **stx,
-\ \ \ \ void\ *context);
+int fi_stx_context(struct fid_domain *domain,
+    struct fi_tx_attr *attr, struct fid_stx **stx,
+    void *context);
 
-int\ fi_srx_context(struct\ fid_domain\ *domain,
-\ \ \ \ struct\ fi_rx_attr\ *attr,\ struct\ fid_ep\ **rx_ep,
-\ \ \ \ void\ *context);
+int fi_srx_context(struct fid_domain *domain,
+    struct fi_rx_attr *attr, struct fid_ep **rx_ep,
+    void *context);
 
-int\ fi_close(struct\ fid\ *ep);
+int fi_close(struct fid *ep);
 
-int\ fi_ep_bind(struct\ fid_ep\ *ep,\ struct\ fid\ *fid,\ uint64_t\ flags);
+int fi_ep_bind(struct fid_ep *ep, struct fid *fid, uint64_t flags);
 
-int\ fi_scalable_ep_bind(struct\ fid_ep\ *sep,\ struct\ fid\ *fid,\ uint64_t\ flags);
+int fi_scalable_ep_bind(struct fid_ep *sep, struct fid *fid, uint64_t flags);
 
-int\ fi_pep_bind(struct\ fid_pep\ *pep,\ struct\ fid\ *fid,\ uint64_t\ flags);
+int fi_pep_bind(struct fid_pep *pep, struct fid *fid, uint64_t flags);
 
-int\ fi_enable(struct\ fid_ep\ *ep);
+int fi_enable(struct fid_ep *ep);
 
-int\ fi_cancel(struct\ fid_ep\ *ep,\ void\ *context);
+int fi_cancel(struct fid_ep *ep, void *context);
 
-int\ fi_ep_alias(struct\ fid_ep\ *ep,\ struct\ fid_ep\ **alias_ep,\ uint64_t\ flags);
+int fi_ep_alias(struct fid_ep *ep, struct fid_ep **alias_ep, uint64_t flags);
 
-int\ fi_control(struct\ fid\ *ep,\ int\ command,\ void\ *arg);
+int fi_control(struct fid *ep, int command, void *arg);
 
-int\ fi_getopt(struct\ fid\ *ep,\ int\ level,\ int\ optname,
-\ \ \ \ void\ *optval,\ size_t\ *optlen);
+int fi_getopt(struct fid *ep, int level, int optname,
+    void *optval, size_t *optlen);
 
-int\ fi_setopt(struct\ fid\ *ep,\ int\ level,\ int\ optname,
-\ \ \ \ const\ void\ *optval,\ size_t\ optlen);
+int fi_setopt(struct fid *ep, int level, int optname,
+    const void *optval, size_t optlen);
 
-uint32_t\ fi_tc_dscp_set(uint8_t\ dscp);
+uint32_t fi_tc_dscp_set(uint8_t dscp);
 
-uint8_t\ fi_tc_dscp_get(uint32_t\ tclass);
+uint8_t fi_tc_dscp_get(uint32_t tclass);
 
-DEPRECATED\ ssize_t\ fi_rx_size_left(struct\ fid_ep\ *ep);
+DEPRECATED ssize_t fi_rx_size_left(struct fid_ep *ep);
 
-DEPRECATED\ ssize_t\ fi_tx_size_left(struct\ fid_ep\ *ep);
-\f[]
+DEPRECATED ssize_t fi_tx_size_left(struct fid_ep *ep);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]fid\f[]
+.B \f[I]fid\f[R]
 On creation, specifies a fabric or access domain.
 On bind, identifies the event queue, completion queue, counter, or
 address vector to bind to the endpoint.
-In other cases, it\[aq]s a fabric identifier of an associated resource.
-.RS
-.RE
+In other cases, it\[cq]s a fabric identifier of an associated resource.
 .TP
-.B \f[I]info\f[]
+.B \f[I]info\f[R]
 Details about the fabric interface endpoint to be opened, obtained from
 fi_getinfo.
-.RS
-.RE
 .TP
-.B \f[I]ep\f[]
+.B \f[I]ep\f[R]
 A fabric endpoint.
-.RS
-.RE
 .TP
-.B \f[I]sep\f[]
+.B \f[I]sep\f[R]
 A scalable fabric endpoint.
-.RS
-.RE
 .TP
-.B \f[I]pep\f[]
+.B \f[I]pep\f[R]
 A passive fabric endpoint.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 Context associated with the endpoint or asynchronous operation.
-.RS
-.RE
 .TP
-.B \f[I]index\f[]
+.B \f[I]index\f[R]
 Index to retrieve a specific transmit/receive context.
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Transmit or receive context attributes.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply to the operation.
-.RS
-.RE
 .TP
-.B \f[I]command\f[]
+.B \f[I]command\f[R]
 Command of control operation to perform on endpoint.
-.RS
-.RE
 .TP
-.B \f[I]arg\f[]
+.B \f[I]arg\f[R]
 Optional control argument.
-.RS
-.RE
 .TP
-.B \f[I]level\f[]
+.B \f[I]level\f[R]
 Protocol level at which the desired option resides.
-.RS
-.RE
 .TP
-.B \f[I]optname\f[]
+.B \f[I]optname\f[R]
 The protocol option to read or set.
-.RS
-.RE
 .TP
-.B \f[I]optval\f[]
+.B \f[I]optval\f[R]
 The option value that was read or to set.
-.RS
-.RE
 .TP
-.B \f[I]optlen\f[]
+.B \f[I]optlen\f[R]
 The size of the optval buffer.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Endpoints are transport level communication portals.
@@ -227,8 +173,8 @@ transfers.
 .PP
 Active endpoints may be connection\-oriented or connectionless, and may
 provide data reliability.
-The data transfer interfaces \-\- messages (fi_msg), tagged messages
-(fi_tagged), RMA (fi_rma), and atomics (fi_atomic) \-\- are associated
+The data transfer interfaces \[en] messages (fi_msg), tagged messages
+(fi_tagged), RMA (fi_rma), and atomics (fi_atomic) \[en] are associated
 with active endpoints.
 In basic configurations, an active endpoint has transmit and receive
 queues.
@@ -353,32 +299,28 @@ reported to the user.
 An active endpoint may direct asynchronous completions to different CQs,
 based on the type of operation.
 This is specified using fi_ep_bind flags.
-The following flags may be OR\[aq]ed together when binding an endpoint
+The following flags may be OR\[cq]ed together when binding an endpoint
 to a completion domain CQ.
 .TP
-.B \f[I]FI_RECV\f[]
+.B \f[I]FI_RECV\f[R]
 Directs the notification of inbound data transfers to the specified
 completion queue.
 This includes received messages.
 This binding automatically includes FI_REMOTE_WRITE, if applicable to
 the endpoint.
-.RS
-.RE
 .TP
-.B \f[I]FI_SELECTIVE_COMPLETION\f[]
+.B \f[I]FI_SELECTIVE_COMPLETION\f[R]
 By default, data transfer operations write CQ completion entries into
 the associated completion queue after they have successfully completed.
 Applications can use this bind flag to selectively enable when
 completions are generated.
 If FI_SELECTIVE_COMPLETION is specified, data transfer operations will
-not generate CQ entries for \f[I]successful\f[] completions unless
+not generate CQ entries for \f[I]successful\f[R] completions unless
 FI_COMPLETION is set as an operational flag for the given operation.
 Operations that fail asynchronously will still generate completions,
 even if a completion is not requested.
-FI_SELECTIVE_COMPLETION must be OR\[aq]ed with FI_TRANSMIT and/or
+FI_SELECTIVE_COMPLETION must be OR\[cq]ed with FI_TRANSMIT and/or
 FI_RECV flags.
-.RS
-.RE
 .PP
 When FI_SELECTIVE_COMPLETION is set, the user must determine when a
 request that does NOT have FI_COMPLETION set has completed indirectly,
@@ -390,12 +332,10 @@ avoid writing a CQ completion entry for every operation.
 See Notes section below for additional information on how this flag
 interacts with the FI_CONTEXT and FI_CONTEXT2 mode bits.
 .TP
-.B \f[I]FI_TRANSMIT\f[]
+.B \f[I]FI_TRANSMIT\f[R]
 Directs the completion of outbound data transfer requests to the
 specified completion queue.
 This includes send message, RMA, and atomic operations.
-.RS
-.RE
 .PP
 An endpoint may optionally be bound to a completion counter.
 Associating an endpoint with a counter is in addition to binding the EP
@@ -403,51 +343,39 @@ with a CQ.
 When binding an endpoint to a counter, the following flags may be
 specified.
 .TP
-.B \f[I]FI_READ\f[]
+.B \f[I]FI_READ\f[R]
 Increments the specified counter whenever an RMA read, atomic fetch, or
 atomic compare operation initiated from the endpoint has completed
 successfully or in error.
-.RS
-.RE
 .TP
-.B \f[I]FI_RECV\f[]
+.B \f[I]FI_RECV\f[R]
 Increments the specified counter whenever a message is received over the
 endpoint.
 Received messages include both tagged and normal message operations.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_READ\f[]
+.B \f[I]FI_REMOTE_READ\f[R]
 Increments the specified counter whenever an RMA read, atomic fetch, or
 atomic compare operation is initiated from a remote endpoint that
 targets the given endpoint.
 Use of this flag requires that the endpoint be created using
 FI_RMA_EVENT.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_WRITE\f[]
+.B \f[I]FI_REMOTE_WRITE\f[R]
 Increments the specified counter whenever an RMA write or base atomic
 operation is initiated from a remote endpoint that targets the given
 endpoint.
 Use of this flag requires that the endpoint be created using
 FI_RMA_EVENT.
-.RS
-.RE
 .TP
-.B \f[I]FI_SEND\f[]
+.B \f[I]FI_SEND\f[R]
 Increments the specified counter whenever a message transfer initiated
 over the endpoint has completed successfully or in error.
 Sent messages include both tagged and normal message operations.
-.RS
-.RE
 .TP
-.B \f[I]FI_WRITE\f[]
+.B \f[I]FI_WRITE\f[R]
 Increments the specified counter whenever an RMA write or base atomic
 operation initiated from the endpoint has completed successfully or in
 error.
-.RS
-.RE
 .PP
 An endpoint may only be bound to a single CQ or counter for a given type
 of operation.
@@ -497,9 +425,9 @@ No specific entry related to fi_cancel itself will be posted.
 .PP
 Cancel uses the context parameter associated with an operation to
 identify the request to cancel.
-Operations posted without a valid context parameter \-\- either no
+Operations posted without a valid context parameter \[en] either no
 context parameter is specified or the context value was ignored by the
-provider \-\- cannot be canceled.
+provider \[en] cannot be canceled.
 If multiple outstanding operations match the context parameter, only one
 will be canceled.
 In this case, the operation which is canceled is provider specific.
@@ -523,7 +451,7 @@ When allocating an alias, an application may configure either the
 transmit or receive operational flags.
 This avoids needing a separate call to fi_control to set those flags.
 The flags passed to fi_ep_alias must include FI_TRANSMIT or FI_RECV (not
-both) with other operational flags OR\[aq]ed in.
+both) with other operational flags OR\[cq]ed in.
 This will override the transmit or receive flags, respectively, for
 operations posted through the alias endpoint.
 All allocated aliases must be closed for the underlying endpoint to be
@@ -546,19 +474,15 @@ endpoint.
 This option only applies to passive endpoints.
 It is used to set the connection request backlog for listening
 endpoints.
-.RS
-.RE
 .TP
-.B **FI_GETOPSFLAG \-\- uint64_t *flags**
+.B **FI_GETOPSFLAG \[en] uint64_t *flags**
 Used to retrieve the current value of flags associated with the data
 transfer operations initiated on the endpoint.
 The control argument must include FI_TRANSMIT or FI_RECV (not both)
 flags to indicate the type of data transfer flags to be returned.
 See below for a list of control flags.
-.RS
-.RE
 .TP
-.B \f[B]FI_GETWAIT \-\- void **\f[]
+.B \f[B]FI_GETWAIT \[en] void **\f[R]
 This command allows the user to retrieve the file descriptor associated
 with a socket endpoint.
 The fi_control arg parameter should be an address where a pointer to the
@@ -566,20 +490,16 @@ returned file descriptor will be written.
 See fi_eq.3 for addition details using fi_control with FI_GETWAIT.
 The file descriptor may be used for notification that the endpoint is
 ready to send or receive data.
-.RS
-.RE
 .TP
-.B **FI_SETOPSFLAG \-\- uint64_t *flags**
+.B **FI_SETOPSFLAG \[en] uint64_t *flags**
 Used to change the data transfer operation flags associated with an
 endpoint.
 The control argument must include FI_TRANSMIT or FI_RECV (not both) to
 indicate the type of data transfer that the flags should apply to, with
-other flags OR\[aq]ed in.
+other flags OR\[cq]ed in.
 The given flags will override the previous transmit and receive
 attributes that were set when the endpoint was created.
 Valid control flags are defined below.
-.RS
-.RE
 .SS fi_getopt / fi_setopt
 .PP
 Endpoint protocol operations may be retrieved using fi_getopt or set
@@ -591,19 +511,16 @@ and implementation specific details of an endpoint.
 .PP
 The following option levels and option names and parameters are defined.
 .PP
-\f[I]FI_OPT_ENDPOINT\f[]
+\f[I]FI_OPT_ENDPOINT\f[R]
 \[bu] .RS 2
 .TP
-.B \f[I]FI_OPT_BUFFERED_LIMIT \- size_t\f[]
+.B \f[I]FI_OPT_BUFFERED_LIMIT \- size_t\f[R]
 Defines the maximum size of a buffered message that will be reported to
 users as part of a receive completion when the FI_BUFFERED_RECV mode is
 enabled on an endpoint.
-.RS
-.RE
-.RE
 .PP
 fi_getopt() will return the currently configured threshold, or the
-provider\[aq]s default threshold if one has not be set by the
+provider\[cq]s default threshold if one has not be set by the
 application.
 fi_setopt() allows an application to configure the threshold.
 If the provider cannot support the requested threshold, it will fail the
@@ -615,11 +532,12 @@ fi_getopt() can then be used to retrieve the set size.
 In most cases, the sending and receiving endpoints must be configured to
 use the same threshold value, and the threshold must be set prior to
 enabling the endpoint.
+.RE
 \[bu] .RS 2
 .TP
-.B \f[I]FI_OPT_BUFFERED_MIN \- size_t\f[]
+.B \f[I]FI_OPT_BUFFERED_MIN \- size_t\f[R]
 Defines the minimum size of a buffered message that will be reported.
-Applications would set this to a size that\[aq]s big enough to decide
+Applications would set this to a size that\[cq]s big enough to decide
 whether to discard or claim a buffered receive or when to claim a
 buffered receive on getting a buffered receive completion.
 The value is typically used by a provider when sending a rendezvous
@@ -627,12 +545,10 @@ protocol request where it would send at least FI_OPT_BUFFERED_MIN bytes
 of application data along with it.
 A smaller sized rendezvous protocol message usually results in better
 latency for the overall transfer of a large message.
-.RS
-.RE
 .RE
 \[bu] .RS 2
 .TP
-.B \f[I]FI_OPT_CM_DATA_SIZE \- size_t\f[]
+.B \f[I]FI_OPT_CM_DATA_SIZE \- size_t\f[R]
 Defines the size of available space in CM messages for user\-defined
 data.
 This value limits the amount of data that applications can exchange
@@ -643,12 +559,10 @@ except in the case of passive endpoints, in which the size reflects the
 maximum size of the data that may be present as part of a connection
 request event.
 This option is read only.
-.RS
-.RE
 .RE
 \[bu] .RS 2
 .TP
-.B \f[I]FI_OPT_MIN_MULTI_RECV \- size_t\f[]
+.B \f[I]FI_OPT_MIN_MULTI_RECV \- size_t\f[R]
 Defines the minimum receive buffer space available when the receive
 buffer is released by the provider (see FI_MULTI_RECV).
 Modifying this value is only guaranteed to set the minimum buffer space
@@ -656,8 +570,6 @@ needed on receives posted after the value has been changed.
 It is recommended that applications that want to override the default
 MIN_MULTI_RECV value set this option before enabling the corresponding
 endpoint.
-.RS
-.RE
 .RE
 .SS fi_tc_dscp_set
 .PP
@@ -704,22 +616,22 @@ receive context attributes as shown below.
 .IP
 .nf
 \f[C]
-struct\ fi_ep_attr\ {
-\ \ \ \ enum\ fi_ep_type\ type;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ protocol;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ protocol_version;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ max_msg_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ msg_prefix_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ max_order_raw_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ max_order_war_size;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ max_order_waw_size;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ mem_tag_format;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ tx_ctx_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ rx_ctx_cnt;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ auth_key_size;
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ *auth_key;
+struct fi_ep_attr {
+    enum fi_ep_type type;
+    uint32_t        protocol;
+    uint32_t        protocol_version;
+    size_t          max_msg_size;
+    size_t          msg_prefix_size;
+    size_t          max_order_raw_size;
+    size_t          max_order_war_size;
+    size_t          max_order_waw_size;
+    uint64_t        mem_tag_format;
+    size_t          tx_ctx_cnt;
+    size_t          rx_ctx_cnt;
+    size_t          auth_key_size;
+    uint8_t         *auth_key;
 };
-\f[]
+\f[R]
 .fi
 .SS type \- Endpoint Type
 .PP
@@ -727,38 +639,30 @@ If specified, indicates the type of fabric interface communication
 desired.
 Supported types are:
 .TP
-.B \f[I]FI_EP_DGRAM\f[]
+.B \f[I]FI_EP_DGRAM\f[R]
 Supports a connectionless, unreliable datagram communication.
 Message boundaries are maintained, but the maximum message size may be
 limited to the fabric MTU.
 Flow control is not guaranteed.
-.RS
-.RE
 .TP
-.B \f[I]FI_EP_MSG\f[]
+.B \f[I]FI_EP_MSG\f[R]
 Provides a reliable, connection\-oriented data transfer service with
 flow control that maintains message boundaries.
-.RS
-.RE
 .TP
-.B \f[I]FI_EP_RDM\f[]
+.B \f[I]FI_EP_RDM\f[R]
 Reliable datagram message.
 Provides a reliable, connectionless data transfer service with flow
 control that maintains message boundaries.
-.RS
-.RE
 .TP
-.B \f[I]FI_EP_SOCK_DGRAM\f[]
+.B \f[I]FI_EP_SOCK_DGRAM\f[R]
 A connectionless, unreliable datagram endpoint with UDP socket\-like
 semantics.
 FI_EP_SOCK_DGRAM is most useful for applications designed around using
 UDP sockets.
 See the SOCKET ENDPOINT section for additional details and restrictions
 that apply to datagram socket endpoints.
-.RS
-.RE
 .TP
-.B \f[I]FI_EP_SOCK_STREAM\f[]
+.B \f[I]FI_EP_SOCK_STREAM\f[R]
 Data streaming endpoint with TCP socket\-like semantics.
 Provides a reliable, connection\-oriented data transfer service that
 does not maintain message boundaries.
@@ -766,15 +670,11 @@ FI_EP_SOCK_STREAM is most useful for applications designed around using
 TCP sockets.
 See the SOCKET ENDPOINT section for additional details and restrictions
 that apply to stream endpoints.
-.RS
-.RE
 .TP
-.B \f[I]FI_EP_UNSPEC\f[]
+.B \f[I]FI_EP_UNSPEC\f[R]
 The type of endpoint is not specified.
 This is usually provided as input, with other attributes of the endpoint
 or the provider selecting the type.
-.RS
-.RE
 .SS Protocol
 .PP
 Specifies the low\-level end to end protocol employed by the provider.
@@ -785,102 +685,72 @@ Provider specific protocols are also allowed.
 Provider specific protocols will be indicated by having the upper bit of
 the protocol value set to one.
 .TP
-.B \f[I]FI_PROTO_GNI\f[]
+.B \f[I]FI_PROTO_GNI\f[R]
 Protocol runs over Cray GNI low\-level interface.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_IB_RDM\f[]
+.B \f[I]FI_PROTO_IB_RDM\f[R]
 Reliable\-datagram protocol implemented over InfiniBand
 reliable\-connected queue pairs.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_IB_UD\f[]
+.B \f[I]FI_PROTO_IB_UD\f[R]
 The protocol runs over Infiniband unreliable datagram queue pairs.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_IWARP\f[]
+.B \f[I]FI_PROTO_IWARP\f[R]
 The protocol runs over the Internet wide area RDMA protocol transport.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_IWARP_RDM\f[]
+.B \f[I]FI_PROTO_IWARP_RDM\f[R]
 Reliable\-datagram protocol implemented over iWarp reliable\-connected
 queue pairs.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_NETWORKDIRECT\f[]
+.B \f[I]FI_PROTO_NETWORKDIRECT\f[R]
 Protocol runs over Microsoft NetworkDirect service provider interface.
 This adds reliable\-datagram semantics over the NetworkDirect
 connection\- oriented endpoint semantics.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_PSMX\f[]
+.B \f[I]FI_PROTO_PSMX\f[R]
 The protocol is based on an Intel proprietary protocol known as PSM,
 performance scaled messaging.
 PSMX is an extended version of the PSM protocol to support the libfabric
 interfaces.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_PSMX2\f[]
+.B \f[I]FI_PROTO_PSMX2\f[R]
 The protocol is based on an Intel proprietary protocol known as PSM2,
 performance scaled messaging version 2.
 PSMX2 is an extended version of the PSM2 protocol to support the
 libfabric interfaces.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_PSMX3\f[]
-The protocol is Intel\[aq]s protocol known as PSM3, performance scaled
+.B \f[I]FI_PROTO_PSMX3\f[R]
+The protocol is Intel\[cq]s protocol known as PSM3, performance scaled
 messaging version 3.
 PSMX3 is implemented over RoCEv2 and verbs.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_RDMA_CM_IB_RC\f[]
+.B \f[I]FI_PROTO_RDMA_CM_IB_RC\f[R]
 The protocol runs over Infiniband reliable\-connected queue pairs, using
 the RDMA CM protocol for connection establishment.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_RXD\f[]
+.B \f[I]FI_PROTO_RXD\f[R]
 Reliable\-datagram protocol implemented over datagram endpoints.
 RXD is a libfabric utility component that adds RDM endpoint semantics
 over DGRAM endpoint semantics.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_RXM\f[]
+.B \f[I]FI_PROTO_RXM\f[R]
 Reliable\-datagram protocol implemented over message endpoints.
 RXM is a libfabric utility component that adds RDM endpoint semantics
 over MSG endpoint semantics.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_SOCK_TCP\f[]
+.B \f[I]FI_PROTO_SOCK_TCP\f[R]
 The protocol is layered over TCP packets.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROTO_UDP\f[]
+.B \f[I]FI_PROTO_UDP\f[R]
 The protocol sends and receives UDP datagrams.
-For example, an endpoint using \f[I]FI_PROTO_UDP\f[] will be able to
+For example, an endpoint using \f[I]FI_PROTO_UDP\f[R] will be able to
 communicate with a remote peer that is using Berkeley
-\f[I]SOCK_DGRAM\f[] sockets using \f[I]IPPROTO_UDP\f[].
-.RS
-.RE
+\f[I]SOCK_DGRAM\f[R] sockets using \f[I]IPPROTO_UDP\f[R].
 .TP
-.B \f[I]FI_PROTO_UNSPEC\f[]
+.B \f[I]FI_PROTO_UNSPEC\f[R]
 The protocol is not specified.
 This is usually provided as input, with other attributes of the socket
 or the provider selecting the actual protocol.
-.RS
-.RE
 .SS protocol_version \- Protocol Version
 .PP
 Identifies which version of the protocol is employed by the provider.
@@ -908,20 +778,31 @@ Data ordering is separate, but dependent on message ordering (defined
 below).
 Data ordering is unspecified where message order is not defined.
 .PP
-Data ordering refers to the access of target memory by subsequent
-operations.
+Data ordering refers to the access of the same target memory by
+subsequent operations.
 When back to back RMA read or write operations access the same
 registered memory location, data ordering indicates whether the second
 operation reads or writes the target memory after the first operation
 has completed.
-Because RMA ordering applies between two operations, and not within a
-single data transfer, ordering is defined per byte\-addressable memory
-location.
+For example, will an RMA read that follows an RMA write read back the
+data that was written?
+Similarly, will an RMA write that follows an RMA read update the target
+buffer after the read has transferred the original data?
+Data ordering answers these questions, even in the presence of errors,
+such as the need to resend data because of lost or corrupted network
+traffic.
+.PP
+RMA ordering applies between two operations, and not within a single
+data transfer.
+Therefore, ordering is defined per byte\-addressable memory location.
 I.e.
 ordering specifies whether location X is accessed by the second
 operation after the first operation.
 Nothing is implied about the completion of the first operation before
 the second operation is initiated.
+For example, if the first operation updates locations X and Y, but the
+second operation only accesses location X, there are no guarantees
+defined relative to location Y and the second operation.
 .PP
 In order to support large data transfers being broken into multiple
 packets and sent using multiple paths through the fabric, data ordering
@@ -931,17 +812,15 @@ values.
 Note that even if data ordering is not maintained, message ordering may
 be.
 .TP
-.B \f[I]max_order_raw_size\f[]
+.B \f[I]max_order_raw_size\f[R]
 Read after write size.
 If set, an RMA or atomic read operation issued after an RMA or atomic
 write operation, both of which are smaller than the size, will be
 ordered.
 Where the target memory locations overlap, the RMA or atomic read
 operation will see the results of the previous RMA or atomic write.
-.RS
-.RE
 .TP
-.B \f[I]max_order_war_size\f[]
+.B \f[I]max_order_war_size\f[R]
 Write after read size.
 If set, an RMA or atomic write operation issued after an RMA or atomic
 read operation, both of which are smaller than the size, will be
@@ -949,18 +828,14 @@ ordered.
 The RMA or atomic read operation will see the initial value of the
 target memory location before a subsequent RMA or atomic write updates
 the value.
-.RS
-.RE
 .TP
-.B \f[I]max_order_waw_size\f[]
+.B \f[I]max_order_waw_size\f[R]
 Write after write size.
 If set, an RMA or atomic write operation issued after an RMA or atomic
 write operation, both of which are smaller than the size, will be
 ordered.
 The target memory location will reflect the results of the second RMA or
 atomic write.
-.RS
-.RE
 .PP
 An order size value of 0 indicates that ordering is not guaranteed.
 A value of \-1 guarantees ordering for any data size.
@@ -973,7 +848,7 @@ fields.
 The mem_tag_format optionally begins with a series of bits set to 0, to
 signify bits which are ignored by the provider.
 Following the initial prefix of ignored bits, the array will consist of
-alternating groups of bits set to all 1\[aq]s or all 0\[aq]s.
+alternating groups of bits set to all 1\[cq]s or all 0\[cq]s.
 Each group of bits corresponds to a tagged field.
 The implication of defining a tagged field is that when a mask is
 applied to the tagged bit array, all bits belonging to a single field
@@ -983,7 +858,7 @@ For example, a mem_tag_format of 0x30FF indicates support for 14 tagged
 bits, separated into 3 fields.
 The first field consists of 2\-bits, the second field 4\-bits, and the
 final field 8\-bits.
-Valid masks for such a tagged field would be a bitwise OR\[aq]ing of
+Valid masks for such a tagged field would be a bitwise OR\[cq]ing of
 zero or more of the following values: 0x3000, 0x0F00, and 0x00FF.
 The provider may not validate the mask provided by the application for
 performance reasons.
@@ -1004,7 +879,7 @@ the tag field of the completion entry.
 .PP
 It is recommended that field sizes be ordered from smallest to largest.
 A generic, unstructured tag and mask can be achieved by requesting a bit
-array consisting of alternating 1\[aq]s and 0\[aq]s.
+array consisting of alternating 1\[cq]s and 0\[cq]s.
 .SS tx_ctx_cnt \- Transmit Context Count
 .PP
 Number of transmit contexts to associate with the endpoint.
@@ -1070,19 +945,19 @@ specified using struct fi_tx_attr.
 .IP
 .nf
 \f[C]
-struct\ fi_tx_attr\ {
-\ \ \ \ uint64_t\ \ caps;
-\ \ \ \ uint64_t\ \ mode;
-\ \ \ \ uint64_t\ \ op_flags;
-\ \ \ \ uint64_t\ \ msg_order;
-\ \ \ \ uint64_t\ \ comp_order;
-\ \ \ \ size_t\ \ \ \ inject_size;
-\ \ \ \ size_t\ \ \ \ size;
-\ \ \ \ size_t\ \ \ \ iov_limit;
-\ \ \ \ size_t\ \ \ \ rma_iov_limit;
-\ \ \ \ uint32_t\ \ tclass;
+struct fi_tx_attr {
+    uint64_t  caps;
+    uint64_t  mode;
+    uint64_t  op_flags;
+    uint64_t  msg_order;
+    uint64_t  comp_order;
+    size_t    inject_size;
+    size_t    size;
+    size_t    iov_limit;
+    size_t    rma_iov_limit;
+    uint32_t  tclass;
 };
-\f[]
+\f[R]
 .fi
 .SS caps \- Capabilities
 .PP
@@ -1145,160 +1020,124 @@ Message ordering requires matching ordering semantics on the receiving
 side of a data transfer operation in order to guarantee that ordering is
 met.
 .TP
-.B \f[I]FI_ORDER_ATOMIC_RAR\f[]
+.B \f[I]FI_ORDER_ATOMIC_RAR\f[R]
 Atomic read after read.
 If set, atomic fetch operations are transmitted in the order submitted
 relative to other atomic fetch operations.
 If not set, atomic fetches may be transmitted out of order from their
 submission.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_RAW\f[]
+.B \f[I]FI_ORDER_ATOMIC_RAW\f[R]
 Atomic read after write.
 If set, atomic fetch operations are transmitted in the order submitted
 relative to atomic update operations.
 If not set, atomic fetches may be transmitted ahead of atomic updates.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_WAR\f[]
+.B \f[I]FI_ORDER_ATOMIC_WAR\f[R]
 RMA write after read.
 If set, atomic update operations are transmitted in the order submitted
 relative to atomic fetch operations.
 If not set, atomic updates may be transmitted ahead of atomic fetches.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_WAW\f[]
+.B \f[I]FI_ORDER_ATOMIC_WAW\f[R]
 RMA write after write.
 If set, atomic update operations are transmitted in the order submitted
 relative to other atomic update operations.
 If not atomic updates may be transmitted out of order from their
 submission.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_NONE\f[]
+.B \f[I]FI_ORDER_NONE\f[R]
 No ordering is specified.
 This value may be used as input in order to obtain the default message
 order supported by the provider.
 FI_ORDER_NONE is an alias for the value 0.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RAR\f[]
+.B \f[I]FI_ORDER_RAR\f[R]
 Read after read.
 If set, RMA and atomic read operations are transmitted in the order
 submitted relative to other RMA and atomic read operations.
 If not set, RMA and atomic reads may be transmitted out of order from
 their submission.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RAS\f[]
+.B \f[I]FI_ORDER_RAS\f[R]
 Read after send.
 If set, RMA and atomic read operations are transmitted in the order
 submitted relative to message send operations, including tagged sends.
 If not set, RMA and atomic reads may be transmitted ahead of sends.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RAW\f[]
+.B \f[I]FI_ORDER_RAW\f[R]
 Read after write.
 If set, RMA and atomic read operations are transmitted in the order
 submitted relative to RMA and atomic write operations.
 If not set, RMA and atomic reads may be transmitted ahead of RMA and
 atomic writes.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RMA_RAR\f[]
+.B \f[I]FI_ORDER_RMA_RAR\f[R]
 RMA read after read.
 If set, RMA read operations are transmitted in the order submitted
 relative to other RMA read operations.
 If not set, RMA reads may be transmitted out of order from their
 submission.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RMA_RAW\f[]
+.B \f[I]FI_ORDER_RMA_RAW\f[R]
 RMA read after write.
 If set, RMA read operations are transmitted in the order submitted
 relative to RMA write operations.
 If not set, RMA reads may be transmitted ahead of RMA writes.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RMA_WAR\f[]
+.B \f[I]FI_ORDER_RMA_WAR\f[R]
 RMA write after read.
 If set, RMA write operations are transmitted in the order submitted
 relative to RMA read operations.
 If not set, RMA writes may be transmitted ahead of RMA reads.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_RMA_WAW\f[]
+.B \f[I]FI_ORDER_RMA_WAW\f[R]
 RMA write after write.
 If set, RMA write operations are transmitted in the order submitted
 relative to other RMA write operations.
 If not set, RMA writes may be transmitted out of order from their
 submission.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_SAR\f[]
+.B \f[I]FI_ORDER_SAR\f[R]
 Send after read.
 If set, message send operations, including tagged sends, are transmitted
 in order submitted relative to RMA and atomic read operations.
 If not set, message sends may be transmitted ahead of RMA and atomic
 reads.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_SAS\f[]
+.B \f[I]FI_ORDER_SAS\f[R]
 Send after send.
 If set, message send operations, including tagged sends, are transmitted
 in the order submitted relative to other message send.
 If not set, message sends may be transmitted out of order from their
 submission.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_SAW\f[]
+.B \f[I]FI_ORDER_SAW\f[R]
 Send after write.
 If set, message send operations, including tagged sends, are transmitted
 in order submitted relative to RMA and atomic write operations.
 If not set, message sends may be transmitted ahead of RMA and atomic
 writes.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_WAR\f[]
+.B \f[I]FI_ORDER_WAR\f[R]
 Write after read.
 If set, RMA and atomic write operations are transmitted in the order
 submitted relative to RMA and atomic read operations.
 If not set, RMA and atomic writes may be transmitted ahead of RMA and
 atomic reads.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_WAS\f[]
+.B \f[I]FI_ORDER_WAS\f[R]
 Write after send.
 If set, RMA and atomic write operations are transmitted in the order
 submitted relative to message send operations, including tagged sends.
 If not set, RMA and atomic writes may be transmitted ahead of sends.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_WAW\f[]
+.B \f[I]FI_ORDER_WAW\f[R]
 Write after write.
 If set, RMA and atomic write operations are transmitted in the order
 submitted relative to other RMA and atomic write operations.
 If not set, RMA and atomic writes may be transmitted out of order from
 their submission.
-.RS
-.RE
 .SS comp_order \- Completion Ordering
 .PP
 Completion ordering refers to the order in which completed requests are
@@ -1328,17 +1167,13 @@ with the constraint that the returned ordering is stricter than that
 specified by the application.
 Supported completion order values are:
 .TP
-.B \f[I]FI_ORDER_NONE\f[]
+.B \f[I]FI_ORDER_NONE\f[R]
 No ordering is defined for completed operations.
 Requests submitted to the transmit context may complete in any order.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_STRICT\f[]
+.B \f[I]FI_ORDER_STRICT\f[R]
 Requests complete in the order in which they are submitted to the
 transmit context.
-.RS
-.RE
 .SS inject_size
 .PP
 The requested inject operation size (see the FI_INJECT flag) that the
@@ -1361,11 +1196,11 @@ transmit operation and a queue entry.
 A single transmit operation may consume multiple queue entries; for
 example, one per scatter\-gather entry.
 Additionally, the size field is intended to guide the allocation of the
-endpoint\[aq]s transmit context.
+endpoint\[cq]s transmit context.
 Specifically, for connectionless endpoints, there may be lower\-level
 queues use to track communication on a per peer basis.
 The sizes of any lower\-level queues may only be significantly smaller
-than the endpoint\[aq]s transmit size, in order to reduce resource
+than the endpoint\[cq]s transmit size, in order to reduce resource
 utilization.
 .SS iov_limit
 .PP
@@ -1391,32 +1226,26 @@ definition.
 If tclass is unset or set to FI_TC_UNSPEC, the endpoint will use the
 default traffic class associated with the domain.
 .TP
-.B \f[I]FI_TC_BEST_EFFORT\f[]
+.B \f[I]FI_TC_BEST_EFFORT\f[R]
 This is the default in the absence of any other local or fabric
 configuration.
 This class carries the traffic for a number of applications executing
 concurrently over the same network infrastructure.
 Even though it is shared, network capacity and resource allocation are
 distributed fairly across the applications.
-.RS
-.RE
 .TP
-.B \f[I]FI_TC_BULK_DATA\f[]
+.B \f[I]FI_TC_BULK_DATA\f[R]
 This class is intended for large data transfers associated with I/O and
 is present to separate sustained I/O transfers from other application
 inter\-process communications.
-.RS
-.RE
 .TP
-.B \f[I]FI_TC_DEDICATED_ACCESS\f[]
+.B \f[I]FI_TC_DEDICATED_ACCESS\f[R]
 This class operates at the highest priority, except the management
 class.
 It carries a high bandwidth allocation, minimum latency targets, and the
 highest scheduling and arbitration priority.
-.RS
-.RE
 .TP
-.B \f[I]FI_TC_LOW_LATENCY\f[]
+.B \f[I]FI_TC_LOW_LATENCY\f[R]
 This class supports low latency, low jitter data patterns typically
 caused by transactional data exchanges, barrier synchronizations, and
 collective operations that are typical of HPC applications.
@@ -1425,33 +1254,25 @@ transfers must achieve for correct or performance operations.
 Fulfillment of such requests in this class will typically require
 accompanying bandwidth and message size limitations so as not to consume
 excessive bandwidth at high priority.
-.RS
-.RE
 .TP
-.B \f[I]FI_TC_NETWORK_CTRL\f[]
+.B \f[I]FI_TC_NETWORK_CTRL\f[R]
 This class is intended for traffic directly related to fabric (network)
 management, which is critical to the correct operation of the network.
 Its use is typically restricted to privileged network management
 applications.
-.RS
-.RE
 .TP
-.B \f[I]FI_TC_SCAVENGER\f[]
+.B \f[I]FI_TC_SCAVENGER\f[R]
 This class is used for data that is desired but does not have strict
 delivery requirements, such as in\-band network or application level
 monitoring data.
 Use of this class indicates that the traffic is considered lower
 priority and should not interfere with higher priority workflows.
-.RS
-.RE
 .TP
-.B \f[I]fi_tc_dscp_set / fi_tc_dscp_get\f[]
+.B \f[I]fi_tc_dscp_set / fi_tc_dscp_get\f[R]
 DSCP values are supported via the DSCP get and set functions.
 The definitions for DSCP values are outside the scope of libfabric.
 See the fi_tc_dscp_set and fi_tc_dscp_get function definitions for
 details on their use.
-.RS
-.RE
 .SH RECEIVE CONTEXT ATTRIBUTES
 .PP
 Attributes specific to the receive capabilities of an endpoint are
@@ -1459,17 +1280,17 @@ specified using struct fi_rx_attr.
 .IP
 .nf
 \f[C]
-struct\ fi_rx_attr\ {
-\ \ \ \ uint64_t\ \ caps;
-\ \ \ \ uint64_t\ \ mode;
-\ \ \ \ uint64_t\ \ op_flags;
-\ \ \ \ uint64_t\ \ msg_order;
-\ \ \ \ uint64_t\ \ comp_order;
-\ \ \ \ size_t\ \ \ \ total_buffered_recv;
-\ \ \ \ size_t\ \ \ \ size;
-\ \ \ \ size_t\ \ \ \ iov_limit;
+struct fi_rx_attr {
+    uint64_t  caps;
+    uint64_t  mode;
+    uint64_t  op_flags;
+    uint64_t  msg_order;
+    uint64_t  comp_order;
+    size_t    total_buffered_recv;
+    size_t    size;
+    size_t    iov_limit;
 };
-\f[]
+\f[R]
 .fi
 .SS caps \- Capabilities
 .PP
@@ -1511,7 +1332,7 @@ Applicable flags are listed in the Operation Flags section.
 .SS msg_order \- Message Ordering
 .PP
 For a description of message ordering, see the msg_order field in the
-\f[I]Transmit Context Attribute\f[] section.
+\f[I]Transmit Context Attribute\f[R] section.
 Receive context message ordering defines the order in which received
 transport message headers are processed when received by an endpoint.
 When ordering is set, it indicates that message headers will be
@@ -1530,28 +1351,22 @@ FI_ORDER_ATOMIC_WAW.
 .SS comp_order \- Completion Ordering
 .PP
 For a description of completion ordering, see the comp_order field in
-the \f[I]Transmit Context Attribute\f[] section.
+the \f[I]Transmit Context Attribute\f[R] section.
 .TP
-.B \f[I]FI_ORDER_DATA\f[]
+.B \f[I]FI_ORDER_DATA\f[R]
 When set, this bit indicates that received data is written into memory
 in order.
 Data ordering applies to memory accessed as part of a single operation
 and between operations if message ordering is guaranteed.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_NONE\f[]
+.B \f[I]FI_ORDER_NONE\f[R]
 No ordering is defined for completed operations.
 Receive operations may complete in any order, regardless of their
 submission order.
-.RS
-.RE
 .TP
-.B \f[I]FI_ORDER_STRICT\f[]
+.B \f[I]FI_ORDER_STRICT\f[R]
 Receive operations complete in the order in which they are processed by
 the receive context, based on the receive side msg_order attribute.
-.RS
-.RE
 .SS total_buffered_recv
 .PP
 This field is supported for backwards compatibility purposes.
@@ -1588,11 +1403,11 @@ operation and a queue entry.
 A single receive operation may consume multiple queue entries; for
 example, one per scatter\-gather entry.
 Additionally, the size field is intended to guide the allocation of the
-endpoint\[aq]s receive context.
+endpoint\[cq]s receive context.
 Specifically, for connectionless endpoints, there may be lower\-level
 queues use to track communication on a per peer basis.
 The sizes of any lower\-level queues may only be significantly smaller
-than the endpoint\[aq]s receive size, in order to reduce resource
+than the endpoint\[cq]s receive size, in order to reduce resource
 utilization.
 .SS iov_limit
 .PP
@@ -1661,7 +1476,7 @@ Receive contexts are often associated with steering flows, that specify
 which incoming packets targeting a scalable endpoint to process.
 However, receive contexts may be targeted directly by the initiator, if
 supported by the underlying protocol.
-Such contexts are referred to as \[aq]named\[aq].
+Such contexts are referred to as `named'.
 Support for named contexts must be indicated by setting the caps
 FI_NAMED_RX_CTX capability when the corresponding endpoint is created.
 Support for named receive contexts is coordinated with address vectors.
@@ -1728,7 +1543,7 @@ scalable set of contexts of the alternate type.
 This call is used to open a shareable transmit context (see above for
 details on the transmit context attributes).
 Endpoints associated with a shared transmit context must use a subset of
-the transmit context\[aq]s attributes.
+the transmit context\[cq]s attributes.
 Note that this is the reverse of the requirement for transmit contexts
 for scalable endpoints.
 .SS fi_srx_context
@@ -1736,7 +1551,7 @@ for scalable endpoints.
 This allocates a shareable receive context (see above for details on the
 receive context attributes).
 Endpoints associated with a shared receive context must use a subset of
-the receive context\[aq]s attributes.
+the receive context\[cq]s attributes.
 Note that this is the reverse of the requirement for receive contexts
 for scalable endpoints.
 .SH SOCKET ENDPOINTS
@@ -1784,63 +1599,51 @@ The file descriptor may be retrieved using fi_control.
 .SH OPERATION FLAGS
 .PP
 Operation flags are obtained by OR\-ing the following flags together.
-Operation flags define the default flags applied to an endpoint\[aq]s
+Operation flags define the default flags applied to an endpoint\[cq]s
 data transfer operations, where a flags parameter is not available.
 Data transfer operations that take flags as input override the op_flags
 value of transmit or receive context attributes of an endpoint.
 .TP
-.B \f[I]FI_COMMIT_COMPLETE\f[]
+.B \f[I]FI_COMMIT_COMPLETE\f[R]
 Indicates that a completion should not be generated (locally or at the
 peer) until the result of an operation have been made persistent.
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
+See \f[C]fi_cq\f[R](3) for additional details on completion semantics.
 .TP
-.B \f[I]FI_COMPLETION\f[]
+.B \f[I]FI_COMPLETION\f[R]
 Indicates that a completion queue entry should be written for data
 transfer operations.
 This flag only applies to operations issued on an endpoint that was
 bound to a completion queue with the FI_SELECTIVE_COMPLETION flag set,
 otherwise, it is ignored.
 See the fi_ep_bind section above for more detail.
-.RS
-.RE
 .TP
-.B \f[I]FI_DELIVERY_COMPLETE\f[]
+.B \f[I]FI_DELIVERY_COMPLETE\f[R]
 Indicates that a completion should be generated when the operation has
 been processed by the destination endpoint(s).
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
+See \f[C]fi_cq\f[R](3) for additional details on completion semantics.
 .TP
-.B \f[I]FI_INJECT\f[]
+.B \f[I]FI_INJECT\f[R]
 Indicates that all outbound data buffers should be returned to the
-user\[aq]s control immediately after a data transfer call returns, even
+user\[cq]s control immediately after a data transfer call returns, even
 if the operation is handled asynchronously.
 This may require that the provider copy the data into a local buffer and
 transfer out of that buffer.
 A provider can limit the total amount of send data that may be buffered
 and/or the size of a single send that can use this flag.
 This limit is indicated using inject_size (see inject_size above).
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT_COMPLETE\f[]
+.B \f[I]FI_INJECT_COMPLETE\f[R]
 Indicates that a completion should be generated when the source
 buffer(s) may be reused.
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
+See \f[C]fi_cq\f[R](3) for additional details on completion semantics.
 .TP
-.B \f[I]FI_MULTICAST\f[]
+.B \f[I]FI_MULTICAST\f[R]
 Indicates that data transfers will target multicast addresses by
 default.
 Any fi_addr_t passed into a data transfer operation will be treated as a
 multicast address.
-.RS
-.RE
 .TP
-.B \f[I]FI_MULTI_RECV\f[]
+.B \f[I]FI_MULTI_RECV\f[R]
 Applies to posted receive operations.
 This flag allows the user to post a single buffer that will receive
 multiple incoming messages.
@@ -1852,15 +1655,11 @@ The placement of received data into the buffer may be subjected to
 provider specific alignment restrictions.
 The buffer will be released by the provider when the available buffer
 space falls below the specified minimum (see FI_OPT_MIN_MULTI_RECV).
-.RS
-.RE
 .TP
-.B \f[I]FI_TRANSMIT_COMPLETE\f[]
+.B \f[I]FI_TRANSMIT_COMPLETE\f[R]
 Indicates that a completion should be generated when the transmit
 operation has completed relative to the local provider.
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
+See \f[C]fi_cq\f[R](3) for additional details on completion semantics.
 .SH NOTES
 .PP
 Users should call fi_close to release all resources allocated to the
@@ -1910,27 +1709,21 @@ On error, a negative value corresponding to fabric errno is returned.
 For fi_cancel, a return value of 0 indicates that the cancel request was
 submitted for processing.
 .PP
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .TP
-.B \f[I]\-FI_EDOMAIN\f[]
+.B \f[I]\-FI_EDOMAIN\f[R]
 A resource domain was not bound to the endpoint or an attempt was made
 to bind multiple domains.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_ENOCQ\f[]
+.B \f[I]\-FI_ENOCQ\f[R]
 The endpoint has not been configured with necessary event queue.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_EOPBADSTATE\f[]
-The endpoint\[aq]s state does not permit the requested operation.
-.RS
-.RE
+.B \f[I]\-FI_EOPBADSTATE\f[R]
+The endpoint\[cq]s state does not permit the requested operation.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_domain\f[](3), \f[C]fi_cq\f[](3)
-\f[C]fi_msg\f[](3), \f[C]fi_tagged\f[](3), \f[C]fi_rma\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_domain\f[R](3), \f[C]fi_cq\f[R](3)
+\f[C]fi_msg\f[R](3), \f[C]fi_tagged\f[R](3), \f[C]fi_rma\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_eq.3 b/man/man3/fi_eq.3
index 47b1bc2..09243d1 100644
--- a/man/man3/fi_eq.3
+++ b/man/man3/fi_eq.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_eq" "3" "2019\-12\-13" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_eq" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,131 +8,93 @@ fi_eq \- Event queue operations
 .TP
 .B fi_eq_open / fi_close
 Open/close an event queue
-.RS
-.RE
 .TP
 .B fi_control
 Control operation of EQ
-.RS
-.RE
 .TP
 .B fi_eq_read / fi_eq_readerr
 Read an event from an event queue
-.RS
-.RE
 .TP
 .B fi_eq_write
 Writes an event to an event queue
-.RS
-.RE
 .TP
 .B fi_eq_sread
 A synchronous (blocking) read of an event queue
-.RS
-.RE
 .TP
 .B fi_eq_strerror
 Converts provider specific error information into a printable string
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_eq_open(struct\ fid_fabric\ *fabric,\ struct\ fi_eq_attr\ *attr,
-\ \ \ \ struct\ fid_eq\ **eq,\ void\ *context);
+int fi_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
+    struct fid_eq **eq, void *context);
 
-int\ fi_close(struct\ fid\ *eq);
+int fi_close(struct fid *eq);
 
-int\ fi_control(struct\ fid\ *eq,\ int\ command,\ void\ *arg);
+int fi_control(struct fid *eq, int command, void *arg);
 
-ssize_t\ fi_eq_read(struct\ fid_eq\ *eq,\ uint32_t\ *event,
-\ \ \ \ void\ *buf,\ size_t\ len,\ uint64_t\ flags);
+ssize_t fi_eq_read(struct fid_eq *eq, uint32_t *event,
+    void *buf, size_t len, uint64_t flags);
 
-ssize_t\ fi_eq_readerr(struct\ fid_eq\ *eq,\ struct\ fi_eq_err_entry\ *buf,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_eq_readerr(struct fid_eq *eq, struct fi_eq_err_entry *buf,
+    uint64_t flags);
 
-ssize_t\ fi_eq_write(struct\ fid_eq\ *eq,\ uint32_t\ event,
-\ \ \ \ const\ void\ *buf,\ size_t\ len,\ uint64_t\ flags);
+ssize_t fi_eq_write(struct fid_eq *eq, uint32_t event,
+    const void *buf, size_t len, uint64_t flags);
 
-ssize_t\ fi_eq_sread(struct\ fid_eq\ *eq,\ uint32_t\ *event,
-\ \ \ \ void\ *buf,\ size_t\ len,\ int\ timeout,\ uint64_t\ flags);
+ssize_t fi_eq_sread(struct fid_eq *eq, uint32_t *event,
+    void *buf, size_t len, int timeout, uint64_t flags);
 
-const\ char\ *\ fi_eq_strerror(struct\ fid_eq\ *eq,\ int\ prov_errno,
-\ \ \ \ \ \ const\ void\ *err_data,\ char\ *buf,\ size_t\ len);
-\f[]
+const char * fi_eq_strerror(struct fid_eq *eq, int prov_errno,
+      const void *err_data, char *buf, size_t len);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]fabric\f[]
+.B \f[I]fabric\f[R]
 Opened fabric descriptor
-.RS
-.RE
 .TP
-.B \f[I]eq\f[]
+.B \f[I]eq\f[R]
 Event queue
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Event queue attributes
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the event queue.
-.RS
-.RE
 .TP
-.B \f[I]event\f[]
+.B \f[I]event\f[R]
 Reported event
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 For read calls, the data buffer to write events into.
 For write calls, an event to insert into the event queue.
 For fi_eq_strerror, an optional buffer that receives printable error
 information.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Length of data buffer
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply to the operation
-.RS
-.RE
 .TP
-.B \f[I]command\f[]
+.B \f[I]command\f[R]
 Command of control operation to perform on EQ.
-.RS
-.RE
 .TP
-.B \f[I]arg\f[]
+.B \f[I]arg\f[R]
 Optional control argument
-.RS
-.RE
 .TP
-.B \f[I]prov_errno\f[]
+.B \f[I]prov_errno\f[R]
 Provider specific error value
-.RS
-.RE
 .TP
-.B \f[I]err_data\f[]
+.B \f[I]err_data\f[R]
 Provider specific error data related to a completion
-.RS
-.RE
 .TP
-.B \f[I]timeout\f[]
+.B \f[I]timeout\f[R]
 Timeout specified in milliseconds
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Event queues are used to report events associated with control
@@ -147,66 +109,54 @@ as listening for connection requests.
 fi_eq_open allocates a new event queue.
 .PP
 The properties and behavior of an event queue are defined by
-\f[C]struct\ fi_eq_attr\f[].
+\f[C]struct fi_eq_attr\f[R].
 .IP
 .nf
 \f[C]
-struct\ fi_eq_attr\ {
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ size;\ \ \ \ \ \ /*\ #\ entries\ for\ EQ\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ flags;\ \ \ \ \ /*\ operation\ flags\ */
-\ \ \ \ enum\ fi_wait_obj\ \ \ \ \ wait_obj;\ \ /*\ requested\ wait\ object\ */
-\ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ signaling_vector;\ /*\ interrupt\ affinity\ */
-\ \ \ \ struct\ fid_wait\ \ \ \ \ *wait_set;\ \ /*\ optional\ wait\ set\ */
+struct fi_eq_attr {
+    size_t               size;      /* # entries for EQ */
+    uint64_t             flags;     /* operation flags */
+    enum fi_wait_obj     wait_obj;  /* requested wait object */
+    int                  signaling_vector; /* interrupt affinity */
+    struct fid_wait     *wait_set;  /* optional wait set */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]size\f[]
+.B \f[I]size\f[R]
 Specifies the minimum size of an event queue.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Flags that control the configuration of the EQ.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WRITE\f[]
+.B \- \f[I]FI_WRITE\f[R]
 Indicates that the application requires support for inserting user
 events into the EQ.
 If this flag is set, then the fi_eq_write operation must be supported by
 the provider.
 If the FI_WRITE flag is not set, then the application may not invoke
 fi_eq_write.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_AFFINITY\f[]
+.B \- \f[I]FI_AFFINITY\f[R]
 Indicates that the signaling_vector field (see below) is valid.
-.RS
-.RE
 .TP
-.B \f[I]wait_obj\f[]
-EQ\[aq]s may be associated with a specific wait object.
+.B \f[I]wait_obj\f[R]
+EQ\[cq]s may be associated with a specific wait object.
 Wait objects allow applications to block until the wait object is
 signaled, indicating that an event is available to be read.
 Users may use fi_control to retrieve the underlying wait object
 associated with an EQ, in order to use it in other system calls.
 The following values may be used to specify the type of wait object
 associated with an EQ:
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_NONE\f[]
+.B \- \f[I]FI_WAIT_NONE\f[R]
 Used to indicate that the user will not block (wait) for events on the
 EQ.
 When FI_WAIT_NONE is specified, the application may not call
 fi_eq_sread.
 This is the default is no wait object is specified.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_UNSPEC\f[]
+.B \- \f[I]FI_WAIT_UNSPEC\f[R]
 Specifies that the user will only wait on the EQ using fabric interface
 calls, such as fi_eq_sread.
 In this case, the underlying provider may select the most appropriate or
@@ -214,49 +164,37 @@ highest performing wait object available, including custom wait
 mechanisms.
 Applications that select FI_WAIT_UNSPEC are not guaranteed to retrieve
 the underlying wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_SET\f[]
+.B \- \f[I]FI_WAIT_SET\f[R]
 Indicates that the event queue should use a wait set object to wait for
 events.
 If specified, the wait_set field must reference an existing wait set
 object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_FD\f[]
+.B \- \f[I]FI_WAIT_FD\f[R]
 Indicates that the EQ should use a file descriptor as its wait
 mechanism.
 A file descriptor wait object must be usable in select, poll, and epoll
 routines.
 However, a provider may signal an FD wait object by marking it as
 readable or with an error.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_MUTEX_COND\f[]
+.B \- \f[I]FI_WAIT_MUTEX_COND\f[R]
 Specifies that the EQ should use a pthread mutex and cond variable as a
 wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_YIELD\f[]
+.B \- \f[I]FI_WAIT_YIELD\f[R]
 Indicates that the EQ will wait without a wait object but instead yield
 on every wait.
 Allows usage of fi_eq_sread through a spin.
-.RS
-.RE
 .TP
-.B \f[I]signaling_vector\f[]
+.B \f[I]signaling_vector\f[R]
 If the FI_AFFINITY flag is set, this indicates the logical cpu number
 (0..max cpu \- 1) that interrupts associated with the EQ should target.
 This field should be treated as a hint to the provider and may be
 ignored if the provider does not support interrupt affinity.
-.RS
-.RE
 .TP
-.B \f[I]wait_set\f[]
+.B \f[I]wait_set\f[R]
 If wait_obj is FI_WAIT_SET, this field references a wait object to which
 the event queue should attach.
 When an event is inserted into the event queue, the corresponding wait
@@ -264,8 +202,6 @@ set will be signaled if all necessary conditions are met.
 The use of a wait_set enables an optimized method of waiting for events
 across multiple event queues.
 This field is ignored if wait_obj is not FI_WAIT_SET.
-.RS
-.RE
 .SS fi_close
 .PP
 The fi_close call releases all resources associated with an event queue.
@@ -281,25 +217,23 @@ Access to the EQ should be serialized across all calls when fi_control
 is invoked, as it may redirect the implementation of EQ operations.
 The following control commands are usable with an EQ.
 .TP
-.B \f[I]FI_GETWAIT (void **)\f[]
+.B \f[I]FI_GETWAIT (void **)\f[R]
 This command allows the user to retrieve the low\-level wait object
 associated with the EQ.
 The format of the wait\-object is specified during EQ creation, through
 the EQ attributes.
 The fi_control arg parameter should be an address where a pointer to the
 returned wait object will be written.
-This should be an \[aq]int *\[aq] for FI_WAIT_FD, or \[aq]struct
-fi_mutex_cond\[aq] for FI_WAIT_MUTEX_COND.
-.RS
-.RE
+This should be an \[cq]int *\[cq] for FI_WAIT_FD, or `struct
+fi_mutex_cond' for FI_WAIT_MUTEX_COND.
 .IP
 .nf
 \f[C]
-struct\ fi_mutex_cond\ {
-\ \ \ \ pthread_mutex_t\ \ \ \ \ *mutex;
-\ \ \ \ pthread_cond_t\ \ \ \ \ \ *cond;
+struct fi_mutex_cond {
+    pthread_mutex_t     *mutex;
+    pthread_cond_t      *cond;
 };
-\f[]
+\f[R]
 .fi
 .SS fi_eq_read
 .PP
@@ -318,26 +252,24 @@ from the EQ.
 The following types of events may be reported to an EQ, along with
 information regarding the format associated with each event.
 .TP
-.B \f[I]Asynchronous Control Operations\f[]
+.B \f[I]Asynchronous Control Operations\f[R]
 Asynchronous control operations are basic requests that simply need to
 generate an event to indicate that they have completed.
 These include the following types of events: memory registration,
 address vector resolution, and multicast joins.
-.RS
-.RE
 .PP
 Control requests report their completion by inserting a
-\f[C]struct\ \ \ fi_eq_entry\f[] into the EQ.
+\f[C]struct   fi_eq_entry\f[R] into the EQ.
 The format of this structure is:
 .IP
 .nf
 \f[C]
-struct\ fi_eq_entry\ {
-\ \ \ \ fid_t\ \ \ \ \ \ \ \ \ \ \ \ fid;\ \ \ \ \ \ \ \ /*\ fid\ associated\ with\ request\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ *context;\ \ \ \ /*\ operation\ context\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ data;\ \ \ \ \ \ \ /*\ completion\-specific\ data\ */
+struct fi_eq_entry {
+    fid_t            fid;        /* fid associated with request */
+    void            *context;    /* operation context */
+    uint64_t         data;       /* completion\-specific data */
 };
-\f[]
+\f[R]
 .fi
 .PP
 For the completion of basic asynchronous control operations, the
@@ -351,34 +283,33 @@ The context field will be set to the context specified as part of the
 operation, if available, otherwise the context will be associated with
 the fabric descriptor.
 The data field will be set as described in the man page for the
-corresponding object type (e.g., see \f[C]fi_av\f[](3) for a description
-of how asynchronous address vector insertions are completed).
+corresponding object type (e.g., see \f[C]fi_av\f[R](3) for a
+description of how asynchronous address vector insertions are
+completed).
 .TP
-.B \f[I]Connection Notification\f[]
+.B \f[I]Connection Notification\f[R]
 Connection notifications are connection management notifications used to
 setup or tear down connections between endpoints.
 There are three connection notification events: FI_CONNREQ,
 FI_CONNECTED, and FI_SHUTDOWN.
 Connection notifications are reported using
-\f[C]struct\ \ \ fi_eq_cm_entry\f[]:
-.RS
-.RE
+\f[C]struct   fi_eq_cm_entry\f[R]:
 .IP
 .nf
 \f[C]
-struct\ fi_eq_cm_entry\ {
-\ \ \ \ fid_t\ \ \ \ \ \ \ \ \ \ \ \ fid;\ \ \ \ \ \ \ \ /*\ fid\ associated\ with\ request\ */
-\ \ \ \ struct\ fi_info\ \ *info;\ \ \ \ \ \ \ /*\ endpoint\ information\ */
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ data[];\ \ \ \ \ /*\ app\ connection\ data\ */
+struct fi_eq_cm_entry {
+    fid_t            fid;        /* fid associated with request */
+    struct fi_info  *info;       /* endpoint information */
+    uint8_t         data[];     /* app connection data */
 };
-\f[]
+\f[R]
 .fi
 .PP
 A connection request (FI_CONNREQ) event indicates that a remote endpoint
 wishes to establish a new connection to a listening, or passive,
 endpoint.
 The fid is the passive endpoint.
-Information regarding the requested, active endpoint\[aq]s capabilities
+Information regarding the requested, active endpoint\[cq]s capabilities
 and attributes are available from the info field.
 The application is responsible for freeing this structure by calling
 fi_freeinfo when it is no longer needed.
@@ -407,8 +338,8 @@ the connecting peer.
 .PP
 If a connection request has been accepted, an FI_CONNECTED event will be
 generated on both sides of the connection.
-The active side \-\- one that called fi_connect() \-\- may receive user
-data as part of the FI_CONNECTED event.
+The active side \[en] one that called fi_connect() \[en] may receive
+user data as part of the FI_CONNECTED event.
 The user data is passed to the connection manager on the passive side
 through the fi_accept call.
 User data is not provided with an FI_CONNECTED event on the listening
@@ -418,16 +349,14 @@ Notification that a remote peer has disconnected from an active endpoint
 is done through the FI_SHUTDOWN event.
 Shutdown notification uses struct fi_eq_cm_entry as declared above.
 The fid field for a shutdown notification refers to the active
-endpoint\[aq]s fid_ep.
+endpoint\[cq]s fid_ep.
 .TP
-.B \f[I]Asynchronous Error Notification\f[]
+.B \f[I]Asynchronous Error Notification\f[R]
 Asynchronous errors are used to report problems with fabric resources.
 Reported errors may be fatal or transient, based on the error, and
 result in the resource becoming disabled.
 Disabled resources will fail operations submitted against them until
 they are explicitly re\-enabled by the application.
-.RS
-.RE
 .PP
 Asynchronous errors may be reported for completion queues and endpoints
 of all types.
@@ -469,7 +398,7 @@ error completion was found or not.
 .PP
 EQs are optimized to report operations which have completed
 successfully.
-Operations which fail are reported \[aq]out of band\[aq].
+Operations which fail are reported `out of band'.
 Such operations are retrieved using the fi_eq_readerr function.
 When an operation that completes with an unexpected error is inserted
 into an EQ, it is placed into a temporary error queue.
@@ -484,16 +413,16 @@ The format of this structure is defined below.
 .IP
 .nf
 \f[C]
-struct\ fi_eq_err_entry\ {
-\ \ \ \ fid_t\ \ \ \ \ \ \ \ \ \ \ \ fid;\ \ \ \ \ \ \ \ /*\ fid\ associated\ with\ error\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ *context;\ \ \ \ /*\ operation\ context\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ data;\ \ \ \ \ \ \ /*\ completion\-specific\ data\ */
-\ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ err;\ \ \ \ \ \ \ \ /*\ positive\ error\ code\ */
-\ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ prov_errno;\ /*\ provider\ error\ code\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ *err_data;\ \ \ /*\ additional\ error\ data\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ err_data_size;\ /*\ size\ of\ err_data\ */
+struct fi_eq_err_entry {
+    fid_t            fid;        /* fid associated with error */
+    void            *context;    /* operation context */
+    uint64_t         data;       /* completion\-specific data */
+    int              err;        /* positive error code */
+    int              prov_errno; /* provider error code */
+    void            *err_data;   /* additional error data */
+    size_t           err_data_size; /* size of err_data */
 };
-\f[]
+\f[R]
 .fi
 .PP
 The fid will reference the fabric descriptor associated with the event.
@@ -503,8 +432,9 @@ The context field will be set to the context specified as part of the
 operation.
 .PP
 The data field will be set as described in the man page for the
-corresponding object type (e.g., see \f[C]fi_av\f[](3) for a description
-of how asynchronous address vector insertions are completed).
+corresponding object type (e.g., see \f[C]fi_av\f[R](3) for a
+description of how asynchronous address vector insertions are
+completed).
 .PP
 The general reason for the error is provided through the err field.
 Provider or operational specific error information may also be available
@@ -532,7 +462,7 @@ The EQ entry data structures share many of the same fields.
 The meanings are the same or similar for all EQ structure formats, with
 specific details described below.
 .TP
-.B \f[I]fid\f[]
+.B \f[I]fid\f[R]
 This corresponds to the fabric descriptor associated with the event.
 The type of fid depends on the event being reported.
 For FI_CONNREQ this will be the fid of the passive endpoint.
@@ -543,60 +473,46 @@ FI_JOIN_COMPLETE will point to the multicast descriptor returned as part
 of the join operation.
 Applications can use fid\->context value to retrieve the context
 associated with the fabric descriptor.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 The context value is set to the context parameter specified with the
 operation that generated the event.
 If no context parameter is associated with the operation, this field
 will be NULL.
-.RS
-.RE
 .TP
-.B \f[I]data\f[]
+.B \f[I]data\f[R]
 Data is an operation specific value or set of bytes.
 For connection events, data is application data exchanged as part of the
 connection protocol.
-.RS
-.RE
 .TP
-.B \f[I]err\f[]
+.B \f[I]err\f[R]
 This err code is a positive fabric errno associated with an event.
 The err value indicates the general reason for an error, if one
 occurred.
 See fi_errno.3 for a list of possible error codes.
-.RS
-.RE
 .TP
-.B \f[I]prov_errno\f[]
+.B \f[I]prov_errno\f[R]
 On an error, prov_errno may contain a provider specific error code.
 The use of this field and its meaning is provider specific.
 It is intended to be used as a debugging aid.
 See fi_eq_strerror for additional details on converting this error value
 into a human readable string.
-.RS
-.RE
 .TP
-.B \f[I]err_data\f[]
+.B \f[I]err_data\f[R]
 On an error, err_data may reference a provider specific amount of data
 associated with an error.
 The use of this field and its meaning is provider specific.
 It is intended to be used as a debugging aid.
 See fi_eq_strerror for additional details on converting this error data
 into a human readable string.
-.RS
-.RE
 .TP
-.B \f[I]err_data_size\f[]
+.B \f[I]err_data_size\f[R]
 On input, err_data_size indicates the size of the err_data buffer in
 bytes.
 On output, err_data_size will be set to the number of bytes copied to
 the err_data buffer.
 The err_data information is typically used with fi_eq_strerror to
 provide details about the type of error that occurred.
-.RS
-.RE
 .PP
 For compatibility purposes, if err_data_size is 0 on input, or the
 fabric was opened with release < 1.5, err_data will be set to a data
@@ -607,8 +523,8 @@ Applications must serialize access to the EQ when processing errors to
 ensure that the buffer referenced by err_data does no change.
 .SH NOTES
 .PP
-If an event queue has been overrun, it will be placed into an
-\[aq]overrun\[aq] state.
+If an event queue has been overrun, it will be placed into an `overrun'
+state.
 Write operations against an overrun EQ will fail with \-FI_EOVERRUN.
 Read operations will continue to return any valid, non\-corrupted
 events, if available.
@@ -621,41 +537,31 @@ additional events once the overrun occurs.
 .B fi_eq_open
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-.RS
-.RE
 .TP
 .B fi_eq_read / fi_eq_readerr
 On success, returns the number of bytes read from the event queue.
 On error, a negative value corresponding to fabric errno is returned.
 If no data is available to be read from the event queue, \-FI_EAGAIN is
 returned.
-.RS
-.RE
 .TP
 .B fi_eq_sread
 On success, returns the number of bytes read from the event queue.
 On error, a negative value corresponding to fabric errno is returned.
 If the timeout expires or the calling thread is signaled and no data is
 available to be read from the event queue, \-FI_EAGAIN is returned.
-.RS
-.RE
 .TP
 .B fi_eq_write
 On success, returns the number of bytes written to the event queue.
 On error, a negative value corresponding to fabric errno is returned.
-.RS
-.RE
 .TP
 .B fi_eq_strerror
 Returns a character string interpretation of the provider specific error
 returned with a completion.
-.RS
-.RE
 .PP
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_cntr\f[](3), \f[C]fi_poll\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_cntr\f[R](3), \f[C]fi_poll\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_errno.3 b/man/man3/fi_errno.3
index 1b1c2f7..919e254 100644
--- a/man/man3/fi_errno.3
+++ b/man/man3/fi_errno.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_errno" "3" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_errno" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -11,234 +11,146 @@ fi_strerror \- Convert fabric error into a printable string
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_errno.h>
+#include <rdma/fi_errno.h>
 
-const\ char\ *fi_strerror(int\ errno);
-\f[]
+const char *fi_strerror(int errno);
+\f[R]
 .fi
 .SH ERRORS
 .TP
-.B \f[I]FI_ENOENT\f[]
+.B \f[I]FI_ENOENT\f[R]
 No such file or directory.
-.RS
-.RE
 .TP
-.B \f[I]FI_EIO\f[]
+.B \f[I]FI_EIO\f[R]
 I/O error
-.RS
-.RE
 .TP
-.B \f[I]FI_E2BIG\f[]
+.B \f[I]FI_E2BIG\f[R]
 Argument list too long.
-.RS
-.RE
 .TP
-.B \f[I]FI_EBADF\f[]
+.B \f[I]FI_EBADF\f[R]
 Bad file number.
-.RS
-.RE
 .TP
-.B \f[I]FI_EAGAIN\f[]
+.B \f[I]FI_EAGAIN\f[R]
 Try again.
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOMEM\f[]
+.B \f[I]FI_ENOMEM\f[R]
 Out of memory.
-.RS
-.RE
 .TP
-.B \f[I]FI_EACCES\f[]
+.B \f[I]FI_EACCES\f[R]
 Permission denied.
-.RS
-.RE
 .TP
-.B \f[I]FI_EBUSY\f[]
+.B \f[I]FI_EBUSY\f[R]
 Device or resource busy
-.RS
-.RE
 .TP
-.B \f[I]FI_ENODEV\f[]
+.B \f[I]FI_ENODEV\f[R]
 No such device
-.RS
-.RE
 .TP
-.B \f[I]FI_EINVAL\f[]
+.B \f[I]FI_EINVAL\f[R]
 Invalid argument
-.RS
-.RE
 .TP
-.B \f[I]FI_EMFILE\f[]
+.B \f[I]FI_EMFILE\f[R]
 Too many open files
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOSPC\f[]
+.B \f[I]FI_ENOSPC\f[R]
 No space left on device
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOSYS\f[]
+.B \f[I]FI_ENOSYS\f[R]
 Function not implemented
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOMSG\f[]
+.B \f[I]FI_ENOMSG\f[R]
 No message of desired type
-.RS
-.RE
 .TP
-.B \f[I]FI_ENODATA\f[]
+.B \f[I]FI_ENODATA\f[R]
 No data available
-.RS
-.RE
 .TP
-.B \f[I]FI_EMSGSIZE\f[]
+.B \f[I]FI_EMSGSIZE\f[R]
 Message too long
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOPROTOOPT\f[]
+.B \f[I]FI_ENOPROTOOPT\f[R]
 Protocol not available
-.RS
-.RE
 .TP
-.B \f[I]FI_EOPNOTSUPP\f[]
+.B \f[I]FI_EOPNOTSUPP\f[R]
 Operation not supported on transport endpoint
-.RS
-.RE
 .TP
-.B \f[I]FI_EADDRINUSE\f[]
+.B \f[I]FI_EADDRINUSE\f[R]
 Address already in use
-.RS
-.RE
 .TP
-.B \f[I]FI_EADDRNOTAVAIL\f[]
+.B \f[I]FI_EADDRNOTAVAIL\f[R]
 Cannot assign requested address
-.RS
-.RE
 .TP
-.B \f[I]FI_ENETDOWN\f[]
+.B \f[I]FI_ENETDOWN\f[R]
 Network is down
-.RS
-.RE
 .TP
-.B \f[I]FI_ENETUNREACH\f[]
+.B \f[I]FI_ENETUNREACH\f[R]
 Network is unreachable
-.RS
-.RE
 .TP
-.B \f[I]FI_ECONNABORTED\f[]
+.B \f[I]FI_ECONNABORTED\f[R]
 Software caused connection abort
-.RS
-.RE
 .TP
-.B \f[I]FI_ECONNRESET\f[]
+.B \f[I]FI_ECONNRESET\f[R]
 Connection reset by peer
-.RS
-.RE
 .TP
-.B \f[I]FI_EISCONN\f[]
+.B \f[I]FI_EISCONN\f[R]
 Transport endpoint is already connected
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOTCONN\f[]
+.B \f[I]FI_ENOTCONN\f[R]
 Transport endpoint is not connected
-.RS
-.RE
 .TP
-.B \f[I]FI_ESHUTDOWN\f[]
+.B \f[I]FI_ESHUTDOWN\f[R]
 Cannot send after transport endpoint shutdown
-.RS
-.RE
 .TP
-.B \f[I]FI_ETIMEDOUT\f[]
+.B \f[I]FI_ETIMEDOUT\f[R]
 Operation timed out
-.RS
-.RE
 .TP
-.B \f[I]FI_ECONNREFUSED\f[]
+.B \f[I]FI_ECONNREFUSED\f[R]
 Connection refused
-.RS
-.RE
 .TP
-.B \f[I]FI_EHOSTUNREACH\f[]
+.B \f[I]FI_EHOSTUNREACH\f[R]
 No route to host
-.RS
-.RE
 .TP
-.B \f[I]FI_EALREADY\f[]
+.B \f[I]FI_EALREADY\f[R]
 Operation already in progress
-.RS
-.RE
 .TP
-.B \f[I]FI_EINPROGRESS\f[]
+.B \f[I]FI_EINPROGRESS\f[R]
 Operation now in progress
-.RS
-.RE
 .TP
-.B \f[I]FI_EREMOTEIO\f[]
+.B \f[I]FI_EREMOTEIO\f[R]
 Remote I/O error
-.RS
-.RE
 .TP
-.B \f[I]FI_ECANCELED\f[]
+.B \f[I]FI_ECANCELED\f[R]
 Operation Canceled
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOKEY\f[]
+.B \f[I]FI_ENOKEY\f[R]
 Required key not available
-.RS
-.RE
 .TP
-.B \f[I]FI_EKEYREJECTED\f[]
+.B \f[I]FI_EKEYREJECTED\f[R]
 Key was rejected by service
-.RS
-.RE
 .TP
-.B \f[I]FI_EOTHER\f[]
+.B \f[I]FI_EOTHER\f[R]
 Unspecified error
-.RS
-.RE
 .TP
-.B \f[I]FI_ETOOSMALL\f[]
+.B \f[I]FI_ETOOSMALL\f[R]
 Provided buffer is too small
-.RS
-.RE
 .TP
-.B \f[I]FI_EOPBADSTATE\f[]
+.B \f[I]FI_EOPBADSTATE\f[R]
 Operation not permitted in current state
-.RS
-.RE
 .TP
-.B \f[I]FI_EAVAIL\f[]
+.B \f[I]FI_EAVAIL\f[R]
 Error available
-.RS
-.RE
 .TP
-.B \f[I]FI_EBADFLAGS\f[]
+.B \f[I]FI_EBADFLAGS\f[R]
 Flags not supported
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOEQ\f[]
+.B \f[I]FI_ENOEQ\f[R]
 Missing or unavailable event queue
-.RS
-.RE
 .TP
-.B \f[I]FI_EDOMAIN\f[]
+.B \f[I]FI_EDOMAIN\f[R]
 Invalid resource domain
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOCQ\f[]
+.B \f[I]FI_ENOCQ\f[R]
 Missing or unavailable completion queue
-.RS
-.RE
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7)
+\f[C]fabric\f[R](7)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_fabric.3 b/man/man3/fi_fabric.3
index 40de16c..af6403d 100644
--- a/man/man3/fi_fabric.3
+++ b/man/man3/fi_fabric.3
@@ -1,92 +1,79 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_fabric" "3" "2020\-10\-20" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_fabric" "3" "2021\-06\-15" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
-fi_fabric \- Fabric domain operations
+fi_fabric \- Fabric network operations
 .TP
 .B fi_fabric / fi_close
-Open / close a fabric domain
-.RS
-.RE
+Open / close a fabric network
 .TP
 .B fi_tostr / fi_tostr_r
 Convert fabric attributes, flags, and capabilities to printable string
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
+#include <rdma/fabric.h>
 
-int\ fi_fabric(struct\ fi_fabric_attr\ *attr,
-\ \ \ \ struct\ fid_fabric\ **fabric,\ void\ *context);
+int fi_fabric(struct fi_fabric_attr *attr,
+    struct fid_fabric **fabric, void *context);
 
-int\ fi_close(struct\ fid\ *fabric);
+int fi_close(struct fid *fabric);
 
-char\ *\ fi_tostr(const\ void\ *data,\ enum\ fi_type\ datatype);
+char * fi_tostr(const void *data, enum fi_type datatype);
 
-char\ *\ fi_tostr(char\ *buf,\ size_t\ len,\ const\ void\ *data,
-\ \ \ \ enum\ fi_type\ datatype);
-\f[]
+char * fi_tostr(char *buf, size_t len, const void *data,
+    enum fi_type datatype);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Attributes of fabric to open.
-.RS
-.RE
 .TP
-.B \f[I]fabric\f[]
-Fabric domain
-.RS
-.RE
+.B \f[I]fabric\f[R]
+Fabric network
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the opened object.
 This context is returned as part of any associated asynchronous event.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Output buffer to write string.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Size in bytes of memory referenced by buf.
-.RS
-.RE
 .TP
-.B \f[I]data\f[]
+.B \f[I]data\f[R]
 Input data to convert into a string.
 The format of data is determined by the datatype parameter.
-.RS
-.RE
 .TP
-.B \f[I]datatype\f[]
+.B \f[I]datatype\f[R]
 Indicates the data to convert to a printable string.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
-A fabric domain represents a collection of hardware and software
+A fabric identifier is used to reference opened fabric resources and
+library related objects.
+.PP
+The fabric network represents a collection of hardware and software
 resources that access a single physical or virtual network.
 All network ports on a system that can communicate with each other
-through their attached networks belong to the same fabric domain.
-A fabric domain shares network addresses and can span multiple
+through their attached networks belong to the same fabric.
+A fabric network shares network addresses and can span multiple
 providers.
+An application must open a fabric network prior to allocating other
+network resources, such as communication endpoints.
 .SS fi_fabric
 .PP
-Opens a fabric provider.
+Opens a fabric network provider.
 The attributes of the fabric provider are specified through the open
 call, and may be obtained by calling fi_getinfo.
 .SS fi_close
 .PP
 The fi_close call is used to release all resources associated with a
-fabric domain or interface.
+fabric object.
 All items associated with the opened fabric must be released prior to
 calling fi_close.
 .SS fi_tostr / fi_tostr_r
@@ -99,133 +86,83 @@ referenced by the data parameter.
 Valid values for the datatype are listed below, along with the
 corresponding datatype or field value.
 .TP
-.B \f[I]FI_TYPE_INFO\f[]
+.B \f[I]FI_TYPE_INFO\f[R]
 struct fi_info, including all substructures and fields
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_EP_TYPE\f[]
+.B \f[I]FI_TYPE_EP_TYPE\f[R]
 struct fi_info::type field
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_EP_CAP\f[]
+.B \f[I]FI_TYPE_EP_CAP\f[R]
 struct fi_info::ep_cap field
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_OP_FLAGS\f[]
+.B \f[I]FI_TYPE_OP_FLAGS\f[R]
 struct fi_info::op_flags field, or general uint64_t flags
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_ADDR_FORMAT\f[]
+.B \f[I]FI_TYPE_ADDR_FORMAT\f[R]
 struct fi_info::addr_format field
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_TX_ATTR\f[]
+.B \f[I]FI_TYPE_TX_ATTR\f[R]
 struct fi_tx_attr
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_RX_ATTR\f[]
+.B \f[I]FI_TYPE_RX_ATTR\f[R]
 struct fi_rx_attr
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_EP_ATTR\f[]
+.B \f[I]FI_TYPE_EP_ATTR\f[R]
 struct fi_ep_attr
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_DOMAIN_ATTR\f[]
+.B \f[I]FI_TYPE_DOMAIN_ATTR\f[R]
 struct fi_domain_attr
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_FABRIC_ATTR\f[]
+.B \f[I]FI_TYPE_FABRIC_ATTR\f[R]
 struct fi_fabric_attr
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_THREADING\f[]
+.B \f[I]FI_TYPE_THREADING\f[R]
 enum fi_threading
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_PROGRESS\f[]
+.B \f[I]FI_TYPE_PROGRESS\f[R]
 enum fi_progress
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_PROTOCOL\f[]
+.B \f[I]FI_TYPE_PROTOCOL\f[R]
 struct fi_ep_attr::protocol field
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_MSG_ORDER\f[]
+.B \f[I]FI_TYPE_MSG_ORDER\f[R]
 struct fi_ep_attr::msg_order field
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_MODE\f[]
+.B \f[I]FI_TYPE_MODE\f[R]
 struct fi_info::mode field
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_AV_TYPE\f[]
+.B \f[I]FI_TYPE_AV_TYPE\f[R]
 enum fi_av_type
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_ATOMIC_TYPE\f[]
+.B \f[I]FI_TYPE_ATOMIC_TYPE\f[R]
 enum fi_datatype
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_ATOMIC_OP\f[]
+.B \f[I]FI_TYPE_ATOMIC_OP\f[R]
 enum fi_op
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_VERSION\f[]
+.B \f[I]FI_TYPE_VERSION\f[R]
 Returns the library version of libfabric in string form.
 The data parameter is ignored.
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_EQ_EVENT\f[]
+.B \f[I]FI_TYPE_EQ_EVENT\f[R]
 uint32_t event parameter returned from fi_eq_read().
-See \f[C]fi_eq(3)\f[] for a list of known values.
-.RS
-.RE
+See \f[C]fi_eq(3)\f[R] for a list of known values.
 .TP
-.B \f[I]FI_TYPE_CQ_EVENT_FLAGS\f[]
+.B \f[I]FI_TYPE_CQ_EVENT_FLAGS\f[R]
 uint64_t flags field in fi_cq_xxx_entry structures.
-See \f[C]fi_cq(3)\f[] for valid flags.
-.RS
-.RE
+See \f[C]fi_cq(3)\f[R] for valid flags.
 .TP
-.B \f[I]FI_TYPE_MR_MODE\f[]
+.B \f[I]FI_TYPE_MR_MODE\f[R]
 struct fi_domain_attr::mr_mode flags
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_OP_TYPE\f[]
+.B \f[I]FI_TYPE_OP_TYPE\f[R]
 enum fi_op_type
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_FID\f[]
+.B \f[I]FI_TYPE_FID\f[R]
 struct fid *
-.RS
-.RE
 .TP
-.B \f[I]FI_TYPE_HMEM_IFACE\f[]
+.B \f[I]FI_TYPE_HMEM_IFACE\f[R]
 enum fi_hmem_iface *
-.RS
-.RE
 .PP
 fi_tostr() will return a pointer to an internal libfabric buffer that
 should not be modified, and will be overwritten the next time fi_tostr()
@@ -235,7 +172,7 @@ fi_tostr() is not thread safe.
 The fi_tostr_r() function is a re\-entrant and thread safe version of
 fi_tostr().
 It writes the string into a buffer provided by the caller.
-fi_tostr_r() returns the start of the caller\[aq]s buffer.
+fi_tostr_r() returns the start of the caller\[cq]s buffer.
 .SH NOTES
 .PP
 The following resources are associated with fabric domains: access
@@ -247,14 +184,14 @@ with a fabric and a fabric provider.
 .IP
 .nf
 \f[C]
-struct\ fi_fabric_attr\ {
-\ \ \ \ struct\ fid_fabric\ *fabric;
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ \ \ *name;
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ \ \ *prov_name;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ \ \ prov_version;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ \ \ api_version;
+struct fi_fabric_attr {
+    struct fid_fabric *fabric;
+    char              *name;
+    char              *prov_name;
+    uint32_t          prov_version;
+    uint32_t          api_version;
 };
-\f[]
+\f[R]
 .fi
 .SS fabric
 .PP
@@ -276,29 +213,30 @@ A fabric identifier.
 The name of the underlying fabric provider.
 .PP
 To request an utility provider layered over a specific core provider,
-both the provider names have to be specified using ";" as delimiter.
+both the provider names have to be specified using \[lq];\[rq] as
+delimiter.
 .PP
-e.g.
-"ofi_rxm;verbs" or "verbs;ofi_rxm"
+e.g.\ \[lq]ofi_rxm;verbs\[rq] or \[lq]verbs;ofi_rxm\[rq]
 .PP
 For debugging and administrative purposes, environment variables can be
 used to control which fabric providers will be registered with
 libfabric.
-Specifying "FI_PROVIDER=foo,bar" will allow any providers with the names
-"foo" or "bar" to be registered.
-Similarly, specifying "FI_PROVIDER=^foo,bar" will prevent any providers
-with the names "foo" or "bar" from being registered.
+Specifying \[lq]FI_PROVIDER=foo,bar\[rq] will allow any providers with
+the names \[lq]foo\[rq] or \[lq]bar\[rq] to be registered.
+Similarly, specifying \[lq]FI_PROVIDER=\[ha]foo,bar\[rq] will prevent
+any providers with the names \[lq]foo\[rq] or \[lq]bar\[rq] from being
+registered.
 Providers which are not registered will not appear in fi_getinfo
 results.
 Applications which need a specific set of providers should implement
-their own filtering of fi_getinfo\[aq]s results rather than relying on
+their own filtering of fi_getinfo\[cq]s results rather than relying on
 these environment variables in a production setting.
 .SS prov_version \- Provider Version
 .PP
 Version information for the fabric provider, in a major.minor format.
 The use of the FI_MAJOR() and FI_MINOR() version macros may be used to
 extract the major and minor version data.
-See \f[C]fi_version(3)\f[].
+See \f[C]fi_version(3)\f[R].
 .PP
 In case of an utility provider layered over a core provider, the version
 would always refer to that of the utility provider.
@@ -306,16 +244,16 @@ would always refer to that of the utility provider.
 .PP
 The interface version requested by the application.
 This value corresponds to the version parameter passed into
-\f[C]fi_getinfo(3)\f[].
+\f[C]fi_getinfo(3)\f[R].
 .SH RETURN VALUE
 .PP
 Returns FI_SUCCESS on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_getinfo\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_eq\f[](3), \f[C]fi_endpoint\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_getinfo\f[R](3), \f[C]fi_domain\f[R](3),
+\f[C]fi_eq\f[R](3), \f[C]fi_endpoint\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_getinfo.3 b/man/man3/fi_getinfo.3
index 20698a6..d62f365 100644
--- a/man/man3/fi_getinfo.3
+++ b/man/man3/fi_getinfo.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_getinfo" "3" "2021\-02\-10" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_getinfo" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -11,51 +11,39 @@ fi_allocinfo, fi_dupinfo \- Allocate / duplicate an fi_info structure
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
+#include <rdma/fabric.h>
 
-int\ fi_getinfo(int\ version,\ const\ char\ *node,\ const\ char\ *service,
-\ \ \ \ \ \ \ \ uint64_t\ flags,\ const\ struct\ fi_info\ *hints,\ struct\ fi_info\ **info);
+int fi_getinfo(int version, const char *node, const char *service,
+        uint64_t flags, const struct fi_info *hints, struct fi_info **info);
 
-void\ fi_freeinfo(struct\ fi_info\ *info);
+void fi_freeinfo(struct fi_info *info);
 
-struct\ fi_info\ *fi_allocinfo(void);
+struct fi_info *fi_allocinfo(void);
 
-struct\ fi_info\ *fi_dupinfo(const\ struct\ fi_info\ *info);
-\f[]
+struct fi_info *fi_dupinfo(const struct fi_info *info);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]version\f[]
+.B \f[I]version\f[R]
 Interface version requested by application.
-.RS
-.RE
 .TP
-.B \f[I]node\f[]
+.B \f[I]node\f[R]
 Optional, name or fabric address to resolve.
-.RS
-.RE
 .TP
-.B \f[I]service\f[]
+.B \f[I]service\f[R]
 Optional, service name or port number of address.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Operation flags for the fi_getinfo call.
-.RS
-.RE
 .TP
-.B \f[I]hints\f[]
+.B \f[I]hints\f[R]
 Reference to an fi_info structure that specifies criteria for selecting
 the returned fabric information.
-.RS
-.RE
 .TP
-.B \f[I]info\f[]
+.B \f[I]info\f[R]
 A pointer to a linked list of fi_info structures containing response
 information.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 fi_getinfo returns information about available fabric services for
@@ -69,7 +57,7 @@ and the call will return \-FI_ENODATA.
 Based on the input hints, node, and service parameters, a list of fabric
 domains and endpoints will be returned.
 Each fi_info structure will describe an endpoint that meets the
-application\[aq]s specified communication criteria.
+application\[cq]s specified communication criteria.
 Each endpoint will be associated with a domain.
 Applications can restrict the number of returned endpoints by including
 additional criteria in their search hints.
@@ -136,85 +124,71 @@ substructures within it.
 .IP
 .nf
 \f[C]
-struct\ fi_info\ {
-\ \ \ \ struct\ fi_info\ \ \ \ \ \ \ \ *next;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ caps;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ mode;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ addr_format;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ src_addrlen;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ dest_addrlen;
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *src_addr;
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *dest_addr;
-\ \ \ \ fid_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ handle;
-\ \ \ \ struct\ fi_tx_attr\ \ \ \ \ *tx_attr;
-\ \ \ \ struct\ fi_rx_attr\ \ \ \ \ *rx_attr;
-\ \ \ \ struct\ fi_ep_attr\ \ \ \ \ *ep_attr;
-\ \ \ \ struct\ fi_domain_attr\ *domain_attr;
-\ \ \ \ struct\ fi_fabric_attr\ *fabric_attr;
-\ \ \ \ struct\ fid_nic\ \ \ \ \ \ \ \ *nic;
+struct fi_info {
+    struct fi_info        *next;
+    uint64_t              caps;
+    uint64_t              mode;
+    uint32_t              addr_format;
+    size_t                src_addrlen;
+    size_t                dest_addrlen;
+    void                  *src_addr;
+    void                  *dest_addr;
+    fid_t                 handle;
+    struct fi_tx_attr     *tx_attr;
+    struct fi_rx_attr     *rx_attr;
+    struct fi_ep_attr     *ep_attr;
+    struct fi_domain_attr *domain_attr;
+    struct fi_fabric_attr *fabric_attr;
+    struct fid_nic        *nic;
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]next\f[]
+.B \f[I]next\f[R]
 Pointer to the next fi_info structure in the list.
 Will be NULL if no more structures exist.
-.RS
-.RE
 .TP
-.B \f[I]caps \- fabric interface capabilities\f[]
+.B \f[I]caps \- fabric interface capabilities\f[R]
 If specified, indicates the desired capabilities of the fabric
 interfaces.
-Supported capabilities are listed in the \f[I]Capabilities\f[] section
+Supported capabilities are listed in the \f[I]Capabilities\f[R] section
 below.
-.RS
-.RE
 .TP
-.B \f[I]mode\f[]
+.B \f[I]mode\f[R]
 Operational modes supported by the application.
-See the \f[I]Mode\f[] section below.
-.RS
-.RE
+See the \f[I]Mode\f[R] section below.
 .TP
-.B \f[I]addr_format \- address format\f[]
+.B \f[I]addr_format \- address format\f[R]
 If specified, indicates the format of addresses referenced by the fabric
 interfaces and data structures.
-Supported formats are listed in the \f[I]Addressing formats\f[] section
+Supported formats are listed in the \f[I]Addressing formats\f[R] section
 below.
-.RS
-.RE
 .TP
-.B \f[I]src_addrlen \- source address length\f[]
+.B \f[I]src_addrlen \- source address length\f[R]
 Indicates the length of the source address.
-This value must be > 0 if \f[I]src_addr\f[] is non\-NULL.
+This value must be > 0 if \f[I]src_addr\f[R] is non\-NULL.
 This field will be ignored in hints if FI_SOURCE flag is set, or
-\f[I]src_addr\f[] is NULL.
-.RS
-.RE
+\f[I]src_addr\f[R] is NULL.
 .TP
-.B \f[I]dest_addrlen \- destination address length\f[]
+.B \f[I]dest_addrlen \- destination address length\f[R]
 Indicates the length of the destination address.
-This value must be > 0 if \f[I]dest_addr\f[] is non\-NULL.
+This value must be > 0 if \f[I]dest_addr\f[R] is non\-NULL.
 This field will be ignored in hints unless the node and service
-parameters are NULL or FI_SOURCE flag is set, or if \f[I]dst_addr\f[] is
-NULL.
-.RS
-.RE
+parameters are NULL or FI_SOURCE flag is set, or if \f[I]dst_addr\f[R]
+is NULL.
 .TP
-.B \f[I]src_addr \- source address\f[]
+.B \f[I]src_addr \- source address\f[R]
 If specified, indicates the source address.
 This field will be ignored in hints if FI_SOURCE flag is set.
 On output a provider shall return an address that corresponds to the
 indicated fabric, domain, node, and/or service fields.
 The format of the address is indicated by the returned
-\f[I]addr_format\f[] field.
+\f[I]addr_format\f[R] field.
 Note that any returned address is only used when opening a local
 endpoint.
 The address is not guaranteed to be usable by a peer process.
-.RS
-.RE
 .TP
-.B \f[I]dest_addr \- destination address\f[]
+.B \f[I]dest_addr \- destination address\f[R]
 If specified, indicates the destination address.
 This field will be ignored in hints unless the node and service
 parameters are NULL or FI_SOURCE flag is set.
@@ -222,10 +196,8 @@ If FI_SOURCE is not specified, on output a provider shall return an
 address the corresponds to the indicated node and/or service fields,
 relative to the fabric and domain.
 Note that any returned address is only usable locally.
-.RS
-.RE
 .TP
-.B \f[I]handle \- provider context handle\f[]
+.B \f[I]handle \- provider context handle\f[R]
 The use of this field is operation specific.
 If hints\->handle is set to struct fid_pep, the hints\->handle will be
 copied to info\->handle on output from fi_getinfo.
@@ -233,14 +205,12 @@ Other values of hints\->handle will be handled in a provider specific
 manner.
 The fi_info::handle field is also used by fi_endpoint() and fi_reject()
 calls when processing connection requests or to inherit another
-endpoint\[aq]s attributes.
-See \f[C]fi_eq\f[](3), \f[C]fi_reject\f[](3), and
-\f[C]fi_endpoint\f[](3).
+endpoint\[cq]s attributes.
+See \f[C]fi_eq\f[R](3), \f[C]fi_reject\f[R](3), and
+\f[C]fi_endpoint\f[R](3).
 The info\->handle field will be ignored by fi_dupinfo and fi_freeinfo.
-.RS
-.RE
 .TP
-.B \f[I]tx_attr \- transmit context attributes\f[]
+.B \f[I]tx_attr \- transmit context attributes\f[R]
 Optionally supplied transmit context attributes.
 Transmit context attributes may be specified and returned as part of
 fi_getinfo.
@@ -250,10 +220,8 @@ On output, the actual transmit context attributes that can be provided
 will be returned.
 Output values will be greater than or equal to the requested input
 values.
-.RS
-.RE
 .TP
-.B \f[I]rx_attr \- receive context attributes\f[]
+.B \f[I]rx_attr \- receive context attributes\f[R]
 Optionally supplied receive context attributes.
 Receive context attributes may be specified and returned as part of
 fi_getinfo.
@@ -263,10 +231,8 @@ On output, the actual receive context attributes that can be provided
 will be returned.
 Output values will be greater than or or equal to the requested input
 values.
-.RS
-.RE
 .TP
-.B \f[I]ep_attr \- endpoint attributes\f[]
+.B \f[I]ep_attr \- endpoint attributes\f[R]
 Optionally supplied endpoint attributes.
 Endpoint attributes may be specified and returned as part of fi_getinfo.
 When provided as hints, requested values of struct fi_ep_attr should be
@@ -274,11 +240,9 @@ set.
 On output, the actual endpoint attributes that can be provided will be
 returned.
 Output values will be greater than or equal to requested input values.
-See \f[C]fi_endpoint\f[](3) for details.
-.RS
-.RE
+See \f[C]fi_endpoint\f[R](3) for details.
 .TP
-.B \f[I]domain_attr \- domain attributes\f[]
+.B \f[I]domain_attr \- domain attributes\f[R]
 Optionally supplied domain attributes.
 Domain attributes may be specified and returned as part of fi_getinfo.
 When provided as hints, requested values of struct fi_domain_attr should
@@ -286,29 +250,23 @@ be set.
 On output, the actual domain attributes that can be provided will be
 returned.
 Output values will be greater than or equal to requested input values.
-See \f[C]fi_domain\f[](3) for details.
-.RS
-.RE
+See \f[C]fi_domain\f[R](3) for details.
 .TP
-.B \f[I]fabric_attr \- fabric attributes\f[]
+.B \f[I]fabric_attr \- fabric attributes\f[R]
 Optionally supplied fabric attributes.
 Fabric attributes may be specified and returned as part of fi_getinfo.
 When provided as hints, requested values of struct fi_fabric_attr should
 be set.
 On output, the actual fabric attributes that can be provided will be
 returned.
-See \f[C]fi_fabric\f[](3) for details.
-.RS
-.RE
+See \f[C]fi_fabric\f[R](3) for details.
 .TP
-.B \f[I]nic \- network interface details\f[]
+.B \f[I]nic \- network interface details\f[R]
 Optional attributes related to the hardware NIC associated with the
 specified fabric, domain, and endpoint data.
 This field is only valid for providers where the corresponding
 attributes are closely associated with a hardware NIC.
-See \f[C]fi_nic\f[](3) for details.
-.RS
-.RE
+See \f[C]fi_nic\f[R](3) for details.
 .SH CAPABILITIES
 .PP
 Interface capabilities are obtained by OR\-ing the following flags
@@ -325,7 +283,7 @@ Applications may use this feature to request a minimal set of
 requirements, then check the returned capabilities to enable additional
 optimizations.
 .TP
-.B \f[I]FI_ATOMIC\f[]
+.B \f[I]FI_ATOMIC\f[R]
 Specifies that the endpoint supports some set of atomic operations.
 Endpoints supporting this capability support operations defined by
 struct fi_ops_atomic.
@@ -334,25 +292,19 @@ initiate and be the target of remote atomic reads and writes.
 Applications can use the FI_READ, FI_WRITE, FI_REMOTE_READ, and
 FI_REMOTE_WRITE flags to restrict the types of atomic operations
 supported by an endpoint.
-.RS
-.RE
 .TP
-.B \f[I]FI_COLLECTIVE\f[]
+.B \f[I]FI_COLLECTIVE\f[R]
 Requests support for collective operations.
 Endpoints that support this capability support the collective operations
-defined in \f[C]fi_collective\f[](3).
-.RS
-.RE
+defined in \f[C]fi_collective\f[R](3).
 .TP
-.B \f[I]FI_DIRECTED_RECV\f[]
+.B \f[I]FI_DIRECTED_RECV\f[R]
 Requests that the communication endpoint use the source address of an
 incoming message when matching it with a receive buffer.
 If this capability is not set, then the src_addr parameter for msg and
 tagged receive operations is ignored.
-.RS
-.RE
 .TP
-.B \f[I]FI_FENCE\f[]
+.B \f[I]FI_FENCE\f[R]
 Indicates that the endpoint support the FI_FENCE flag on data transfer
 operations.
 Support requires tracking that all previous transmit requests to a
@@ -361,16 +313,12 @@ operation.
 Fenced operations are often used to enforce ordering between operations
 that are not otherwise guaranteed by the underlying provider or
 protocol.
-.RS
-.RE
 .TP
-.B \f[I]FI_HMEM\f[]
+.B \f[I]FI_HMEM\f[R]
 Specifies that the endpoint should support transfers to and from device
 memory.
-.RS
-.RE
 .TP
-.B \f[I]FI_LOCAL_COMM\f[]
+.B \f[I]FI_LOCAL_COMM\f[R]
 Indicates that the endpoint support host local communication.
 This flag may be used in conjunction with FI_REMOTE_COMM to indicate
 that local and remote communication are required.
@@ -380,17 +328,13 @@ affects performance.
 Providers that set FI_LOCAL_COMM but not FI_REMOTE_COMM, for example a
 shared memory provider, may only be used to communication between
 processes on the same system.
-.RS
-.RE
 .TP
-.B \f[I]FI_MSG\f[]
+.B \f[I]FI_MSG\f[R]
 Specifies that an endpoint should support sending and receiving messages
 or datagrams.
 Message capabilities imply support for send and/or receive queues.
 Endpoints supporting this capability support operations defined by
 struct fi_ops_msg.
-.RS
-.RE
 .PP
 The caps may be used to specify or restrict the type of messaging
 operations that are supported.
@@ -399,67 +343,51 @@ and receive messages.
 Applications can use the FI_SEND and FI_RECV flags to optimize an
 endpoint as send\-only or receive\-only.
 .TP
-.B \f[I]FI_MULTICAST\f[]
+.B \f[I]FI_MULTICAST\f[R]
 Indicates that the endpoint support multicast data transfers.
 This capability must be paired with FI_MSG.
 Applications can use FI_SEND and FI_RECV to optimize multicast as
 send\-only or receive\-only.
-.RS
-.RE
 .TP
-.B \f[I]FI_MULTI_RECV\f[]
+.B \f[I]FI_MULTI_RECV\f[R]
 Specifies that the endpoint must support the FI_MULTI_RECV flag when
 posting receive buffers.
-.RS
-.RE
 .TP
-.B \f[I]FI_NAMED_RX_CTX\f[]
+.B \f[I]FI_NAMED_RX_CTX\f[R]
 Requests that endpoints which support multiple receive contexts allow an
 initiator to target (or name) a specific receive context as part of a
 data transfer operation.
-.RS
-.RE
 .TP
-.B \f[I]FI_READ\f[]
+.B \f[I]FI_READ\f[R]
 Indicates that the user requires an endpoint capable of initiating reads
 against remote memory regions.
 This flag requires that FI_RMA and/or FI_ATOMIC be set.
-.RS
-.RE
 .TP
-.B \f[I]FI_RECV\f[]
+.B \f[I]FI_RECV\f[R]
 Indicates that the user requires an endpoint capable of receiving
 message data transfers.
 Message transfers include base message operations as well as tagged
 message functionality.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_COMM\f[]
+.B \f[I]FI_REMOTE_COMM\f[R]
 Indicates that the endpoint support communication with endpoints located
 at remote nodes (across the fabric).
 See FI_LOCAL_COMM for additional details.
 Providers that set FI_REMOTE_COMM but not FI_LOCAL_COMM, for example
 NICs that lack loopback support, cannot be used to communicate with
 processes on the same system.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_READ\f[]
+.B \f[I]FI_REMOTE_READ\f[R]
 Indicates that the user requires an endpoint capable of receiving read
 memory operations from remote endpoints.
 This flag requires that FI_RMA and/or FI_ATOMIC be set.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_WRITE\f[]
+.B \f[I]FI_REMOTE_WRITE\f[R]
 Indicates that the user requires an endpoint capable of receiving write
 memory operations from remote endpoints.
 This flag requires that FI_RMA and/or FI_ATOMIC be set.
-.RS
-.RE
 .TP
-.B \f[I]FI_RMA\f[]
+.B \f[I]FI_RMA\f[R]
 Specifies that the endpoint should support RMA read and write
 operations.
 Endpoints supporting this capability support operations defined by
@@ -469,43 +397,33 @@ initiate and be the target of remote memory reads and writes.
 Applications can use the FI_READ, FI_WRITE, FI_REMOTE_READ, and
 FI_REMOTE_WRITE flags to restrict the types of RMA operations supported
 by an endpoint.
-.RS
-.RE
 .TP
-.B \f[I]FI_RMA_EVENT\f[]
+.B \f[I]FI_RMA_EVENT\f[R]
 Requests that an endpoint support the generation of completion events
 when it is the target of an RMA and/or atomic operation.
 This flag requires that FI_REMOTE_READ and/or FI_REMOTE_WRITE be enabled
 on the endpoint.
-.RS
-.RE
 .TP
-.B \f[I]FI_RMA_PMEM\f[]
-Indicates that the provider is \[aq]persistent memory aware\[aq] and
-supports RMA operations to and from persistent memory.
+.B \f[I]FI_RMA_PMEM\f[R]
+Indicates that the provider is `persistent memory aware' and supports
+RMA operations to and from persistent memory.
 Persistent memory aware providers must support registration of memory
 that is backed by non\- volatile memory, RMA transfers to/from
 persistent memory, and enhanced completion semantics.
 This flag requires that FI_RMA be set.
 This capability is experimental.
-.RS
-.RE
 .TP
-.B \f[I]FI_SEND\f[]
+.B \f[I]FI_SEND\f[R]
 Indicates that the user requires an endpoint capable of sending message
 data transfers.
 Message transfers include base message operations as well as tagged
 message functionality.
-.RS
-.RE
 .TP
-.B \f[I]FI_SHARED_AV\f[]
+.B \f[I]FI_SHARED_AV\f[R]
 Requests or indicates support for address vectors which may be shared
 among multiple processes.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOURCE\f[]
+.B \f[I]FI_SOURCE\f[R]
 Requests that the endpoint return source addressing data as part of its
 completion data.
 This capability only applies to connectionless endpoints.
@@ -515,10 +433,8 @@ available in the underlying protocol in order to provide the requested
 data, which may adversely affect performance.
 The performance impact may be greater for address vectors of type
 FI_AV_TABLE.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOURCE_ERR\f[]
+.B \f[I]FI_SOURCE_ERR\f[R]
 Must be paired with FI_SOURCE.
 When specified, this requests that raw source addressing data be
 returned as part of completion data for any address that has not been
@@ -526,10 +442,8 @@ inserted into the local address vector.
 Use of this capability may require the provider to validate incoming
 source address data against addresses stored in the local address
 vector, which may adversely affect performance.
-.RS
-.RE
 .TP
-.B \f[I]FI_TAGGED\f[]
+.B \f[I]FI_TAGGED\f[R]
 Specifies that the endpoint should handle tagged message transfers.
 Tagged message transfers associate a user\-specified key or tag with
 each message that is used for matching purposes at the remote side.
@@ -539,34 +453,26 @@ In the absence of any relevant flags, FI_TAGGED implies the ability to
 send and receive tagged messages.
 Applications can use the FI_SEND and FI_RECV flags to optimize an
 endpoint as send\-only or receive\-only.
-.RS
-.RE
 .TP
-.B \f[I]FI_TRIGGER\f[]
+.B \f[I]FI_TRIGGER\f[R]
 Indicates that the endpoint should support triggered operations.
 Endpoints support this capability must meet the usage model as described
 by fi_trigger.3.
-.RS
-.RE
 .TP
-.B \f[I]FI_VARIABLE_MSG\f[]
+.B \f[I]FI_VARIABLE_MSG\f[R]
 Requests that the provider must notify a receiver when a variable length
 message is ready to be received prior to attempting to place the data.
 Such notification will include the size of the message and any
 associated message tag (for FI_TAGGED).
-See \[aq]Variable Length Messages\[aq] in fi_msg.3 for full details.
+See `Variable Length Messages' in fi_msg.3 for full details.
 Variable length messages are any messages larger than an endpoint
 configurable size.
 This flag requires that FI_MSG and/or FI_TAGGED be set.
-.RS
-.RE
 .TP
-.B \f[I]FI_WRITE\f[]
+.B \f[I]FI_WRITE\f[R]
 Indicates that the user requires an endpoint capable of initiating
 writes against remote memory regions.
 This flag requires that FI_RMA and/or FI_ATOMIC be set.
-.RS
-.RE
 .PP
 Capabilities may be grouped into three general categories: primary,
 secondary, and primary modifiers.
@@ -612,10 +518,10 @@ achieve high\-performance.
 Mode bits that remain set indicate application requirements for using
 the fabric interfaces created using the returned fi_info.
 The set of modes are listed below.
-If a NULL hints structure is provided, then the provider\[aq]s supported
+If a NULL hints structure is provided, then the provider\[cq]s supported
 set of modes will be returned in the info structure(s).
 .TP
-.B \f[I]FI_ASYNC_IOV\f[]
+.B \f[I]FI_ASYNC_IOV\f[R]
 Applications can reference multiple data buffers as part of a single
 operation through the use of IO vectors (SGEs).
 Typically, the contents of an IO vector are copied by the provider into
@@ -627,10 +533,8 @@ buffering needed for the IO vectors.
 When set, an application must not modify an IO vector of length > 1,
 including any related memory descriptor array, until the associated
 operation has completed.
-.RS
-.RE
 .TP
-.B \f[I]FI_BUFFERED_RECV\f[]
+.B \f[I]FI_BUFFERED_RECV\f[R]
 The buffered receive mode bit indicates that the provider owns the data
 buffer(s) that are accessed by the networking layer for received
 messages.
@@ -639,12 +543,10 @@ buffer into the application buffer.
 Applications that can handle message processing from network allocated
 data buffers can set this mode bit to avoid copies.
 For full details on application requirements to support this mode, see
-the \[aq]Buffered Receives\[aq] section in \f[C]fi_msg\f[](3).
+the `Buffered Receives' section in \f[C]fi_msg\f[R](3).
 This mode bit applies to FI_MSG and FI_TAGGED receive operations.
-.RS
-.RE
 .TP
-.B \f[I]FI_CONTEXT\f[]
+.B \f[I]FI_CONTEXT\f[R]
 Specifies that the provider requires that applications use struct
 fi_context as their per operation context parameter for operations that
 generated full completions.
@@ -660,28 +562,23 @@ Doing so is likely to result in stack corruption that will be difficult
 to debug.
 Users should not update or interpret the fields in this structure, or
 reuse it until the original operation has completed.
-If an operation does not generate a completion (i.e.
-the endpoint was configured with FI_SELECTIVE_COMPLETION and the
-operation was not initiated with the FI_COMPLETION flag) then the
-context parameter is ignored by the fabric provider.
+If an operation does not generate a completion (i.e.\ the endpoint was
+configured with FI_SELECTIVE_COMPLETION and the operation was not
+initiated with the FI_COMPLETION flag) then the context parameter is
+ignored by the fabric provider.
 The structure is specified in rdma/fabric.h.
-.RS
-.RE
 .TP
-.B \f[I]FI_CONTEXT2\f[]
-This bit is similar to FI_CONTEXT, but doubles the provider\[aq]s
+.B \f[I]FI_CONTEXT2\f[R]
+This bit is similar to FI_CONTEXT, but doubles the provider\[cq]s
 requirement on the size of the per context structure.
 When set, this specifies that the provider requires that applications
 use struct fi_context2 as their per operation context parameter.
 Or, optionally, an application can provide an array of two fi_context
-structures (e.g.
-struct fi_context[2]) instead.
+structures (e.g.\ struct fi_context[2]) instead.
 The requirements for using struct fi_context2 are identical as defined
 for FI_CONTEXT above.
-.RS
-.RE
 .TP
-.B \f[I]FI_LOCAL_MR\f[]
+.B \f[I]FI_LOCAL_MR\f[R]
 The provider is optimized around having applications register memory for
 locally accessed data buffers.
 Data buffers used in send and receive operations and as the source
@@ -690,12 +587,10 @@ application for access domains opened with this capability.
 This flag is defined for compatibility and is ignored if the application
 version is 1.5 or later and the domain mr_mode is set to anything other
 than FI_MR_BASIC or FI_MR_SCALABLE.
-See the domain attribute mr_mode \f[C]fi_domain\f[](3) and
-\f[C]fi_mr\f[](3).
-.RS
-.RE
+See the domain attribute mr_mode \f[C]fi_domain\f[R](3) and
+\f[C]fi_mr\f[R](3).
 .TP
-.B \f[I]FI_MSG_PREFIX\f[]
+.B \f[I]FI_MSG_PREFIX\f[R]
 Message prefix mode indicates that an application will provide buffer
 space in front of all message send and receive buffers for use by the
 provider.
@@ -705,8 +600,6 @@ The contents of the prefix space should be treated as opaque.
 The use of FI_MSG_PREFIX may improve application performance over
 certain providers by reducing the number of IO vectors referenced by
 underlying hardware and eliminating provider buffer allocation.
-.RS
-.RE
 .PP
 FI_MSG_PREFIX only applies to send and receive operations, including
 tagged sends and receives.
@@ -736,34 +629,27 @@ For scatter\-gather send/recv operations, the prefix buffer must be a
 contiguous region, though it may or may not be directly adjacent to the
 payload portion of the buffer.
 .TP
-.B \f[I]FI_NOTIFY_FLAGS_ONLY\f[]
+.B \f[I]FI_NOTIFY_FLAGS_ONLY\f[R]
 This bit indicates that general completion flags may not be set by the
 provider, and are not needed by the application.
 If specified, completion flags which simply report the type of operation
-that completed (e.g.
-send or receive) may not be set.
+that completed (e.g.\ send or receive) may not be set.
 However, completion flags that are used for remote notifications will
 still be set when applicable.
-See \f[C]fi_cq\f[](3) for details on which completion flags are valid
+See \f[C]fi_cq\f[R](3) for details on which completion flags are valid
 when this mode bit is enabled.
-.RS
-.RE
 .TP
-.B \f[I]FI_RESTRICTED_COMP\f[]
+.B \f[I]FI_RESTRICTED_COMP\f[R]
 This bit indicates that the application will only share completion
 queues and counters among endpoints, transmit contexts, and receive
 contexts that have the same set of capability flags.
-.RS
-.RE
 .TP
-.B \f[I]FI_RX_CQ_DATA\f[]
+.B \f[I]FI_RX_CQ_DATA\f[R]
 This mode bit only applies to data transfers that set FI_REMOTE_CQ_DATA.
 When set, a data transfer that carries remote CQ data will consume a
 receive buffer at the target.
 This is true even for operations that would normally not consume posted
 receive buffers, such as RMA write operations.
-.RS
-.RE
 .SH ADDRESSING FORMATS
 .PP
 Multiple fabric interfaces take as input either a source or destination
@@ -777,54 +663,40 @@ these operations.
 A provider may support one or more of the following addressing formats.
 In some cases, a selected addressing format may need to be translated or
 mapped into an address which is native to the fabric.
-See \f[C]fi_av\f[](3).
+See \f[C]fi_av\f[R](3).
 .TP
-.B \f[I]FI_ADDR_BGQ\f[]
+.B \f[I]FI_ADDR_BGQ\f[R]
 Address is an IBM proprietary format that is used with their Blue Gene Q
 systems.
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_EFA\f[]
+.B \f[I]FI_ADDR_EFA\f[R]
 Address is an Amazon Elastic Fabric Adapter (EFA) proprietary format.
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_GNI\f[]
+.B \f[I]FI_ADDR_GNI\f[R]
 Address is a Cray proprietary format that is used with their GNI
 protocol.
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_PSMX\f[]
+.B \f[I]FI_ADDR_PSMX\f[R]
 Address is an Intel proprietary format used with their Performance
 Scaled Messaging protocol.
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_PSMX2\f[]
+.B \f[I]FI_ADDR_PSMX2\f[R]
 Address is an Intel proprietary format used with their Performance
 Scaled Messaging protocol version 2.
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_PSMX3\f[]
+.B \f[I]FI_ADDR_PSMX3\f[R]
 Address is an Intel proprietary format used with their Performance
 Scaled Messaging protocol version 3.
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_STR\f[]
+.B \f[I]FI_ADDR_STR\f[R]
 Address is a formatted character string.
 The length and content of the string is address and/or provider
 specific, but in general follows a URI model:
-.RS
-.RE
 .IP
 .nf
 \f[C]
 address_format[://[node][:[service][/[field3]...][?[key=value][&k2=v2]...]]]
-\f[]
+\f[R]
 .fi
 .PP
 Examples: \- fi_sockaddr://10.31.6.12:7471 \-
@@ -835,7 +707,7 @@ Since the string formatted address does not contain any provider
 information, the prov_name field of the fabric attribute structure
 should be used to filter by provider if necessary.
 .TP
-.B \f[I]FI_FORMAT_UNSPEC\f[]
+.B \f[I]FI_FORMAT_UNSPEC\f[R]
 FI_FORMAT_UNSPEC indicates that a provider specific address format
 should be selected.
 Provider specific addresses may be protocol specific or a vendor
@@ -846,50 +718,36 @@ FI_FORMAT_UNSPEC targets apps which make use of an out of band address
 exchange.
 Applications which use FI_FORMAT_UNSPEC may use fi_getname() to obtain a
 provider specific address assigned to an allocated endpoint.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKADDR\f[]
+.B \f[I]FI_SOCKADDR\f[R]
 Address is of type sockaddr.
 The specific socket address format will be determined at run time by
 interfaces examining the sa_family field.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKADDR_IB\f[]
+.B \f[I]FI_SOCKADDR_IB\f[R]
 Address is of type sockaddr_ib (defined in Linux kernel source)
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKADDR_IN\f[]
+.B \f[I]FI_SOCKADDR_IN\f[R]
 Address is of type sockaddr_in (IPv4).
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKADDR_IN6\f[]
+.B \f[I]FI_SOCKADDR_IN6\f[R]
 Address is of type sockaddr_in6 (IPv6).
-.RS
-.RE
 .TP
-.B \f[I]FI_ADDR_PSMX\f[]
+.B \f[I]FI_ADDR_PSMX\f[R]
 Address is an Intel proprietary format that is used with their PSMX
 (extended performance scaled messaging) protocol.
-.RS
-.RE
 .SH FLAGS
 .PP
 The operation of the fi_getinfo call may be controlled through the use
 of input flags.
 Valid flags include the following.
 .TP
-.B \f[I]FI_NUMERICHOST\f[]
+.B \f[I]FI_NUMERICHOST\f[R]
 Indicates that the node parameter is a numeric string representation of
 a fabric address, such as a dotted decimal IP address.
 Use of this flag will suppress any lengthy name resolution protocol.
-.RS
-.RE
 .TP
-.B \f[I]FI_PROV_ATTR_ONLY\f[]
+.B \f[I]FI_PROV_ATTR_ONLY\f[R]
 Indicates that the caller is only querying for what providers are
 potentially available.
 All providers will return exactly one fi_info struct, regardless of
@@ -898,23 +756,19 @@ The returned fi_info struct will contain default values for all members,
 with the exception of fabric_attr.
 The fabric_attr member will have the prov_name and prov_version values
 filled in.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOURCE\f[]
+.B \f[I]FI_SOURCE\f[R]
 Indicates that the node and service parameters specify the local source
 address to associate with an endpoint.
 If specified, either the node and/or service parameter must be
 non\-NULL.
 This flag is often used with passive endpoints.
-.RS
-.RE
 .SH RETURN VALUE
 .PP
 fi_getinfo() returns 0 on success.
 On error, fi_getinfo() returns a negative value corresponding to fabric
 errno.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .PP
 fi_allocinfo() returns a pointer to a new fi_info structure on success,
 or NULL on error.
@@ -925,34 +779,28 @@ Both calls require that the returned fi_info structure be freed via
 fi_freeinfo().
 .SH ERRORS
 .TP
-.B \f[I]FI_EBADFLAGS\f[]
+.B \f[I]FI_EBADFLAGS\f[R]
 The specified endpoint or domain capability or operation flags are
 invalid.
-.RS
-.RE
 .TP
-.B \f[I]FI_ENODATA\f[]
+.B \f[I]FI_ENODATA\f[R]
 Indicates that no providers could be found which support the requested
 fabric information.
-.RS
-.RE
 .TP
-.B \f[I]FI_ENOMEM\f[]
+.B \f[I]FI_ENOMEM\f[R]
 Indicates that there was insufficient memory to complete the operation.
-.RS
-.RE
 .SH NOTES
 .PP
 If hints are provided, the operation will be controlled by the values
 that are supplied in the various fields (see section on
-\f[I]fi_info\f[]).
+\f[I]fi_info\f[R]).
 Applications that require specific communication interfaces, domains,
 capabilities or other requirements, can specify them using fields in
-\f[I]hints\f[].
-Libfabric returns a linked list in \f[I]info\f[] that points to a list
+\f[I]hints\f[R].
+Libfabric returns a linked list in \f[I]info\f[R] that points to a list
 of matching interfaces.
-\f[I]info\f[] is set to NULL if there are no communication interfaces or
-none match the input hints.
+\f[I]info\f[R] is set to NULL if there are no communication interfaces
+or none match the input hints.
 .PP
 If node is provided, fi_getinfo will attempt to resolve the fabric
 address to the given node.
@@ -964,11 +812,11 @@ by fi_getinfo.
 If neither node, service or hints are provided, then fi_getinfo simply
 returns the list all available communication interfaces.
 .PP
-Multiple threads may call \f[C]fi_getinfo\f[] simultaneously, without
+Multiple threads may call \f[C]fi_getinfo\f[R] simultaneously, without
 any requirement for serialization.
 .SH SEE ALSO
 .PP
-\f[C]fi_open\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_nic\f[](3)
+\f[C]fi_open\f[R](3), \f[C]fi_endpoint\f[R](3), \f[C]fi_domain\f[R](3),
+\f[C]fi_nic\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_mr.3 b/man/man3/fi_mr.3
index c790188..e4ac603 100644
--- a/man/man3/fi_mr.3
+++ b/man/man3/fi_mr.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_mr" "3" "2020\-10\-01" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_mr" "3" "2021\-07\-07" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,162 +8,116 @@ fi_mr \- Memory region operations
 .TP
 .B fi_mr_reg / fi_mr_regv / fi_mr_regattr
 Register local memory buffers for direct fabric access
-.RS
-.RE
 .TP
 .B fi_close
 Deregister registered memory buffers.
-.RS
-.RE
 .TP
 .B fi_mr_desc
 Return a local descriptor associated with a registered memory region
-.RS
-.RE
 .TP
 .B fi_mr_key
 Return the remote key needed to access a registered memory region
-.RS
-.RE
 .TP
 .B fi_mr_raw_attr
 Return raw memory region attributes.
-.RS
-.RE
 .TP
 .B fi_mr_map_raw
 Converts a raw memory region key into a key that is usable for data
 transfer operations.
-.RS
-.RE
 .TP
 .B fi_mr_unmap_key
 Releases a previously mapped raw memory region key.
-.RS
-.RE
 .TP
 .B fi_mr_bind
 Associate a registered memory region with a completion counter or an
 endpoint.
-.RS
-.RE
 .TP
 .B fi_mr_refresh
 Updates the memory pages associated with a memory region.
-.RS
-.RE
 .TP
 .B fi_mr_enable
 Enables a memory region for use.
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_mr_reg(struct\ fid_domain\ *domain,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ uint64_t\ access,\ uint64_t\ offset,\ uint64_t\ requested_key,
-\ \ \ \ uint64_t\ flags,\ struct\ fid_mr\ **mr,\ void\ *context);
+int fi_mr_reg(struct fid_domain *domain, const void *buf, size_t len,
+    uint64_t access, uint64_t offset, uint64_t requested_key,
+    uint64_t flags, struct fid_mr **mr, void *context);
 
-int\ fi_mr_regv(struct\ fid_domain\ *domain,\ const\ struct\ iovec\ *\ iov,
-\ \ \ \ size_t\ count,\ uint64_t\ access,\ uint64_t\ offset,\ uint64_t\ requested_key,
-\ \ \ \ uint64_t\ flags,\ struct\ fid_mr\ **mr,\ void\ *context);
+int fi_mr_regv(struct fid_domain *domain, const struct iovec * iov,
+    size_t count, uint64_t access, uint64_t offset, uint64_t requested_key,
+    uint64_t flags, struct fid_mr **mr, void *context);
 
-int\ fi_mr_regattr(struct\ fid_domain\ *domain,\ const\ struct\ fi_mr_attr\ *attr,
-\ \ \ \ uint64_t\ flags,\ struct\ fid_mr\ **mr);
+int fi_mr_regattr(struct fid_domain *domain, const struct fi_mr_attr *attr,
+    uint64_t flags, struct fid_mr **mr);
 
-int\ fi_close(struct\ fid\ *mr);
+int fi_close(struct fid *mr);
 
-void\ *\ fi_mr_desc(struct\ fid_mr\ *mr);
+void * fi_mr_desc(struct fid_mr *mr);
 
-uint64_t\ fi_mr_key(struct\ fid_mr\ *mr);
+uint64_t fi_mr_key(struct fid_mr *mr);
 
-int\ fi_mr_raw_attr(struct\ fid_mr\ *mr,\ uint64_t\ *base_addr,
-\ \ \ \ uint8_t\ *raw_key,\ size_t\ *key_size,\ uint64_t\ flags);
+int fi_mr_raw_attr(struct fid_mr *mr, uint64_t *base_addr,
+    uint8_t *raw_key, size_t *key_size, uint64_t flags);
 
-int\ fi_mr_map_raw(struct\ fid_domain\ *domain,\ uint64_t\ base_addr,
-\ \ \ \ uint8_t\ *raw_key,\ size_t\ key_size,\ uint64_t\ *key,\ uint64_t\ flags);
+int fi_mr_map_raw(struct fid_domain *domain, uint64_t base_addr,
+    uint8_t *raw_key, size_t key_size, uint64_t *key, uint64_t flags);
 
-int\ fi_mr_unmap_key(struct\ fid_domain\ *domain,\ uint64_t\ key);
+int fi_mr_unmap_key(struct fid_domain *domain, uint64_t key);
 
-int\ fi_mr_bind(struct\ fid_mr\ *mr,\ struct\ fid\ *bfid,\ uint64_t\ flags);
+int fi_mr_bind(struct fid_mr *mr, struct fid *bfid, uint64_t flags);
 
-int\ fi_mr_refresh(struct\ fid_mr\ *mr,\ const\ struct\ iovec\ *iov,\ size,\ count,
-\ \ \ \ uint64_t\ flags)
+int fi_mr_refresh(struct fid_mr *mr, const struct iovec *iov,
+    size_t count, uint64_t flags);
 
-int\ fi_mr_enable(struct\ fid_mr\ *mr);
-\f[]
+int fi_mr_enable(struct fid_mr *mr);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]domain\f[]
+.B \f[I]domain\f[R]
 Resource domain
-.RS
-.RE
 .TP
-.B \f[I]mr\f[]
+.B \f[I]mr\f[R]
 Memory region
-.RS
-.RE
 .TP
-.B \f[I]bfid\f[]
+.B \f[I]bfid\f[R]
 Fabric identifier of an associated resource.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified context associated with the memory region.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Memory buffer to register with the fabric hardware.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Length of memory buffer to register.
 Must be > 0.
-.RS
-.RE
 .TP
-.B \f[I]iov\f[]
+.B \f[I]iov\f[R]
 Vectored memory buffer.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Count of vectored buffer entries.
-.RS
-.RE
 .TP
-.B \f[I]access\f[]
+.B \f[I]access\f[R]
 Memory access permissions associated with registration
-.RS
-.RE
 .TP
-.B \f[I]offset\f[]
+.B \f[I]offset\f[R]
 Optional specified offset for accessing specified registered buffers.
 This parameter is reserved for future use and must be 0.
-.RS
-.RE
 .TP
-.B \f[I]requested_key\f[]
+.B \f[I]requested_key\f[R]
 Optional requested remote key associated with registered buffers.
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Memory region attributes
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply to the operation.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Registered memory regions associate memory buffers with permissions
@@ -179,15 +133,13 @@ order to use memory buffers with libfabric interfaces.
 .PP
 The following apply to memory registration.
 .TP
-.B \f[I]Default Memory Registration\f[]
+.B \f[I]Default Memory Registration\f[R]
 If no mr_mode bits are set, the default behaviors describe below are
 followed.
 Historically, these defaults were collectively referred to as scalable
 memory registration.
 The default requirements are outlined below, followed by definitions of
 how each mr_mode bit alters the definition.
-.RS
-.RE
 .PP
 Compatibility: For library versions 1.4 and earlier, this was indicated
 by setting mr_mode to FI_MR_SCALABLE and the fi_info mode bit
@@ -224,8 +176,8 @@ The key size is restricted to a maximum of 8 bytes.
 .PP
 With scalable registration, locally accessed data buffers are not
 registered.
-This includes source buffers for all transmit operations \-\- sends,
-tagged sends, RMA, and atomics \-\- as well as buffers posted for
+This includes source buffers for all transmit operations \[en] sends,
+tagged sends, RMA, and atomics \[en] as well as buffers posted for
 receive and tagged receive operations.
 .PP
 Although the default memory registration behavior is convenient for
@@ -244,20 +196,17 @@ Importantly, applications that choose to support an mr_mode must be
 prepared to handle the case where the mr_mode is not required.
 A provider will clear an mr_mode bit if it is not needed.
 .TP
-.B \f[I]FI_MR_LOCAL\f[]
+.B \f[I]FI_MR_LOCAL\f[R]
 When the FI_MR_LOCAL mode bit is set, applications must register all
 data buffers that will be accessed by the local hardware and provide a
 valid desc parameter into applicable data transfer operations.
 When FI_MR_LOCAL is zero, applications are not required to register data
-buffers before using them for local operations (e.g.
-send and receive data buffers).
+buffers before using them for local operations (e.g.\ send and receive
+data buffers).
 The desc parameter into data transfer operations will be ignored in this
-case, unless otherwise required (e.g.
-se FI_MR_HMEM).
+case, unless otherwise required (e.g.\ se FI_MR_HMEM).
 It is recommended that applications pass in NULL for desc when not
 required.
-.RS
-.RE
 .PP
 A provider may hide local registration requirements from applications by
 making use of an internal registration cache or similar mechanisms.
@@ -272,33 +221,27 @@ Note: the FI_MR_LOCAL mr_mode bit replaces the FI_LOCAL_MR fi_info mode
 bit.
 When FI_MR_LOCAL is set, FI_LOCAL_MR is ignored.
 .TP
-.B \f[I]FI_MR_RAW\f[]
+.B \f[I]FI_MR_RAW\f[R]
 Raw memory regions are used to support providers with keys larger than
 64\-bits or require setup at the peer.
 When the FI_MR_RAW bit is set, applications must use fi_mr_raw_attr()
 locally and fi_mr_map_raw() at the peer before targeting a memory region
 as part of any data transfer request.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_VIRT_ADDR\f[]
+.B \f[I]FI_MR_VIRT_ADDR\f[R]
 The FI_MR_VIRT_ADDR bit indicates that the provider references memory
 regions by virtual address, rather than a 0\-based offset.
 Peers that target memory regions registered with FI_MR_VIRT_ADDR specify
-the destination memory buffer using the target\[aq]s virtual address,
+the destination memory buffer using the target\[cq]s virtual address,
 with any offset into the region specified as virtual address + offset.
 Support of this bit typically implies that peers must exchange
 addressing data prior to initiating any RMA or atomic operation.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_ALLOCATED\f[]
+.B \f[I]FI_MR_ALLOCATED\f[R]
 When set, all registered memory regions must be backed by physical
 memory pages at the time the registration call is made.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_PROV_KEY\f[]
+.B \f[I]FI_MR_PROV_KEY\f[R]
 This memory region mode indicates that the provider does not support
 application requested MR keys.
 MR keys are returned by the provider.
@@ -306,10 +249,8 @@ Applications that support FI_MR_PROV_KEY can obtain the provider key
 using fi_mr_key(), unless FI_MR_RAW is also set.
 The returned key should then be exchanged with peers prior to initiating
 an RMA or atomic operation.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_MMU_NOTIFY\f[]
+.B \f[I]FI_MR_MMU_NOTIFY\f[R]
 FI_MR_MMU_NOTIFY is typically set by providers that support memory
 registration against memory regions that are not necessarily backed by
 allocated physical pages at the time the memory registration occurs.
@@ -321,21 +262,17 @@ now back the region.
 The notification is necessary for providers that cannot hook directly
 into the operating system page tables or memory management unit.
 See fi_mr_refresh() for notification details.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_RMA_EVENT\f[]
+.B \f[I]FI_MR_RMA_EVENT\f[R]
 This mode bit indicates that the provider must configure memory regions
 that are associated with RMA events prior to their use.
 This includes all memory regions that are associated with completion
 counters.
 When set, applications must indicate if a memory region will be
-associated with a completion counter as part of the region\[aq]s
+associated with a completion counter as part of the region\[cq]s
 creation.
 This is done by passing in the FI_RMA_EVENT flag to the memory
 registration call.
-.RS
-.RE
 .PP
 Such memory regions will be created in a disabled state and must be
 associated with all completion counters prior to being enabled.
@@ -343,27 +280,22 @@ To enable a memory region, the application must call fi_mr_enable().
 After calling fi_mr_enable(), no further resource bindings may be made
 to the memory region.
 .TP
-.B \f[I]FI_MR_ENDPOINT\f[]
+.B \f[I]FI_MR_ENDPOINT\f[R]
 This mode bit indicates that the provider associates memory regions with
 endpoints rather than domains.
 Memory regions that are registered with the provider are created in a
 disabled state and must be bound to an endpoint prior to being enabled.
 To bind the MR with an endpoint, the application must use fi_mr_bind().
 To enable the memory region, the application must call fi_mr_enable().
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_HMEM\f[]
+.B \f[I]FI_MR_HMEM\f[R]
 This mode bit is associated with the FI_HMEM capability.
 If FI_MR_HMEM is set, the application must register buffers that were
 allocated using a device call and provide a valid desc parameter into
 applicable data transfer operations even if they are only used for local
-operations (e.g.
-send and receive data buffers).
+operations (e.g.\ send and receive data buffers).
 Device memory must be registered using the fi_mr_regattr call, with the
 iface and device fields filled out.
-.RS
-.RE
 .PP
 If FI_MR_HMEM is set, but FI_MR_LOCAL is unset, only device buffers must
 be registered when used locally.
@@ -372,7 +304,7 @@ must either be valid or NULL.
 Similarly, if FI_MR_LOCAL is set, but FI_MR_HMEM is not, the desc
 parameter must either be valid or NULL.
 .TP
-.B \f[I]Basic Memory Registration\f[]
+.B \f[I]Basic Memory Registration\f[R]
 Basic memory registration was deprecated in libfabric version 1.5, but
 is supported for backwards compatibility.
 Basic memory registration is indicated by setting mr_mode equal to
@@ -382,8 +314,6 @@ Unlike other mr_mode bits, if FI_MR_BASIC is set on input to
 fi_getinfo(), it will not be cleared by the provider.
 That is, setting mr_mode equal to FI_MR_BASIC forces basic registration
 if the provider supports it.
-.RS
-.RE
 .PP
 The behavior of basic registration is equivalent to requiring the
 following mr_mode bits: FI_MR_VIRT_ADDR, FI_MR_ALLOCATED, and
@@ -392,8 +322,8 @@ Additionally, providers that support basic registration usually require
 the (deprecated) fi_info mode bit FI_LOCAL_MR, which was incorporated
 into the FI_MR_LOCAL mr_mode bit.
 .PP
-The registrations functions \-\- fi_mr_reg, fi_mr_regv, and
-fi_mr_regattr \-\- are used to register one or more memory regions with
+The registrations functions \[en] fi_mr_reg, fi_mr_regv, and
+fi_mr_regattr \[en] are used to register one or more memory regions with
 fabric resources.
 The main difference between registration functions are the number and
 type of parameters that they accept as input.
@@ -549,13 +479,11 @@ with endpoints (see FI_MR_ENDPOINT).
 When binding with a counter, the type of events tracked against the
 memory region is based on the bitwise OR of the following flags.
 .TP
-.B \f[I]FI_REMOTE_WRITE\f[]
+.B \f[I]FI_REMOTE_WRITE\f[R]
 Generates an event whenever a remote RMA write or atomic operation
 modifies the memory region.
 Use of this flag requires that the endpoint through which the MR is
 accessed be created with the FI_RMA_EVENT capability.
-.RS
-.RE
 .PP
 When binding the memory region to an endpoint, flags should be 0.
 .SS fi_mr_refresh
@@ -595,23 +523,23 @@ passed directly into calls as function parameters.
 .IP
 .nf
 \f[C]
-struct\ fi_mr_attr\ {
-\ \ \ \ const\ struct\ iovec\ *mr_iov;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ iov_count;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ access;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ offset;
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ requested_key;
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *context;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ auth_key_size;
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ \ \ \ *auth_key;
-\ \ \ \ enum\ fi_hmem_iface\ iface;
-\ \ \ \ union\ {
-\ \ \ \ \ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ reserved;
-\ \ \ \ \ \ \ \ int\ \ \ \ \ \ \ \ \ \ \ \ \ \ cuda;
-\ \ \ \ \ \ \ \ int\ \ \ \ \ \ ze
-\ \ \ \ }\ device;
+struct fi_mr_attr {
+    const struct iovec *mr_iov;
+    size_t             iov_count;
+    uint64_t           access;
+    uint64_t           offset;
+    uint64_t           requested_key;
+    void               *context;
+    size_t             auth_key_size;
+    uint8_t            *auth_key;
+    enum fi_hmem_iface iface;
+    union {
+        uint64_t         reserved;
+        int              cuda;
+        int      ze
+    } device;
 };
-\f[]
+\f[R]
 .fi
 .SS mr_iov
 .PP
@@ -623,55 +551,43 @@ The number of entries in the iovec is specified by iov_count.
 The number of entries in the mr_iov array.
 The maximum number of memory buffers that may be associated with a
 single memory region is specified as the mr_iov_limit domain attribute.
-See \f[C]fi_domain(3)\f[].
+See \f[C]fi_domain(3)\f[R].
 .SS access
 .PP
-Indicates the type of \f[I]operations\f[] that the local or a peer
+Indicates the type of \f[I]operations\f[R] that the local or a peer
 endpoint may perform on registered memory region.
 Supported access permissions are the bitwise OR of the following flags:
 .TP
-.B \f[I]FI_SEND\f[]
+.B \f[I]FI_SEND\f[R]
 The memory buffer may be used in outgoing message data transfers.
 This includes fi_msg and fi_tagged send operations.
-.RS
-.RE
 .TP
-.B \f[I]FI_RECV\f[]
+.B \f[I]FI_RECV\f[R]
 The memory buffer may be used to receive inbound message transfers.
 This includes fi_msg and fi_tagged receive operations.
-.RS
-.RE
 .TP
-.B \f[I]FI_READ\f[]
+.B \f[I]FI_READ\f[R]
 The memory buffer may be used as the result buffer for RMA read and
 atomic operations on the initiator side.
 Note that from the viewpoint of the application, the memory buffer is
 being written into by the network.
-.RS
-.RE
 .TP
-.B \f[I]FI_WRITE\f[]
+.B \f[I]FI_WRITE\f[R]
 The memory buffer may be used as the source buffer for RMA write and
 atomic operations on the initiator side.
 Note that from the viewpoint of the application, the endpoint is reading
 from the memory buffer and copying the data onto the network.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_READ\f[]
+.B \f[I]FI_REMOTE_READ\f[R]
 The memory buffer may be used as the source buffer of an RMA read
 operation on the target side.
 The contents of the memory buffer are not modified by such operations.
-.RS
-.RE
 .TP
-.B \f[I]FI_REMOTE_WRITE\f[]
+.B \f[I]FI_REMOTE_WRITE\f[R]
 The memory buffer may be used as the target buffer of an RMA write or
 atomic operation.
 The contents of the memory buffer may be modified as a result of such
 operations.
-.RS
-.RE
 .PP
 Note that some providers may not enforce fine grained access
 permissions.
@@ -723,48 +639,36 @@ and manage the memory region.
 This field is ignored unless the application has requested the FI_HMEM
 capability.
 .TP
-.B \f[I]FI_HMEM_SYSTEM\f[]
+.B \f[I]FI_HMEM_SYSTEM\f[R]
 Uses standard operating system calls and libraries, such as malloc,
 calloc, realloc, mmap, and free.
-.RS
-.RE
 .TP
-.B \f[I]FI_HMEM_CUDA\f[]
+.B \f[I]FI_HMEM_CUDA\f[R]
 Uses Nvidia CUDA interfaces such as cuMemAlloc, cuMemAllocHost,
 cuMemAllocManaged, cuMemFree, cudaMalloc, cudaFree.
-.RS
-.RE
 .TP
-.B \f[I]FI_HMEM_ROCR\f[]
+.B \f[I]FI_HMEM_ROCR\f[R]
 Uses AMD ROCR interfaces such as hsa_memory_allocate and
 hsa_memory_free.
-.RS
-.RE
 .TP
-.B \f[I]FI_HMEM_ZE\f[]
+.B \f[I]FI_HMEM_ZE\f[R]
 Uses Intel L0 ZE interfaces such as zeDriverAllocSharedMem,
 zeDriverFreeMem.
-.RS
-.RE
 .SS device
 .PP
 Reserved 64 bits for device identifier if using non\-standard HMEM
 interface.
 This field is ignore unless the iface field is valid.
 .TP
-.B \f[I]cuda\f[]
+.B \f[I]cuda\f[R]
 For FI_HMEM_CUDA, this is equivalent to CUdevice (int).
-.RS
-.RE
 .TP
-.B \f[I]ze\f[]
+.B \f[I]ze\f[R]
 For FI_HMEM_ZE, this is equivalent to the ze_device_handle_t index
 (int).
-.RS
-.RE
 .SH NOTES
 .PP
-Direct access to an application\[aq]s memory by a remote peer requires
+Direct access to an application\[cq]s memory by a remote peer requires
 that the application register the targeted memory buffer(s).
 This is typically done by calling one of the fi_mr_reg* routines.
 For FI_MR_PROV_KEY, the provider will return a key that must be used by
@@ -791,50 +695,87 @@ footprint overhead, making it less desirable for highly scalable apps.
 .PP
 The follow flag may be specified to any memory registration call.
 .TP
-.B \f[I]FI_RMA_EVENT\f[]
+.B \f[I]FI_RMA_EVENT\f[R]
 This flag indicates that the specified memory region will be associated
 with a completion counter used to count RMA operations that access the
 MR.
-.RS
-.RE
 .TP
-.B \f[I]FI_RMA_PMEM\f[]
+.B \f[I]FI_RMA_PMEM\f[R]
 This flag indicates that the underlying memory region is backed by
 persistent memory and will be used in RMA operations.
 It must be specified if persistent completion semantics or persistent
 data transfers are required when accessing the registered region.
-.RS
-.RE
+.TP
+.B \f[I]FI_HMEM_DEVICE_ONLY\f[R]
+This flag indicates that the memory is only accessible by a device.
+Which device is specified by the fi_mr_attr fields iface and device.
+This refers to memory regions that were allocated using a device API
+AllocDevice call (as opposed to using the host allocation or
+unified/shared memory allocation).
+.SH MEMORY DOMAINS
+.PP
+Memory domains identify the physical separation of memory which may or
+may not be accessible through the same virtual address space.
+Traditionally, applications only dealt with a single memory domain, that
+of host memory tightly coupled with the system CPUs.
+With the introduction of device and non\-uniform memory subsystems,
+applications often need to be aware of which memory domain a particular
+virtual address maps to.
+.PP
+As a general rule, separate physical devices can be considered to have
+their own memory domains.
+For example, a NIC may have user accessible memory, and would be
+considered a separate memory domain from memory on a GPU.
+Both the NIC and GPU memory domains are separate from host system
+memory.
+Individual GPUs or computation accelerators may have distinct memory
+domains, or may be connected in such a way (e.g.\ a GPU specific fabric)
+that all GPUs would belong to the same memory domain.
+Unfortunately, identifying memory domains is specific to each system and
+its physical and/or virtual configuration.
+.PP
+Understanding memory domains in heterogenous memory environments is
+important as it can impact data ordering and visibility as viewed by an
+application.
+It is also important to understand which memory domain an application is
+most tightly coupled to.
+In most cases, applications are tightly coupled to host memory.
+However, an application running directly on a GPU or NIC may be more
+tightly coupled to memory associated with those devices.
+.PP
+Memory regions are often associated with a single memory domain.
+The domain is often indicated by the fi_mr_attr iface and device fields.
+Though it is possible for physical pages backing a virtual memory region
+to migrate between memory domains based on access patterns.
+For example, the physical pages referenced by a virtual address range
+could migrate between host memory and GPU memory, depending on which
+computational unit is actively using it.
+.PP
+See the \f[C]fi_endpoint\f[R](3) and \f[C]fi_cq\f[R](3) man pages for
+addition discussion on message, data, and completion ordering semantics,
+including the impact of memory domains.
 .SH RETURN VALUES
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
 .PP
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .TP
-.B \f[I]\-FI_ENOKEY\f[]
+.B \f[I]\-FI_ENOKEY\f[R]
 The requested_key is already in use.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_EKEYREJECTED\f[]
+.B \f[I]\-FI_EKEYREJECTED\f[R]
 The requested_key is not available.
 They key may be out of the range supported by the provider, or the
 provider may not support user\-requested memory registration keys.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_ENOSYS\f[]
+.B \f[I]\-FI_ENOSYS\f[R]
 Returned by fi_mr_bind if the provider does not support reporting events
 based on access to registered memory regions.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_EBADFLAGS\f[]
+.B \f[I]\-FI_EBADFLAGS\f[R]
 Returned if the specified flags are not supported by the provider.
-.RS
-.RE
 .SH MEMORY REGISTRATION CACHE
 .PP
 Many hardware NICs accessed by libfabric require that data buffers be
@@ -856,11 +797,11 @@ or device memory cannot be cached.
 .PP
 As a general rule, if hardware requires the FI_MR_LOCAL mode bit
 described above, but this is not supported by the application, a memory
-registration cache \f[I]may\f[] be in use.
+registration cache \f[I]may\f[R] be in use.
 The following environment variables may be used to configure
 registration caches.
 .TP
-.B \f[I]FI_MR_CACHE_MAX_SIZE\f[]
+.B \f[I]FI_MR_CACHE_MAX_SIZE\f[R]
 This defines the total number of bytes for all memory regions that may
 be tracked by the cache.
 If not set, the cache has no limit on how many bytes may be registered
@@ -868,10 +809,8 @@ and cached.
 Setting this will reduce the amount of memory that is not actively being
 used as part of a data transfer that is registered with a provider.
 By default, the cache size is unlimited.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_CACHE_MAX_COUNT\f[]
+.B \f[I]FI_MR_CACHE_MAX_COUNT\f[R]
 This defines the total number of memory regions that may be registered
 with the cache.
 If not set, a default limit is chosen.
@@ -879,10 +818,8 @@ Setting this will reduce the number of regions that are registered,
 regardless of their size, which are not actively being used as part of a
 data transfer.
 Setting this to zero will disable registration caching.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_CACHE_MONITOR\f[]
+.B \f[I]FI_MR_CACHE_MONITOR\f[R]
 The cache monitor is responsible for detecting system memory
 (FI_HMEM_SYSTEM) changes made between the virtual addresses used by an
 application and the underlying physical pages.
@@ -895,31 +832,40 @@ deallocation calls which may result in the mappings changing, such as
 malloc, mmap, free, etc.
 Note that memhooks operates at the elf linker layer, and does not use
 glibc memory hooks.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_CUDA_CACHE_MONITOR_ENABLED\f[]
+.B \f[I]FI_MR_CUDA_CACHE_MONITOR_ENABLED\f[R]
 The CUDA cache monitor is responsible for detecting CUDA device memory
 (FI_HMEM_CUDA) changes made between the device virtual addresses used by
 an application and the underlying device physical pages.
 Valid monitor options are: 0 or 1.
 Note that the CUDA memory monitor requires a CUDA toolkit version with
 unified virtual addressing enabled.
-.RS
-.RE
 .TP
-.B \f[I]FI_MR_ROCR_CACHE_MONITOR_ENABLED\f[]
+.B \f[I]FI_MR_ROCR_CACHE_MONITOR_ENABLED\f[R]
 The ROCR cache monitor is responsible for detecting ROCR device memory
 (FI_HMEM_ROCR) changes made between the device virtual addresses used by
 an application and the underlying device physical pages.
 Valid monitor options are: 0 or 1.
 Note that the ROCR memory monitor requires a ROCR version with unified
 virtual addressing enabled.
-.RS
-.RE
+.TP
+.B \f[I]FI_MR_ZE_CACHE_MONITOR_ENABLED\f[R]
+The ZE cache monitor is responsible for detecting ZE device memory
+(FI_HMEM_ZE) changes made between the device virtual addresses used by
+an application and the underlying device physical pages.
+Valid monitor options are: 0 or 1.
+.PP
+More direct access to the internal registration cache is possible
+through the fi_open() call, using the \[lq]mr_cache\[rq] service name.
+Once opened, custom memory monitors may be installed.
+A memory monitor is a component of the cache responsible for detecting
+changes in virtual to physical address mappings.
+Some level of control over the cache is possible through the above
+mentioned environment variables.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_rma\f[](3), \f[C]fi_msg\f[](3), \f[C]fi_atomic\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_rma\f[R](3), \f[C]fi_msg\f[R](3),
+\f[C]fi_atomic\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_msg.3 b/man/man3/fi_msg.3
index 237c2a0..68159a8 100644
--- a/man/man3/fi_msg.3
+++ b/man/man3/fi_msg.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_msg" "3" "2020\-10\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_msg" "3" "2021\-03\-23" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,8 +8,6 @@ fi_msg \- Message data transfer operations
 .TP
 .B fi_recv / fi_recvv / fi_recvmsg
 Post a buffer to receive an incoming message
-.RS
-.RE
 .PP
 fi_send / fi_sendv / fi_sendmsg fi_inject / fi_senddata : Initiate an
 operation to send a message
@@ -17,110 +15,86 @@ operation to send a message
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_endpoint.h>
+#include <rdma/fi_endpoint.h>
 
-ssize_t\ fi_recv(struct\ fid_ep\ *ep,\ void\ *\ buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ fi_addr_t\ src_addr,\ void\ *context);
+ssize_t fi_recv(struct fid_ep *ep, void * buf, size_t len,
+    void *desc, fi_addr_t src_addr, void *context);
 
-ssize_t\ fi_recvv(struct\ fid_ep\ *ep,\ const\ struct\ iovec\ *iov,\ void\ **desc,
-\ \ \ \ size_t\ count,\ fi_addr_t\ src_addr,\ void\ *context);
+ssize_t fi_recvv(struct fid_ep *ep, const struct iovec *iov, void **desc,
+    size_t count, fi_addr_t src_addr, void *context);
 
-ssize_t\ fi_recvmsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_recvmsg(struct fid_ep *ep, const struct fi_msg *msg,
+    uint64_t flags);
 
-ssize_t\ fi_send(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ fi_addr_t\ dest_addr,\ void\ *context);
+ssize_t fi_send(struct fid_ep *ep, const void *buf, size_t len,
+    void *desc, fi_addr_t dest_addr, void *context);
 
-ssize_t\ fi_sendv(struct\ fid_ep\ *ep,\ const\ struct\ iovec\ *iov,
-\ \ \ \ void\ **desc,\ size_t\ count,\ fi_addr_t\ dest_addr,\ void\ *context);
+ssize_t fi_sendv(struct fid_ep *ep, const struct iovec *iov,
+    void **desc, size_t count, fi_addr_t dest_addr, void *context);
 
-ssize_t\ fi_sendmsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_sendmsg(struct fid_ep *ep, const struct fi_msg *msg,
+    uint64_t flags);
 
-ssize_t\ fi_inject(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ fi_addr_t\ dest_addr);
+ssize_t fi_inject(struct fid_ep *ep, const void *buf, size_t len,
+    fi_addr_t dest_addr);
 
-ssize_t\ fi_senddata(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ uint64_t\ data,\ fi_addr_t\ dest_addr,\ void\ *context);
+ssize_t fi_senddata(struct fid_ep *ep, const void *buf, size_t len,
+    void *desc, uint64_t data, fi_addr_t dest_addr, void *context);
 
-ssize_t\ fi_injectdata(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ uint64_t\ data,\ fi_addr_t\ dest_addr);
-\f[]
+ssize_t fi_injectdata(struct fid_ep *ep, const void *buf, size_t len,
+    uint64_t data, fi_addr_t dest_addr);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]ep\f[]
+.B \f[I]ep\f[R]
 Fabric endpoint on which to initiate send or post receive buffer.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Data buffer to send or receive.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Length of data buffer to send or receive, specified in bytes.
-Valid transfers are from 0 bytes up to the endpoint\[aq]s max_msg_size.
-.RS
-.RE
+Valid transfers are from 0 bytes up to the endpoint\[cq]s max_msg_size.
 .TP
-.B \f[I]iov\f[]
+.B \f[I]iov\f[R]
 Vectored data buffer.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Count of vectored data entries.
-.RS
-.RE
 .TP
-.B \f[I]desc\f[]
+.B \f[I]desc\f[R]
 Descriptor associated with the data buffer.
-See \f[C]fi_mr\f[](3).
-.RS
-.RE
+See \f[C]fi_mr\f[R](3).
 .TP
-.B \f[I]data\f[]
+.B \f[I]data\f[R]
 Remote CQ data to transfer with the sent message.
-.RS
-.RE
 .TP
-.B \f[I]dest_addr\f[]
+.B \f[I]dest_addr\f[R]
 Destination address for connectionless transfers.
 Ignored for connected endpoints.
-.RS
-.RE
 .TP
-.B \f[I]src_addr\f[]
+.B \f[I]src_addr\f[R]
 Source address to receive from for connectionless transfers.
 Applies only to connectionless endpoints with the FI_DIRECTED_RECV
 capability enabled, otherwise this field is ignored.
 If set to FI_ADDR_UNSPEC, any source address may match.
-.RS
-.RE
 .TP
-.B \f[I]msg\f[]
+.B \f[I]msg\f[R]
 Message descriptor for send and receive operations.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply for the send or receive operation.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified pointer to associate with the operation.
 This parameter is ignored if the operation will not generate a
 successful completion, unless an op flag specifies the context parameter
 be used for required input.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
-The send functions \-\- fi_send, fi_sendv, fi_sendmsg, fi_inject, and
-fi_senddata \-\- are used to transmit a message from one endpoint to
+The send functions \[en] fi_send, fi_sendv, fi_sendmsg, fi_inject, and
+fi_senddata \[en] are used to transmit a message from one endpoint to
 another endpoint.
 The main difference between send functions are the number and type of
 parameters that they accept as input.
@@ -128,7 +102,7 @@ Otherwise, they perform the same general function.
 Messages sent using fi_msg operations are received by a remote endpoint
 into a buffer posted to receive such messages.
 .PP
-The receive functions \-\- fi_recv, fi_recvv, fi_recvmsg \-\- post a
+The receive functions \[en] fi_recv, fi_recvv, fi_recvmsg \[en] post a
 data buffer to an endpoint to receive inbound messages.
 Similar to the send operations, receive operations operate
 asynchronously.
@@ -162,19 +136,19 @@ parameter to a remote endpoint as a single message.
 The fi_sendmsg call supports data transfers over both connected and
 connectionless endpoints, with the ability to control the send operation
 per call through the use of flags.
-The fi_sendmsg function takes a \f[C]struct\ fi_msg\f[] as input.
+The fi_sendmsg function takes a \f[C]struct fi_msg\f[R] as input.
 .IP
 .nf
 \f[C]
-struct\ fi_msg\ {
-\ \ \ \ const\ struct\ iovec\ *msg_iov;\ /*\ scatter\-gather\ array\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **desc;\ \ \ /*\ local\ request\ descriptors\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ iov_count;/*\ #\ elements\ in\ iov\ */
-\ \ \ \ fi_addr_t\ \ \ \ \ \ \ \ \ \ addr;\ \ \ \ \ /*\ optional\ endpoint\ address\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *context;\ /*\ user\-defined\ context\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ data;\ \ \ \ \ /*\ optional\ message\ data\ */
+struct fi_msg {
+    const struct iovec *msg_iov; /* scatter\-gather array */
+    void               **desc;   /* local request descriptors */
+    size_t             iov_count;/* # elements in iov */
+    fi_addr_t          addr;     /* optional endpoint address */
+    void               *context; /* user\-defined context */
+    uint64_t           data;     /* optional message data */
 };
-\f[]
+\f[R]
 .fi
 .SS fi_inject
 .PP
@@ -233,49 +207,39 @@ fi_endpoint.3).
 The following list of flags are usable with fi_recvmsg and/or
 fi_sendmsg.
 .TP
-.B \f[I]FI_REMOTE_CQ_DATA\f[]
+.B \f[I]FI_REMOTE_CQ_DATA\f[R]
 Applies to fi_sendmsg and fi_senddata.
 Indicates that remote CQ data is available and should be sent as part of
 the request.
 See fi_getinfo for additional details on FI_REMOTE_CQ_DATA.
-.RS
-.RE
 .TP
-.B \f[I]FI_CLAIM\f[]
+.B \f[I]FI_CLAIM\f[R]
 Applies to posted receive operations for endpoints configured for
 FI_BUFFERED_RECV or FI_VARIABLE_MSG.
 This flag is used to retrieve a message that was buffered by the
 provider.
 See the Buffered Receives section for details.
-.RS
-.RE
 .TP
-.B \f[I]FI_COMPLETION\f[]
+.B \f[I]FI_COMPLETION\f[R]
 Indicates that a completion entry should be generated for the specified
 operation.
 The endpoint must be bound to a completion queue with
 FI_SELECTIVE_COMPLETION that corresponds to the specified operation, or
 this flag is ignored.
-.RS
-.RE
 .TP
-.B \f[I]FI_DISCARD\f[]
+.B \f[I]FI_DISCARD\f[R]
 Applies to posted receive operations for endpoints configured for
 FI_BUFFERED_RECV or FI_VARIABLE_MSG.
 This flag is used to free a message that was buffered by the provider.
 See the Buffered Receives section for details.
-.RS
-.RE
 .TP
-.B \f[I]FI_MORE\f[]
+.B \f[I]FI_MORE\f[R]
 Indicates that the user has additional requests that will immediately be
 posted after the current call returns.
 Use of this flag may improve performance by enabling the provider to
 optimize its access to the fabric hardware.
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT\f[]
+.B \f[I]FI_INJECT\f[R]
 Applies to fi_sendmsg.
 Indicates that the outbound data buffer should be returned to user
 immediately after the send call returns, even if the operation is
@@ -283,10 +247,8 @@ handled asynchronously.
 This may require that the underlying provider implementation copy the
 data into a local buffer and transfer out of that buffer.
 This flag can only be used with messages smaller than inject_size.
-.RS
-.RE
 .TP
-.B \f[I]FI_MULTI_RECV\f[]
+.B \f[I]FI_MULTI_RECV\f[R]
 Applies to posted receive operations.
 This flag allows the user to post a single buffer that will receive
 multiple incoming messages.
@@ -296,40 +258,36 @@ Use of this flag may cause a single posted receive operation to generate
 multiple events as messages are placed into the buffer.
 The placement of received data into the buffer may be subjected to
 provider specific alignment restrictions.
-.RS
-.RE
 .PP
 The buffer will be released by the provider when the available buffer
 space falls below the specified minimum (see FI_OPT_MIN_MULTI_RECV).
 Note that an entry to the associated receive completion queue will
 always be generated when the buffer has been consumed, even if other
-receive completions have been suppressed (i.e.
-the Rx context has been configured for FI_SELECTIVE_COMPLETION).
-See the FI_MULTI_RECV completion flag \f[C]fi_cq\f[](3).
+receive completions have been suppressed (i.e.\ the Rx context has been
+configured for FI_SELECTIVE_COMPLETION).
+See the FI_MULTI_RECV completion flag \f[C]fi_cq\f[R](3).
 .TP
-.B \f[I]FI_INJECT_COMPLETE\f[]
+.B \f[I]FI_INJECT_COMPLETE\f[R]
 Applies to fi_sendmsg.
 Indicates that a completion should be generated when the source
 buffer(s) may be reused.
-.RS
-.RE
 .TP
-.B \f[I]FI_TRANSMIT_COMPLETE\f[]
-Applies to fi_sendmsg.
-Indicates that a completion should not be generated until the operation
-has been successfully transmitted and is no longer being tracked by the
-provider.
-.RS
-.RE
+.B \f[I]FI_TRANSMIT_COMPLETE\f[R]
+Applies to fi_sendmsg and fi_recvmsg.
+For sends, indicates that a completion should not be generated until the
+operation has been successfully transmitted and is no longer being
+tracked by the provider.
+For receive operations, indicates that a completion may be generated as
+soon as the message has been processed by the local provider, even if
+the message data may not be visible to all processing elements.
+See \f[C]fi_cq\f[R](3) for target side completion semantics.
 .TP
-.B \f[I]FI_DELIVERY_COMPLETE\f[]
+.B \f[I]FI_DELIVERY_COMPLETE\f[R]
 Applies to fi_sendmsg.
 Indicates that a completion should be generated when the operation has
 been processed by the destination.
-.RS
-.RE
 .TP
-.B \f[I]FI_FENCE\f[]
+.B \f[I]FI_FENCE\f[R]
 Applies to transmits.
 Indicates that the requested operation, also known as the fenced
 operation, and any operation posted after the fenced operation will be
@@ -337,21 +295,17 @@ deferred until all previous operations targeting the same peer endpoint
 have completed.
 Operations posted after the fencing will see and/or replace the results
 of any operations initiated prior to the fenced operation.
-.RS
-.RE
 .PP
 The ordering of operations starting at the posting of the fenced
 operation (inclusive) to the posting of a subsequent fenced operation
-(exclusive) is controlled by the endpoint\[aq]s ordering semantics.
+(exclusive) is controlled by the endpoint\[cq]s ordering semantics.
 .TP
-.B \f[I]FI_MULTICAST\f[]
+.B \f[I]FI_MULTICAST\f[R]
 Applies to transmits.
 This flag indicates that the address specified as the data transfer
 destination is a multicast address.
 This flag must be used in all multicast transfers, in conjunction with a
 multicast fi_addr_t.
-.RS
-.RE
 .SH Buffered Receives
 .PP
 Buffered receives indicate that the networking layer allocates and
@@ -379,28 +333,28 @@ buffers, the CQ entry op_context will point to a struct fi_recv_context.
 .IP
 .nf
 \f[C]
-struct\ fi_recv_context\ {
-\ \ \ \ struct\ fid_ep\ *ep;
-\ \ \ \ void\ *context;
+struct fi_recv_context {
+    struct fid_ep *ep;
+    void *context;
 };
-\f[]
+\f[R]
 .fi
 .PP
-The \[aq]ep\[aq] field will point to the receiving endpoint or Rx
-context, and \[aq]context\[aq] will be NULL.
-The CQ entry\[aq]s \[aq]buf\[aq] will point to a provider managed buffer
-where the start of the received message is located, and \[aq]len\[aq]
-will be set to the total size of the message.
+The `ep' field will point to the receiving endpoint or Rx context, and
+`context' will be NULL.
+The CQ entry\[cq]s `buf' will point to a provider managed buffer where
+the start of the received message is located, and `len' will be set to
+the total size of the message.
 .PP
 The maximum sized message that a provider can buffer is limited by an
 FI_OPT_BUFFERED_LIMIT.
 This threshold can be obtained and may be adjusted by the application
 using the fi_getopt and fi_setopt calls, respectively.
 Any adjustments must be made prior to enabling the endpoint.
-The CQ entry \[aq]buf\[aq] will point to a buffer of received data.
+The CQ entry `buf' will point to a buffer of received data.
 If the sent message is larger than the buffered amount, the CQ entry
-\[aq]flags\[aq] will have the FI_MORE bit set.
-When the FI_MORE bit is set, \[aq]buf\[aq] will reference at least
+`flags' will have the FI_MORE bit set.
+When the FI_MORE bit is set, `buf' will reference at least
 FI_OPT_BUFFERED_MIN bytes of data (see fi_endpoint.3 for more info).
 .PP
 After being notified that a buffered receive has arrived, applications
@@ -413,21 +367,21 @@ regardless of message size.
 To claim a message, an application must post a receive operation with
 the FI_CLAIM flag set.
 The struct fi_recv_context returned as part of the notification must be
-provided as the receive operation\[aq]s context.
-The struct fi_recv_context contains a \[aq]context\[aq] field.
+provided as the receive operation\[cq]s context.
+The struct fi_recv_context contains a `context' field.
 Applications may modify this field prior to claiming the message.
 When the claim operation completes, a standard receive completion entry
 will be generated on the completion queue.
-The \[aq]context\[aq] of the associated CQ entry will be set to the
-\[aq]context\[aq] value passed in through the fi_recv_context structure,
-and the CQ entry flags will have the FI_CLAIM bit set.
+The `context' of the associated CQ entry will be set to the `context'
+value passed in through the fi_recv_context structure, and the CQ entry
+flags will have the FI_CLAIM bit set.
 .PP
 Buffered receives that are not claimed must be discarded by the
 application when it is done processing the CQ entry data.
 To discard a message, an application must post a receive operation with
 the FI_DISCARD flag set.
 The struct fi_recv_context returned as part of the notification must be
-provided as the receive operation\[aq]s context.
+provided as the receive operation\[cq]s context.
 When the FI_DISCARD flag is set for a receive operation, the receive
 input buffer(s) and length parameters are ignored.
 .PP
@@ -436,8 +390,8 @@ manner.
 Failure to do so may result in increased memory usage for network
 buffering or communication stalls.
 Once a buffered receive has been claimed or discarded, the original CQ
-entry \[aq]buf\[aq] or struct fi_recv_context data may no longer be
-accessed by the application.
+entry `buf' or struct fi_recv_context data may no longer be accessed by
+the application.
 .PP
 The use of the FI_CLAIM and FI_DISCARD operation flags is also described
 with respect to tagged message transfers in fi_tagged.3.
@@ -459,10 +413,8 @@ It is most commonly used when the size of message transfers varies
 greatly, with very large messages interspersed with much smaller
 messages, making receive side message buffering difficult to manage.
 Variable messages are not subject to max message length restrictions
-(i.e.
-struct fi_ep_attr::max_msg_size limits), and may be up to the maximum
-value of size_t (e.g.
-SIZE_MAX) in length.
+(i.e.\ struct fi_ep_attr::max_msg_size limits), and may be up to the
+maximum value of size_t (e.g.\ SIZE_MAX) in length.
 .PP
 Variable length messages support requests that the provider allocate and
 manage the network message buffers.
@@ -489,19 +441,17 @@ buffer length.
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .PP
 See the discussion below for details handling FI_EAGAIN.
 .SH ERRORS
 .TP
-.B \f[I]\-FI_EAGAIN\f[]
+.B \f[I]\-FI_EAGAIN\f[R]
 Indicates that the underlying provider currently lacks the resources
 needed to initiate the requested operation.
 The reasons for a provider returning FI_EAGAIN are varied.
 However, common reasons include insufficient internal buffering or full
 processing queues.
-.RS
-.RE
 .PP
 Insufficient internal buffering is often associated with operations that
 use FI_INJECT.
@@ -524,7 +474,7 @@ employed, as acknowledgements or flow control messages may need to be
 processed in order to resume execution.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_cq\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_cq\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_nic.3 b/man/man3/fi_nic.3
index 8e4bfc2..897c14b 100644
--- a/man/man3/fi_nic.3
+++ b/man/man3/fi_nic.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_nic" "3" "2020\-04\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_nic" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,7 +9,7 @@ fi_nic \- Fabric network interface card attributes
 .PP
 The fid_nic structure defines attributes for a struct fi_info that is
 directly associated with underlying networking hardware and may be
-returned directly from calling \f[C]fi_getinfo\f[](3).
+returned directly from calling \f[C]fi_getinfo\f[R](3).
 The format of fid_nic and the related substructures are defined below.
 .PP
 Note that not all fields of all structures may be available.
@@ -18,156 +18,124 @@ will be set to NULL or 0.
 .IP
 .nf
 \f[C]
-struct\ fid_nic\ {
-\ \ \ \ struct\ fid\ \ \ \ \ \ \ \ \ \ \ \ \ fid;
-\ \ \ \ struct\ fi_device_attr\ *device_attr;
-\ \ \ \ struct\ fi_bus_attr\ \ \ \ *bus_attr;
-\ \ \ \ struct\ fi_link_attr\ \ \ *link_attr;
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *prov_attr;
+struct fid_nic {
+    struct fid             fid;
+    struct fi_device_attr *device_attr;
+    struct fi_bus_attr    *bus_attr;
+    struct fi_link_attr   *link_attr;
+    void                  *prov_attr;
 };
 
-struct\ fi_device_attr\ {
-\ \ \ \ char\ *name;
-\ \ \ \ char\ *device_id;
-\ \ \ \ char\ *device_version;
-\ \ \ \ char\ *vendor_id;
-\ \ \ \ char\ *driver;
-\ \ \ \ char\ *firmware;
+struct fi_device_attr {
+    char *name;
+    char *device_id;
+    char *device_version;
+    char *vendor_id;
+    char *driver;
+    char *firmware;
 };
 
-struct\ fi_pci_attr\ {
-\ \ \ \ uint16_t\ domain_id;
-\ \ \ \ uint8_t\ \ bus_id;
-\ \ \ \ uint8_t\ \ device_id;
-\ \ \ \ uint8_t\ \ function_id;
+struct fi_pci_attr {
+    uint16_t domain_id;
+    uint8_t  bus_id;
+    uint8_t  device_id;
+    uint8_t  function_id;
 };
 
-struct\ fi_bus_attr\ {
-\ \ \ \ enum\ fi_bus_type\ \ \ \ \ \ \ bus_type;
-\ \ \ \ union\ {
-\ \ \ \ \ \ \ \ struct\ fi_pci_attr\ pci;
-\ \ \ \ }\ attr;
+struct fi_bus_attr {
+    enum fi_bus_type       bus_type;
+    union {
+        struct fi_pci_attr pci;
+    } attr;
 };
 
-struct\ fi_link_attr\ {
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *address;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ mtu;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ speed;
-\ \ \ \ enum\ fi_link_state\ state;
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *network_type;
+struct fi_link_attr {
+    char               *address;
+    size_t             mtu;
+    size_t             speed;
+    enum fi_link_state state;
+    char               *network_type;
 };
-\f[]
+\f[R]
 .fi
 .SS Device Attributes
 .PP
 Device attributes are used to identify the specific virtual or hardware
 NIC associated with an fi_info structure.
 .TP
-.B \f[I]name\f[]
+.B \f[I]name\f[R]
 The operating system name associated with the device.
-This may be a logical network interface name (e.g.
-eth0 or eno1) or an absolute filename.
-.RS
-.RE
+This may be a logical network interface name (e.g.\ eth0 or eno1) or an
+absolute filename.
 .TP
-.B \f[I]device_id\f[]
+.B \f[I]device_id\f[R]
 This is a vendor specific identifier for the device or product.
-.RS
-.RE
 .TP
-.B \f[I]device_version\f[]
+.B \f[I]device_version\f[R]
 Indicates the version of the device.
-.RS
-.RE
 .TP
-.B \f[I]vendor_id\f[]
+.B \f[I]vendor_id\f[R]
 Indicates the name of the vendor that distributes the NIC.
-.RS
-.RE
 .TP
-.B \f[I]driver\f[]
+.B \f[I]driver\f[R]
 The name of the driver associated with the device
-.RS
-.RE
 .TP
-.B \f[I]firmware\f[]
-The device\[aq]s firmware version.
-.RS
-.RE
+.B \f[I]firmware\f[R]
+The device\[cq]s firmware version.
 .SS Bus Attributes
 .PP
 The bus attributes are used to identify the physical location of the NIC
 in the system.
 .TP
-.B \f[I]bus_type\f[]
+.B \f[I]bus_type\f[R]
 Indicates the type of system bus where the NIC is located.
 Valid values are FI_BUS_PCI or FI_BUS_UNKNOWN.
-.RS
-.RE
 .TP
-.B \f[I]attr.pci.domain_id\f[]
+.B \f[I]attr.pci.domain_id\f[R]
 The domain where the PCI bus is located.
 Valid only if bus_type is FI_BUS_PCI.
-.RS
-.RE
 .TP
-.B \f[I]attr.pci.bus_id\f[]
+.B \f[I]attr.pci.bus_id\f[R]
 The PCI bus identifier where the device is located.
 Valid only if bus_type is FI_BUS_PCI.
-.RS
-.RE
 .TP
-.B \f[I]attr.pci.device_id\f[]
+.B \f[I]attr.pci.device_id\f[R]
 The identifier on the PCI bus where the device is located.
 Valid only if bus_type is FI_BUS_PCI.
-.RS
-.RE
 .TP
-.B \f[I]attr.pci.function_id\f[]
+.B \f[I]attr.pci.function_id\f[R]
 The function on the device being referenced.
 Valid only if bus_type is FI_BUS_PCI.
-.RS
-.RE
 .SS Link Attributes
 .PP
 Link attributes describe low\-level details about the network connection
 into the fabric.
 .TP
-.B \f[I]address\f[]
+.B \f[I]address\f[R]
 The primary link\-level address associated with the NIC, such as a MAC
 address.
 If multiple addresses are available, only one will be reported.
-.RS
-.RE
 .TP
-.B \f[I]mtu\f[]
+.B \f[I]mtu\f[R]
 The maximum transfer unit of link level frames or packets, in bytes.
-.RS
-.RE
 .TP
-.B \f[I]speed\f[]
+.B \f[I]speed\f[R]
 The active link data rate, given in bits per second.
-.RS
-.RE
 .TP
-.B \f[I]state\f[]
+.B \f[I]state\f[R]
 The current physical port state.
 Possible values are FI_LINK_UNKNOWN, FI_LINK_DOWN, and FI_LINK_UP, to
 indicate if the port state is unknown or not applicable (unknown),
 inactive (down), or active (up).
-.RS
-.RE
 .TP
-.B \f[I]network_type\f[]
+.B \f[I]network_type\f[R]
 Specifies the type of network interface currently active, such as
 Ethernet or InfiniBand.
-.RS
-.RE
 .SS Provider Attributes
 .PP
 Provider attributes reference provider specific details of the device.
 These attributes are both provider and device specific.
-The attributes can be interpreted by \f[C]fi_tostr\f[](3).
+The attributes can be interpreted by \f[C]fi_tostr\f[R](3).
 Applications may also use the other attribute fields, such as related
 fi_fabric_attr: prov_name field, to determine an appropriate structure
 to cast the attributes.
@@ -177,10 +145,10 @@ specific header file included with libfabric package.
 .SH NOTES
 .PP
 The fid_nic structure is returned as part of a call to
-\f[C]fi_getinfo\f[](3).
-It is automatically freed as part of calling \f[C]fi_freeinfo\f[](3)
+\f[C]fi_getinfo\f[R](3).
+It is automatically freed as part of calling \f[C]fi_freeinfo\f[R](3)
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3)
+\f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_open.3 b/man/man3/fi_open.3
index b356493..f72423b 100644
--- a/man/man3/fi_open.3
+++ b/man/man3/fi_open.3
@@ -1 +1 @@
-.so man3/fi_domain.3
+.so man3/fi_provider.3
diff --git a/man/man3/fi_poll.3 b/man/man3/fi_poll.3
index 787c139..8d41f7a 100644
--- a/man/man3/fi_poll.3
+++ b/man/man3/fi_poll.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_poll" "3" "2020\-04\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_poll" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,128 +8,92 @@ fi_poll \- Polling and wait set operations
 .TP
 .B fi_poll_open / fi_close
 Open/close a polling set
-.RS
-.RE
 .TP
 .B fi_poll_add / fi_poll_del
 Add/remove a completion queue or counter to/from a poll set.
-.RS
-.RE
 .TP
 .B fi_poll
 Poll for progress and events across multiple completion queues and
 counters.
-.RS
-.RE
 .TP
 .B fi_wait_open / fi_close
 Open/close a wait set
-.RS
-.RE
 .TP
 .B fi_wait
 Waits for one or more wait objects in a set to be signaled.
-.RS
-.RE
 .TP
 .B fi_trywait
 Indicate when it is safe to block on wait objects using native OS calls.
-.RS
-.RE
 .TP
 .B fi_control
 Control wait set operation or attributes.
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_domain.h>
+#include <rdma/fi_domain.h>
 
-int\ fi_poll_open(struct\ fid_domain\ *domain,\ struct\ fi_poll_attr\ *attr,
-\ \ \ \ struct\ fid_poll\ **pollset);
+int fi_poll_open(struct fid_domain *domain, struct fi_poll_attr *attr,
+    struct fid_poll **pollset);
 
-int\ fi_close(struct\ fid\ *pollset);
+int fi_close(struct fid *pollset);
 
-int\ fi_poll_add(struct\ fid_poll\ *pollset,\ struct\ fid\ *event_fid,
-\ \ \ \ uint64_t\ flags);
+int fi_poll_add(struct fid_poll *pollset, struct fid *event_fid,
+    uint64_t flags);
 
-int\ fi_poll_del(struct\ fid_poll\ *pollset,\ struct\ fid\ *event_fid,
-\ \ \ \ uint64_t\ flags);
+int fi_poll_del(struct fid_poll *pollset, struct fid *event_fid,
+    uint64_t flags);
 
-int\ fi_poll(struct\ fid_poll\ *pollset,\ void\ **context,\ int\ count);
+int fi_poll(struct fid_poll *pollset, void **context, int count);
 
-int\ fi_wait_open(struct\ fid_fabric\ *fabric,\ struct\ fi_wait_attr\ *attr,
-\ \ \ \ struct\ fid_wait\ **waitset);
+int fi_wait_open(struct fid_fabric *fabric, struct fi_wait_attr *attr,
+    struct fid_wait **waitset);
 
-int\ fi_close(struct\ fid\ *waitset);
+int fi_close(struct fid *waitset);
 
-int\ fi_wait(struct\ fid_wait\ *waitset,\ int\ timeout);
+int fi_wait(struct fid_wait *waitset, int timeout);
 
-int\ fi_trywait(struct\ fid_fabric\ *fabric,\ struct\ fid\ **fids,\ size_t\ count);
+int fi_trywait(struct fid_fabric *fabric, struct fid **fids, size_t count);
 
-int\ fi_control(struct\ fid\ *waitset,\ int\ command,\ void\ *arg);
-\f[]
+int fi_control(struct fid *waitset, int command, void *arg);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]fabric\f[]
+.B \f[I]fabric\f[R]
 Fabric provider
-.RS
-.RE
 .TP
-.B \f[I]domain\f[]
+.B \f[I]domain\f[R]
 Resource domain
-.RS
-.RE
 .TP
-.B \f[I]pollset\f[]
+.B \f[I]pollset\f[R]
 Event poll set
-.RS
-.RE
 .TP
-.B \f[I]waitset\f[]
+.B \f[I]waitset\f[R]
 Wait object set
-.RS
-.RE
 .TP
-.B \f[I]attr\f[]
+.B \f[I]attr\f[R]
 Poll or wait set attributes
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 On success, an array of user context values associated with completion
 queues or counters.
-.RS
-.RE
 .TP
-.B \f[I]fids\f[]
+.B \f[I]fids\f[R]
 An array of fabric descriptors, each one associated with a native wait
 object.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Number of entries in context or fids array.
-.RS
-.RE
 .TP
-.B \f[I]timeout\f[]
+.B \f[I]timeout\f[R]
 Time to wait for a signal, in milliseconds.
-.RS
-.RE
 .TP
-.B \f[I]command\f[]
+.B \f[I]command\f[R]
 Command of control operation to perform on the wait set.
-.RS
-.RE
 .TP
-.B \f[I]arg\f[]
+.B \f[I]arg\f[R]
 Optional control argument.
-.RS
-.RE
 .SH DESCRIPTION
 .SS fi_poll_open
 .PP
@@ -142,17 +106,15 @@ A poll set is defined with the following attributes.
 .IP
 .nf
 \f[C]
-struct\ fi_poll_attr\ {
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ flags;\ \ \ \ \ /*\ operation\ flags\ */
+struct fi_poll_attr {
+    uint64_t             flags;     /* operation flags */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Flags that set the default operation of the poll set.
 The use of this field is reserved and must be set to 0 by the caller.
-.RS
-.RE
 .SS fi_close
 .PP
 The fi_close call releases all resources associated with a poll set.
@@ -172,7 +134,7 @@ If events might have occurred, contexts associated with the completion
 queues and/or counters are returned.
 Completion queues will return their context if they are not empty.
 The context associated with a counter will be returned if the
-counter\[aq]s success value or error value have changed since the last
+counter\[cq]s success value or error value have changed since the last
 time fi_poll, fi_cntr_set, or fi_cntr_add were called.
 The number of contexts is limited to the size of the context array,
 indicated by the count parameter.
@@ -199,24 +161,22 @@ fi_wait_attr.
 .IP
 .nf
 \f[C]
-struct\ fi_wait_attr\ {
-\ \ \ \ enum\ fi_wait_obj\ \ \ \ \ wait_obj;\ \ /*\ requested\ wait\ object\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ flags;\ \ \ \ \ /*\ operation\ flags\ */
+struct fi_wait_attr {
+    enum fi_wait_obj     wait_obj;  /* requested wait object */
+    uint64_t             flags;     /* operation flags */
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]wait_obj\f[]
+.B \f[I]wait_obj\f[R]
 Wait sets are associated with specific wait object(s).
 Wait objects allow applications to block until the wait object is
 signaled, indicating that an event is available to be read.
 The following values may be used to specify the type of wait object
 associated with a wait set: FI_WAIT_UNSPEC, FI_WAIT_FD,
 FI_WAIT_MUTEX_COND, and FI_WAIT_YIELD.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_UNSPEC\f[]
+.B \- \f[I]FI_WAIT_UNSPEC\f[R]
 Specifies that the user will only wait on the wait set using fabric
 interface calls, such as fi_wait.
 In this case, the underlying provider may select the most appropriate or
@@ -224,10 +184,8 @@ highest performing wait object available, including custom wait
 mechanisms.
 Applications that select FI_WAIT_UNSPEC are not guaranteed to retrieve
 the underlying wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_FD\f[]
+.B \- \f[I]FI_WAIT_FD\f[R]
 Indicates that the wait set should use a single file descriptor as its
 wait mechanism, as exposed to the application.
 Internally, this may require the use of epoll in order to support
@@ -236,16 +194,12 @@ File descriptor wait objects must be usable in the POSIX select(2) and
 poll(2), and Linux epoll(7) routines (if available).
 Provider signal an FD wait object by marking it as readable or with an
 error.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_MUTEX_COND\f[]
+.B \- \f[I]FI_WAIT_MUTEX_COND\f[R]
 Specifies that the wait set should use a pthread mutex and cond variable
 as a wait object.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_POLLFD\f[]
+.B \- \f[I]FI_WAIT_POLLFD\f[R]
 This option is similar to FI_WAIT_FD, but allows the wait mechanism to
 use multiple file descriptors as its wait mechanism, as viewed by the
 application.
@@ -255,20 +209,14 @@ for events.
 The file descriptors must be usable in the POSIX select(2) and poll(2)
 routines, and match directly to being used with poll.
 See the NOTES section below for details on using pollfd.
-.RS
-.RE
 .TP
-.B \- \f[I]FI_WAIT_YIELD\f[]
+.B \- \f[I]FI_WAIT_YIELD\f[R]
 Indicates that the wait set will wait without a wait object but instead
 yield on every wait.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Flags that set the default operation of the wait set.
 The use of this field is reserved and must be set to 0 by the caller.
-.RS
-.RE
 .SS fi_close
 .PP
 The fi_close call releases all resources associated with a wait set.
@@ -295,19 +243,19 @@ conjunction with the OS select(2) call.
 .IP
 .nf
 \f[C]
-fi_control(&cq\->fid,\ FI_GETWAIT,\ (void\ *)\ &fd);
+fi_control(&cq\->fid, FI_GETWAIT, (void *) &fd);
 FD_ZERO(&fds);
-FD_SET(fd,\ &fds);
+FD_SET(fd, &fds);
 
-while\ (1)\ {
-\ \ \ \ if\ (fi_trywait(&cq,\ 1)\ ==\ FI_SUCCESS)
-\ \ \ \ \ \ \ \ select(fd\ +\ 1,\ &fds,\ NULL,\ &fds,\ &timeout);
+while (1) {
+    if (fi_trywait(&cq, 1) == FI_SUCCESS)
+        select(fd + 1, &fds, NULL, &fds, &timeout);
 
-\ \ \ \ do\ {
-\ \ \ \ \ \ \ \ ret\ =\ fi_cq_read(cq,\ &comp,\ 1);
-\ \ \ \ }\ while\ (ret\ >\ 0);
+    do {
+        ret = fi_cq_read(cq, &comp, 1);
+    } while (ret > 0);
 }
-\f[]
+\f[R]
 .fi
 .PP
 fi_trywait() will return FI_SUCCESS if it is safe to block on the wait
@@ -325,7 +273,7 @@ The following types of fabric descriptors may be passed into fi_trywait:
 event queues, completion queues, counters, and wait sets.
 Applications that wish to use native wait calls should select specific
 wait objects when allocating such resources.
-For example, by setting the item\[aq]s creation attribute wait_obj value
+For example, by setting the item\[cq]s creation attribute wait_obj value
 to FI_WAIT_FD.
 .PP
 In the case the wait object to check belongs to a wait set, only the
@@ -347,37 +295,30 @@ fi_control is invoked, as it may redirect the implementation of wait set
 operations.
 The following control commands are usable with a wait set or fid.
 .TP
-.B \f[I]FI_GETWAIT (void **)\f[]
+.B \f[I]FI_GETWAIT (void **)\f[R]
 This command allows the user to retrieve the low\-level wait object
 associated with a wait set or fid.
 The format of the wait set is specified during wait set creation,
 through the wait set attributes.
 The fi_control arg parameter should be an address where a pointer to the
 returned wait object will be written.
-This should be an \[aq]int *\[aq] for FI_WAIT_FD, \[aq]struct
-fi_mutex_cond\[aq] for FI_WAIT_MUTEX_COND, or \[aq]struct
-fi_wait_pollfd\[aq] for FI_WAIT_POLLFD.
+This should be an \[cq]int *\[cq] for FI_WAIT_FD, `struct fi_mutex_cond'
+for FI_WAIT_MUTEX_COND, or `struct fi_wait_pollfd' for FI_WAIT_POLLFD.
 Support for FI_GETWAIT is provider specific.
-.RS
-.RE
 .TP
-.B \f[I]FI_GETWAITOBJ (enum fi_wait_obj *)\f[]
+.B \f[I]FI_GETWAITOBJ (enum fi_wait_obj *)\f[R]
 This command returns the type of wait object associated with a wait set
 or fid.
-.RS
-.RE
 .SH RETURN VALUES
 .PP
 Returns FI_SUCCESS on success.
 On error, a negative value corresponding to fabric errno is returned.
 .PP
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .TP
 .B fi_poll
 On success, if events are available, returns the number of entries
 written to the context array.
-.RS
-.RE
 .SH NOTES
 .PP
 In many situations, blocking calls may need to wait on signals sent to a
@@ -395,7 +336,7 @@ mechanism.
 A significant different between using POLLFD versus FD wait objects is
 that with FI_WAIT_POLLFD, the file descriptors may change dynamically.
 As an example, the file descriptors associated with a completion
-queues\[aq] wait set may change as endpoint associations with the CQ are
+queues\[cq] wait set may change as endpoint associations with the CQ are
 added and removed.
 .PP
 Struct fi_wait_pollfd is used to retrieve all file descriptors for fids
@@ -403,15 +344,15 @@ using FI_WAIT_POLLFD to support blocking calls.
 .IP
 .nf
 \f[C]
-struct\ fi_wait_pollfd\ {
-\ \ \ \ uint64_t\ \ \ \ \ \ change_index;
-\ \ \ \ size_t\ \ \ \ \ \ \ \ nfds;
-\ \ \ \ struct\ pollfd\ *fd;
+struct fi_wait_pollfd {
+    uint64_t      change_index;
+    size_t        nfds;
+    struct pollfd *fd;
 };
-\f[]
+\f[R]
 .fi
 .TP
-.B \f[I]change_index\f[]
+.B \f[I]change_index\f[R]
 The change_index may be used to determine if there have been any changes
 to the file descriptor list.
 Anytime a file descriptor is added, removed, or its events are updated,
@@ -423,10 +364,8 @@ fi_control() to retrieve the current change_index and compare that
 against its cached value.
 If the values differ, then the app should update its file descriptor
 list prior to blocking.
-.RS
-.RE
 .TP
-.B \f[I]nfds\f[]
+.B \f[I]nfds\f[R]
 On input to fi_control(), this indicates the number of entries in the
 struct pollfd * array.
 On output, this will be set to the number of entries needed to store the
@@ -435,18 +374,14 @@ If the input value is smaller than the output value, fi_control() will
 return the error \-FI_ETOOSMALL.
 Note that setting nfds = 0 allows an efficient way of checking the
 change_index.
-.RS
-.RE
 .TP
-.B \f[I]fd\f[]
+.B \f[I]fd\f[R]
 This points to an array of struct pollfd entries.
 The number of entries is specified through the nfds field.
 If the number of needed entries is less than or equal to the number of
 entries available, the struct pollfd array will be filled out with a
 list of file descriptors and corresponding events that can be used in
 the select(2) and poll(2) calls.
-.RS
-.RE
 .PP
 The change_index is updated only when the file descriptors associated
 with the pollfd file set has changed.
@@ -456,7 +391,7 @@ The use of the fi_trywait() function is still required if accessing wait
 objects directly.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_domain\f[](3), \f[C]fi_cntr\f[](3),
-\f[C]fi_eq\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_domain\f[R](3), \f[C]fi_cntr\f[R](3),
+\f[C]fi_eq\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_provider.3 b/man/man3/fi_provider.3
new file mode 100644
index 0000000..1ae81a8
--- /dev/null
+++ b/man/man3/fi_provider.3
@@ -0,0 +1,252 @@
+.\" Automatically generated by Pandoc 2.5
+.\"
+.TH "fi_provider" "3" "2021\-09\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
+.hy
+.SH NAME
+.PP
+fi_prov_ini \- External provider entry point
+.TP
+.B fi_param_define / fi_param_get
+Register and retrieve environment variables with the libfabric core
+.TP
+.B fi_log_enabled / fi_log_ready / fi_log
+Control and output debug logging information.
+.TP
+.B fi_open / fi_close
+Open a named library object
+.TP
+.B fi_export_fid / fi_import_fid
+Share a fabric object between different providers or resources
+.SH SYNOPSIS
+.IP
+.nf
+\f[C]
+#include <rdma/fabric.h>
+#include <rdma/prov/fi_prov.h>
+
+struct fi_provider* fi_prov_ini(void);
+
+int fi_param_define(const struct fi_provider *provider, const char *param_name,
+    enum fi_param_type type, const char *help_string_fmt, ...);
+
+int fi_param_get_str(struct fi_provider *provider, const char *param_name,
+    char **value);
+
+int fi_param_get_int(struct fi_provider *provider, const char *param_name,
+    int *value);
+
+int fi_param_get_bool(struct fi_provider *provider, const char *param_name,
+    int *value);
+
+int fi_param_get_size_t(struct fi_provider *provider, const char *param_name,
+    size_t *value);
+\f[R]
+.fi
+.IP
+.nf
+\f[C]
+#include <rdma/fabric.h>
+#include <rdma/prov/fi_prov.h>
+#include <rdma/prov/fi_log.h>
+
+int fi_log_enabled(const struct fi_provider *prov, enum fi_log_level level,
+    enum fi_log_subsys subsys);
+
+int fi_log_ready(const struct fi_provider *prov, enum fi_log_level level,
+    enum fi_log_subsys subsys, uint64_t *showtime);
+
+void fi_log(const struct fi_provider *prov, enum fi_log_level level,
+    enum fi_log_subsys subsys, const char *func, int line,
+    const char *fmt, ...);
+\f[R]
+.fi
+.IP
+.nf
+\f[C]
+#include <rdma/fabric.h>
+
+int fi_open(uint32_t version, const char *name, void *attr,
+    size_t attr_len, uint64_t flags, struct fid **fid, void *context);
+
+int fi_close(struct fid *fid);
+\f[R]
+.fi
+.IP
+.nf
+\f[C]
+#include <rdma/fabric.h>
+#include <rdma/fi_ext.h>
+
+int fi_export_fid(struct fid *fid, uint64_t flags,
+    struct fid **expfid, void *context);
+
+int fi_import_fid(struct fid *fid, struct fid *expfid, uint64_t flags);
+\f[R]
+.fi
+.SH ARGUMENTS
+.TP
+.B \f[I]provider\f[R]
+Reference to the provider.
+.TP
+.B \f[I]version\f[R]
+API version requested by application.
+.TP
+.B \f[I]name\f[R]
+Well\-known name of the library object to open.
+.TP
+.B \f[I]attr\f[R]
+Optional attributes of object to open.
+.TP
+.B \f[I]attr_len\f[R]
+Size of any attribute structure passed to fi_open.
+Should be 0 if no attributes are give.
+.TP
+.B \f[I]fid\f[R]
+Returned fabric identifier for opened object.
+.SH DESCRIPTION
+.PP
+A fabric provider implements the application facing software interfaces
+needed to access network specific protocols, drivers, and hardware.
+The interfaces and structures defined by this man page are exported by
+the libfabric library, but are targeted for provider implementations,
+rather than for direct use by most applications.
+.PP
+Integrated providers are those built directly into the libfabric library
+itself.
+External providers are loaded dynamically by libfabric at initialization
+time.
+External providers must be in a standard library path or in the
+libfabric library search path as specified by environment variable.
+Additionally, external providers must be named with the suffix
+\[lq]\-fi.so\[rq] at the end of the name.
+.PP
+Named objects are special purpose resources which are accessible
+directly to applications.
+They may be used to enhance or modify the behavior of library core.
+For details, see the fi_open call below.
+.SS fi_prov_ini
+.PP
+This entry point must be defined by external providers.
+On loading, libfabric will invoke fi_prov_ini() to retrieve the
+provider\[cq]s fi_provider structure.
+Additional interactions between the libfabric core and the provider will
+be through the interfaces defined by that struct.
+.SS fi_param_define
+.PP
+Defines a configuration parameter for use by a specified provider.
+The help_string and param_name arguments must be non\-NULL, help_string
+must additionally be non\-empty.
+They are copied internally and may be freed after calling
+fi_param_define.
+.SS fi_param_get
+.PP
+Gets the value of a configuration parameter previously defined using
+fi_param_define().
+The value comes from the environment variable name of the form FI__, all
+converted to upper case.
+.PP
+If the parameter was previously defined and the user set a value,
+FI_SUCCESS is returned and (*value) points to the retrieved value.
+.PP
+If the parameter name was previously defined, but the user did not set a
+value, \-FI_ENODATA is returned and the value of (*value) is unchanged.
+.PP
+If the parameter name was not previously defined via fi_param_define(),
+\-FI_ENOENT will be returned and the value of (*value) is unchanged.
+.PP
+If the value in the environment is not valid for the parameter type,
+\-FI_EINVAL will be returned and the value of (*value) is unchanged.
+.SS fi_log_enabled / fi_log_ready / fi_log
+.PP
+These functions control debug and informational logging output.
+Providers typically access these functions through the FI_LOG and
+related macros in fi_log.h and do not call these function directly.
+.SS fi_open
+.PP
+Open a library resource using a well\-known name.
+This feature allows applications and providers a mechanism which can be
+used to modify or enhance core library services and behavior.
+The details are specific based on the requested object name.
+Most applications will not need this level of control.
+.PP
+The library API version known to the application should be provided
+through the version parameter.
+The use of attributes is object dependent.
+If required, attributes should be provided through the attr parameter,
+with attr_len set to the size of the referenced attribute structure.
+The following is a list of published names, along with descriptions of
+the service or resource to which they correspond.
+.TP
+.B \f[I]mr_cache\f[R]
+The mr_cache object references the internal memory registration cache
+used by the different providers.
+Additional information on the cache is available in the
+\f[C]fi_mr(3)\f[R] man page.
+.SS fi_export_fid / fi_import_fid
+.PP
+Generally, fabric objects are allocated and managed entirely by a single
+provider.
+Typically only the application facing software interfaces of a fabric
+object are defined, for example, the message or tagged operations of an
+endpoint.
+The fi_export_fid and fi_import_fid calls provide a a mechanism by which
+provider facing APIs may be accessed.
+This allows the creation of fid objects that are shareable between
+providers, or for library plug\-in services.
+The ability to export a shareable object is object and provider
+implementation dependent.
+.PP
+Shareable fids typically contain at least 3 main components: a base fid,
+a set of exporter defined ops, and a set of importer defined ops.
+.SH NOTES
+.PP
+TODO
+.SH PROVIDER INTERFACE
+.PP
+The fi_provider structure defines entry points for the libfabric core to
+use to access the provider.
+All other calls into a provider are through function pointers associated
+with allocated objects.
+.IP
+.nf
+\f[C]
+struct fi_provider {
+    uint32_t version;
+    uint32_t fi_version;
+    struct fi_context context;
+    const char *name;
+    int (*getinfo)(uint32_t version, const char *node, const char *service,
+            uint64_t flags, const struct fi_info *hints,
+            struct fi_info **info);
+    int (*fabric)(struct fi_fabric_attr *attr, struct fid_fabric **fabric,
+            void *context);
+    void    (*cleanup)(void);
+};
+\f[R]
+.fi
+.SS version
+.PP
+The provider version.
+For providers integrated with the library, this is often the same as the
+library version.
+.SS fi_version
+.PP
+The library interface version that the provider was implemented against.
+The provider\[cq]s fi_version must be greater than or equal to an
+application\[cq]s requested api version for the application to use the
+provider.
+It is a provider\[cq]s responsibility to support older versions of the
+api if it wishes to supports legacy applications.
+For integrated providers
+.SS TODO
+.SH RETURN VALUE
+.PP
+Returns FI_SUCCESS on success.
+On error, a negative value corresponding to fabric errno is returned.
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
+.SH ERRORS
+.SH SEE ALSO
+.PP
+\f[C]fabric\f[R](7), \f[C]fi_getinfo\f[R](3) \f[C]fi_mr\f[R](3),
+.SH AUTHORS
+OpenFabrics.
diff --git a/man/man3/fi_rma.3 b/man/man3/fi_rma.3
index 4988c9e..bf05937 100644
--- a/man/man3/fi_rma.3
+++ b/man/man3/fi_rma.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_rma" "3" "2020\-10\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_rma" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,8 +8,6 @@ fi_rma \- Remote memory access operations
 .TP
 .B fi_read / fi_readv / fi_readmsg
 Initiates a read from remote memory
-.RS
-.RE
 .PP
 fi_write / fi_writev / fi_writemsg fi_inject_write / fi_writedata :
 Initiate a write to remote memory
@@ -17,121 +15,93 @@ Initiate a write to remote memory
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_rma.h>
+#include <rdma/fi_rma.h>
 
-ssize_t\ fi_read(struct\ fid_ep\ *ep,\ void\ *buf,\ size_t\ len,\ void\ *desc,
-\ \ \ \ fi_addr_t\ src_addr,\ uint64_t\ addr,\ uint64_t\ key,\ void\ *context);
+ssize_t fi_read(struct fid_ep *ep, void *buf, size_t len, void *desc,
+    fi_addr_t src_addr, uint64_t addr, uint64_t key, void *context);
 
-ssize_t\ fi_readv(struct\ fid_ep\ *ep,\ const\ struct\ iovec\ *iov,\ void\ **desc,
-\ \ \ \ size_t\ count,\ fi_addr_t\ src_addr,\ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ void\ *context);
+ssize_t fi_readv(struct fid_ep *ep, const struct iovec *iov, void **desc,
+    size_t count, fi_addr_t src_addr, uint64_t addr, uint64_t key,
+    void *context);
 
-ssize_t\ fi_readmsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg_rma\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg,
+    uint64_t flags);
 
-ssize_t\ fi_write(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ fi_addr_t\ dest_addr,\ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ void\ *context);
+ssize_t fi_write(struct fid_ep *ep, const void *buf, size_t len,
+    void *desc, fi_addr_t dest_addr, uint64_t addr, uint64_t key,
+    void *context);
 
-ssize_t\ fi_writev(struct\ fid_ep\ *ep,\ const\ struct\ iovec\ *iov,\ void\ **desc,
-\ \ \ \ size_t\ count,\ fi_addr_t\ dest_addr,\ uint64_t\ addr,\ uint64_t\ key,
-\ \ \ \ void\ *context);
+ssize_t fi_writev(struct fid_ep *ep, const struct iovec *iov, void **desc,
+    size_t count, fi_addr_t dest_addr, uint64_t addr, uint64_t key,
+    void *context);
 
-ssize_t\ fi_writemsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg_rma\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_writemsg(struct fid_ep *ep, const struct fi_msg_rma *msg,
+    uint64_t flags);
 
-ssize_t\ fi_inject_write(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ fi_addr_t\ dest_addr,\ uint64_t\ addr,\ uint64_t\ key);
+ssize_t fi_inject_write(struct fid_ep *ep, const void *buf, size_t len,
+    fi_addr_t dest_addr, uint64_t addr, uint64_t key);
 
-ssize_t\ fi_writedata(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ uint64_t\ data,\ fi_addr_t\ dest_addr,\ uint64_t\ addr,
-\ \ \ \ uint64_t\ key,\ void\ *context);
+ssize_t fi_writedata(struct fid_ep *ep, const void *buf, size_t len,
+    void *desc, uint64_t data, fi_addr_t dest_addr, uint64_t addr,
+    uint64_t key, void *context);
 
-ssize_t\ fi_inject_writedata(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ uint64_t\ data,\ fi_addr_t\ dest_addr,\ uint64_t\ addr,\ uint64_t\ key);
-\f[]
+ssize_t fi_inject_writedata(struct fid_ep *ep, const void *buf, size_t len,
+    uint64_t data, fi_addr_t dest_addr, uint64_t addr, uint64_t key);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]ep\f[]
+.B \f[I]ep\f[R]
 Fabric endpoint on which to initiate read or write operation.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Local data buffer to read into (read target) or write from (write
 source)
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Length of data to read or write, specified in bytes.
-Valid transfers are from 0 bytes up to the endpoint\[aq]s max_msg_size.
-.RS
-.RE
+Valid transfers are from 0 bytes up to the endpoint\[cq]s max_msg_size.
 .TP
-.B \f[I]iov\f[]
+.B \f[I]iov\f[R]
 Vectored data buffer.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Count of vectored data entries.
-.RS
-.RE
 .TP
-.B \f[I]addr\f[]
+.B \f[I]addr\f[R]
 Address of remote memory to access.
 This will be the virtual address of the remote region in the case of
 FI_MR_BASIC, or the offset from the starting address in the case of
 FI_MR_SCALABLE.
-.RS
-.RE
 .TP
-.B \f[I]key\f[]
+.B \f[I]key\f[R]
 Protection key associated with the remote memory.
-.RS
-.RE
 .TP
-.B \f[I]desc\f[]
-Descriptor associated with the local data buffer See \f[C]fi_mr\f[](3).
-.RS
-.RE
+.B \f[I]desc\f[R]
+Descriptor associated with the local data buffer See \f[C]fi_mr\f[R](3).
 .TP
-.B \f[I]data\f[]
+.B \f[I]data\f[R]
 Remote CQ data to transfer with the operation.
-.RS
-.RE
 .TP
-.B \f[I]dest_addr\f[]
+.B \f[I]dest_addr\f[R]
 Destination address for connectionless write transfers.
 Ignored for connected endpoints.
-.RS
-.RE
 .TP
-.B \f[I]src_addr\f[]
+.B \f[I]src_addr\f[R]
 Source address to read from for connectionless transfers.
 Ignored for connected endpoints.
-.RS
-.RE
 .TP
-.B \f[I]msg\f[]
+.B \f[I]msg\f[R]
 Message descriptor for read and write operations.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply for the read or write operation.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified pointer to associate with the operation.
 This parameter is ignored if the operation will not generate a
 successful completion, unless an op flag specifies the context parameter
 be used for required input.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 RMA (remote memory access) operations are used to transfer data directly
@@ -139,15 +109,16 @@ between a local data buffer and a remote data buffer.
 RMA transfers occur on a byte level granularity, and no message
 boundaries are maintained.
 .PP
-The write functions \-\- fi_write, fi_writev, fi_writemsg,
-fi_inject_write, and fi_writedata \-\- are used to transmit data into a
+The write functions \[en] fi_write, fi_writev, fi_writemsg,
+fi_inject_write, and fi_writedata \[en] are used to transmit data into a
 remote memory buffer.
 The main difference between write functions are the number and type of
 parameters that they accept as input.
 Otherwise, they perform the same general function.
 .PP
-The read functions \-\- fi_read, fi_readv, and fi_readmsg \-\- are used
-to transfer data from a remote memory region into local data buffer(s).
+The read functions \[en] fi_read, fi_readv, and fi_readmsg \[en] are
+used to transfer data from a remote memory region into local data
+buffer(s).
 Similar to the write operations, read operations operate asynchronously.
 Users should not touch the posted data buffer(s) until the read
 operation has completed.
@@ -182,29 +153,29 @@ The fi_writemsg function takes a struct fi_msg_rma as input.
 .IP
 .nf
 \f[C]
-struct\ fi_msg_rma\ {
-\ \ \ \ const\ struct\ iovec\ *msg_iov;\ \ \ \ \ /*\ local\ scatter\-gather\ array\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **desc;\ \ \ \ \ \ \ /*\ operation\ descriptor\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ iov_count;\ \ \ \ /*\ #\ elements\ in\ msg_iov\ */
-\ \ \ \ fi_addr_t\ \ \ \ \ \ \ \ \ \ addr;\ \ \ \ \ \ \ \ /*\ optional\ endpoint\ address\ */
-\ \ \ \ const\ struct\ fi_rma_iov\ *rma_iov;/*\ remote\ SGL\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ rma_iov_count;/*\ #\ elements\ in\ rma_iov\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *context;\ \ \ \ \ /*\ user\-defined\ context\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ data;\ \ \ \ \ \ \ \ \ /*\ optional\ immediate\ data\ */
+struct fi_msg_rma {
+    const struct iovec *msg_iov;     /* local scatter\-gather array */
+    void               **desc;       /* operation descriptor */
+    size_t             iov_count;    /* # elements in msg_iov */
+    fi_addr_t          addr;        /* optional endpoint address */
+    const struct fi_rma_iov *rma_iov;/* remote SGL */
+    size_t             rma_iov_count;/* # elements in rma_iov */
+    void               *context;     /* user\-defined context */
+    uint64_t           data;         /* optional immediate data */
 };
 
-struct\ fi_rma_iov\ {
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ addr;\ \ \ \ \ \ \ \ \ /*\ target\ RMA\ address\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ len;\ \ \ \ \ \ \ \ \ \ /*\ size\ of\ target\ buffer\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ key;\ \ \ \ \ \ \ \ \ \ /*\ access\ key\ */
+struct fi_rma_iov {
+    uint64_t           addr;         /* target RMA address */
+    size_t             len;          /* size of target buffer */
+    uint64_t           key;          /* access key */
 };
-\f[]
+\f[R]
 .fi
 .SS fi_inject_write
 .PP
 The write inject call is an optimized version of fi_write.
 It provides similar completion semantics as fi_inject
-\f[C]fi_msg\f[](3).
+\f[C]fi_msg\f[R](3).
 .SS fi_writedata
 .PP
 The write data call is similar to fi_write, but allows for the sending
@@ -239,32 +210,26 @@ fi_endpoint.3).
 The following list of flags are usable with fi_readmsg and/or
 fi_writemsg.
 .TP
-.B \f[I]FI_REMOTE_CQ_DATA\f[]
+.B \f[I]FI_REMOTE_CQ_DATA\f[R]
 Applies to fi_writemsg and fi_writedata.
 Indicates that remote CQ data is available and should be sent as part of
 the request.
 See fi_getinfo for additional details on FI_REMOTE_CQ_DATA.
-.RS
-.RE
 .TP
-.B \f[I]FI_COMPLETION\f[]
+.B \f[I]FI_COMPLETION\f[R]
 Indicates that a completion entry should be generated for the specified
 operation.
 The endpoint must be bound to a completion queue with
 FI_SELECTIVE_COMPLETION that corresponds to the specified operation, or
 this flag is ignored.
-.RS
-.RE
 .TP
-.B \f[I]FI_MORE\f[]
+.B \f[I]FI_MORE\f[R]
 Indicates that the user has additional requests that will immediately be
 posted after the current call returns.
 Use of this flag may improve performance by enabling the provider to
 optimize its access to the fabric hardware.
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT\f[]
+.B \f[I]FI_INJECT\f[R]
 Applies to fi_writemsg.
 Indicates that the outbound data buffer should be returned to user
 immediately after the write call returns, even if the operation is
@@ -272,39 +237,29 @@ handled asynchronously.
 This may require that the underlying provider implementation copy the
 data into a local buffer and transfer out of that buffer.
 This flag can only be used with messages smaller than inject_size.
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT_COMPLETE\f[]
+.B \f[I]FI_INJECT_COMPLETE\f[R]
 Applies to fi_writemsg.
 Indicates that a completion should be generated when the source
 buffer(s) may be reused.
-.RS
-.RE
 .TP
-.B \f[I]FI_TRANSMIT_COMPLETE\f[]
+.B \f[I]FI_TRANSMIT_COMPLETE\f[R]
 Applies to fi_writemsg.
 Indicates that a completion should not be generated until the operation
 has been successfully transmitted and is no longer being tracked by the
 provider.
-.RS
-.RE
 .TP
-.B \f[I]FI_DELIVERY_COMPLETE\f[]
+.B \f[I]FI_DELIVERY_COMPLETE\f[R]
 Applies to fi_writemsg.
 Indicates that a completion should be generated when the operation has
 been processed by the destination.
-.RS
-.RE
 .TP
-.B \f[I]FI_COMMIT_COMPLETE\f[]
+.B \f[I]FI_COMMIT_COMPLETE\f[R]
 Applies to fi_writemsg when targeting persistent memory regions.
 Indicates that a completion should be generated only after the result of
 the operation has been made durable.
-.RS
-.RE
 .TP
-.B \f[I]FI_FENCE\f[]
+.B \f[I]FI_FENCE\f[R]
 Applies to transmits.
 Indicates that the requested operation, also known as the fenced
 operation, and any operation posted after the fenced operation will be
@@ -312,26 +267,23 @@ deferred until all previous operations targeting the same peer endpoint
 have completed.
 Operations posted after the fencing will see and/or replace the results
 of any operations initiated prior to the fenced operation.
-.RS
-.RE
 .PP
 The ordering of operations starting at the posting of the fenced
 operation (inclusive) to the posting of a subsequent fenced operation
-(exclusive) is controlled by the endpoint\[aq]s ordering semantics.
+(exclusive) is controlled by the endpoint\[cq]s ordering semantics.
 .SH RETURN VALUE
 .PP
 Returns 0 on success.
 On error, a negative value corresponding to fabric errno is returned.
-Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[].
+Fabric errno values are defined in \f[C]rdma/fi_errno.h\f[R].
 .SH ERRORS
 .TP
-.B \f[I]\-FI_EAGAIN\f[]
-See \f[C]fi_msg\f[](3) for a detailed description of handling FI_EAGAIN.
-.RS
-.RE
+.B \f[I]\-FI_EAGAIN\f[R]
+See \f[C]fi_msg\f[R](3) for a detailed description of handling
+FI_EAGAIN.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_cq\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_cq\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_tagged.3 b/man/man3/fi_tagged.3
index d865714..e7f0f06 100644
--- a/man/man3/fi_tagged.3
+++ b/man/man3/fi_tagged.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_tagged" "3" "2020\-10\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_tagged" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -8,129 +8,97 @@ fi_tagged \- Tagged data transfer operations
 .TP
 .B fi_trecv / fi_trecvv / fi_trecvmsg
 Post a buffer to receive an incoming message
-.RS
-.RE
 .TP
 .B fi_tsend / fi_tsendv / fi_tsendmsg / fi_tinject / fi_tsenddata
 Initiate an operation to send a message
-.RS
-.RE
 .SH SYNOPSIS
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_tagged.h>
+#include <rdma/fi_tagged.h>
 
-ssize_t\ fi_trecv(struct\ fid_ep\ *ep,\ void\ *buf,\ size_t\ len,\ void\ *desc,
-\ \ \ \ fi_addr_t\ src_addr,\ uint64_t\ tag,\ uint64_t\ ignore,\ void\ *context);
+ssize_t fi_trecv(struct fid_ep *ep, void *buf, size_t len, void *desc,
+    fi_addr_t src_addr, uint64_t tag, uint64_t ignore, void *context);
 
-ssize_t\ fi_trecvv(struct\ fid_ep\ *ep,\ const\ struct\ iovec\ *iov,\ void\ **desc,
-\ \ \ \ size_t\ count,\ fi_addr_t\ src_addr,\ uint64_t\ tag,\ uint64_t\ ignore,
-\ \ \ \ void\ *context);
+ssize_t fi_trecvv(struct fid_ep *ep, const struct iovec *iov, void **desc,
+    size_t count, fi_addr_t src_addr, uint64_t tag, uint64_t ignore,
+    void *context);
 
-ssize_t\ fi_trecvmsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg_tagged\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_trecvmsg(struct fid_ep *ep, const struct fi_msg_tagged *msg,
+    uint64_t flags);
 
-ssize_t\ fi_tsend(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ fi_addr_t\ dest_addr,\ uint64_t\ tag,\ void\ *context);
+ssize_t fi_tsend(struct fid_ep *ep, const void *buf, size_t len,
+    void *desc, fi_addr_t dest_addr, uint64_t tag, void *context);
 
-ssize_t\ fi_tsendv(struct\ fid_ep\ *ep,\ const\ struct\ iovec\ *iov,
-\ \ \ \ void\ **desc,\ size_t\ count,\ fi_addr_t\ dest_addr,\ uint64_t\ tag,
-\ \ \ \ void\ *context);
+ssize_t fi_tsendv(struct fid_ep *ep, const struct iovec *iov,
+    void **desc, size_t count, fi_addr_t dest_addr, uint64_t tag,
+    void *context);
 
-ssize_t\ fi_tsendmsg(struct\ fid_ep\ *ep,\ const\ struct\ fi_msg_tagged\ *msg,
-\ \ \ \ uint64_t\ flags);
+ssize_t fi_tsendmsg(struct fid_ep *ep, const struct fi_msg_tagged *msg,
+    uint64_t flags);
 
-ssize_t\ fi_tinject(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ fi_addr_t\ dest_addr,\ uint64_t\ tag);
+ssize_t fi_tinject(struct fid_ep *ep, const void *buf, size_t len,
+    fi_addr_t dest_addr, uint64_t tag);
 
-ssize_t\ fi_tsenddata(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ void\ *desc,\ uint64_t\ data,\ fi_addr_t\ dest_addr,\ uint64_t\ tag,
-\ \ \ \ void\ *context);
+ssize_t fi_tsenddata(struct fid_ep *ep, const void *buf, size_t len,
+    void *desc, uint64_t data, fi_addr_t dest_addr, uint64_t tag,
+    void *context);
 
-ssize_t\ fi_tinjectdata(struct\ fid_ep\ *ep,\ const\ void\ *buf,\ size_t\ len,
-\ \ \ \ uint64_t\ data,\ fi_addr_t\ dest_addr,\ uint64_t\ tag);
-\f[]
+ssize_t fi_tinjectdata(struct fid_ep *ep, const void *buf, size_t len,
+    uint64_t data, fi_addr_t dest_addr, uint64_t tag);
+\f[R]
 .fi
 .SH ARGUMENTS
 .TP
-.B \f[I]fid\f[]
+.B \f[I]fid\f[R]
 Fabric endpoint on which to initiate tagged communication operation.
-.RS
-.RE
 .TP
-.B \f[I]buf\f[]
+.B \f[I]buf\f[R]
 Data buffer to send or receive.
-.RS
-.RE
 .TP
-.B \f[I]len\f[]
+.B \f[I]len\f[R]
 Length of data buffer to send or receive.
-.RS
-.RE
 .TP
-.B \f[I]iov\f[]
+.B \f[I]iov\f[R]
 Vectored data buffer.
-.RS
-.RE
 .TP
-.B \f[I]count\f[]
+.B \f[I]count\f[R]
 Count of vectored data entries.
-.RS
-.RE
 .TP
-.B \f[I]tag\f[]
+.B \f[I]tag\f[R]
 Tag associated with the message.
-.RS
-.RE
 .TP
-.B \f[I]ignore\f[]
+.B \f[I]ignore\f[R]
 Mask of bits to ignore applied to the tag for receive operations.
-.RS
-.RE
 .TP
-.B \f[I]desc\f[]
+.B \f[I]desc\f[R]
 Memory descriptor associated with the data buffer.
-See \f[C]fi_mr\f[](3).
-.RS
-.RE
+See \f[C]fi_mr\f[R](3).
 .TP
-.B \f[I]data\f[]
+.B \f[I]data\f[R]
 Remote CQ data to transfer with the sent data.
-.RS
-.RE
 .TP
-.B \f[I]dest_addr\f[]
+.B \f[I]dest_addr\f[R]
 Destination address for connectionless transfers.
 Ignored for connected endpoints.
-.RS
-.RE
 .TP
-.B \f[I]src_addr\f[]
+.B \f[I]src_addr\f[R]
 Source address to receive from for connectionless transfers.
 Applies only to connectionless endpoints with the FI_DIRECTED_RECV
 capability enabled, otherwise this field is ignored.
 If set to FI_ADDR_UNSPEC, any source address may match.
-.RS
-.RE
 .TP
-.B \f[I]msg\f[]
+.B \f[I]msg\f[R]
 Message descriptor for send and receive operations.
-.RS
-.RE
 .TP
-.B \f[I]flags\f[]
+.B \f[I]flags\f[R]
 Additional flags to apply for the send or receive operation.
-.RS
-.RE
 .TP
-.B \f[I]context\f[]
+.B \f[I]context\f[R]
 User specified pointer to associate with the operation.
 This parameter is ignored if the operation will not generate a
 successful completion, unless an op flag specifies the context parameter
 be used for required input.
-.RS
-.RE
 .SH DESCRIPTION
 .PP
 Tagged messages are data transfers which carry a key or tag with the
@@ -143,22 +111,22 @@ This can be stated as:
 .IP
 .nf
 \f[C]
-send_tag\ &\ ~ignore\ ==\ recv_tag\ &\ ~ignore
-\f[]
+send_tag & \[ti]ignore == recv_tag & \[ti]ignore
+\f[R]
 .fi
 .PP
 In general, message tags are checked against receive buffers in the
 order in which messages have been posted to the endpoint.
 See the ordering discussion below for more details.
 .PP
-The send functions \-\- fi_tsend, fi_tsendv, fi_tsendmsg, fi_tinject,
-and fi_tsenddata \-\- are used to transmit a tagged message from one
+The send functions \[en] fi_tsend, fi_tsendv, fi_tsendmsg, fi_tinject,
+and fi_tsenddata \[en] are used to transmit a tagged message from one
 endpoint to another endpoint.
 The main difference between send functions are the number and type of
 parameters that they accept as input.
 Otherwise, they perform the same general function.
 .PP
-The receive functions \-\- fi_trecv, fi_trecvv, fi_recvmsg \-\- post a
+The receive functions \[en] fi_trecv, fi_trecvv, fi_recvmsg \[en] post a
 data buffer to an endpoint to receive inbound tagged messages.
 Similar to the send operations, receive operations operate
 asynchronously.
@@ -203,23 +171,23 @@ The fi_tsendmsg function takes a struct fi_msg_tagged as input.
 .IP
 .nf
 \f[C]
-struct\ fi_msg_tagged\ {
-\ \ \ \ const\ struct\ iovec\ *msg_iov;\ /*\ scatter\-gather\ array\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *desc;\ \ \ \ /*\ data\ descriptor\ */
-\ \ \ \ size_t\ \ \ \ \ \ \ \ \ \ \ \ \ iov_count;/*\ #\ elements\ in\ msg_iov\ */
-\ \ \ \ fi_addr_t\ \ \ \ \ \ \ \ \ \ addr;\ \ \ \ /*\ optional\ endpoint\ address\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ tag;\ \ \ \ \ \ /*\ tag\ associated\ with\ message\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ ignore;\ \ \ /*\ mask\ applied\ to\ tag\ for\ receives\ */
-\ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *context;\ /*\ user\-defined\ context\ */
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ data;\ \ \ \ \ /*\ optional\ immediate\ data\ */
+struct fi_msg_tagged {
+    const struct iovec *msg_iov; /* scatter\-gather array */
+    void               *desc;    /* data descriptor */
+    size_t             iov_count;/* # elements in msg_iov */
+    fi_addr_t          addr;    /* optional endpoint address */
+    uint64_t           tag;      /* tag associated with message */
+    uint64_t           ignore;   /* mask applied to tag for receives */
+    void               *context; /* user\-defined context */
+    uint64_t           data;     /* optional immediate data */
 };
-\f[]
+\f[R]
 .fi
 .SS fi_tinject
 .PP
 The tagged inject call is an optimized version of fi_tsend.
 It provides similar completion semantics as fi_inject
-\f[C]fi_msg\f[](3).
+\f[C]fi_msg\f[R](3).
 .SS fi_tsenddata
 .PP
 The tagged send data call is similar to fi_tsend, but allows for the
@@ -260,32 +228,26 @@ fi_endpoint).
 The following list of flags are usable with fi_trecvmsg and/or
 fi_tsendmsg.
 .TP
-.B \f[I]FI_REMOTE_CQ_DATA\f[]
+.B \f[I]FI_REMOTE_CQ_DATA\f[R]
 Applies to fi_tsendmsg and fi_tsenddata.
 Indicates that remote CQ data is available and should be sent as part of
 the request.
 See fi_getinfo for additional details on FI_REMOTE_CQ_DATA.
-.RS
-.RE
 .TP
-.B \f[I]FI_COMPLETION\f[]
+.B \f[I]FI_COMPLETION\f[R]
 Indicates that a completion entry should be generated for the specified
 operation.
 The endpoint must be bound to a completion queue with
 FI_SELECTIVE_COMPLETION that corresponds to the specified operation, or
 this flag is ignored.
-.RS
-.RE
 .TP
-.B \f[I]FI_MORE\f[]
+.B \f[I]FI_MORE\f[R]
 Indicates that the user has additional requests that will immediately be
 posted after the current call returns.
 Use of this flag may improve performance by enabling the provider to
 optimize its access to the fabric hardware.
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT\f[]
+.B \f[I]FI_INJECT\f[R]
 Applies to fi_tsendmsg.
 Indicates that the outbound data buffer should be returned to user
 immediately after the send call returns, even if the operation is
@@ -293,33 +255,25 @@ handled asynchronously.
 This may require that the underlying provider implementation copy the
 data into a local buffer and transfer out of that buffer.
 This flag can only be used with messages smaller than inject_size.
-.RS
-.RE
 .TP
-.B \f[I]FI_INJECT_COMPLETE\f[]
+.B \f[I]FI_INJECT_COMPLETE\f[R]
 Applies to fi_tsendmsg.
 Indicates that a completion should be generated when the source
 buffer(s) may be reused.
-.RS
-.RE
 .TP
-.B \f[I]FI_TRANSMIT_COMPLETE\f[]
+.B \f[I]FI_TRANSMIT_COMPLETE\f[R]
 Applies to fi_tsendmsg.
 Indicates that a completion should not be generated until the operation
 has been successfully transmitted and is no longer being tracked by the
 provider.
-.RS
-.RE
 .TP
-.B \f[I]FI_MATCH_COMPLETE\f[]
+.B \f[I]FI_MATCH_COMPLETE\f[R]
 Applies to fi_tsendmsg.
 Indicates that a completion should be generated only after the message
 has either been matched with a tagged buffer or was discarded by the
 target application.
-.RS
-.RE
 .TP
-.B \f[I]FI_FENCE\f[]
+.B \f[I]FI_FENCE\f[R]
 Applies to transmits.
 Indicates that the requested operation, also known as the fenced
 operation, and any operation posted after the fenced operation will be
@@ -327,16 +281,14 @@ deferred until all previous operations targeting the same peer endpoint
 have completed.
 Operations posted after the fencing will see and/or replace the results
 of any operations initiated prior to the fenced operation.
-.RS
-.RE
 .PP
 The ordering of operations starting at the posting of the fenced
 operation (inclusive) to the posting of a subsequent fenced operation
-(exclusive) is controlled by the endpoint\[aq]s ordering semantics.
+(exclusive) is controlled by the endpoint\[cq]s ordering semantics.
 .PP
 The following flags may be used with fi_trecvmsg.
 .TP
-.B \f[I]FI_PEEK\f[]
+.B \f[I]FI_PEEK\f[R]
 The peek flag may be used to see if a specified message has arrived.
 A peek request is often useful on endpoints that have provider allocated
 buffering enabled (see fi_rx_attr total_buffered_recv).
@@ -349,16 +301,14 @@ endpoint.
 If no message is found matching the tags specified in the peek request,
 then a completion queue error entry with err field set to FI_ENOMSG will
 be available.
-.RS
-.RE
 .PP
 If a peek request locates a matching message, the operation will
 complete successfully.
 The returned completion data will indicate the meta\-data associated
 with the message, such as the message length, completion flags,
 available CQ data, tag, and source address.
-The data available is subject to the completion entry format (e.g.
-struct fi_cq_tagged_entry).
+The data available is subject to the completion entry format
+(e.g.\ struct fi_cq_tagged_entry).
 .PP
 An application may supply a buffer if it desires to receive data as a
 part of the peek operation.
@@ -369,23 +319,21 @@ if peek operations desire to obtain a copy of the data.
 The returned data is limited to the size of the input buffer(s) or the
 message size, if smaller.
 A provider indicates if data is available by setting the buf field of
-the CQ entry to the user\[aq]s first input buffer.
+the CQ entry to the user\[cq]s first input buffer.
 If buf is NULL, no data was available to return.
 A provider may return NULL even if the peek operation completes
 successfully.
 Note that the CQ entry len field will reference the size of the message,
 not necessarily the size of the returned data.
 .TP
-.B \f[I]FI_CLAIM\f[]
+.B \f[I]FI_CLAIM\f[R]
 If this flag is used in conjunction with FI_PEEK, it indicates if the
-peek request completes successfully \-\- indicating that a matching
-message was located \-\- the message is claimed by caller.
+peek request completes successfully \[en] indicating that a matching
+message was located \[en] the message is claimed by caller.
 Claimed messages can only be retrieved using a subsequent, paired
 receive operation with the FI_CLAIM flag set.
 A receive operation with the FI_CLAIM flag set, but FI_PEEK not set is
 used to retrieve a previously claimed message.
-.RS
-.RE
 .PP
 In order to use the FI_CLAIM flag, an application must supply a struct
 fi_context structure as the context for the receive operation, or a
@@ -399,17 +347,15 @@ When set, it is used to retrieve a tagged message that was buffered by
 the provider.
 See Buffered Tagged Receives section for details.
 .TP
-.B \f[I]FI_DISCARD\f[]
+.B \f[I]FI_DISCARD\f[R]
 This flag may be used in conjunction with either FI_PEEK or FI_CLAIM.
 If this flag is used in conjunction with FI_PEEK, it indicates if the
-peek request completes successfully \-\- indicating that a matching
-message was located \-\- the message is discarded by the provider, as
+peek request completes successfully \[en] indicating that a matching
+message was located \[en] the message is discarded by the provider, as
 the data is not needed by the application.
 This flag may also be used in conjunction with FI_CLAIM in order to
 discard a message previously claimed using an FI_PEEK + FI_CLAIM
 request.
-.RS
-.RE
 .PP
 This flag also applies to endpoints configured for FI_BUFFERED_RECV or
 FI_VARIABLE_MSG.
@@ -421,7 +367,7 @@ If this flag is set, the input buffer(s) and length parameters are
 ignored.
 .SH Buffered Tagged Receives
 .PP
-See \f[C]fi_msg\f[](3) for an introduction to buffered receives.
+See \f[C]fi_msg\f[R](3) for an introduction to buffered receives.
 The handling of buffered receives differs between fi_msg operations and
 fi_tagged.
 Although the provider is responsible for allocating and managing network
@@ -433,8 +379,8 @@ specified tags.
 When FI_BUFFERED_RECV is enabled, the application posts the tags that
 will be used for matching purposes.
 Tags are posted using fi_trecv, fi_trecvv, and fi_trecvmsg; however,
-parameters related to the input buffers are ignored (e.g.
-buf, len, iov, desc).
+parameters related to the input buffers are ignored (e.g.\ buf, len,
+iov, desc).
 When a provider receives a message for which there is a matching tag, it
 will write an entry to the completion queue associated with the
 receiving endpoint.
@@ -445,51 +391,46 @@ The op_context field will point to a struct fi_recv_context.
 .IP
 .nf
 \f[C]
-struct\ fi_recv_context\ {
-\ \ \ \ struct\ fid_ep\ *ep;
-\ \ \ \ void\ *context;
+struct fi_recv_context {
+    struct fid_ep *ep;
+    void *context;
 };
-\f[]
+\f[R]
 .fi
 .PP
-The \[aq]ep\[aq] field will be NULL.
-The \[aq]context\[aq] field will match the application context specified
-when posting the tag.
-Other fields are set as defined in \f[C]fi_msg\f[](3).
+The `ep' field will be NULL.
+The `context' field will match the application context specified when
+posting the tag.
+Other fields are set as defined in \f[C]fi_msg\f[R](3).
 .PP
 After being notified that a buffered receive has arrived, applications
 must either claim or discard the message as described in
-\f[C]fi_msg\f[](3).
+\f[C]fi_msg\f[R](3).
 .SH Variable Length Tagged Messages
 .PP
-Variable length messages are defined in \f[C]fi_msg\f[](3).
+Variable length messages are defined in \f[C]fi_msg\f[R](3).
 The requirements for handling variable length tagged messages is
 identical to those defined above for buffered tagged receives.
 .SH RETURN VALUE
 .PP
 The tagged send and receive calls return 0 on success.
-On error, a negative value corresponding to fabric \f[I]errno \f[] is
+On error, a negative value corresponding to fabric \f[I]errno \f[R] is
 returned.
-Fabric errno values are defined in \f[C]fi_errno.h\f[].
+Fabric errno values are defined in \f[C]fi_errno.h\f[R].
 .SH ERRORS
 .TP
-.B \f[I]\-FI_EAGAIN\f[]
-See \f[C]fi_msg\f[](3) for a detailed description of handling FI_EAGAIN.
-.RS
-.RE
+.B \f[I]\-FI_EAGAIN\f[R]
+See \f[C]fi_msg\f[R](3) for a detailed description of handling
+FI_EAGAIN.
 .TP
-.B \f[I]\-FI_EINVAL\f[]
+.B \f[I]\-FI_EINVAL\f[R]
 Indicates that an invalid argument was supplied by the user.
-.RS
-.RE
 .TP
-.B \f[I]\-FI_EOTHER\f[]
+.B \f[I]\-FI_EOTHER\f[R]
 Indicates that an unspecified error occurred.
-.RS
-.RE
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
-\f[C]fi_cq\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3), \f[C]fi_cq\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_trigger.3 b/man/man3/fi_trigger.3
index 590b077..ee22d7c 100644
--- a/man/man3/fi_trigger.3
+++ b/man/man3/fi_trigger.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_trigger" "3" "2019\-09\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_trigger" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,8 +9,8 @@ fi_trigger \- Triggered operations
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_trigger.h>
-\f[]
+#include <rdma/fi_trigger.h>
+\f[R]
 .fi
 .SH DESCRIPTION
 .PP
@@ -50,22 +50,22 @@ The format of struct fi_triggered_context[2] is described below.
 .IP
 .nf
 \f[C]
-struct\ fi_triggered_context\ {
-\ \ \ \ enum\ fi_trigger_event\ \ \ \ \ \ \ \ \ event_type;\ \ \ /*\ trigger\ type\ */
-\ \ \ \ union\ {
-\ \ \ \ \ \ \ \ struct\ fi_trigger_threshold\ threshold;
-\ \ \ \ \ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *internal[3];\ /*\ reserved\ */
-\ \ \ \ }\ trigger;
+struct fi_triggered_context {
+    enum fi_trigger_event         event_type;   /* trigger type */
+    union {
+        struct fi_trigger_threshold threshold;
+        void                        *internal[3]; /* reserved */
+    } trigger;
 };
 
-struct\ fi_triggered_context2\ {
-\ \ \ \ enum\ fi_trigger_event\ \ \ \ \ \ \ \ \ event_type;\ \ \ /*\ trigger\ type\ */
-\ \ \ \ union\ {
-\ \ \ \ \ \ \ \ struct\ fi_trigger_threshold\ threshold;
-\ \ \ \ \ \ \ \ void\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ *internal[7];\ /*\ reserved\ */
-\ \ \ \ }\ trigger;
+struct fi_triggered_context2 {
+    enum fi_trigger_event         event_type;   /* trigger type */
+    union {
+        struct fi_trigger_threshold threshold;
+        void                        *internal[7]; /* reserved */
+    } trigger;
 };
-\f[]
+\f[R]
 .fi
 .PP
 The triggered context indicates the type of event assigned to the
@@ -75,20 +75,18 @@ event type.
 .PP
 The following trigger events are defined.
 .TP
-.B \f[I]FI_TRIGGER_THRESHOLD\f[]
+.B \f[I]FI_TRIGGER_THRESHOLD\f[R]
 This indicates that the data transfer operation will be deferred until
 an event counter crosses an application specified threshold value.
 The threshold is specified using struct fi_trigger_threshold:
-.RS
-.RE
 .IP
 .nf
 \f[C]
-struct\ fi_trigger_threshold\ {
-\ \ \ \ struct\ fid_cntr\ *cntr;\ /*\ event\ counter\ to\ check\ */
-\ \ \ \ size_t\ threshold;\ \ \ \ \ \ /*\ threshold\ value\ */
+struct fi_trigger_threshold {
+    struct fid_cntr *cntr; /* event counter to check */
+    size_t threshold;      /* threshold value */
 };
-\f[]
+\f[R]
 .fi
 .PP
 Threshold operations are triggered in the order of the threshold values.
@@ -121,26 +119,26 @@ The format of the deferred work request is as follows:
 .IP
 .nf
 \f[C]
-struct\ fi_deferred_work\ {
-\ \ \ \ struct\ fi_context2\ \ \ \ context;
+struct fi_deferred_work {
+    struct fi_context2    context;
 
-\ \ \ \ uint64_t\ \ \ \ \ \ \ \ \ \ \ \ \ \ threshold;
-\ \ \ \ struct\ fid_cntr\ \ \ \ \ \ \ *triggering_cntr;
-\ \ \ \ struct\ fid_cntr\ \ \ \ \ \ \ *completion_cntr;
+    uint64_t              threshold;
+    struct fid_cntr       *triggering_cntr;
+    struct fid_cntr       *completion_cntr;
 
-\ \ \ \ enum\ fi_trigger_op\ \ \ \ op_type;
+    enum fi_trigger_op    op_type;
 
-\ \ \ \ union\ {
-\ \ \ \ \ \ \ \ struct\ fi_op_msg\ \ \ \ \ \ \ \ \ \ \ \ *msg;
-\ \ \ \ \ \ \ \ struct\ fi_op_tagged\ \ \ \ \ \ \ \ \ *tagged;
-\ \ \ \ \ \ \ \ struct\ fi_op_rma\ \ \ \ \ \ \ \ \ \ \ \ *rma;
-\ \ \ \ \ \ \ \ struct\ fi_op_atomic\ \ \ \ \ \ \ \ \ *atomic;
-\ \ \ \ \ \ \ \ struct\ fi_op_fetch_atomic\ \ \ *fetch_atomic;
-\ \ \ \ \ \ \ \ struct\ fi_op_compare_atomic\ *compare_atomic;
-\ \ \ \ \ \ \ \ struct\ fi_op_cntr\ \ \ \ \ \ \ \ \ \ \ *cntr;
-\ \ \ \ }\ op;
+    union {
+        struct fi_op_msg            *msg;
+        struct fi_op_tagged         *tagged;
+        struct fi_op_rma            *rma;
+        struct fi_op_atomic         *atomic;
+        struct fi_op_fetch_atomic   *fetch_atomic;
+        struct fi_op_compare_atomic *compare_atomic;
+        struct fi_op_cntr           *cntr;
+    } op;
 };
-\f[]
+\f[R]
 .fi
 .PP
 Once a work request has been posted to the deferred work queue, it will
@@ -158,7 +156,7 @@ The completion_cntr field must be NULL for counter operations.
 Because deferred work targets support of collective communication
 operations, posted work requests do not generate any completions at the
 endpoint by default.
-For example, completed operations are not written to the EP\[aq]s
+For example, completed operations are not written to the EP\[cq]s
 completion queue or update the EP counter (unless the EP counter is
 explicitly referenced as the completion_cntr).
 An application may request EP completions by specifying the
@@ -166,13 +164,13 @@ FI_COMPLETION flag as part of the operation.
 .PP
 It is the responsibility of the application to detect and handle
 situations that occur which could result in a deferred work
-request\[aq]s condition not being met.
+request\[cq]s condition not being met.
 For example, if a work request is dependent upon the successful
 completion of a data transfer operation, which fails, then the
 application must cancel the work request.
 .PP
 To submit a deferred work request, applications should use the
-domain\[aq]s fi_control function with command FI_QUEUE_WORK and struct
+domain\[cq]s fi_control function with command FI_QUEUE_WORK and struct
 fi_deferred_work as the fi_control arg parameter.
 To cancel a deferred work request, use fi_control with command
 FI_CANCEL_WORK and the corresponding struct fi_deferred_work to cancel.
@@ -191,7 +189,7 @@ If a specific request is not supported by the provider, it will fail the
 operation with \-FI_ENOSYS.
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_alias\f[](3),
-\f[C]fi_cntr\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_alias\f[R](3), \f[C]fi_cntr\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man3/fi_version.3 b/man/man3/fi_version.3
index 27e5080..f36afdc 100644
--- a/man/man3/fi_version.3
+++ b/man/man3/fi_version.3
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_version" "3" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_version" "3" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,14 +9,14 @@ fi_version \- Version of the library interfaces
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
+#include <rdma/fabric.h>
 
-uint32_t\ fi_version(void);
+uint32_t fi_version(void);
 
 FI_MAJOR(version)
 
 FI_MINOR(version)
-\f[]
+\f[R]
 .fi
 .SH DESCRIPTION
 .PP
@@ -34,6 +34,6 @@ The upper 16\-bits of the version correspond to the major number, and
 the lower 16\-bits correspond with the minor number.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fabric.7 b/man/man7/fabric.7
index c4b812b..32c996c 100644
--- a/man/man7/fabric.7
+++ b/man/man7/fabric.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fabric" "7" "2020\-07\-21" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fabric" "7" "2021\-09\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -9,15 +9,15 @@ fabric \- Fabric Interface Library
 .IP
 .nf
 \f[C]
-#include\ <rdma/fabric.h>
-\f[]
+#include <rdma/fabric.h>
+\f[R]
 .fi
 .PP
 Libfabric is a high\-performance fabric software library designed to
 provide low\-latency interfaces to fabric hardware.
 .SH OVERVIEW
 .PP
-Libfabric provides \[aq]process direct I/O\[aq] to application software
+Libfabric provides `process direct I/O' to application software
 communicating across fabric software and hardware.
 Process direct I/O, historically referred to as RDMA, allows an
 application to directly access network resources without operating
@@ -26,7 +26,7 @@ Data transfers can occur directly to and from application memory.
 .PP
 There are two components to the libfabric software:
 .TP
-.B \f[I]Fabric Providers\f[]
+.B \f[I]Fabric Providers\f[R]
 Conceptually, a fabric provider may be viewed as a local hardware NIC
 driver, though a provider is not limited by this definition.
 The first component of libfabric is a general purpose framework that is
@@ -35,19 +35,15 @@ All fabric hardware devices and their software drivers are required to
 support this framework.
 Devices and the drivers that plug into the libfabric framework are
 referred to as fabric providers, or simply providers.
-Provider details may be found in \f[C]fi_provider\f[](7).
-.RS
-.RE
+Provider details may be found in \f[C]fi_provider\f[R](7).
 .TP
-.B \f[I]Fabric Interfaces\f[]
+.B \f[I]Fabric Interfaces\f[R]
 The second component is a set of communication operations.
 Libfabric defines several sets of communication functions that providers
 can support.
 It is not required that providers implement all the interfaces that are
 defined; however, providers clearly indicate which interfaces they do
 support.
-.RS
-.RE
 .SH FABRIC INTERFACES
 .PP
 The fabric interfaces are designed such that they are cohesive and not
@@ -74,15 +70,13 @@ resources.
 This involves listing all the interfaces available, obtaining the
 capabilities of the interfaces and opening a provider.
 .TP
-.B \f[I]fi_getinfo \- Fabric Information\f[]
+.B \f[I]fi_getinfo \- Fabric Information\f[R]
 The fi_getinfo call is the base call used to discover and request fabric
 services offered by the system.
 Applications can use this call to indicate the type of communication
 that they desire.
 The results from fi_getinfo, fi_info, are used to reserve and configure
 fabric resources.
-.RS
-.RE
 .PP
 fi_getinfo returns a list of fi_info structures.
 Each structure references a single fabric provider, indicating the
@@ -91,7 +85,7 @@ resources.
 A fabric provider may include multiple fi_info structures in the
 returned list.
 .TP
-.B \f[I]fi_fabric \- Fabric Domain\f[]
+.B \f[I]fi_fabric \- Fabric Domain\f[R]
 A fabric domain represents a collection of hardware and software
 resources that access a single physical or virtual network.
 All network ports on a system that can communicate with each other
@@ -99,61 +93,47 @@ through the fabric belong to the same fabric domain.
 A fabric domain shares network addresses and can span multiple
 providers.
 libfabric supports systems connected to multiple fabrics.
-.RS
-.RE
 .TP
-.B \f[I]fi_domain \- Access Domains\f[]
+.B \f[I]fi_domain \- Access Domains\f[R]
 An access domain represents a single logical connection into a fabric.
 It may map to a single physical or virtual NIC or a port.
 An access domain defines the boundary across which fabric resources may
 be associated.
 Each access domain belongs to a single fabric domain.
-.RS
-.RE
 .TP
-.B \f[I]fi_endpoint \- Fabric Endpoint\f[]
+.B \f[I]fi_endpoint \- Fabric Endpoint\f[R]
 A fabric endpoint is a communication portal.
 An endpoint may be either active or passive.
 Passive endpoints are used to listen for connection requests.
 Active endpoints can perform data transfers.
 Endpoints are configured with specific communication capabilities and
 data transfer interfaces.
-.RS
-.RE
 .TP
-.B \f[I]fi_eq \- Event Queue\f[]
+.B \f[I]fi_eq \- Event Queue\f[R]
 Event queues, are used to collect and report the completion of
 asynchronous operations and events.
 Event queues report events that are not directly associated with data
 transfer operations.
-.RS
-.RE
 .TP
-.B \f[I]fi_cq \- Completion Queue\f[]
+.B \f[I]fi_cq \- Completion Queue\f[R]
 Completion queues are high\-performance event queues used to report the
 completion of data transfer operations.
-.RS
-.RE
 .TP
-.B \f[I]fi_cntr \- Event Counters\f[]
+.B \f[I]fi_cntr \- Event Counters\f[R]
 Event counters are used to report the number of completed asynchronous
 operations.
 Event counters are considered light\-weight, in that a completion simply
 increments a counter, rather than placing an entry into an event queue.
-.RS
-.RE
 .TP
-.B \f[I]fi_mr \- Memory Region\f[]
+.B \f[I]fi_mr \- Memory Region\f[R]
 Memory regions describe application local memory buffers.
 In order for fabric resources to access application memory, the
 application must first grant permission to the fabric provider by
 constructing a memory region.
 Memory regions are required for specific types of data transfer
 operations, such as RMA transfers (see below).
-.RS
-.RE
 .TP
-.B \f[I]fi_av \- Address Vector\f[]
+.B \f[I]fi_av \- Address Vector\f[R]
 Address vectors are used to map higher level addresses, such as IP
 addresses, which may be more natural for an application to use, into
 fabric specific addresses.
@@ -161,8 +141,6 @@ The use of address vectors allows providers to reduce the amount of
 memory required to maintain large address look\-up tables, and eliminate
 expensive address resolution and look\-up methods during data transfer
 operations.
-.RS
-.RE
 .SH DATA TRANSFER INTERFACES
 .PP
 Fabric endpoints are associated with multiple data transfer interfaces.
@@ -171,150 +149,110 @@ communication, with an endpoint allowing the different interfaces to be
 used in conjunction.
 The following data transfer interfaces are defined by libfabric.
 .TP
-.B \f[I]fi_msg \- Message Queue\f[]
+.B \f[I]fi_msg \- Message Queue\f[R]
 Message queues expose a simple, message\-based FIFO queue interface to
 the application.
 Message data transfers allow applications to send and receive data with
 message boundaries being maintained.
-.RS
-.RE
 .TP
-.B \f[I]fi_tagged \- Tagged Message Queues\f[]
+.B \f[I]fi_tagged \- Tagged Message Queues\f[R]
 Tagged message lists expose send/receive data transfer operations built
 on the concept of tagged messaging.
 The tagged message queue is conceptually similar to standard message
 queues, but with the addition of 64\-bit tags for each message.
 Sent messages are matched with receive buffers that are tagged with a
 similar value.
-.RS
-.RE
 .TP
-.B \f[I]fi_rma \- Remote Memory Access\f[]
+.B \f[I]fi_rma \- Remote Memory Access\f[R]
 RMA transfers are one\-sided operations that read or write data directly
 to a remote memory region.
 Other than defining the appropriate memory region, RMA operations do not
 require interaction at the target side for the data transfer to
 complete.
-.RS
-.RE
 .TP
-.B \f[I]fi_atomic \- Atomic\f[]
+.B \f[I]fi_atomic \- Atomic\f[R]
 Atomic operations can perform one of several operations on a remote
 memory region.
 Atomic operations include well\-known functionality, such as atomic\-add
 and compare\-and\-swap, plus several other pre\-defined calls.
 Unlike other data transfer interfaces, atomic operations are aware of
 the data formatting at the target memory region.
-.RS
-.RE
 .SH LOGGING INTERFACE
 .PP
 Logging can be controlled using the FI_LOG_LEVEL, FI_LOG_PROV, and
 FI_LOG_SUBSYS environment variables.
 .TP
-.B \f[I]FI_LOG_LEVEL\f[]
+.B \f[I]FI_LOG_LEVEL\f[R]
 FI_LOG_LEVEL controls the amount of logging data that is output.
 The following log levels are defined.
-.RS
-.RE
 .TP
-.B \- \f[I]Warn\f[]
+.B \- \f[I]Warn\f[R]
 Warn is the least verbose setting and is intended for reporting errors
 or warnings.
-.RS
-.RE
 .TP
-.B \- \f[I]Trace\f[]
+.B \- \f[I]Trace\f[R]
 Trace is more verbose and is meant to include non\-detailed output
 helpful to tracing program execution.
-.RS
-.RE
 .TP
-.B \- \f[I]Info\f[]
+.B \- \f[I]Info\f[R]
 Info is high traffic and meant for detailed output.
-.RS
-.RE
 .TP
-.B \- \f[I]Debug\f[]
+.B \- \f[I]Debug\f[R]
 Debug is high traffic and is likely to impact application performance.
 Debug output is only available if the library has been compiled with
 debugging enabled.
-.RS
-.RE
 .TP
-.B \f[I]FI_LOG_PROV\f[]
+.B \f[I]FI_LOG_PROV\f[R]
 The FI_LOG_PROV environment variable enables or disables logging from
 specific providers.
 Providers can be enabled by listing them in a comma separated fashion.
-If the list begins with the \[aq]^\[aq] symbol, then the list will be
+If the list begins with the `\[ha]' symbol, then the list will be
 negated.
 By default all providers are enabled.
-.RS
-.RE
 .PP
 Example: To enable logging from the psm and sockets provider:
-FI_LOG_PROV="psm,sockets"
+FI_LOG_PROV=\[lq]psm,sockets\[rq]
 .PP
 Example: To enable logging from providers other than psm:
-FI_LOG_PROV="^psm"
+FI_LOG_PROV=\[lq]\[ha]psm\[rq]
 .TP
-.B \f[I]FI_LOG_SUBSYS\f[]
+.B \f[I]FI_LOG_SUBSYS\f[R]
 The FI_LOG_SUBSYS environment variable enables or disables logging at
 the subsystem level.
 The syntax for enabling or disabling subsystems is similar to that used
 for FI_LOG_PROV.
 The following subsystems are defined.
-.RS
-.RE
 .TP
-.B \- \f[I]core\f[]
+.B \- \f[I]core\f[R]
 Provides output related to the core framework and its management of
 providers.
-.RS
-.RE
 .TP
-.B \- \f[I]fabric\f[]
+.B \- \f[I]fabric\f[R]
 Provides output specific to interactions associated with the fabric
 object.
-.RS
-.RE
 .TP
-.B \- \f[I]domain\f[]
+.B \- \f[I]domain\f[R]
 Provides output specific to interactions associated with the domain
 object.
-.RS
-.RE
 .TP
-.B \- \f[I]ep_ctrl\f[]
+.B \- \f[I]ep_ctrl\f[R]
 Provides output specific to endpoint non\-data transfer operations, such
 as CM operations.
-.RS
-.RE
 .TP
-.B \- \f[I]ep_data\f[]
+.B \- \f[I]ep_data\f[R]
 Provides output specific to endpoint data transfer operations.
-.RS
-.RE
 .TP
-.B \- \f[I]av\f[]
+.B \- \f[I]av\f[R]
 Provides output specific to address vector operations.
-.RS
-.RE
 .TP
-.B \- \f[I]cq\f[]
+.B \- \f[I]cq\f[R]
 Provides output specific to completion queue operations.
-.RS
-.RE
 .TP
-.B \- \f[I]eq\f[]
+.B \- \f[I]eq\f[R]
 Provides output specific to event queue operations.
-.RS
-.RE
 .TP
-.B \- \f[I]mr\f[]
+.B \- \f[I]mr\f[R]
 Provides output specific to memory registration.
-.RS
-.RE
 .SH PROVIDER INSTALLATION AND SELECTION
 .PP
 The libfabric build scripts will install all providers that are
@@ -325,29 +263,29 @@ library initialization and respond appropriately to application queries.
 .PP
 Users can enable or disable available providers through build
 configuration options.
-See \[aq]configure \-\-help\[aq] for details.
+See `configure \[en]help' for details.
 In general, a specific provider can be controlled using the configure
-option \[aq]\-\-enable\-\[aq].
-For example, \[aq]\-\-enable\-udp\[aq] (or
-\[aq]\-\-enable\-udp=yes\[aq]) will add the udp provider to the build.
-To disable the provider, \[aq]\-\-enable\-udp=no\[aq] can be used.
+option `\[en]enable\-'.
+For example, `\[en]enable\-udp' (or `\[en]enable\-udp=yes') will add the
+udp provider to the build.
+To disable the provider, `\[en]enable\-udp=no' can be used.
 .PP
 Providers can also be enable or disabled at run time using the
 FI_PROVIDER environment variable.
 The FI_PROVIDER variable is set to a comma separated list of providers
 to include.
-If the list begins with the \[aq]^\[aq] symbol, then the list will be
+If the list begins with the `\[ha]' symbol, then the list will be
 negated.
 .PP
 Example: To enable the udp and tcp providers only, set:
-FI_PROVIDER="udp,tcp"
+FI_PROVIDER=\[lq]udp,tcp\[rq]
 .PP
 The fi_info utility, which is included as part of the libfabric package,
 can be used to retrieve information about which providers are available
 in the system.
 Additionally, it can retrieve a list of all environment variables that
 may be used to configure libfabric and each provider.
-See \f[C]fi_info\f[](1) for more details.
+See \f[C]fi_info\f[R](1) for more details.
 .SH ENVIRONMENT VARIABLE CONTROLS
 .PP
 Core features of libfabric and its providers may be configured by an
@@ -361,8 +299,9 @@ obtain the full list of variables that may be set, along with a brief
 description of their use.
 .PP
 A full list of variables available may be obtained by running the
-fi_info application, with the \-e or \-\-env command line option.
+fi_info application, with the \-e or \[en]env command line option.
 .SH NOTES
+.SS System Calls
 .PP
 Because libfabric is designed to provide applications direct access to
 fabric hardware, there are limits on how libfabric resources may be used
@@ -373,7 +312,7 @@ Although limits are provider specific, the following restrictions apply
 to many providers and should be adhered to by applications desiring
 portability across providers.
 .TP
-.B \f[I]fork\f[]
+.B \f[I]fork\f[R]
 Fabric resources are not guaranteed to be available by child processes.
 This includes objects, such as endpoints and completion queues, as well
 as application controlled data buffers which have been assigned to the
@@ -381,8 +320,30 @@ network.
 For example, data buffers that have been registered with a fabric domain
 may not be available in a child process because of copy on write
 restrictions.
-.RS
-.RE
+.SS CUDA deadlock
+.PP
+In some cases, calls to \f[C]cudaMemcpy\f[R] within libfabric may result
+in a deadlock.
+This typically occurs when a CUDA kernel blocks until a
+\f[C]cudaMemcpy\f[R] on the host completes.
+To avoid this deadlock, \f[C]cudaMemcpy\f[R] may be disabled by setting
+\f[C]FI_HMEM_CUDA_ENABLE_XFER=0\f[R].
+If this environment variable is set and there is a call to
+\f[C]cudaMemcpy\f[R] with libfabric, a warning will be emitted and no
+copy will occur.
+Note that not all providers support this option.
+.PP
+Another mechanism which can be used to avoid deadlock is Nvidia\[cq]s
+gdrcopy.
+Using gdrcopy requires an external library and kernel module available
+at https://github.com/NVIDIA/gdrcopy.
+Libfabric must be configured with gdrcopy support using the
+\f[C]\-\-with\-gdrcopy\f[R] option, and be run with
+\f[C]FI_HMEM_CUDA_USE_GDRCOPY=1\f[R].
+This may be used in conjunction with the above option to provide a
+method for copying to/from CUDA device memory when \f[C]cudaMemcpy\f[R]
+cannot be used.
+Again, this may not be supported by all providers.
 .SH ABI CHANGES
 .PP
 libfabric releases maintain compatibility with older releases, so that
@@ -414,54 +375,55 @@ These changes included adding the fields to the following data
 structures.
 The 1.1 ABI was exported by libfabric versions 1.5 and 1.6.
 .TP
-.B \f[I]fi_fabric_attr\f[]
+.B \f[I]fi_fabric_attr\f[R]
 Added api_version
-.RS
-.RE
 .TP
-.B \f[I]fi_domain_attr\f[]
+.B \f[I]fi_domain_attr\f[R]
 Added cntr_cnt, mr_iov_limit, caps, mode, auth_key, auth_key_size,
 max_err_data, and mr_cnt fields.
 The mr_mode field was also changed from an enum to an integer flag
 field.
-.RS
-.RE
 .TP
-.B \f[I]fi_ep_attr\f[]
+.B \f[I]fi_ep_attr\f[R]
 Added auth_key_size and auth_key fields.
-.RS
-.RE
 .SS ABI 1.2
 .PP
 The 1.2 ABI version was exported by libfabric versions 1.7 and 1.8, and
 expanded the following structure.
 .TP
-.B \f[I]fi_info\f[]
+.B \f[I]fi_info\f[R]
 The fi_info structure was expanded to reference a new fabric object,
 fid_nic.
 When available, the fid_nic references a new set of attributes related
 to network hardware details.
-.RS
-.RE
 .SS ABI 1.3
 .PP
-The 1.3 ABI is also the current ABI version.
-All libfabric releases starting at 1.9 export this ABI.
+The 1.3 ABI version was exported by libfabric versions 1.9, 1.10, and
+1.11.
+Added new fields to the following attributes:
 .TP
-.B \f[I]fi_domain_attr\f[]
+.B \f[I]fi_domain_attr\f[R]
 Added tclass
-.RS
-.RE
 .TP
-.B \f[I]fi_tx_attr\f[]
+.B \f[I]fi_tx_attr\f[R]
 Added tclass
-.RS
-.RE
+.SS ABI 1.4
+.PP
+The 1.4 ABI version was exported by libfabric 1.12.
+Added fi_tostr_r, a thread\-safe (re\-entrant) version of fi_tostr.
+.SS ABI 1.5
+.PP
+ABI version starting with libfabric 1.13.
+Added new fi_open API call.
+.SS ABI 1.6
+.PP
+ABI version starting with libfabric 1.14.
+Added fi_log_ready for providers.
 .SH SEE ALSO
 .PP
-\f[C]fi_info\f[](1), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3),
-\f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3), \f[C]fi_av\f[](3),
-\f[C]fi_eq\f[](3), \f[C]fi_cq\f[](3), \f[C]fi_cntr\f[](3),
-\f[C]fi_mr\f[](3)
+\f[C]fi_info\f[R](1), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3),
+\f[C]fi_endpoint\f[R](3), \f[C]fi_domain\f[R](3), \f[C]fi_av\f[R](3),
+\f[C]fi_eq\f[R](3), \f[C]fi_cq\f[R](3), \f[C]fi_cntr\f[R](3),
+\f[C]fi_mr\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_bgq.7 b/man/man7/fi_bgq.7
index 33e5156..8ea28e7 100644
--- a/man/man7/fi_bgq.7
+++ b/man/man7/fi_bgq.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_bgq" "7" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_bgq" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -12,48 +12,44 @@ that makes direct use of the unique hardware features such as the
 Messaging Unit (MU), Base Address Table (BAT), and L2 Atomics.
 .PP
 The purpose of this provider is to demonstrate the scalability and
-performance of libfabric, providing an "extreme scale" development
-environment for applications and middleware using the libfabric API, and
-to support a functional and performant version of MPI3 on Blue Gene/Q
-via MPICH CH4.
+performance of libfabric, providing an \[lq]extreme scale\[rq]
+development environment for applications and middleware using the
+libfabric API, and to support a functional and performant version of
+MPI3 on Blue Gene/Q via MPICH CH4.
 .SH SUPPORTED FEATURES
 .PP
 The bgq provider supports most features defined for the libfabric API.
 Key features include:
 .TP
-.B \f[I]Endpoint types\f[]
+.B \f[I]Endpoint types\f[R]
 The Blue Gene/Q hardware is connectionless and reliable.
-Therefore, the bgq provider only supports the \f[I]FI_EP_RDM\f[]
+Therefore, the bgq provider only supports the \f[I]FI_EP_RDM\f[R]
 endpoint type.
-.RS
-.RE
 .TP
-.B \f[I]Capabilities\f[]
-Supported capabilities include \f[I]FI_MSG\f[], \f[I]FI_RMA\f[],
-\f[I]FI_TAGGED\f[], \f[I]FI_ATOMIC\f[], \f[I]FI_NAMED_RX_CTX\f[],
-\f[I]FI_READ\f[], \f[I]FI_WRITE\f[], \f[I]FI_SEND\f[], \f[I]FI_RECV\f[],
-\f[I]FI_REMOTE_READ\f[], \f[I]FI_REMOTE_WRITE\f[],
-\f[I]FI_MULTI_RECV\f[], \f[I]FI_DIRECTED_RECV\f[], \f[I]FI_SOURCE\f[]
-and \f[I]FI_FENCE\f[].
-.RS
-.RE
+.B \f[I]Capabilities\f[R]
+Supported capabilities include \f[I]FI_MSG\f[R], \f[I]FI_RMA\f[R],
+\f[I]FI_TAGGED\f[R], \f[I]FI_ATOMIC\f[R], \f[I]FI_NAMED_RX_CTX\f[R],
+\f[I]FI_READ\f[R], \f[I]FI_WRITE\f[R], \f[I]FI_SEND\f[R],
+\f[I]FI_RECV\f[R], \f[I]FI_REMOTE_READ\f[R], \f[I]FI_REMOTE_WRITE\f[R],
+\f[I]FI_MULTI_RECV\f[R], \f[I]FI_DIRECTED_RECV\f[R], \f[I]FI_SOURCE\f[R]
+and \f[I]FI_FENCE\f[R].
 .PP
 Notes on FI_DIRECTED_RECV capability: The immediate data which is sent
-within the \f[I]senddata\f[] call to support FI_DIRECTED_RECV for BGQ
+within the \f[I]senddata\f[R] call to support FI_DIRECTED_RECV for BGQ
 must be exactly 4 bytes, which BGQ uses to completely identify the
 source address to an exascale\-level number of ranks for tag matching on
 the recv and can be managed within the MU packet.
 Therefore the domain attribute cq_data_size is set to 4 which is the OFI
 standard minimum.
 .TP
-.B \f[I]Modes\f[]
-The bgq provider requires \f[I]FI_CONTEXT\f[] and \f[I]FI_ASYNC_IOV\f[]
-.RS
-.RE
+.B \f[I]Modes\f[R]
+The bgq provider requires \f[I]FI_CONTEXT\f[R] and
+\f[I]FI_ASYNC_IOV\f[R]
 .TP
-.B \f[I]Memory registration modes\f[]
+.B \f[I]Memory registration modes\f[R]
 Both FI_MR_SCALABLE and FI_MR_BASIC are supported, specified at
-configuration time with the "\-\-with\-bgq\-mr" configure option.
+configuration time with the \[lq]\[en]with\-bgq\-mr\[rq] configure
+option.
 The base address table utilized by FI_MR_SCALABLE for rdma transfers is
 completely software emulated, supporting FI_ATOMIC, FI_READ, FI_WRITE,
 FI_REMOTE_READ, and FI_REMOTE_WRITE capabilities.
@@ -62,57 +58,43 @@ other rdma transfers are still software emulated but the use of a base
 address table is no longer required as the offset is now the virtual
 address of the memory from the application and the key is the delta from
 which the physical address can be computed if necessary.
-.RS
-.RE
 .TP
-.B \f[I]Additional features\f[]
-Supported additional features include \f[I]FABRIC_DIRECT\f[],
-\f[I]scalable endpoints\f[], and \f[I]counters\f[].
-.RS
-.RE
+.B \f[I]Additional features\f[R]
+Supported additional features include \f[I]FABRIC_DIRECT\f[R],
+\f[I]scalable endpoints\f[R], and \f[I]counters\f[R].
 .TP
-.B \f[I]Progress\f[]
-Both progress modes, \f[I]FI_PROGRESS_AUTO\f[] and
-\f[I]FI_PROGRESS_MANUAL\f[], are supported.
-The progress mode may be specified via the "\-\-with\-bgq\-progress"
-configure option.
-.RS
-.RE
+.B \f[I]Progress\f[R]
+Both progress modes, \f[I]FI_PROGRESS_AUTO\f[R] and
+\f[I]FI_PROGRESS_MANUAL\f[R], are supported.
+The progress mode may be specified via the
+\[lq]\[en]with\-bgq\-progress\[rq] configure option.
 .TP
-.B \f[I]Address vector\f[]
-Only the \f[I]FI_AV_MAP\f[] address vector format is supported.
-.RS
-.RE
+.B \f[I]Address vector\f[R]
+Only the \f[I]FI_AV_MAP\f[R] address vector format is supported.
 .SH UNSUPPORTED FEATURES
 .TP
-.B \f[I]Endpoint types\f[]
-Unsupported endpoint types include \f[I]FI_EP_DGRAM\f[] and
-\f[I]FI_EP_MSG\f[]
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+Unsupported endpoint types include \f[I]FI_EP_DGRAM\f[R] and
+\f[I]FI_EP_MSG\f[R]
 .TP
-.B \f[I]Capabilities\f[]
-The bgq provider does not support the \f[I]FI_RMA_EVENT\f[], and
-\f[I]FI_TRIGGER\f[] capabilities.
-.RS
-.RE
+.B \f[I]Capabilities\f[R]
+The bgq provider does not support the \f[I]FI_RMA_EVENT\f[R], and
+\f[I]FI_TRIGGER\f[R] capabilities.
 .TP
-.B \f[I]Address vector\f[]
-The bgq provider does not support the \f[I]FI_AV_TABLE\f[] address
+.B \f[I]Address vector\f[R]
+The bgq provider does not support the \f[I]FI_AV_TABLE\f[R] address
 vector format.
-Support for \f[I]FI_AV_TABLE\f[] may be added in the future.
-.RS
-.RE
+Support for \f[I]FI_AV_TABLE\f[R] may be added in the future.
 .SH LIMITATIONS
 .PP
-The bgq provider only supports \f[I]FABRIC_DIRECT\f[].
-The size of the fi_context structure for \f[I]FI_CONTEXT\f[] is too
+The bgq provider only supports \f[I]FABRIC_DIRECT\f[R].
+The size of the fi_context structure for \f[I]FI_CONTEXT\f[R] is too
 small to be useful.
-In the \[aq]direct\[aq] mode the bgq provider can re\-define the struct
+In the `direct' mode the bgq provider can re\-define the struct
 fi_context to a larger size \- currently 64 bytes which is the L1 cache
 size.
 .PP
-The fi_context structure for \f[I]FI_CONTEXT\f[] must be aligned to 8
+The fi_context structure for \f[I]FI_CONTEXT\f[R] must be aligned to 8
 bytes.
 This requirement is because the bgq provider will use MU network atomics
 to track completions and the memory used with MU atomic operations must
@@ -120,18 +102,18 @@ be aligned to 8 bytes.
 Unfortunately, the libfabric API has no mechanism for applications to
 programmatically determine these alignment requirements.
 Because unaligned MU atomics operations are a fatal error, the bgq
-provider will assert on the alignment for "debug" builds (i.e., the
-\[aq]\-DNDEBUG\[aq] pre\-processor flag is not specified).
+provider will assert on the alignment for \[lq]debug\[rq] builds (i.e.,
+the `\-DNDEBUG' pre\-processor flag is not specified).
 .PP
-The progress thread used for \f[I]FI_PROGRESS_AUTO\f[] effectively
+The progress thread used for \f[I]FI_PROGRESS_AUTO\f[R] effectively
 limits the maximum number of ranks\-per\-node to 32.
 However for FI_PROGRESS_MANUAL the maximum is 64.
 .PP
 For FI_MR_SCALABLE mr mode the memory region key size (mr_key_size) is 2
-\f[I]bytes\f[]; Valid key values are 0..2^16\-1.
+\f[I]bytes\f[R]; Valid key values are 0..2\[ha]16\-1.
 .PP
-It is invalid to register memory at the base virtual address "0" with a
-length of "UINTPTR_MAX" (or equivalent).
+It is invalid to register memory at the base virtual address \[lq]0\[rq]
+with a length of \[lq]UINTPTR_MAX\[rq] (or equivalent).
 The Blue Gene/Q hardware operates on 37\-bit physical addresses and all
 virtual addresses specified in the libfabric API, such as the location
 of source/destination data and remote memory locations, must be
@@ -147,6 +129,6 @@ The fi_trecvv() and fi_recvv() functions are currently not supported.
 No runtime parameters are currently defined.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_direct.7 b/man/man7/fi_direct.7
index d73cba4..90637ae 100644
--- a/man/man7/fi_direct.7
+++ b/man/man7/fi_direct.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_direct" "7" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_direct" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -11,8 +11,8 @@ fi_direct \- Direct fabric provider access
 \f[C]
 \-DFABRIC_DIRECT
 
-#define\ FABRIC_DIRECT
-\f[]
+#define FABRIC_DIRECT
+\f[R]
 .fi
 .PP
 Fabric direct provides a mechanism for applications to compile against a
@@ -37,12 +37,12 @@ part of their build.
 In general, the use of fabric direct does not require application source
 code changes, and, instead, is limited to the build process.
 .PP
-Providers supporting fabric direct must install \[aq]direct\[aq]
-versions of all libfabric header files.
+Providers supporting fabric direct must install `direct' versions of all
+libfabric header files.
 For convenience, the libfabric sources contain sample header files that
 may be modified by a provider.
-The \[aq]direct\[aq] header file names have \[aq]fi_direct\[aq] as their
-prefix: fi_direct.h, fi_direct_endpoint.h, etc.
+The `direct' header file names have `fi_direct' as their prefix:
+fi_direct.h, fi_direct_endpoint.h, etc.
 .PP
 Direct providers are prohibited from overriding or modifying existing
 data structures.
@@ -60,21 +60,18 @@ modes, if those capabilities are supported.
 The following #define values may be used by an application to test for
 provider support of supported features.
 .TP
-.B \f[I]FI_DIRECT_CONTEXT\f[]
+.B \f[I]FI_DIRECT_CONTEXT\f[R]
 The provider sets FI_CONTEXT or FI_CONTEXT2 for fi_info:mode.
 See fi_getinfo for additional details.
 When FI_DIRECT_CONTEXT is defined, applications should use struct
 fi_context in their definitions, even if FI_CONTEXT2 is set.
-.RS
-.RE
 .TP
-.B \f[I]FI_DIRECT_LOCAL_MR\f[]
+.B \f[I]FI_DIRECT_LOCAL_MR\f[R]
 The provider sets FI_LOCAL_MR for fi_info:mode.
 See fi_getinfo for additional details.
-.RS
-.RE
 .SH SEE ALSO
 .PP
-\f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3)
+\f[C]fi_getinfo\f[R](3), \f[C]fi_endpoint\f[R](3),
+\f[C]fi_domain\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_efa.7 b/man/man7/fi_efa.7
index d5f3499..7b773b5 100644
--- a/man/man7/fi_efa.7
+++ b/man/man7/fi_efa.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_efa" "7" "2020\-09\-01" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_efa" "7" "2021\-09\-17" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -15,240 +15,196 @@ hardware access from userspace (OS bypass).
 .PP
 The following features are supported:
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports endpoint type \f[I]FI_EP_DGRAM\f[], and
-\f[I]FI_EP_RDM\f[] on a new Scalable (unordered) Reliable Datagram
+.B \f[I]Endpoint types\f[R]
+The provider supports endpoint type \f[I]FI_EP_DGRAM\f[R], and
+\f[I]FI_EP_RDM\f[R] on a new Scalable (unordered) Reliable Datagram
 protocol (SRD).
 SRD provides support for reliable datagrams and more complete error
 handling than typically seen with other Reliable Datagram (RD)
 implementations.
 The EFA provider provides segmentation, reassembly of out\-of\-order
 packets to provide send\-after\-send ordering guarantees to applications
-via its \f[I]FI_EP_RDM\f[] endpoint.
-.RS
-.RE
+via its \f[I]FI_EP_RDM\f[R] endpoint.
 .TP
-.B \f[I]RDM Endpoint capabilities\f[]
+.B \f[I]RDM Endpoint capabilities\f[R]
 The following data transfer interfaces are supported via the
-\f[I]FI_EP_RDM\f[] endpoint: \f[I]FI_MSG\f[], \f[I]FI_TAGGED\f[], and
-\f[I]FI_RMA\f[].
-\f[I]FI_SEND\f[], \f[I]FI_RECV\f[], \f[I]FI_DIRECTED_RECV\f[],
-\f[I]FI_MULTI_RECV\f[], and \f[I]FI_SOURCE\f[] capabilities are
+\f[I]FI_EP_RDM\f[R] endpoint: \f[I]FI_MSG\f[R], \f[I]FI_TAGGED\f[R], and
+\f[I]FI_RMA\f[R].
+\f[I]FI_SEND\f[R], \f[I]FI_RECV\f[R], \f[I]FI_DIRECTED_RECV\f[R],
+\f[I]FI_MULTI_RECV\f[R], and \f[I]FI_SOURCE\f[R] capabilities are
 supported.
 The endpoint provides send\-after\-send guarantees for data operations.
-The \f[I]FI_EP_RDM\f[] endpoint does not have a maximum message size.
-.RS
-.RE
+The \f[I]FI_EP_RDM\f[R] endpoint does not have a maximum message size.
 .TP
-.B \f[I]DGRAM Endpoint capabilities\f[]
-The DGRAM endpoint only supports \f[I]FI_MSG\f[] capability with a
+.B \f[I]DGRAM Endpoint capabilities\f[R]
+The DGRAM endpoint only supports \f[I]FI_MSG\f[R] capability with a
 maximum message size of the MTU of the underlying hardware
 (approximately 8 KiB).
-.RS
-.RE
 .TP
-.B \f[I]Address vectors\f[]
-The provider supports \f[I]FI_AV_TABLE\f[] and \f[I]FI_AV_MAP\f[]
+.B \f[I]Address vectors\f[R]
+The provider supports \f[I]FI_AV_TABLE\f[R] and \f[I]FI_AV_MAP\f[R]
 address vector types.
-\f[I]FI_EVENT\f[] is unsupported.
-.RS
-.RE
-.TP
-.B \f[I]Completion events\f[]
-The provider supports \f[I]FI_CQ_FORMAT_CONTEXT\f[],
-\f[I]FI_CQ_FORMAT_MSG\f[], and \f[I]FI_CQ_FORMAT_DATA\f[].
-\f[I]FI_CQ_FORMAT_TAGGED\f[] is supported on the RDM endpoint.
+\f[I]FI_EVENT\f[R] is unsupported.
+.TP
+.B \f[I]Completion events\f[R]
+The provider supports \f[I]FI_CQ_FORMAT_CONTEXT\f[R],
+\f[I]FI_CQ_FORMAT_MSG\f[R], and \f[I]FI_CQ_FORMAT_DATA\f[R].
+\f[I]FI_CQ_FORMAT_TAGGED\f[R] is supported on the RDM endpoint.
 Wait objects are not currently supported.
-.RS
-.RE
 .TP
-.B \f[I]Modes\f[]
-The provider requires the use of \f[I]FI_MSG_PREFIX\f[] when running
-over the DGRAM endpoint, and requires \f[I]FI_MR_LOCAL\f[] for all
+.B \f[I]Modes\f[R]
+The provider requires the use of \f[I]FI_MSG_PREFIX\f[R] when running
+over the DGRAM endpoint, and requires \f[I]FI_MR_LOCAL\f[R] for all
 memory registrations on the DGRAM endpoint.
-.RS
-.RE
 .TP
-.B \f[I]Memory registration modes\f[]
+.B \f[I]Memory registration modes\f[R]
 The RDM endpoint does not require memory registration for send and
-receive operations, i.e.
-it does not require \f[I]FI_MR_LOCAL\f[].
-Applications may specify \f[I]FI_MR_LOCAL\f[] in the MR mode flags in
+receive operations, i.e.\ it does not require \f[I]FI_MR_LOCAL\f[R].
+Applications may specify \f[I]FI_MR_LOCAL\f[R] in the MR mode flags in
 order to use descriptors provided by the application.
-The \f[I]FI_EP_DGRAM\f[] endpoint only supports \f[I]FI_MR_LOCAL\f[].
-.RS
-.RE
+The \f[I]FI_EP_DGRAM\f[R] endpoint only supports \f[I]FI_MR_LOCAL\f[R].
 .TP
-.B \f[I]Progress\f[]
-The RDM endpoint supports both \f[I]FI_PROGRESS_AUTO\f[] and
-\f[I]FI_PROGRESS_MANUAL\f[], with the default set to auto.
+.B \f[I]Progress\f[R]
+The RDM endpoint supports both \f[I]FI_PROGRESS_AUTO\f[R] and
+\f[I]FI_PROGRESS_MANUAL\f[R], with the default set to auto.
 However, receive side data buffers are not modified outside of
 completion processing routines.
-The DGRAM endpoint only supports \f[I]FI_PROGRESS_MANUAL\f[].
-.RS
-.RE
-.TP
-.B \f[I]Threading\f[]
-The RDM endpoint supports \f[I]FI_THREAD_SAFE\f[], the DGRAM endpoint
-supports \f[I]FI_THREAD_DOMAIN\f[], i.e.
-the provider is not thread safe when using the DGRAM endpoint.
-.RS
-.RE
+The DGRAM endpoint only supports \f[I]FI_PROGRESS_MANUAL\f[R].
+.TP
+.B \f[I]Threading\f[R]
+The RDM endpoint supports \f[I]FI_THREAD_SAFE\f[R], the DGRAM endpoint
+supports \f[I]FI_THREAD_DOMAIN\f[R], i.e.\ the provider is not thread
+safe when using the DGRAM endpoint.
 .SH LIMITATIONS
 .PP
-The DGRAM endpoint does not support \f[I]FI_ATOMIC\f[] interfaces.
+The DGRAM endpoint does not support \f[I]FI_ATOMIC\f[R] interfaces.
 For RMA operations, completion events for RMA targets
-(\f[I]FI_RMA_EVENT\f[]) is not supported.
+(\f[I]FI_RMA_EVENT\f[R]) is not supported.
 The DGRAM endpoint does not fully protect against resource overruns, so
 resource management is disabled for this endpoint
-(\f[I]FI_RM_DISABLED\f[]).
+(\f[I]FI_RM_DISABLED\f[R]).
 .PP
 No support for selective completions.
 .PP
 No support for counters for the DGRAM endpoint.
 .PP
 No support for inject.
+.SH PROVIDER SPECIFIC ENDPOINT LEVEL OPTION
+.TP
+.B \f[I]FI_OPT_EFA_RNR_RETRY\f[R]
+Defines the number of RNR retry.
+The application can use it to reset RNR retry counter via the call to
+fi_setopt.
+Note that this option must be set before the endpoint is enabled.
+Otherwise, the call will fail.
+Also note that this option only applies to RDM endpoint.
 .SH RUNTIME PARAMETERS
 .TP
-.B \f[I]FI_EFA_TX_SIZE\f[]
+.B \f[I]FI_EFA_TX_SIZE\f[R]
 Maximum number of transmit operations before the provider returns
 \-FI_EAGAIN.
 For only the RDM endpoint, this parameter will cause transmit operations
 to be queued when this value is set higher than the default and the
 transmit queue is full.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_RX_SIZE\f[]
+.B \f[I]FI_EFA_RX_SIZE\f[R]
 Maximum number of receive operations before the provider returns
 \-FI_EAGAIN.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_TX_IOV_LIMIT\f[]
+.B \f[I]FI_EFA_TX_IOV_LIMIT\f[R]
 Maximum number of IOVs for a transmit operation.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_RX_IOV_LIMIT\f[]
+.B \f[I]FI_EFA_RX_IOV_LIMIT\f[R]
 Maximum number of IOVs for a receive operation.
-.RS
-.RE
 .SH RUNTIME PARAMETERS SPECIFIC TO RDM ENDPOINT
 .PP
 These OFI runtime parameters apply only to the RDM endpoint.
 .TP
-.B \f[I]FI_EFA_RX_WINDOW_SIZE\f[]
+.B \f[I]FI_EFA_RX_WINDOW_SIZE\f[R]
 Maximum number of MTU\-sized messages that can be in flight from any
 single endpoint as part of long message data transfer.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_TX_QUEUE_SIZE\f[]
+.B \f[I]FI_EFA_TX_QUEUE_SIZE\f[R]
 Depth of transmit queue opened with the NIC.
 This may not be set to a value greater than what the NIC supports.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_RECVWIN_SIZE\f[]
+.B \f[I]FI_EFA_RECVWIN_SIZE\f[R]
 Size of out of order reorder buffer (in messages).
 Messages received out of this window will result in an error.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_CQ_SIZE\f[]
+.B \f[I]FI_EFA_CQ_SIZE\f[R]
 Size of any cq created, in number of entries.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_MR_CACHE_ENABLE\f[]
+.B \f[I]FI_EFA_MR_CACHE_ENABLE\f[R]
 Enables using the mr cache and in\-line registration instead of a bounce
-buffer for iov\[aq]s larger than max_memcpy_size.
+buffer for iov\[cq]s larger than max_memcpy_size.
 Defaults to true.
 When disabled, only uses a bounce buffer
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_MR_MAX_CACHED_COUNT\f[]
+.B \f[I]FI_EFA_MR_MAX_CACHED_COUNT\f[R]
 Sets the maximum number of memory registrations that can be cached at
 any time.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_MR_MAX_CACHED_SIZE\f[]
+.B \f[I]FI_EFA_MR_MAX_CACHED_SIZE\f[R]
 Sets the maximum amount of memory that cached memory registrations can
 hold onto at any time.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_MAX_MEMCPY_SIZE\f[]
+.B \f[I]FI_EFA_MAX_MEMCPY_SIZE\f[R]
 Threshold size switch between using memory copy into a pre\-registered
 bounce buffer and memory registration on the user buffer.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_MTU_SIZE\f[]
+.B \f[I]FI_EFA_MTU_SIZE\f[R]
 Overrides the default MTU size of the device.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_RX_COPY_UNEXP\f[]
+.B \f[I]FI_EFA_RX_COPY_UNEXP\f[R]
 Enables the use of a separate pool of bounce\-buffers to copy unexpected
 messages out of the pre\-posted receive buffers.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_RX_COPY_OOO\f[]
+.B \f[I]FI_EFA_RX_COPY_OOO\f[R]
 Enables the use of a separate pool of bounce\-buffers to copy
 out\-of\-order RTS packets out of the pre\-posted receive buffers.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_MAX_TIMEOUT\f[]
+.B \f[I]FI_EFA_MAX_TIMEOUT\f[R]
 Maximum timeout (us) for backoff to a peer after a receiver not ready
 error.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_TIMEOUT_INTERVAL\f[]
+.B \f[I]FI_EFA_TIMEOUT_INTERVAL\f[R]
 Time interval (us) for the base timeout to use for exponential backoff
 to a peer after a receiver not ready error.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_ENABLE_SHM_TRANSFER\f[]
+.B \f[I]FI_EFA_ENABLE_SHM_TRANSFER\f[R]
 Enable SHM provider to provide the communication across all intra\-node
 processes.
 SHM transfer will be disabled in the case where
-\f[C]ptrace\ protection\f[] is turned on.
+\f[C]ptrace protection\f[R] is turned on.
 You can turn it off to enable shm transfer.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_SHM_AV_SIZE\f[]
-Defines the maximum number of entries in SHM provider\[aq]s address
+.B \f[I]FI_EFA_SHM_AV_SIZE\f[R]
+Defines the maximum number of entries in SHM provider\[cq]s address
 vector.
-.RS
-.RE
 .TP
-.B \f[I]FI_EFA_SHM_MAX_MEDIUM_SIZE\f[]
+.B \f[I]FI_EFA_SHM_MAX_MEDIUM_SIZE\f[R]
 Defines the switch point between small/medium message and large message.
 The message larger than this switch point will be transferred with large
 message protocol.
-.RS
-.RE
+NOTE: This parameter is now deprecated.
 .TP
-.B \f[I]FI_EFA_INTER_MAX_MEDIUM_MESSAGE_SIZE\f[]
+.B \f[I]FI_EFA_INTER_MAX_MEDIUM_MESSAGE_SIZE\f[R]
 The maximum size for inter EFA messages to be sent by using medium
 message protocol.
 Messages which can fit in one packet will be sent as eager message.
 Messages whose sizes are smaller than this value will be sent using
 medium message protocol.
 Other messages will be sent using CTS based long message protocol.
-.RS
-.RE
+.TP
+.B \f[I]FI_EFA_FORK_SAFE\f[R]
+Enable fork() support.
+This may have a small performance impact and should only be set when
+required.
+Applications that require to register regions backed by huge pages and
+also require fork support are not supported.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_gni.7 b/man/man7/fi_gni.7
index d25f7d8..06573a5 100644
--- a/man/man7/fi_gni.7
+++ b/man/man7/fi_gni.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_gni" "7" "2019\-04\-29" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_gni" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -32,72 +32,54 @@ Any other value will result in a return value of \-FI_EINVAL.
 The GNI provider supports the following features defined for the
 libfabric API:
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports the \f[I]FI_EP_RDM\f[], \f[I]FI_EP_DGRAM\f[],
-\f[I]FI_EP_MSG\f[] endpoint types, including scalable endpoints.
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports the \f[I]FI_EP_RDM\f[R], \f[I]FI_EP_DGRAM\f[R],
+\f[I]FI_EP_MSG\f[R] endpoint types, including scalable endpoints.
 .TP
-.B \f[I]Address vectors\f[]
-The provider implements both the \f[I]FI_AV_MAP\f[] and
-\f[I]FI_AV_TABLE\f[] address vector types.
+.B \f[I]Address vectors\f[R]
+The provider implements both the \f[I]FI_AV_MAP\f[R] and
+\f[I]FI_AV_TABLE\f[R] address vector types.
 FI_EVENT is unsupported.
-.RS
-.RE
 .TP
-.B \f[I]Memory registration modes\f[]
+.B \f[I]Memory registration modes\f[R]
 The provider implements basic and scalable memory registration modes.
-.RS
-.RE
 .TP
-.B \f[I]Data transfer operations\f[]
+.B \f[I]Data transfer operations\f[R]
 The following data transfer interfaces are supported for all endpoint
-types: \f[I]FI_ATOMIC\f[], \f[I]FI_MSG\f[], \f[I]FI_RMA\f[],
-\f[I]FI_TAGGED\f[].
+types: \f[I]FI_ATOMIC\f[R], \f[I]FI_MSG\f[R], \f[I]FI_RMA\f[R],
+\f[I]FI_TAGGED\f[R].
 See DATA TRANSFER OPERATIONS below for more details.
-.RS
-.RE
-.TP
-.B \f[I]Completion events\f[]
-The GNI provider supports \f[I]FI_CQ_FORMAT_CONTEXT\f[],
-\f[I]FI_CQ_FORMAT_MSG\f[], \f[I]FI_CQ_FORMAT_DATA\f[] and
-\f[I]FI_CQ_FORMAT_TAGGED\f[] with wait objects of type
-\f[I]FI_WAIT_NONE\f[], \f[I]FI_WAIT_UNSPEC\f[], \f[I]FI_WAIT_SET\f[].
-.RS
-.RE
-.TP
-.B \f[I]Modes\f[]
+.TP
+.B \f[I]Completion events\f[R]
+The GNI provider supports \f[I]FI_CQ_FORMAT_CONTEXT\f[R],
+\f[I]FI_CQ_FORMAT_MSG\f[R], \f[I]FI_CQ_FORMAT_DATA\f[R] and
+\f[I]FI_CQ_FORMAT_TAGGED\f[R] with wait objects of type
+\f[I]FI_WAIT_NONE\f[R], \f[I]FI_WAIT_UNSPEC\f[R], \f[I]FI_WAIT_SET\f[R].
+.TP
+.B \f[I]Modes\f[R]
 The GNI provider does not require any operation modes.
-.RS
-.RE
 .TP
-.B \f[I]Progress\f[]
+.B \f[I]Progress\f[R]
 For both control and data progress, the GNI provider supports both
-\f[I]FI_PROGRESS_AUTO\f[] and \f[I]FI_PROGRESS_MANUAL\f[], with a
-default set to \f[I]FI_PROGRESS_AUTO\f[].
+\f[I]FI_PROGRESS_AUTO\f[R] and \f[I]FI_PROGRESS_MANUAL\f[R], with a
+default set to \f[I]FI_PROGRESS_AUTO\f[R].
 Note that for data progress, progression is only performed when data
 transfers use the rendezvous protocol.
-.RS
-.RE
 .TP
-.B \f[I]Wait Objects\f[]
+.B \f[I]Wait Objects\f[R]
 The GNI provider specifically supports wait object types
-\f[I]FI_WAIT_UNSPEC\f[], and \f[I]FI_WAIT_SET\f[].
+\f[I]FI_WAIT_UNSPEC\f[R], and \f[I]FI_WAIT_SET\f[R].
 A wait object must be used when calling fi_cntr_wait, fi_cq_sread/from,
 fi_eq_sread/from, fi_wait.
 The GNI provider spawns an internal wait progress thread that is woken
 up when clients utilize the wait system (e.g., calling fi_wait).
-.RS
-.RE
 .TP
-.B \f[I]Additional Features\f[]
+.B \f[I]Additional Features\f[R]
 The GNI provider also supports the following capabilities and features:
-\- \f[I]FI_MULTI_RECV\f[] \- \f[I]FI_SOURCE\f[] \- \f[I]FI_FENCE\f[] \-
-\f[I]FI_RM_ENABLED\f[] \- \f[I]FI_RMA_EVENT\f[] \-
-\f[I]FI_REMOTE_CQ_DATA\f[] \- \f[I]FABRIC_DIRECT\f[] compilation mode \-
-\f[I]FI_MORE\f[] (For FI_RMA)
-.RS
-.RE
+\- \f[I]FI_MULTI_RECV\f[R] \- \f[I]FI_SOURCE\f[R] \- \f[I]FI_FENCE\f[R]
+\- \f[I]FI_RM_ENABLED\f[R] \- \f[I]FI_RMA_EVENT\f[R] \-
+\f[I]FI_REMOTE_CQ_DATA\f[R] \- \f[I]FABRIC_DIRECT\f[R] compilation mode
+\- \f[I]FI_MORE\f[R] (For FI_RMA)
 .SH DATA TRANSFER OPERATIONS
 .SS FI_ATOMIC
 .PP
@@ -108,13 +90,14 @@ integer and floating point values.
 Specifically,
 .SS Basic (fi_atomic, etc.)
 .IP \[bu] 2
-\f[I]FI_MIN\f[], \f[I]FI_MAX\f[] (no unsigned)
+\f[I]FI_MIN\f[R], \f[I]FI_MAX\f[R] (no unsigned)
 .IP \[bu] 2
-\f[I]FI_SUM\f[] (no 64\-bit floating point)
+\f[I]FI_SUM\f[R] (no 64\-bit floating point)
 .IP \[bu] 2
-\f[I]FI_BOR\f[], \f[I]FI_BAND\f[], \f[I]FI_BXOR\f[] (no floating point)
+\f[I]FI_BOR\f[R], \f[I]FI_BAND\f[R], \f[I]FI_BXOR\f[R] (no floating
+point)
 .IP \[bu] 2
-\f[I]FI_ATOMIC_WRITE\f[]
+\f[I]FI_ATOMIC_WRITE\f[R]
 .SS Fetching (fi_fetch_atomic, etc.)
 .IP \[bu] 2
 All of the basic operations as above
@@ -127,255 +110,199 @@ FI_CSWAP
 FI_MSWAP
 .SS FI_MSG
 .PP
-All \f[I]FI_MSG\f[] operations are supported.
+All \f[I]FI_MSG\f[R] operations are supported.
 .SS FI_RMA
 .PP
-All \f[I]FI_RMA\f[] operations are supported.
+All \f[I]FI_RMA\f[R] operations are supported.
 .SS FI_TAGGED
 .PP
-All \f[I]FI_TAGGED\f[] operations are supported except
-\f[C]fi_tinjectdata\f[].
+All \f[I]FI_TAGGED\f[R] operations are supported except
+\f[C]fi_tinjectdata\f[R].
 .SH GNI EXTENSIONS
 .PP
 The GNI provider exposes low\-level tuning parameters via domain,
-endpoint and fabric level \f[C]fi_open_ops\f[] interfaces.
-The domain extensions have been named \f[I]FI_GNI_DOMAIN_OPS_1\f[].
-The endpoint extensions have been named \f[I]FI_GNI_EP_OPS_1\f[].
-The fabric extensions have been named \f[I]FI_GNI_FABRIC_OPS_1\f[] and
-\f[I]FI_GNI_FABRIC_OPS_2\f[].
+endpoint and fabric level \f[C]fi_open_ops\f[R] interfaces.
+The domain extensions have been named \f[I]FI_GNI_DOMAIN_OPS_1\f[R].
+The endpoint extensions have been named \f[I]FI_GNI_EP_OPS_1\f[R].
+The fabric extensions have been named \f[I]FI_GNI_FABRIC_OPS_1\f[R] and
+\f[I]FI_GNI_FABRIC_OPS_2\f[R].
 The flags parameter is currently ignored.
-The fi_open_ops function takes a \f[C]struct\ fi_gni_ops_domain\f[] or a
-\f[C]struct\ fi_gni_ops_ep\f[] parameter respectively and populates it
+The fi_open_ops function takes a \f[C]struct fi_gni_ops_domain\f[R] or a
+\f[C]struct fi_gni_ops_ep\f[R] parameter respectively and populates it
 with the following:
 .IP
 .nf
 \f[C]
-struct\ fi_gni_ops_fab\ {
-\ \ \ \ int\ (*set_val)(struct\ fid\ *fid,\ fab_ops_val_t\ t,\ void\ *val);
-\ \ \ \ int\ (*get_val)(struct\ fid\ *fid,\ fab_ops_val_t\ t,\ void\ *val);
+struct fi_gni_ops_fab {
+    int (*set_val)(struct fid *fid, fab_ops_val_t t, void *val);
+    int (*get_val)(struct fid *fid, fab_ops_val_t t, void *val);
 };
 
-struct\ fi_gni_auth_key_ops_fab\ {
-\ \ \ \ int\ (*set_val)(uint8_t\ *auth_key,\ size_t\ auth_keylen,\ gnix_auth_key_opt_t\ opt,\ void\ *val);
-\ \ \ \ int\ (*get_val)(uint8_t\ *auth_key,\ size_t\ auth_keylen,\ gnix_auth_key_opt_t\ opt,\ void\ *val);
+struct fi_gni_auth_key_ops_fab {
+    int (*set_val)(uint8_t *auth_key, size_t auth_keylen, gnix_auth_key_opt_t opt, void *val);
+    int (*get_val)(uint8_t *auth_key, size_t auth_keylen, gnix_auth_key_opt_t opt, void *val);
 };
 
-struct\ fi_gni_ops_domain\ {
-\ \ \ \ int\ (*set_val)(struct\ fid\ *fid,\ dom_ops_val_t\ t,\ void\ *val);
-\ \ \ \ int\ (*get_val)(struct\ fid\ *fid,\ dom_ops_val_t\ t,\ void\ *val);
-\ \ \ \ int\ (*flush_cache)(struct\ fid\ *fid);
+struct fi_gni_ops_domain {
+    int (*set_val)(struct fid *fid, dom_ops_val_t t, void *val);
+    int (*get_val)(struct fid *fid, dom_ops_val_t t, void *val);
+    int (*flush_cache)(struct fid *fid);
 };
 
-struct\ fi_gni_ops_ep\ {
-\ \ \ \ int\ (*set_val)(struct\ fid\ *fid,\ dom_ops_val_t\ t,\ void\ *val);
-\ \ \ \ int\ (*get_val)(struct\ fid\ *fid,\ dom_ops_val_t\ t,\ void\ *val);
-\ \ \ \ \ \ \ \ size_t\ (*native_amo)(struct\ fid_ep\ *ep,\ const\ void\ *buf,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ size_t\ count,void\ *desc,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ fi_addr_t\ dest_addr,\ uint64_t\ addr,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ uint64_t\ key,\ enum\ fi_datatype\ datatype,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ enum\ gnix_fab_req_type\ req_type,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ void\ *context);
+struct fi_gni_ops_ep {
+    int (*set_val)(struct fid *fid, dom_ops_val_t t, void *val);
+    int (*get_val)(struct fid *fid, dom_ops_val_t t, void *val);
+        size_t (*native_amo)(struct fid_ep *ep, const void *buf,
+                             size_t count,void *desc,
+                             fi_addr_t dest_addr, uint64_t addr,
+                             uint64_t key, enum fi_datatype datatype,
+                             enum gnix_fab_req_type req_type,
+                             void *context);
 };
-\f[]
+\f[R]
 .fi
 .PP
-The \f[C]set_val\f[] function sets the value of a given parameter; the
-\f[C]get_val\f[] function returns the current value.
+The \f[C]set_val\f[R] function sets the value of a given parameter; the
+\f[C]get_val\f[R] function returns the current value.
 .PP
-For \f[I]FI_GNI_FABRIC_OPS_1\f[], the currently supported values are:
+For \f[I]FI_GNI_FABRIC_OPS_1\f[R], the currently supported values are:
 .TP
-.B \f[I]GNI_WAIT_THREAD_SLEEP\f[]
+.B \f[I]GNI_WAIT_THREAD_SLEEP\f[R]
 Time in seconds for which the progress thread will sleep between periods
 of inactivity.
-.RS
-.RE
 .TP
-.B \f[I]GNI_DEFAULT_USER_REGISTRATION_LIMIT\f[]
+.B \f[I]GNI_DEFAULT_USER_REGISTRATION_LIMIT\f[R]
 The number of user registrations that an authorization key is limited to
 when using the scalable memory mode, if not specified by the user during
 init.
-.RS
-.RE
 .TP
-.B \f[I]GNI_DEFAULT_PROV_REGISTRATION_LIMIT\f[]
+.B \f[I]GNI_DEFAULT_PROV_REGISTRATION_LIMIT\f[R]
 The number of provider registration that an authorization key is limited
 to when using the scalable memory mode, if not specified by the user
 during init.
-.RS
-.RE
 .TP
-.B \f[I]GNI_WAIT_SHARED_MEMORY_TIMEOUT\f[]
+.B \f[I]GNI_WAIT_SHARED_MEMORY_TIMEOUT\f[R]
 The number of seconds that the provider should wait when attempting to
-open mmap\[aq]d shared memory files for internal mappings.
-.RS
-.RE
+open mmap\[cq]d shared memory files for internal mappings.
 .PP
-For \f[I]FI_GNI_FABRIC_OPS_2\f[], the currently supported values are:
+For \f[I]FI_GNI_FABRIC_OPS_2\f[R], the currently supported values are:
 .TP
-.B \f[I]GNIX_USER_KEY_LIMIT\f[]
+.B \f[I]GNIX_USER_KEY_LIMIT\f[R]
 The number of user registrations that an authorization key is limited to
 when using the scalable memory mode.
 This may only be set prior to the first use of an authorization key in
 the initialization of a domain, endpoint, or memory registration.
-.RS
-.RE
 .TP
-.B \f[I]GNIX_PROV_KEY_LIMIT\f[]
+.B \f[I]GNIX_PROV_KEY_LIMIT\f[R]
 The number of provider registrations that an authorization key is
 limited to when using the scalable memory mode.
 This may only be set prior to the first use of an authorization key in
 the initialization of a domain, endpoint, or memory registration.
-.RS
-.RE
 .PP
-For \f[I]FI_GNI_DOMAIN_OPS_1\f[], the currently supported values are:
+For \f[I]FI_GNI_DOMAIN_OPS_1\f[R], the currently supported values are:
 .TP
-.B \f[I]GNI_MSG_RENDEZVOUS_THRESHOLD\f[]
+.B \f[I]GNI_MSG_RENDEZVOUS_THRESHOLD\f[R]
 Threshold message size at which a rendezvous protocol is used for
-\f[I]FI_MSG\f[] data transfers.
+\f[I]FI_MSG\f[R] data transfers.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_RMA_RDMA_THRESHOLD\f[]
-Threshold message size at which RDMA is used for \f[I]FI_RMA\f[] data
+.B \f[I]GNI_RMA_RDMA_THRESHOLD\f[R]
+Threshold message size at which RDMA is used for \f[I]FI_RMA\f[R] data
 transfers.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_CONN_TABLE_INITIAL_SIZE\f[]
+.B \f[I]GNI_CONN_TABLE_INITIAL_SIZE\f[R]
 Initial size of the internal table data structure used to manage
 connections.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_CONN_TABLE_MAX_SIZE\f[]
+.B \f[I]GNI_CONN_TABLE_MAX_SIZE\f[R]
 Maximum size of the internal table data structure used to manage
 connections.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_CONN_TABLE_STEP_SIZE\f[]
+.B \f[I]GNI_CONN_TABLE_STEP_SIZE\f[R]
 Step size for increasing the size of the internal table data structure
 used to manage internal GNI connections.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_VC_ID_TABLE_CAPACITY\f[]
+.B \f[I]GNI_VC_ID_TABLE_CAPACITY\f[R]
 Size of the virtual channel (VC) table used for managing remote
 connections.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MBOX_PAGE_SIZE\f[]
+.B \f[I]GNI_MBOX_PAGE_SIZE\f[R]
 Page size for GNI SMSG mailbox allocations.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MBOX_NUM_PER_SLAB\f[]
+.B \f[I]GNI_MBOX_NUM_PER_SLAB\f[R]
 Number of GNI SMSG mailboxes per allocation slab.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MBOX_MAX_CREDIT\f[]
+.B \f[I]GNI_MBOX_MAX_CREDIT\f[R]
 Maximum number of credits per GNI SMSG mailbox.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MBOX_MSG_MAX_SIZE\f[]
+.B \f[I]GNI_MBOX_MSG_MAX_SIZE\f[R]
 Maximum size of GNI SMSG messages.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_RX_CQ_SIZE\f[]
+.B \f[I]GNI_RX_CQ_SIZE\f[R]
 Recommended GNI receive CQ size.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_TX_CQ_SIZE\f[]
+.B \f[I]GNI_TX_CQ_SIZE\f[R]
 Recommended GNI transmit CQ size.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MAX_RETRANSMITS\f[]
+.B \f[I]GNI_MAX_RETRANSMITS\f[R]
 Maximum number of message retransmits before failure.
 The value is of type uint32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MR_CACHE_LAZY_DEREG\f[]
+.B \f[I]GNI_MR_CACHE_LAZY_DEREG\f[R]
 Enable or disable lazy deregistration of memory.
 The value is of type int32_t.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MR_CACHE\f[]
+.B \f[I]GNI_MR_CACHE\f[R]
 Select the type of cache that the domain will use.
-Valid choices are the following: \[aq]internal\[aq], \[aq]udreg\[aq], or
-\[aq]none\[aq].
-\[aq]internal\[aq] refers to the GNI provider internal registration
+Valid choices are the following: `internal', `udreg', or `none'.
+`internal' refers to the GNI provider internal registration cache.
+`udreg' refers to a user level dreg library based cache.
+Lastly, `none' refers to device direct registration without a provider
 cache.
-\[aq]udreg\[aq] refers to a user level dreg library based cache.
-Lastly, \[aq]none\[aq] refers to device direct registration without a
-provider cache.
-.RS
-.RE
 .TP
-.B \f[I]GNI_MR_HARD_REG_LIMIT\f[]
+.B \f[I]GNI_MR_HARD_REG_LIMIT\f[R]
 Maximum number of registrations.
 Applies only to the GNI provider cache.
 The value is of type int32_t (\-1 for no limit).
-.RS
-.RE
 .TP
-.B \f[I]GNI_MR_SOFT_REG_LIMIT\f[]
+.B \f[I]GNI_MR_SOFT_REG_LIMIT\f[R]
 Soft cap on the registration limit.
 Applies only to the GNI provider cache.
 The value is of type int32_t (\-1 for no limit).
-.RS
-.RE
 .TP
-.B \f[I]GNI_MR_HARD_STALE_REG_LIMIT\f[]
+.B \f[I]GNI_MR_HARD_STALE_REG_LIMIT\f[R]
 Maximum number of stale registrations to be held in cache.
 This applies to the GNI provider cache and the udreg cache.
 The value is of type int32_t (\-1 for no limit for the GNI provider
 cache and udreg cache values must be greater than 0).
-.RS
-.RE
 .TP
-.B \f[I]GNI_MR_UDREG_LIMIT\f[]
+.B \f[I]GNI_MR_UDREG_LIMIT\f[R]
 Maximum number of registrations.
 Applies only to the udreg cache.
 The value is of type int32_t.
 The value must be greater than 0.
-.RS
-.RE
 .TP
-.B \f[I]GNI_XPMEM_ENABLE\f[]
+.B \f[I]GNI_XPMEM_ENABLE\f[R]
 Enable or disable use of XPMEM for on node messages using the GNI
 provider internal rendezvous protocol.
 The value is of type bool.
-.RS
-.RE
 .TP
-.B \f[I]GNI_DGRAM_PROGRESS_TIMEOUT\f[]
+.B \f[I]GNI_DGRAM_PROGRESS_TIMEOUT\f[R]
 Controls timeout value in milliseconds for the control progress thread.
 The value is of type uint32_t.
-.RS
-.RE
 .PP
-The \f[C]flush_cache\f[] function allows the user to flush any stale
+The \f[C]flush_cache\f[R] function allows the user to flush any stale
 registration cache entries from the cache.
 This has the effect of removing registrations from the cache that have
 been deregistered with the provider, but still exist in case that they
@@ -385,29 +312,27 @@ of the stale memory registrations and frees any memory related to those
 stale registrations.
 Only the provider\-level registration struct is freed, not the user
 buffer associated with the registration.
-The parameter for \f[C]flush_cache\f[] is a struct fid pointer to a
+The parameter for \f[C]flush_cache\f[R] is a struct fid pointer to a
 fi_domain.
 The memory registration cache is tied to the domain, so issuing a
-\f[C]flush_cache\f[] to the domain will flush the registration cache of
+\f[C]flush_cache\f[R] to the domain will flush the registration cache of
 the domain.
 .PP
-For \f[I]FI_GNI_EP_OPS_1\f[], the currently supported values are:
-\f[I]GNI_HASH_TAG_IMPL\f[] : Use a hashlist for the tag list
+For \f[I]FI_GNI_EP_OPS_1\f[R], the currently supported values are:
+\f[I]GNI_HASH_TAG_IMPL\f[R] : Use a hashlist for the tag list
 implementation.
 The value is of type uint32_t.
 .PP
-The \f[C]native_amo\f[] function allows the user to call GNI native
+The \f[C]native_amo\f[R] function allows the user to call GNI native
 atomics that are not implemented in the libfabric API.
 The parameters for native_amo are the same as the fi_atomic function but
 adds the following parameter:
 .TP
-.B \f[I]enum gnix_fab_req_type req_type\f[]
-The req_type\[aq]s supported with this call are GNIX_FAB_RQ_NAMO_AX (AND
+.B \f[I]enum gnix_fab_req_type req_type\f[R]
+The req_type\[cq]s supported with this call are GNIX_FAB_RQ_NAMO_AX (AND
 and XOR), and GNIX_FAB_RQ_NAMO_AX_S (AND and XOR 32 bit),
 GNIX_FAB_RQ_NAMO_FAX (Fetch AND and XOR) and GNIX_FAB_RQ_NAMO_FAX_S
 (Fetch AND and XOR 32 bit).
-.RS
-.RE
 .SH NOTES
 .PP
 The default address format is FI_ADDR_GNI.
@@ -416,25 +341,25 @@ passing.
 FI_ADDR_STR is always parsed and converted to FI_ADDR_GNI for use within
 the GNI provider.
 .PP
-\f[I]FI_ADDR_STR\f[] is formatted as follows:
+\f[I]FI_ADDR_STR\f[R] is formatted as follows:
 gni;node;service;GNIX_AV_STR_ADDR_VERSION;device_addr;cdm_id;name_type;cm_nic_cdm_id;cookie;rx_ctx_cnt;key_offset
 .PP
-The GNI provider sets the domain attribute \f[I]cntr_cnt\f[] to the CQ
+The GNI provider sets the domain attribute \f[I]cntr_cnt\f[R] to the CQ
 limit divided by 2.
 .PP
-The GNI provider sets the domain attribute \f[I]cq_cnt\f[] to the CQ
+The GNI provider sets the domain attribute \f[I]cq_cnt\f[R] to the CQ
 limit divided by 2.
 .PP
-The GNI provider sets the domain attribute \f[I]ep_cnt\f[] to SIZE_MAX.
+The GNI provider sets the domain attribute \f[I]ep_cnt\f[R] to SIZE_MAX.
 .PP
 Completion queue events may report unknown source address information
-when using \f[I]FI_SOURCE\f[].
-If \f[I]FI_SOURCE_ERR\f[] is also specified, the source address
+when using \f[I]FI_SOURCE\f[R].
+If \f[I]FI_SOURCE_ERR\f[R] is also specified, the source address
 information will be reported in the err_data member of the struct
 fi_cq_err_entry populated by fi_cq_readerr.
 The err_data member will contain the source address information in the
 FI_ADDR_GNI address format.
-In order to populate the remote peer\[aq]s address vector with this
+In order to populate the remote peer\[cq]s address vector with this
 mechanism, the application must call fi_cq_readerr to get the source
 address followed by fi_av_insert on the populated err_data member.
 .PP
@@ -446,7 +371,7 @@ buffer may be generated out of order with respect to the offset into the
 buffer into which the messages were received.
 .PP
 The GNI provider can use a maximum of 4K memory registrations per
-\f[I]node\f[] when using scalable memory registration.
+\f[I]node\f[R] when using scalable memory registration.
 Please consider this limitation when placing multiple processes on each
 node.
 .PP
@@ -493,7 +418,7 @@ Setting TMPDIR to a non\-NULL value with change the directory for the
 authorization key mapping file, and setting GNIX_AK_FILENAME to a
 non\-NULL value will change the filename.
 The default path for the authorization key mapping file is
-\[aq]/tmp/gnix_vmdh_info\[aq].
+`/tmp/gnix_vmdh_info'.
 The recommendation is that the user should not change these environment
 variables unless necessary.
 .SH KNOWN BUGS
@@ -510,13 +435,13 @@ FI_OPT_MULTI_RECV is set to 0 and will return \-FI_EINVAL if an
 application attempts to set this value to zero.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_open_ops\f[](3), \f[C]fi_provider\f[](7),
-\f[C]fi_getinfo\f[](3) \f[C]fi_atomic\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_open_ops\f[R](3), \f[C]fi_provider\f[R](7),
+\f[C]fi_getinfo\f[R](3) \f[C]fi_atomic\f[R](3)
 .PP
-For more information on uGNI, see \f[I]Using the GNI and DMAPP APIs\f[]
+For more information on uGNI, see \f[I]Using the GNI and DMAPP APIs\f[R]
 (S\-2446\-3103, Cray Inc.).
 For more information on the GNI provider, see \f[I]An Implementation of
-OFI libfabric in Support of Multithreaded PGAS Solutions\f[] (PGAS
-\[aq]15).
+OFI libfabric in Support of Multithreaded PGAS Solutions\f[R] (PGAS
+\[cq]15).
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_hook.7 b/man/man7/fi_hook.7
index c01a43c..c87554c 100644
--- a/man/man7/fi_hook.7
+++ b/man/man7/fi_hook.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_hook" "7" "2019\-07\-19" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_hook" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -21,20 +21,17 @@ available hooking providers.
 When multiple hooks are specified, the names must be separated by a
 semi\-colon.
 To obtain a list of hooking providers available on the current system,
-one can use the fi_info utility with the \[aq]\-\-env\[aq] command line
-option.
-Hooking providers are usually identified by \[aq]hook\[aq] appearing in
-the provider name.
+one can use the fi_info utility with the `\[en]env' command line option.
+Hooking providers are usually identified by `hook' appearing in the
+provider name.
 .PP
 Known hooking providers include the following:
 .TP
-.B \f[I]ofi_hook_perf\f[]
-This hooks \[aq]fast path\[aq] data operation calls.
+.B \f[I]ofi_hook_perf\f[R]
+This hooks `fast path' data operation calls.
 Performance data is captured on call entrance and exit, in order to
 provide an average of how long each call takes to complete.
 See the PERFORMANCE HOOKS section for available performance data.
-.RS
-.RE
 .SH PERFORMANCE HOOKS
 .PP
 The hook provider allows capturing inline performance data by accessing
@@ -53,16 +50,12 @@ The environment variable FI_PERF_CNTR is used to identify which
 performance counter is tracked.
 The following counters are available:
 .TP
-.B \f[I]cpu_cycles\f[]
+.B \f[I]cpu_cycles\f[R]
 Counts the number of CPU cycles each function takes to complete.
-.RS
-.RE
 .TP
-.B \f[I]cpu_instr\f[]
+.B \f[I]cpu_instr\f[R]
 Counts the number of CPU instructions each function takes to complete.
 This is the default performance counter if none is specified.
-.RS
-.RE
 .SH LIMITATIONS
 .PP
 Hooking functionality is not available for providers built using the
@@ -74,6 +67,6 @@ Application that use FI_TRIGGER operations that attempt to hook calls
 will likely crash.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_mlx.7 b/man/man7/fi_mlx.7
index 1a3df04..3f80d23 100644
--- a/man/man7/fi_mlx.7
+++ b/man/man7/fi_mlx.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_mlx" "7" "2019\-09\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_mlx" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -11,6 +11,6 @@ The mlx provider was deprecated and removed in libfabric 1.9 due to a
 lack of a maintainer.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7),
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_mrail.7 b/man/man7/fi_mrail.7
index 4b82177..f3cc234 100644
--- a/man/man7/fi_mrail.7
+++ b/man/man7/fi_mrail.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_mrail" "7" "2020\-04\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_mrail" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -32,31 +32,23 @@ FI_RMA capability.
 below).
 .SH SUPPORTED FEATURES
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports only \f[I]FI_EP_RDM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports only \f[I]FI_EP_RDM\f[R].
 .TP
-.B \f[I]Endpoint capabilities\f[]
-The following data transfer interface is supported: \f[I]FI_MSG\f[],
-\f[I]FI_TAGGED\f[], \f[I]FI_RMA\f[].
-.RS
-.RE
+.B \f[I]Endpoint capabilities\f[R]
+The following data transfer interface is supported: \f[I]FI_MSG\f[R],
+\f[I]FI_TAGGED\f[R], \f[I]FI_RMA\f[R].
 .TP
 .B # LIMITATIONS
 Limitations of the underlying provider may show up as that of mrail
 provider.
-.RS
-.RE
-mrail provider doesn\[aq]t allow pass\-through of any mode bits to the
+mrail provider doesn\[cq]t allow pass\-through of any mode bits to the
 underlying provider.
-.RS
-.RE
 .SS Unsupported features
 .PP
 The following are the major libfabric features that are not supported.
-Any other feature not listed in "Supported features" can be assumed as
-unsupported.
+Any other feature not listed in \[lq]Supported features\[rq] can be
+assumed as unsupported.
 .IP \[bu] 2
 FI_ATOMIC
 .IP \[bu] 2
@@ -73,7 +65,7 @@ Triggered operations
 .PP
 For messages (FI_MSG, FI_TAGGED), the provider uses different policies
 to send messages over one or more rails based on message size (See
-\f[I]FI_OFI_MRIAL_CONFIG\f[] in the RUNTIME PARAMETERS section).
+\f[I]FI_OFI_MRIAL_CONFIG\f[R] in the RUNTIME PARAMETERS section).
 Ordering is guaranteed through the use of sequence numbers.
 .PP
 For RMA, the data is striped equally across all rails.
@@ -81,34 +73,28 @@ For RMA, the data is striped equally across all rails.
 .PP
 The ofi_mrail provider checks for the following environment variables.
 .TP
-.B \f[I]FI_OFI_MRAIL_ADDR\f[]
+.B \f[I]FI_OFI_MRAIL_ADDR\f[R]
 Comma delimited list of individual rail addresses.
 Each address can be an address in FI_ADDR_STR format, a host name, an IP
 address, or a netdev interface name.
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_MRAIL_ADDR_STRC\f[]
+.B \f[I]FI_OFI_MRAIL_ADDR_STRC\f[R]
 Deprecated.
-Replaced by \f[I]FI_OFI_MRAIL_ADDR\f[].
-.RS
-.RE
+Replaced by \f[I]FI_OFI_MRAIL_ADDR\f[R].
 .TP
-.B \f[I]FI_OFI_MRAIL_CONFIG\f[]
-Comma separated list of \f[C]<max_size>:<policy>\f[] pairs, sorted in
-ascending order of \f[C]<max_size>\f[].
+.B \f[I]FI_OFI_MRAIL_CONFIG\f[R]
+Comma separated list of \f[C]<max_size>:<policy>\f[R] pairs, sorted in
+ascending order of \f[C]<max_size>\f[R].
 Each pair indicated the rail sharing policy to be used for messages up
-to the size \f[C]<max_size>\f[] and not covered by all previous pairs.
-The value of \f[C]<policy>\f[] can be \f[I]fixed\f[] (a fixed rail is
-used), \f[I]round\-robin\f[] (one rail per message, selected in
-round\-robin fashion), or \f[I]striping\f[] (striping across all the
+to the size \f[C]<max_size>\f[R] and not covered by all previous pairs.
+The value of \f[C]<policy>\f[R] can be \f[I]fixed\f[R] (a fixed rail is
+used), \f[I]round\-robin\f[R] (one rail per message, selected in
+round\-robin fashion), or \f[I]striping\f[R] (striping across all the
 rails).
-The default configuration is \f[C]16384:fixed,ULONG_MAX:striping\f[].
+The default configuration is \f[C]16384:fixed,ULONG_MAX:striping\f[R].
 The value ULONG_MAX can be input as \-1.
-.RS
-.RE
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_netdir.7 b/man/man7/fi_netdir.7
index 15031b2..f247381 100644
--- a/man/man7/fi_netdir.7
+++ b/man/man7/fi_netdir.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_netdir" "7" "2019\-11\-20" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_netdir" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -25,124 +25,88 @@ service provider interface (SPI) for their hardware.
 The Network Direct provider support the following features defined for
 the libfabric API:
 .TP
-.B \f[I]Endpoint types\f[]
+.B \f[I]Endpoint types\f[R]
 The provider support the FI_EP_MSG endpoint types.
-.RS
-.RE
 .TP
-.B \f[I]Memory registration modes\f[]
-The provider implements the \f[I]FI_MR_BASIC\f[] memory registration
+.B \f[I]Memory registration modes\f[R]
+The provider implements the \f[I]FI_MR_BASIC\f[R] memory registration
 mode.
-.RS
-.RE
 .TP
-.B \f[I]Data transfer operations\f[]
+.B \f[I]Data transfer operations\f[R]
 The following data transfer interfaces are supported for the following
-endpoint types: \f[I]FI_MSG\f[], \f[I]FI_RMA\f[].
+endpoint types: \f[I]FI_MSG\f[R], \f[I]FI_RMA\f[R].
 See DATA TRANSFER OPERATIONS below for more details.
-.RS
-.RE
 .TP
-.B \f[I]Modes\f[]
+.B \f[I]Modes\f[R]
 The Network Direct provider requires applications to support the
 following modes: * FI_LOCAL_MR for all applications.
-.RS
-.RE
 .TP
-.B \f[I]Addressing Formats\f[]
+.B \f[I]Addressing Formats\f[R]
 Supported addressing formats include FI_SOCKADDR, FI_SOCKADDR_IN,
 FI_SOCKADDR_IN6
-.RS
-.RE
 .TP
-.B \f[I]Progress\f[]
+.B \f[I]Progress\f[R]
 The Network Direct provider supports FI_PROGRESS_AUTO: Asynchronous
 operations make forward progress automatically.
-.RS
-.RE
 .TP
-.B \f[I]Operation flags\f[]
+.B \f[I]Operation flags\f[R]
 The provider supports FI_INJECT, FI_COMPLETION, FI_TRANSMIT_COMPLETE,
 FI_INJECT_COMPLETE, FI_DELIVERY_COMPLETE, FI_SELECTIVE_COMPLETION
-.RS
-.RE
 .TP
-.B \f[I]Completion ordering\f[]
+.B \f[I]Completion ordering\f[R]
 RX/TX contexts: FI_ORDER_STRICT
-.RS
-.RE
 .TP
-.B \f[I]Other supported features\f[]
+.B \f[I]Other supported features\f[R]
 Multiple input/output vector (IOV) is supported for FI_RMA read/write
 and FI_MSG receive/transmit operations.
-.RS
-.RE
 .SH LIMITATIONS
 .TP
-.B \f[I]Memory Regions\f[]
+.B \f[I]Memory Regions\f[R]
 Only FI_MR_BASIC mode is supported.
 Adding regions via s/g list is supported only up to a s/g list size of
 1.
 No support for binding memory regions to a counter.
-.RS
-.RE
 .TP
-.B \f[I]Wait objects\f[]
+.B \f[I]Wait objects\f[R]
 Wait object and wait sets are not supported.
-.RS
-.RE
 .TP
-.B \f[I]Resource Management\f[]
+.B \f[I]Resource Management\f[R]
 Application has to make sure CQs are not overrun as this cannot be
 detected by the provider.
-.RS
-.RE
 .TP
-.B \f[I]Unsupported Endpoint types\f[]
+.B \f[I]Unsupported Endpoint types\f[R]
 FI_EP_DGRAM, FI_EP_RDM
-.RS
-.RE
 .TP
-.B \f[I]Other unsupported features\f[]
+.B \f[I]Other unsupported features\f[R]
 Scalable endpoints, FABRIC_DIRECT
-.RS
-.RE
 .TP
-.B \f[I]Unsupported features specific to MSG endpoints\f[]
+.B \f[I]Unsupported features specific to MSG endpoints\f[R]
 FI_SOURCE, FI_TAGGED, FI_CLAIM, fi_ep_alias, shared TX context,
 operations.
-.RS
-.RE
 .SH RUNTIME PARAMETERS
 .PP
 The Network Direct provider checks for the following environment
 variables.
 .SS Variables specific to RDM endpoints
 .TP
-.B \f[I]FI_NETDIR_INLINETHR\f[]
+.B \f[I]FI_NETDIR_INLINETHR\f[R]
 The size of the (default: 8 Kbyte): * Transmitted data that can be
 inlined * Preposted data for the unexpected receive queue
-.RS
-.RE
 .TP
-.B \f[I]FI_NETDIR_PREPOSTCNT\f[]
+.B \f[I]FI_NETDIR_PREPOSTCNT\f[R]
 The number of pre\-registered buffers between the endpoints that are not
 require internal ACK messages, must be a power of 2 (default: 8).
-.RS
-.RE
 .TP
-.B \f[I]FI_NETDIR_PREPOSTBUFCNT\f[]
+.B \f[I]FI_NETDIR_PREPOSTBUFCNT\f[R]
 The number of preposted arrays of buffers, must be a power of 2
 (default: 1).
-.RS
-.RE
 .SS Environment variables notes
 .PP
 The fi_info utility would give the up\-to\-date information on
 environment variables: fi_info \-p netdir \-e
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_open_ops\f[](3), \f[C]fi_provider\f[](7),
-\f[C]fi_getinfo\f[](3) \f[C]fi_atomic\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_open_ops\f[R](3), \f[C]fi_provider\f[R](7),
+\f[C]fi_getinfo\f[R](3) \f[C]fi_atomic\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_provider.7 b/man/man7/fi_provider.7
index 326b11e..ef050cf 100644
--- a/man/man7/fi_provider.7
+++ b/man/man7/fi_provider.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_provider" "7" "2021\-02\-10" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_provider" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -20,98 +20,74 @@ This distribution of libfabric contains the following providers
 (although more may be available via run\-time plug\-ins):
 .SS Core providers
 .TP
-.B \f[I]GNI\f[]
+.B \f[I]GNI\f[R]
 A provider for the Aries interconnect in Cray XC(TM) systems utilizing
-the user\-space \f[I]Generic Networking Interface\f[].
-See \f[C]fi_gni\f[](7) for more information.
-.RS
-.RE
+the user\-space \f[I]Generic Networking Interface\f[R].
+See \f[C]fi_gni\f[R](7) for more information.
 .TP
-.B \f[I]PSM\f[]
+.B \f[I]PSM\f[R]
 High\-speed InfiniBand networking from Intel.
-See \f[C]fi_psm\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_psm\f[R](7) for more information.
 .TP
-.B \f[I]PSM2\f[]
+.B \f[I]PSM2\f[R]
 High\-speed Omni\-Path networking from Intel.
-See \f[C]fi_psm2\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_psm2\f[R](7) for more information.
 .TP
-.B \f[I]PSM3\f[]
+.B \f[I]PSM3\f[R]
 High\-speed Ethernet networking from Intel.
-See \f[C]fi_psm3\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_psm3\f[R](7) for more information.
 .TP
-.B \f[I]Sockets\f[]
+.B \f[I]Sockets\f[R]
 A general purpose provider that can be used on any network that supports
 TCP/UDP sockets.
 This provider is not intended to provide performance improvements over
 regular TCP/UDP sockets, but rather to allow developers to write, test,
 and debug application code even on platforms that do not have
 high\-speed networking.
-See \f[C]fi_sockets\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_sockets\f[R](7) for more information.
 .TP
-.B \f[I]usNIC\f[]
+.B \f[I]usNIC\f[R]
 Ultra low latency Ethernet networking over Cisco userspace VIC adapters.
-See \f[C]fi_usnic\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_usnic\f[R](7) for more information.
 .TP
-.B \f[I]Verbs\f[]
+.B \f[I]Verbs\f[R]
 This provider uses the Linux Verbs API for network transport.
 Application performance is, obviously expected to be similar to that of
 the native Linux Verbs API.
 Analogous to the Sockets provider, the Verbs provider is intended to
 enable developers to write, test, and debug application code on
 platforms that only have Linux Verbs\-based networking.
-See \f[C]fi_verbs\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_verbs\f[R](7) for more information.
 .TP
-.B \f[I]Blue Gene/Q\f[]
-See \f[C]fi_bgq\f[](7) for more information.
-.RS
-.RE
+.B \f[I]Blue Gene/Q\f[R]
+See \f[C]fi_bgq\f[R](7) for more information.
 .TP
-.B \f[I]EFA\f[]
+.B \f[I]EFA\f[R]
 A provider for the Amazon EC2 Elastic Fabric Adapter
 (EFA) (https://aws.amazon.com/hpc/efa/), a custom\-built OS bypass
 hardware interface for inter\-instance communication on EC2.
-See \f[C]fi_efa\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_efa\f[R](7) for more information.
 .TP
-.B \f[I]SHM\f[]
+.B \f[I]SHM\f[R]
 A provider for intranode communication using shared memory.
 The provider makes use of the Linux kernel feature Cross Memory Attach
-(CMA) which allows processes to have full access to another process\[aq]
+(CMA) which allows processes to have full access to another process\[cq]
 address space.
-See \f[C]fi_shm\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_shm\f[R](7) for more information.
 .SS Utility providers
 .TP
-.B \f[I]RxM\f[]
+.B \f[I]RxM\f[R]
 The RxM provider (ofi_rxm) is an utility provider that supports RDM
 endpoints emulated over MSG endpoints of a core provider.
-See \f[C]fi_rxm\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_rxm\f[R](7) for more information.
 .TP
-.B \f[I]RxD\f[]
+.B \f[I]RxD\f[R]
 The RxD provider (ofi_rxd) is a utility provider that supports RDM
 endpoints emulated over DGRAM endpoints of a core provider.
-See \f[C]fi_rxd\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_rxd\f[R](7) for more information.
 .SS Special providers
 .TP
-.B \f[I]Hook\f[]
+.B \f[I]Hook\f[R]
 The hook provider is a special type of provider that can layer over any
 other provider, unless FI_FABRIC_DIRECT is used.
 The hook provider is always available, but has no impact unless enabled.
@@ -119,9 +95,7 @@ When enabled, the hook provider will intercept all calls to the
 underlying core or utility provider(s).
 The hook provider is useful for capturing performance data or providing
 debugging information, even in release builds of the library.
-See \f[C]fi_hook\f[](7) for more information.
-.RS
-.RE
+See \f[C]fi_hook\f[R](7) for more information.
 .SH CORE VERSUS UTILITY PROVIDERS
 .PP
 Core providers implement the libfabric interfaces directly over
@@ -142,9 +116,9 @@ datagram endpoints.
 The utility providers will not layer over the sockets provider unless it
 is explicitly requested.
 .PP
-Utility providers show up as a component in the core provider\[aq]s
+Utility providers show up as a component in the core provider\[cq]s
 component list.
-See \f[C]fi_fabric\f[](3).
+See \f[C]fi_fabric\f[R](3).
 Utility providers are enabled automatically for core providers that do
 not support the feature set requested by an application.
 .SH PROVIDER REQUIREMENTS
@@ -172,15 +146,18 @@ All endpoints must support the message queue data transfer interface
 .IP \[bu] 2
 An endpoint that advertises support for a specific endpoint capability
 must support the corresponding data transfer interface.
+.RS 2
 .IP \[bu] 2
 FI_ATOMIC \- fi_ops_atomic
 .IP \[bu] 2
 FI_RMA \- fi_ops_rma
 .IP \[bu] 2
 FI_TAGGED \- fi_ops_tagged
+.RE
 .IP \[bu] 2
 Endpoints must support all transmit and receive operations for any data
 transfer interface that they support.
+.RS 2
 .IP \[bu] 2
 Exception: If an operation is only usable for an operation that the
 provider does not support, and support for that operation is conveyed
@@ -192,12 +169,13 @@ For example, if the provider does not support injected data, it can set
 the attribute inject_size = 0, and fail all fi_inject operations.
 .RE
 .IP \[bu] 2
-The framework supplies wrappers around the \[aq]msg\[aq] operations that
-can be used.
+The framework supplies wrappers around the `msg' operations that can be
+used.
 For example, the framework implements the sendv() msg operation by
 calling sendmsg().
 Providers may reference the general operation, and supply on the
 sendmsg() implementation.
+.RE
 .IP \[bu] 2
 Providers must set all operations to an implementation.
 Function pointers may not be left NULL or uninitialized.
@@ -205,12 +183,14 @@ The framework supplies empty functions that return \-FI_ENOSYS which can
 be used for this purpose.
 .IP \[bu] 2
 Endpoints must support the CM interface as follows:
+.RS 2
 .IP \[bu] 2
 FI_EP_MSG endpoints must support all CM operations.
 .IP \[bu] 2
 FI_EP_DGRAM endpoints must support CM getname and setname.
 .IP \[bu] 2
 FI_EP_RDM endpoints must support CM getname and setname.
+.RE
 .IP \[bu] 2
 Providers that support connectionless endpoints must support all AV
 operations (fi_ops_av).
@@ -219,6 +199,7 @@ Providers that support memory registration, must support all MR
 operations (fi_ops_mr).
 .IP \[bu] 2
 Providers should support both completion queues and counters.
+.RS 2
 .IP \[bu] 2
 If FI_RMA_EVENT is not supported, counter support is limited to local
 events only.
@@ -230,10 +211,11 @@ Providers that support FI_REMOTE_CQ_DATA shall support
 FI_CQ_FORMAT_DATA.
 .IP \[bu] 2
 Providers that support FI_TAGGED shall support FI_CQ_FORMAT_TAGGED.
+.RE
 .IP \[bu] 2
 A provider is expected to be forward compatible, and must be able to be
-compiled against expanded \f[C]fi_xxx_ops\f[] structures that define new
-functions added after the provider was written.
+compiled against expanded \f[C]fi_xxx_ops\f[R] structures that define
+new functions added after the provider was written.
 Any unknown functions must be set to NULL.
 .IP \[bu] 2
 Providers shall document in their man page which features they support,
@@ -249,55 +231,41 @@ Logging is performed using the FI_ERR, FI_LOG, and FI_DEBUG macros.
 .IP
 .nf
 \f[C]
-#define\ FI_ERR(prov_name,\ subsystem,\ ...)
+#define FI_ERR(prov_name, subsystem, ...)
 
-#define\ FI_LOG(prov_name,\ prov,\ level,\ subsystem,\ ...)
+#define FI_LOG(prov_name, prov, level, subsystem, ...)
 
-#define\ FI_DEBUG(prov_name,\ subsystem,\ ...)
-\f[]
+#define FI_DEBUG(prov_name, subsystem, ...)
+\f[R]
 .fi
 .SS ARGUMENTS
 .TP
-.B \f[I]prov_name\f[]
+.B \f[I]prov_name\f[R]
 String representing the provider name.
-.RS
-.RE
 .TP
-.B \f[I]prov\f[]
+.B \f[I]prov\f[R]
 Provider context structure.
-.RS
-.RE
 .TP
-.B \f[I]level\f[]
+.B \f[I]level\f[R]
 Log level associated with log statement.
-.RS
-.RE
 .TP
-.B \f[I]subsystem\f[]
+.B \f[I]subsystem\f[R]
 Subsystem being logged from.
-.RS
-.RE
 .SS DESCRIPTION
 .TP
-.B \f[I]FI_ERR\f[]
+.B \f[I]FI_ERR\f[R]
 Always logged.
-.RS
-.RE
 .TP
-.B \f[I]FI_LOG\f[]
+.B \f[I]FI_LOG\f[R]
 Logged if the intended provider, log level, and subsystem parameters
 match the user supplied values.
-.RS
-.RE
 .TP
-.B \f[I]FI_DEBUG\f[]
-Logged if configured with the \-\-enable\-debug flag.
-.RS
-.RE
+.B \f[I]FI_DEBUG\f[R]
+Logged if configured with the \[en]enable\-debug flag.
 .SH SEE ALSO
 .PP
-\f[C]fi_gni\f[](7), \f[C]fi_hook\f[](7), \f[C]fi_psm\f[](7),
-\f[C]fi_sockets\f[](7), \f[C]fi_usnic\f[](7), \f[C]fi_verbs\f[](7),
-\f[C]fi_bgq\f[](7),
+\f[C]fi_gni\f[R](7), \f[C]fi_hook\f[R](7), \f[C]fi_psm\f[R](7),
+\f[C]fi_sockets\f[R](7), \f[C]fi_usnic\f[R](7), \f[C]fi_verbs\f[R](7),
+\f[C]fi_bgq\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_psm.7 b/man/man7/fi_psm.7
index d63b6c7..7746ccc 100644
--- a/man/man7/fi_psm.7
+++ b/man/man7/fi_psm.7
@@ -1,96 +1,87 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_psm" "7" "2021\-02\-10" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_psm" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
 fi_psm \- The PSM Fabric Provider
 .SH OVERVIEW
 .PP
-The \f[I]psm\f[] provider runs over the PSM 1.x interface that is
+The \f[I]psm\f[R] provider runs over the PSM 1.x interface that is
 currently supported by the Intel TrueScale Fabric.
 PSM provides tag\-matching message queue functions that are optimized
 for MPI implementations.
 PSM also has limited Active Message support, which is not officially
 published but is quite stable and well documented in the source code
 (part of the OFED release).
-The \f[I]psm\f[] provider makes use of both the tag\-matching message
+The \f[I]psm\f[R] provider makes use of both the tag\-matching message
 queue functions and the Active Message functions to support a variety of
 libfabric data transfer APIs, including tagged message queue, message
 queue, RMA, and atomic operations.
 .PP
-The \f[I]psm\f[] provider can work with the psm2\-compat library, which
+The \f[I]psm\f[R] provider can work with the psm2\-compat library, which
 exposes a PSM 1.x interface over the Intel Omni\-Path Fabric.
 .SH LIMITATIONS
 .PP
-The \f[I]psm\f[] provider doesn\[aq]t support all the features defined
+The \f[I]psm\f[R] provider doesn\[cq]t support all the features defined
 in the libfabric API.
 Here are some of the limitations:
 .TP
 .B Endpoint types
-Only support non\-connection based types \f[I]FI_DGRAM\f[] and
-\f[I]FI_RDM\f[]
-.RS
-.RE
+Only support non\-connection based types \f[I]FI_DGRAM\f[R] and
+\f[I]FI_RDM\f[R]
 .TP
 .B Endpoint capabilities
 Endpoints can support any combination of data transfer capabilities
-\f[I]FI_TAGGED\f[], \f[I]FI_MSG\f[], \f[I]FI_ATOMICS\f[], and
-\f[I]FI_RMA\f[].
-These capabilities can be further refined by \f[I]FI_SEND\f[],
-\f[I]FI_RECV\f[], \f[I]FI_READ\f[], \f[I]FI_WRITE\f[],
-\f[I]FI_REMOTE_READ\f[], and \f[I]FI_REMOTE_WRITE\f[] to limit the
+\f[I]FI_TAGGED\f[R], \f[I]FI_MSG\f[R], \f[I]FI_ATOMICS\f[R], and
+\f[I]FI_RMA\f[R].
+These capabilities can be further refined by \f[I]FI_SEND\f[R],
+\f[I]FI_RECV\f[R], \f[I]FI_READ\f[R], \f[I]FI_WRITE\f[R],
+\f[I]FI_REMOTE_READ\f[R], and \f[I]FI_REMOTE_WRITE\f[R] to limit the
 direction of operations.
 The limitation is that no two endpoints can have overlapping receive or
 RMA target capabilities in any of the above categories.
-For example it is fine to have two endpoints with \f[I]FI_TAGGED\f[] |
-\f[I]FI_SEND\f[], one endpoint with \f[I]FI_TAGGED\f[] |
-\f[I]FI_RECV\f[], one endpoint with \f[I]FI_MSG\f[], one endpoint with
-\f[I]FI_RMA\f[] | \f[I]FI_ATOMICS\f[].
-But it is not allowed to have two endpoints with \f[I]FI_TAGGED\f[], or
-two endpoints with \f[I]FI_RMA\f[].
-.RS
-.RE
-.PP
-\f[I]FI_MULTI_RECV\f[] is supported for non\-tagged message queue only.
-.PP
-Other supported capabilities include \f[I]FI_TRIGGER\f[].
+For example it is fine to have two endpoints with \f[I]FI_TAGGED\f[R] |
+\f[I]FI_SEND\f[R], one endpoint with \f[I]FI_TAGGED\f[R] |
+\f[I]FI_RECV\f[R], one endpoint with \f[I]FI_MSG\f[R], one endpoint with
+\f[I]FI_RMA\f[R] | \f[I]FI_ATOMICS\f[R].
+But it is not allowed to have two endpoints with \f[I]FI_TAGGED\f[R], or
+two endpoints with \f[I]FI_RMA\f[R].
+.PP
+\f[I]FI_MULTI_RECV\f[R] is supported for non\-tagged message queue only.
+.PP
+Other supported capabilities include \f[I]FI_TRIGGER\f[R].
 .TP
 .B Modes
-\f[I]FI_CONTEXT\f[] is required for the \f[I]FI_TAGGED\f[] and
-\f[I]FI_MSG\f[] capabilities.
+\f[I]FI_CONTEXT\f[R] is required for the \f[I]FI_TAGGED\f[R] and
+\f[I]FI_MSG\f[R] capabilities.
 That means, any request belonging to these two categories that generates
 a completion must pass as the operation context a valid pointer to type
-\f[I]struct fi_context\f[], and the space referenced by the pointer must
-remain untouched until the request has completed.
-If none of \f[I]FI_TAGGED\f[] and \f[I]FI_MSG\f[] is asked for, the
-\f[I]FI_CONTEXT\f[] mode is not required.
-.RS
-.RE
+\f[I]struct fi_context\f[R], and the space referenced by the pointer
+must remain untouched until the request has completed.
+If none of \f[I]FI_TAGGED\f[R] and \f[I]FI_MSG\f[R] is asked for, the
+\f[I]FI_CONTEXT\f[R] mode is not required.
 .TP
 .B Progress
-The \f[I]psm\f[] provider requires manual progress.
-The application is expected to call \f[I]fi_cq_read\f[] or
-\f[I]fi_cntr_read\f[] function from time to time when no other libfabric
-function is called to ensure progress is made in a timely manner.
+The \f[I]psm\f[R] provider requires manual progress.
+The application is expected to call \f[I]fi_cq_read\f[R] or
+\f[I]fi_cntr_read\f[R] function from time to time when no other
+libfabric function is called to ensure progress is made in a timely
+manner.
 The provider does support auto progress mode.
 However, the performance can be significantly impacted if the
 application purely depends on the provider to make auto progress.
-.RS
-.RE
 .TP
 .B Unsupported features
 These features are unsupported: connection management, scalable
 endpoint, passive endpoint, shared receive context, send/inject with
 immediate data.
-.RS
-.RE
 .SH RUNTIME PARAMETERS
 .PP
-The \f[I]psm\f[] provider checks for the following environment
+The \f[I]psm\f[R] provider checks for the following environment
 variables:
 .TP
-.B \f[I]FI_PSM_UUID\f[]
+.B \f[I]FI_PSM_UUID\f[R]
 PSM requires that each job has a unique ID (UUID).
 All the processes in the same job need to use the same UUID in order to
 be able to talk to each other.
@@ -98,35 +89,31 @@ The PSM reference manual advises to keep UUID unique to each job.
 In practice, it generally works fine to reuse UUID as long as (1) no two
 jobs with the same UUID are running at the same time; and (2) previous
 jobs with the same UUID have exited normally.
-If running into "resource busy" or "connection failure" issues with
-unknown reason, it is advisable to manually set the UUID to a value
-different from the default.
-.RS
-.RE
+If running into \[lq]resource busy\[rq] or \[lq]connection failure\[rq]
+issues with unknown reason, it is advisable to manually set the UUID to
+a value different from the default.
 .PP
 The default UUID is 0FFF0FFF\-0000\-0000\-0000\-0FFF0FFF0FFF.
 .TP
-.B \f[I]FI_PSM_NAME_SERVER\f[]
-The \f[I]psm\f[] provider has a simple built\-in name server that can be
-used to resolve an IP address or host name into a transport address
-needed by the \f[I]fi_av_insert\f[] call.
+.B \f[I]FI_PSM_NAME_SERVER\f[R]
+The \f[I]psm\f[R] provider has a simple built\-in name server that can
+be used to resolve an IP address or host name into a transport address
+needed by the \f[I]fi_av_insert\f[R] call.
 The main purpose of this name server is to allow simple client\-server
-type applications (such as those in \f[I]fabtests\f[]) to be written
+type applications (such as those in \f[I]fabtests\f[R]) to be written
 purely with libfabric, without using any out\-of\-band communication
 mechanism.
 For such applications, the server would run first to allow endpoints be
 created and registered with the name server, and then the client would
-call \f[I]fi_getinfo\f[] with the \f[I]node\f[] parameter set to the IP
-address or host name of the server.
-The resulting \f[I]fi_info\f[] structure would have the transport
-address of the endpoint created by the server in the \f[I]dest_addr\f[]
+call \f[I]fi_getinfo\f[R] with the \f[I]node\f[R] parameter set to the
+IP address or host name of the server.
+The resulting \f[I]fi_info\f[R] structure would have the transport
+address of the endpoint created by the server in the \f[I]dest_addr\f[R]
 field.
-Optionally the \f[I]service\f[] parameter can be used in addition to
-\f[I]node\f[].
-Notice that the \f[I]service\f[] number is interpreted by the provider
+Optionally the \f[I]service\f[R] parameter can be used in addition to
+\f[I]node\f[R].
+Notice that the \f[I]service\f[R] number is interpreted by the provider
 and is not a TCP/IP port number.
-.RS
-.RE
 .PP
 The name server is on by default.
 It can be turned off by setting the variable to 0.
@@ -136,17 +123,15 @@ created when the name server is on.
 The provider detects OpenMPI and MPICH runs and changes the default
 setting to off.
 .TP
-.B \f[I]FI_PSM_TAGGED_RMA\f[]
+.B \f[I]FI_PSM_TAGGED_RMA\f[R]
 The RMA functions are implemented on top of the PSM Active Message
 functions.
 The Active Message functions have limit on the size of data can be
 transferred in a single message.
 Large transfers can be divided into small chunks and be pipe\-lined.
 However, the bandwidth is sub\-optimal by doing this way.
-.RS
-.RE
 .PP
-The \f[I]psm\f[] provider use PSM tag\-matching message queue functions
+The \f[I]psm\f[R] provider use PSM tag\-matching message queue functions
 to achieve higher bandwidth for large size RMA.
 For this purpose, a bit is reserved from the tag space to separate the
 RMA traffic from the regular tagged message queue.
@@ -154,63 +139,53 @@ RMA traffic from the regular tagged message queue.
 The option is on by default.
 To turn it off set the variable to 0.
 .TP
-.B \f[I]FI_PSM_AM_MSG\f[]
-The \f[I]psm\f[] provider implements the non\-tagged message queue over
+.B \f[I]FI_PSM_AM_MSG\f[R]
+The \f[I]psm\f[R] provider implements the non\-tagged message queue over
 the PSM tag\-matching message queue.
 One tag bit is reserved for this purpose.
 Alternatively, the non\-tagged message queue can be implemented over
 Active Message.
 This experimental feature has slightly larger latency.
-.RS
-.RE
 .PP
 This option is off by default.
 To turn it on set the variable to 1.
 .TP
-.B \f[I]FI_PSM_DELAY\f[]
+.B \f[I]FI_PSM_DELAY\f[R]
 Time (seconds) to sleep before closing PSM endpoints.
 This is a workaround for a bug in some versions of PSM library.
-.RS
-.RE
 .PP
 The default setting is 1.
 .TP
-.B \f[I]FI_PSM_TIMEOUT\f[]
+.B \f[I]FI_PSM_TIMEOUT\f[R]
 Timeout (seconds) for gracefully closing PSM endpoints.
 A forced closing will be issued if timeout expires.
-.RS
-.RE
 .PP
 The default setting is 5.
 .TP
-.B \f[I]FI_PSM_PROG_INTERVAL\f[]
+.B \f[I]FI_PSM_PROG_INTERVAL\f[R]
 When auto progress is enabled (asked via the hints to
-\f[I]fi_getinfo\f[]), a progress thread is created to make progress
+\f[I]fi_getinfo\f[R]), a progress thread is created to make progress
 calls from time to time.
 This option set the interval (microseconds) between progress calls.
-.RS
-.RE
 .PP
 The default setting is 1 if affinity is set, or 1000 if not.
-See \f[I]FI_PSM_PROG_AFFINITY\f[].
+See \f[I]FI_PSM_PROG_AFFINITY\f[R].
 .TP
-.B \f[I]FI_PSM_PROG_AFFINITY\f[]
+.B \f[I]FI_PSM_PROG_AFFINITY\f[R]
 When set, specify the set of CPU cores to set the progress thread
 affinity to.
 The format is
-\f[C]<start>[:<end>[:<stride>]][,<start>[:<end>[:<stride>]]]*\f[], where
-each triplet \f[C]<start>:<end>:<stride>\f[] defines a block of
+\f[C]<start>[:<end>[:<stride>]][,<start>[:<end>[:<stride>]]]*\f[R],
+where each triplet \f[C]<start>:<end>:<stride>\f[R] defines a block of
 core_ids.
-Both \f[C]<start>\f[] and \f[C]<end>\f[] can be either the
-\f[C]core_id\f[] (when >=0) or \f[C]core_id\ \-\ num_cores\f[] (when
+Both \f[C]<start>\f[R] and \f[C]<end>\f[R] can be either the
+\f[C]core_id\f[R] (when >=0) or \f[C]core_id \- num_cores\f[R] (when
 <0).
-.RS
-.RE
 .PP
 By default affinity is not set.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_psm2\f[](7),
-\f[C]fi_psm3\f[](7),
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_psm2\f[R](7),
+\f[C]fi_psm3\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_psm2.7 b/man/man7/fi_psm2.7
index 4644f29..e2983c0 100644
--- a/man/man7/fi_psm2.7
+++ b/man/man7/fi_psm2.7
@@ -1,98 +1,90 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_psm2" "7" "2021\-02\-10" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_psm2" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
 fi_psm2 \- The PSM2 Fabric Provider
 .SH OVERVIEW
 .PP
-The \f[I]psm2\f[] provider runs over the PSM 2.x interface that is
+The \f[I]psm2\f[R] provider runs over the PSM 2.x interface that is
 supported by the Intel Omni\-Path Fabric.
 PSM 2.x has all the PSM 1.x features plus a set of new functions with
 enhanced capabilities.
-Since PSM 1.x and PSM 2.x are not ABI compatible the \f[I]psm2\f[]
-provider only works with PSM 2.x and doesn\[aq]t support Intel TrueScale
+Since PSM 1.x and PSM 2.x are not ABI compatible the \f[I]psm2\f[R]
+provider only works with PSM 2.x and doesn\[cq]t support Intel TrueScale
 Fabric.
 .SH LIMITATIONS
 .PP
-The \f[I]psm2\f[] provider doesn\[aq]t support all the features defined
+The \f[I]psm2\f[R] provider doesn\[cq]t support all the features defined
 in the libfabric API.
 Here are some of the limitations:
 .TP
 .B Endpoint types
-Only support non\-connection based types \f[I]FI_DGRAM\f[] and
-\f[I]FI_RDM\f[]
-.RS
-.RE
+Only support non\-connection based types \f[I]FI_DGRAM\f[R] and
+\f[I]FI_RDM\f[R]
 .TP
 .B Endpoint capabilities
 Endpoints can support any combination of data transfer capabilities
-\f[I]FI_TAGGED\f[], \f[I]FI_MSG\f[], \f[I]FI_ATOMICS\f[], and
-\f[I]FI_RMA\f[].
-These capabilities can be further refined by \f[I]FI_SEND\f[],
-\f[I]FI_RECV\f[], \f[I]FI_READ\f[], \f[I]FI_WRITE\f[],
-\f[I]FI_REMOTE_READ\f[], and \f[I]FI_REMOTE_WRITE\f[] to limit the
+\f[I]FI_TAGGED\f[R], \f[I]FI_MSG\f[R], \f[I]FI_ATOMICS\f[R], and
+\f[I]FI_RMA\f[R].
+These capabilities can be further refined by \f[I]FI_SEND\f[R],
+\f[I]FI_RECV\f[R], \f[I]FI_READ\f[R], \f[I]FI_WRITE\f[R],
+\f[I]FI_REMOTE_READ\f[R], and \f[I]FI_REMOTE_WRITE\f[R] to limit the
 direction of operations.
-.RS
-.RE
 .PP
-\f[I]FI_MULTI_RECV\f[] is supported for non\-tagged message queue only.
+\f[I]FI_MULTI_RECV\f[R] is supported for non\-tagged message queue only.
 .PP
 Scalable endpoints are supported if the underlying PSM2 library supports
 multiple endpoints.
 This condition must be satisfied both when the provider is built and
 when the provider is used.
-See the \f[I]Scalable endpoints\f[] section for more information.
+See the \f[I]Scalable endpoints\f[R] section for more information.
 .PP
-Other supported capabilities include \f[I]FI_TRIGGER\f[],
-\f[I]FI_REMOTE_CQ_DATA\f[], \f[I]FI_RMA_EVENT\f[], \f[I]FI_SOURCE\f[],
-and \f[I]FI_SOURCE_ERR\f[].
-Furthermore, \f[I]FI_NAMED_RX_CTX\f[] is supported when scalable
+Other supported capabilities include \f[I]FI_TRIGGER\f[R],
+\f[I]FI_REMOTE_CQ_DATA\f[R], \f[I]FI_RMA_EVENT\f[R],
+\f[I]FI_SOURCE\f[R], and \f[I]FI_SOURCE_ERR\f[R].
+Furthermore, \f[I]FI_NAMED_RX_CTX\f[R] is supported when scalable
 endpoints are enabled.
 .TP
 .B Modes
-\f[I]FI_CONTEXT\f[] is required for the \f[I]FI_TAGGED\f[] and
-\f[I]FI_MSG\f[] capabilities.
+\f[I]FI_CONTEXT\f[R] is required for the \f[I]FI_TAGGED\f[R] and
+\f[I]FI_MSG\f[R] capabilities.
 That means, any request belonging to these two categories that generates
 a completion must pass as the operation context a valid pointer to type
-\f[I]struct fi_context\f[], and the space referenced by the pointer must
-remain untouched until the request has completed.
-If none of \f[I]FI_TAGGED\f[] and \f[I]FI_MSG\f[] is asked for, the
-\f[I]FI_CONTEXT\f[] mode is not required.
-.RS
-.RE
+\f[I]struct fi_context\f[R], and the space referenced by the pointer
+must remain untouched until the request has completed.
+If none of \f[I]FI_TAGGED\f[R] and \f[I]FI_MSG\f[R] is asked for, the
+\f[I]FI_CONTEXT\f[R] mode is not required.
 .TP
 .B Progress
-The \f[I]psm2\f[] provider requires manual progress.
-The application is expected to call \f[I]fi_cq_read\f[] or
-\f[I]fi_cntr_read\f[] function from time to time when no other libfabric
-function is called to ensure progress is made in a timely manner.
+The \f[I]psm2\f[R] provider requires manual progress.
+The application is expected to call \f[I]fi_cq_read\f[R] or
+\f[I]fi_cntr_read\f[R] function from time to time when no other
+libfabric function is called to ensure progress is made in a timely
+manner.
 The provider does support auto progress mode.
 However, the performance can be significantly impacted if the
 application purely depends on the provider to make auto progress.
-.RS
-.RE
 .TP
 .B Scalable endpoints
 Scalable endpoints support depends on the multi\-EP feature of the
-\f[I]PSM2\f[] library.
-If the \f[I]PSM2\f[] library supports this feature, the availability is
-further controlled by an environment variable \f[I]PSM2_MULTI_EP\f[].
-The \f[I]psm2\f[] provider automatically sets this variable to 1 if it
+\f[I]PSM2\f[R] library.
+If the \f[I]PSM2\f[R] library supports this feature, the availability is
+further controlled by an environment variable \f[I]PSM2_MULTI_EP\f[R].
+The \f[I]psm2\f[R] provider automatically sets this variable to 1 if it
 is not set.
-The feature can be disabled explicitly by setting \f[I]PSM2_MULTI_EP\f[]
-to 0.
-.RS
-.RE
+The feature can be disabled explicitly by setting
+\f[I]PSM2_MULTI_EP\f[R] to 0.
 .PP
 When creating a scalable endpoint, the exact number of contexts
-requested should be set in the "fi_info" structure passed to the
-\f[I]fi_scalable_ep\f[] function.
-This number should be set in "fi_info\->ep_attr\->tx_ctx_cnt" or
-"fi_info\->ep_attr\->rx_ctx_cnt" or both, whichever greater is used.
-The \f[I]psm2\f[] provider allocates all requested contexts upfront when
-the scalable endpoint is created.
+requested should be set in the \[lq]fi_info\[rq] structure passed to the
+\f[I]fi_scalable_ep\f[R] function.
+This number should be set in \[lq]fi_info\->ep_attr\->tx_ctx_cnt\[rq] or
+\[lq]fi_info\->ep_attr\->rx_ctx_cnt\[rq] or both, whichever greater is
+used.
+The \f[I]psm2\f[R] provider allocates all requested contexts upfront
+when the scalable endpoint is created.
 The same context is used for both Tx and Rx.
 .PP
 For optimal performance, it is advised to avoid having multiple threads
@@ -105,7 +97,7 @@ supported.
 Instead, individual tx context or rx context of the scalable endpoint
 should be used.
 Similarly, using the address of the scalable endpoint as the source
-address or destination address doesn\[aq]t collectively address all the
+address or destination address doesn\[cq]t collectively address all the
 tx/rx contexts.
 It addresses only the first tx/rx context, instead.
 .TP
@@ -117,20 +109,16 @@ The reason is that Rx capability always requires a PSM context, which
 can also be automatically used for Tx.
 As the result, allocating a shared Tx context for Rx capable endpoints
 actually consumes one extra context instead of saving some.
-.RS
-.RE
 .TP
 .B Unsupported features
 These features are unsupported: connection management, passive endpoint,
 and shared receive context.
-.RS
-.RE
 .SH RUNTIME PARAMETERS
 .PP
-The \f[I]psm2\f[] provider checks for the following environment
+The \f[I]psm2\f[R] provider checks for the following environment
 variables:
 .TP
-.B \f[I]FI_PSM2_UUID\f[]
+.B \f[I]FI_PSM2_UUID\f[R]
 PSM requires that each job has a unique ID (UUID).
 All the processes in the same job need to use the same UUID in order to
 be able to talk to each other.
@@ -138,43 +126,39 @@ The PSM reference manual advises to keep UUID unique to each job.
 In practice, it generally works fine to reuse UUID as long as (1) no two
 jobs with the same UUID are running at the same time; and (2) previous
 jobs with the same UUID have exited normally.
-If running into "resource busy" or "connection failure" issues with
-unknown reason, it is advisable to manually set the UUID to a value
-different from the default.
-.RS
-.RE
+If running into \[lq]resource busy\[rq] or \[lq]connection failure\[rq]
+issues with unknown reason, it is advisable to manually set the UUID to
+a value different from the default.
 .PP
 The default UUID is 00FF00FF\-0000\-0000\-0000\-00FF0F0F00FF.
 .PP
 It is possible to create endpoints with UUID different from the one set
 here.
-To achieve that, set \[aq]info\->ep_attr\->auth_key\[aq] to the uuid
-value and \[aq]info\->ep_attr\->auth_key_size\[aq] to its size (16
-bytes) when calling fi_endpoint() or fi_scalable_ep().
+To achieve that, set `info\->ep_attr\->auth_key' to the uuid value and
+`info\->ep_attr\->auth_key_size' to its size (16 bytes) when calling
+fi_endpoint() or fi_scalable_ep().
 It is still true that an endpoint can only communicate with endpoints
 with the same UUID.
 .TP
-.B \f[I]FI_PSM2_NAME_SERVER\f[]
-The \f[I]psm2\f[] provider has a simple built\-in name server that can
+.B \f[I]FI_PSM2_NAME_SERVER\f[R]
+The \f[I]psm2\f[R] provider has a simple built\-in name server that can
 be used to resolve an IP address or host name into a transport address
-needed by the \f[I]fi_av_insert\f[] call.
+needed by the \f[I]fi_av_insert\f[R] call.
 The main purpose of this name server is to allow simple client\-server
-type applications (such as those in \f[I]fabtests\f[]) to be written
+type applications (such as those in \f[I]fabtests\f[R]) to be written
 purely with libfabric, without using any out\-of\-band communication
 mechanism.
 For such applications, the server would run first to allow endpoints be
 created and registered with the name server, and then the client would
-call \f[I]fi_getinfo\f[] with the \f[I]node\f[] parameter set to the IP
-address or host name of the server.
-The resulting \f[I]fi_info\f[] structure would have the transport
-address of the endpoint created by the server in the \f[I]dest_addr\f[]
+call \f[I]fi_getinfo\f[R] with the \f[I]node\f[R] parameter set to the
+IP address or host name of the server.
+The resulting \f[I]fi_info\f[R] structure would have the transport
+address of the endpoint created by the server in the \f[I]dest_addr\f[R]
 field.
-Optionally the \f[I]service\f[] parameter can be used in addition to
-\f[I]node\f[].
-Notice that the \f[I]service\f[] number is interpreted by the provider
+Optionally the \f[I]service\f[R] parameter can be used in addition to
+\f[I]node\f[R].
+Notice that the \f[I]service\f[R] number is interpreted by the provider
 and is not a TCP/IP port number.
-.RS
-.RE
 .PP
 The name server is on by default.
 It can be turned off by setting the variable to 0.
@@ -184,93 +168,77 @@ created when the name server is on.
 The provider detects OpenMPI and MPICH runs and changes the default
 setting to off.
 .TP
-.B \f[I]FI_PSM2_TAGGED_RMA\f[]
+.B \f[I]FI_PSM2_TAGGED_RMA\f[R]
 The RMA functions are implemented on top of the PSM Active Message
 functions.
 The Active Message functions have limit on the size of data can be
 transferred in a single message.
 Large transfers can be divided into small chunks and be pipe\-lined.
 However, the bandwidth is sub\-optimal by doing this way.
-.RS
-.RE
 .PP
-The \f[I]psm2\f[] provider use PSM tag\-matching message queue functions
-to achieve higher bandwidth for large size RMA.
+The \f[I]psm2\f[R] provider use PSM tag\-matching message queue
+functions to achieve higher bandwidth for large size RMA.
 It takes advantage of the extra tag bits available in PSM2 to separate
 the RMA traffic from the regular tagged message queue.
 .PP
 The option is on by default.
 To turn it off set the variable to 0.
 .TP
-.B \f[I]FI_PSM2_DELAY\f[]
+.B \f[I]FI_PSM2_DELAY\f[R]
 Time (seconds) to sleep before closing PSM endpoints.
 This is a workaround for a bug in some versions of PSM library.
-.RS
-.RE
 .PP
 The default setting is 0.
 .TP
-.B \f[I]FI_PSM2_TIMEOUT\f[]
+.B \f[I]FI_PSM2_TIMEOUT\f[R]
 Timeout (seconds) for gracefully closing PSM endpoints.
 A forced closing will be issued if timeout expires.
-.RS
-.RE
 .PP
 The default setting is 5.
 .TP
-.B \f[I]FI_PSM2_CONN_TIMEOUT\f[]
+.B \f[I]FI_PSM2_CONN_TIMEOUT\f[R]
 Timeout (seconds) for establishing connection between two PSM endpoints.
-.RS
-.RE
 .PP
 The default setting is 5.
 .TP
-.B \f[I]FI_PSM2_PROG_INTERVAL\f[]
+.B \f[I]FI_PSM2_PROG_INTERVAL\f[R]
 When auto progress is enabled (asked via the hints to
-\f[I]fi_getinfo\f[]), a progress thread is created to make progress
+\f[I]fi_getinfo\f[R]), a progress thread is created to make progress
 calls from time to time.
 This option set the interval (microseconds) between progress calls.
-.RS
-.RE
 .PP
 The default setting is 1 if affinity is set, or 1000 if not.
-See \f[I]FI_PSM2_PROG_AFFINITY\f[].
+See \f[I]FI_PSM2_PROG_AFFINITY\f[R].
 .TP
-.B \f[I]FI_PSM2_PROG_AFFINITY\f[]
+.B \f[I]FI_PSM2_PROG_AFFINITY\f[R]
 When set, specify the set of CPU cores to set the progress thread
 affinity to.
 The format is
-\f[C]<start>[:<end>[:<stride>]][,<start>[:<end>[:<stride>]]]*\f[], where
-each triplet \f[C]<start>:<end>:<stride>\f[] defines a block of
+\f[C]<start>[:<end>[:<stride>]][,<start>[:<end>[:<stride>]]]*\f[R],
+where each triplet \f[C]<start>:<end>:<stride>\f[R] defines a block of
 core_ids.
-Both \f[C]<start>\f[] and \f[C]<end>\f[] can be either the
-\f[C]core_id\f[] (when >=0) or \f[C]core_id\ \-\ num_cores\f[] (when
+Both \f[C]<start>\f[R] and \f[C]<end>\f[R] can be either the
+\f[C]core_id\f[R] (when >=0) or \f[C]core_id \- num_cores\f[R] (when
 <0).
-.RS
-.RE
 .PP
 By default affinity is not set.
 .TP
-.B \f[I]FI_PSM2_INJECT_SIZE\f[]
+.B \f[I]FI_PSM2_INJECT_SIZE\f[R]
 Maximum message size allowed for fi_inject and fi_tinject calls.
 This is an experimental feature to allow some applications to override
 default inject size limitation.
 When the inject size is larger than the default value, some inject calls
 might block.
-.RS
-.RE
 .PP
 The default setting is 64.
 .TP
-.B \f[I]FI_PSM2_LOCK_LEVEL\f[]
+.B \f[I]FI_PSM2_LOCK_LEVEL\f[R]
 When set, dictate the level of locking being used by the provider.
 Level 2 means all locks are enabled.
 Level 1 disables some locks and is suitable for runs that limit the
 access to each PSM2 context to a single thread.
 Level 0 disables all locks and thus is only suitable for single threaded
 runs.
-.RS
-.RE
 .PP
 To use level 0 or level 1, wait object and auto progress mode cannot be
 used because they introduce internal threads that may break the
@@ -278,7 +246,7 @@ conditions needed for these levels.
 .PP
 The default setting is 2.
 .TP
-.B \f[I]FI_PSM2_LAZY_CONN\f[]
+.B \f[I]FI_PSM2_LAZY_CONN\f[R]
 There are two strategies on when to establish connections between the
 PSM2 endpoints that OFI endpoints are built on top of.
 In eager connection mode, connections are established when addresses are
@@ -287,8 +255,6 @@ In lazy connection mode, connections are established when addresses are
 used the first time in communication.
 Eager connection mode has slightly lower critical path overhead but lazy
 connection mode scales better.
-.RS
-.RE
 .PP
 This option controls how the two connection modes are used.
 When set to 1, lazy connection mode is always used.
@@ -300,11 +266,11 @@ PSM2_MULTI_EP=0; and (2) the address vector type is FI_AV_MAP.
 .PP
 The default setting is 0.
 .TP
-.B \f[I]FI_PSM2_DISCONNECT\f[]
+.B \f[I]FI_PSM2_DISCONNECT\f[R]
 The provider has a mechanism to automatically send disconnection
 notifications to all connected peers before the local endpoint is
 closed.
-As the response, the peers call \f[I]psm2_ep_disconnect\f[] to clean up
+As the response, the peers call \f[I]psm2_ep_disconnect\f[R] to clean up
 the connection state at their side.
 This allows the same PSM2 epid be used by different dynamically started
 processes (clients) to communicate with the same peer (server).
@@ -312,8 +278,6 @@ This mechanism, however, introduce extra overhead to the finalization
 phase.
 For applications that never reuse epids within the same session such
 overhead is unnecessary.
-.RS
-.RE
 .PP
 This option controls whether the automatic disconnection notification
 mechanism should be enabled.
@@ -322,50 +286,46 @@ set this option to 1, but the server should set it to 0.
 .PP
 The default setting is 0.
 .TP
-.B \f[I]FI_PSM2_TAG_LAYOUT\f[]
+.B \f[I]FI_PSM2_TAG_LAYOUT\f[R]
 Select how the 96\-bit PSM2 tag bits are organized.
-Currently three choices are available: \f[I]tag60\f[] means 32\-4\-60
+Currently three choices are available: \f[I]tag60\f[R] means 32\-4\-60
 partitioning for CQ data, internal protocol flags, and application tag.
-\f[I]tag64\f[] means 4\-28\-64 partitioning for internal protocol flags,
-CQ data, and application tag.
-\f[I]auto\f[] means to choose either \f[I]tag60\f[] or \f[I]tag64\f[]
-based on the hints passed to fi_getinfo \-\- \f[I]tag60\f[] is used if
+\f[I]tag64\f[R] means 4\-28\-64 partitioning for internal protocol
+flags, CQ data, and application tag.
+\f[I]auto\f[R] means to choose either \f[I]tag60\f[R] or \f[I]tag64\f[R]
+based on the hints passed to fi_getinfo \[en] \f[I]tag60\f[R] is used if
 remote CQ data support is requested explicitly, either by passing
-non\-zero value via \f[I]hints\->domain_attr\->cq_data_size\f[] or by
-including \f[I]FI_REMOTE_CQ_DATA\f[] in \f[I]hints\->caps\f[], otherwise
-\f[I]tag64\f[] is used.
-If \f[I]tag64\f[] is the result of automatic selection,
-\f[I]fi_getinfo\f[] also returns a second instance of the provider with
-\f[I]tag60\f[] layout.
-.RS
-.RE
+non\-zero value via \f[I]hints\->domain_attr\->cq_data_size\f[R] or by
+including \f[I]FI_REMOTE_CQ_DATA\f[R] in \f[I]hints\->caps\f[R],
+otherwise \f[I]tag64\f[R] is used.
+If \f[I]tag64\f[R] is the result of automatic selection,
+\f[I]fi_getinfo\f[R] also returns a second instance of the provider with
+\f[I]tag60\f[R] layout.
 .PP
-The default setting is \f[I]auto\f[].
+The default setting is \f[I]auto\f[R].
 .PP
 Notice that if the provider is compiled with macro
-\f[I]PSMX2_TAG_LAYOUT\f[] defined to 1 (means \f[I]tag60\f[]) or 2
-(means \f[I]tag64\f[]), the choice is fixed at compile time and this
+\f[I]PSMX2_TAG_LAYOUT\f[R] defined to 1 (means \f[I]tag60\f[R]) or 2
+(means \f[I]tag64\f[R]), the choice is fixed at compile time and this
 runtime option will be disabled.
 .SH PSM2 EXTENSIONS
 .PP
-The \f[I]psm2\f[] provider supports limited low level parameter setting
+The \f[I]psm2\f[R] provider supports limited low level parameter setting
 through the fi_set_val() and fi_get_val() functions.
 Currently the following parameters can be set via the domain fid:
 \[bu] .RS 2
 .TP
 .B FI_PSM2_DISCONNECT *
-Overwite the global runtime parameter \f[I]FI_PSM2_DISCONNECT\f[] for
+Overwite the global runtime parameter \f[I]FI_PSM2_DISCONNECT\f[R] for
 this domain.
-See the \f[I]RUNTIME PARAMETERS\f[] section for details.
-.RS
-.RE
+See the \f[I]RUNTIME PARAMETERS\f[R] section for details.
 .RE
 .PP
 Valid parameter names are defined in the header file
-\f[I]rdma/fi_ext_psm2.h\f[].
+\f[I]rdma/fi_ext_psm2.h\f[R].
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_psm\f[](7),
-\f[C]fi_psm3\f[](7),
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_psm\f[R](7),
+\f[C]fi_psm3\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_psm3.7 b/man/man7/fi_psm3.7
index 86e4ce0..1bf0914 100644
--- a/man/man7/fi_psm3.7
+++ b/man/man7/fi_psm3.7
@@ -1,94 +1,86 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_psm3" "7" "2021\-02\-10" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_psm3" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
 fi_psm3 \- The PSM3 Fabric Provider
 .SH OVERVIEW
 .PP
-The \f[I]psm3\f[] provider implements a Performance Scaled Messaging
+The \f[I]psm3\f[R] provider implements a Performance Scaled Messaging
 capability which supports Intel RoCEv2 capable NICs.
 PSM3 represents an Ethernet and standard RoCEv2 enhancement of previous
 PSM implementations.
 .SH SUPPORTED FEATURES
 .PP
-The \f[I]psm3\f[] provider supports a subset of all the features defined
-in the libfabric API.
+The \f[I]psm3\f[R] provider supports a subset of all the features
+defined in the libfabric API.
 .TP
 .B Endpoint types
-Supports non\-connection based types \f[I]FI_DGRAM\f[] and
-\f[I]FI_RDM\f[].
-.RS
-.RE
+Supports non\-connection based types \f[I]FI_DGRAM\f[R] and
+\f[I]FI_RDM\f[R].
 .TP
 .B Endpoint capabilities
 Endpoints can support any combination of data transfer capabilities
-\f[I]FI_TAGGED\f[], \f[I]FI_MSG\f[], \f[I]FI_ATOMICS\f[], and
-\f[I]FI_RMA\f[].
-These capabilities can be further refined by \f[I]FI_SEND\f[],
-\f[I]FI_RECV\f[], \f[I]FI_READ\f[], \f[I]FI_WRITE\f[],
-\f[I]FI_REMOTE_READ\f[], and \f[I]FI_REMOTE_WRITE\f[] to limit the
+\f[I]FI_TAGGED\f[R], \f[I]FI_MSG\f[R], \f[I]FI_ATOMICS\f[R], and
+\f[I]FI_RMA\f[R].
+These capabilities can be further refined by \f[I]FI_SEND\f[R],
+\f[I]FI_RECV\f[R], \f[I]FI_READ\f[R], \f[I]FI_WRITE\f[R],
+\f[I]FI_REMOTE_READ\f[R], and \f[I]FI_REMOTE_WRITE\f[R] to limit the
 direction of operations.
-.RS
-.RE
 .PP
-\f[I]FI_MULTI_RECV\f[] is supported for non\-tagged message queue only.
+\f[I]FI_MULTI_RECV\f[R] is supported for non\-tagged message queue only.
 .PP
 Scalable endpoints are supported if the underlying PSM3 library supports
 multiple endpoints.
 This condition must be satisfied both when the provider is built and
 when the provider is used.
-See the \f[I]Scalable endpoints\f[] section for more information.
+See the \f[I]Scalable endpoints\f[R] section for more information.
 .PP
-Other supported capabilities include \f[I]FI_TRIGGER\f[],
-\f[I]FI_REMOTE_CQ_DATA\f[], \f[I]FI_RMA_EVENT\f[], \f[I]FI_SOURCE\f[],
-and \f[I]FI_SOURCE_ERR\f[].
-Furthermore, \f[I]FI_NAMED_RX_CTX\f[] is supported when scalable
+Other supported capabilities include \f[I]FI_TRIGGER\f[R],
+\f[I]FI_REMOTE_CQ_DATA\f[R], \f[I]FI_RMA_EVENT\f[R],
+\f[I]FI_SOURCE\f[R], and \f[I]FI_SOURCE_ERR\f[R].
+Furthermore, \f[I]FI_NAMED_RX_CTX\f[R] is supported when scalable
 endpoints are enabled.
 .TP
 .B Modes
-\f[I]FI_CONTEXT\f[] is required for the \f[I]FI_TAGGED\f[] and
-\f[I]FI_MSG\f[] capabilities.
+\f[I]FI_CONTEXT\f[R] is required for the \f[I]FI_TAGGED\f[R] and
+\f[I]FI_MSG\f[R] capabilities.
 That means, any request belonging to these two categories that generates
 a completion must pass as the operation context a valid pointer to type
-\f[I]struct fi_context\f[], and the space referenced by the pointer must
-remain untouched until the request has completed.
-If none of \f[I]FI_TAGGED\f[] and \f[I]FI_MSG\f[] is asked for, the
-\f[I]FI_CONTEXT\f[] mode is not required.
-.RS
-.RE
+\f[I]struct fi_context\f[R], and the space referenced by the pointer
+must remain untouched until the request has completed.
+If none of \f[I]FI_TAGGED\f[R] and \f[I]FI_MSG\f[R] is asked for, the
+\f[I]FI_CONTEXT\f[R] mode is not required.
 .TP
 .B Progress
-The \f[I]psm3\f[] provider performs optimal with manual progress.
-By default, the application is expected to call \f[I]fi_cq_read\f[] or
-\f[I]fi_cntr_read\f[] function from time to time when no other libfabric
-function is called to ensure progress is made in a timely manner.
+The \f[I]psm3\f[R] provider performs optimal with manual progress.
+By default, the application is expected to call \f[I]fi_cq_read\f[R] or
+\f[I]fi_cntr_read\f[R] function from time to time when no other
+libfabric function is called to ensure progress is made in a timely
+manner.
 The provider does support auto progress mode.
 However, the performance can be significantly impacted if the
 application purely depends on the provider to make auto progress.
-.RS
-.RE
 .TP
 .B Scalable endpoints
 Scalable endpoints support depends on the multi\-EP feature of the
-\f[I]PSM3\f[] library.
-If the \f[I]PSM3\f[] library supports this feature, the availability is
-further controlled by an environment variable \f[I]PSM3_MULTI_EP\f[].
-The \f[I]psm3\f[] provider automatically sets this variable to 1 if it
+\f[I]PSM3\f[R] library.
+If the \f[I]PSM3\f[R] library supports this feature, the availability is
+further controlled by an environment variable \f[I]PSM3_MULTI_EP\f[R].
+The \f[I]psm3\f[R] provider automatically sets this variable to 1 if it
 is not set.
-The feature can be disabled explicitly by setting \f[I]PSM3_MULTI_EP\f[]
-to 0.
-.RS
-.RE
+The feature can be disabled explicitly by setting
+\f[I]PSM3_MULTI_EP\f[R] to 0.
 .PP
 When creating a scalable endpoint, the exact number of contexts
-requested should be set in the "fi_info" structure passed to the
-\f[I]fi_scalable_ep\f[] function.
-This number should be set in "fi_info\->ep_attr\->tx_ctx_cnt" or
-"fi_info\->ep_attr\->rx_ctx_cnt" or both, whichever greater is used.
-The \f[I]psm3\f[] provider allocates all requested contexts upfront when
-the scalable endpoint is created.
+requested should be set in the \[lq]fi_info\[rq] structure passed to the
+\f[I]fi_scalable_ep\f[R] function.
+This number should be set in \[lq]fi_info\->ep_attr\->tx_ctx_cnt\[rq] or
+\[lq]fi_info\->ep_attr\->rx_ctx_cnt\[rq] or both, whichever greater is
+used.
+The \f[I]psm3\f[R] provider allocates all requested contexts upfront
+when the scalable endpoint is created.
 The same context is used for both Tx and Rx.
 .PP
 For optimal performance, it is advised to avoid having multiple threads
@@ -101,26 +93,24 @@ supported.
 Instead, individual tx context or rx context of the scalable endpoint
 should be used.
 Similarly, using the address of the scalable endpoint as the source
-address or destination address doesn\[aq]t collectively address all the
+address or destination address doesn\[cq]t collectively address all the
 tx/rx contexts.
 It addresses only the first tx/rx context, instead.
 .SH LIMITATIONS
 .PP
-The \f[I]psm3\f[] provider doesn\[aq]t support all the features defined
+The \f[I]psm3\f[R] provider doesn\[cq]t support all the features defined
 in the libfabric API.
 Here are some of the limitations not listed above:
 .TP
 .B Unsupported features
 These features are unsupported: connection management, passive endpoint,
 and shared receive context.
-.RS
-.RE
 .SH RUNTIME PARAMETERS
 .PP
-The \f[I]psm3\f[] provider checks for the following environment
+The \f[I]psm3\f[R] provider checks for the following environment
 variables:
 .TP
-.B \f[I]FI_PSM3_UUID\f[]
+.B \f[I]FI_PSM3_UUID\f[R]
 PSM requires that each job has a unique ID (UUID).
 All the processes in the same job need to use the same UUID in order to
 be able to talk to each other.
@@ -128,43 +118,39 @@ The PSM reference manual advises to keep UUID unique to each job.
 In practice, it generally works fine to reuse UUID as long as (1) no two
 jobs with the same UUID are running at the same time; and (2) previous
 jobs with the same UUID have exited normally.
-If running into "resource busy" or "connection failure" issues with
-unknown reason, it is advisable to manually set the UUID to a value
-different from the default.
-.RS
-.RE
+If running into \[lq]resource busy\[rq] or \[lq]connection failure\[rq]
+issues with unknown reason, it is advisable to manually set the UUID to
+a value different from the default.
 .PP
 The default UUID is 00FF00FF\-0000\-0000\-0000\-00FF0F0F00FF.
 .PP
 It is possible to create endpoints with UUID different from the one set
 here.
-To achieve that, set \[aq]info\->ep_attr\->auth_key\[aq] to the uuid
-value and \[aq]info\->ep_attr\->auth_key_size\[aq] to its size (16
-bytes) when calling fi_endpoint() or fi_scalable_ep().
+To achieve that, set `info\->ep_attr\->auth_key' to the uuid value and
+`info\->ep_attr\->auth_key_size' to its size (16 bytes) when calling
+fi_endpoint() or fi_scalable_ep().
 It is still true that an endpoint can only communicate with endpoints
 with the same UUID.
 .TP
-.B \f[I]FI_PSM3_NAME_SERVER\f[]
-The \f[I]psm3\f[] provider has a simple built\-in name server that can
+.B \f[I]FI_PSM3_NAME_SERVER\f[R]
+The \f[I]psm3\f[R] provider has a simple built\-in name server that can
 be used to resolve an IP address or host name into a transport address
-needed by the \f[I]fi_av_insert\f[] call.
+needed by the \f[I]fi_av_insert\f[R] call.
 The main purpose of this name server is to allow simple client\-server
-type applications (such as those in \f[I]fabtests\f[]) to be written
+type applications (such as those in \f[I]fabtests\f[R]) to be written
 purely with libfabric, without using any out\-of\-band communication
 mechanism.
 For such applications, the server would run first to allow endpoints be
 created and registered with the name server, and then the client would
-call \f[I]fi_getinfo\f[] with the \f[I]node\f[] parameter set to the IP
-address or host name of the server.
-The resulting \f[I]fi_info\f[] structure would have the transport
-address of the endpoint created by the server in the \f[I]dest_addr\f[]
+call \f[I]fi_getinfo\f[R] with the \f[I]node\f[R] parameter set to the
+IP address or host name of the server.
+The resulting \f[I]fi_info\f[R] structure would have the transport
+address of the endpoint created by the server in the \f[I]dest_addr\f[R]
 field.
-Optionally the \f[I]service\f[] parameter can be used in addition to
-\f[I]node\f[].
-Notice that the \f[I]service\f[] number is interpreted by the provider
+Optionally the \f[I]service\f[R] parameter can be used in addition to
+\f[I]node\f[R].
+Notice that the \f[I]service\f[R] number is interpreted by the provider
 and is not a TCP/IP port number.
-.RS
-.RE
 .PP
 The name server is on by default.
 It can be turned off by setting the variable to 0.
@@ -174,93 +160,77 @@ created when the name server is on.
 The provider detects OpenMPI and MPICH runs and changes the default
 setting to off.
 .TP
-.B \f[I]FI_PSM3_TAGGED_RMA\f[]
+.B \f[I]FI_PSM3_TAGGED_RMA\f[R]
 The RMA functions are implemented on top of the PSM Active Message
 functions.
 The Active Message functions have limit on the size of data can be
 transferred in a single message.
 Large transfers can be divided into small chunks and be pipe\-lined.
 However, the bandwidth is sub\-optimal by doing this way.
-.RS
-.RE
 .PP
-The \f[I]psm3\f[] provider use PSM tag\-matching message queue functions
-to achieve higher bandwidth for large size RMA.
+The \f[I]psm3\f[R] provider use PSM tag\-matching message queue
+functions to achieve higher bandwidth for large size RMA.
 It takes advantage of the extra tag bits available in PSM3 to separate
 the RMA traffic from the regular tagged message queue.
 .PP
 The option is on by default.
 To turn it off set the variable to 0.
 .TP
-.B \f[I]FI_PSM3_DELAY\f[]
+.B \f[I]FI_PSM3_DELAY\f[R]
 Time (seconds) to sleep before closing PSM endpoints.
 This is a workaround for a bug in some versions of PSM library.
-.RS
-.RE
 .PP
 The default setting is 0.
 .TP
-.B \f[I]FI_PSM3_TIMEOUT\f[]
+.B \f[I]FI_PSM3_TIMEOUT\f[R]
 Timeout (seconds) for gracefully closing PSM endpoints.
 A forced closing will be issued if timeout expires.
-.RS
-.RE
 .PP
 The default setting is 5.
 .TP
-.B \f[I]FI_PSM3_CONN_TIMEOUT\f[]
+.B \f[I]FI_PSM3_CONN_TIMEOUT\f[R]
 Timeout (seconds) for establishing connection between two PSM endpoints.
-.RS
-.RE
 .PP
 The default setting is 5.
 .TP
-.B \f[I]FI_PSM3_PROG_INTERVAL\f[]
+.B \f[I]FI_PSM3_PROG_INTERVAL\f[R]
 When auto progress is enabled (asked via the hints to
-\f[I]fi_getinfo\f[]), a progress thread is created to make progress
+\f[I]fi_getinfo\f[R]), a progress thread is created to make progress
 calls from time to time.
 This option set the interval (microseconds) between progress calls.
-.RS
-.RE
 .PP
 The default setting is 1 if affinity is set, or 1000 if not.
-See \f[I]FI_PSM3_PROG_AFFINITY\f[].
+See \f[I]FI_PSM3_PROG_AFFINITY\f[R].
 .TP
-.B \f[I]FI_PSM3_PROG_AFFINITY\f[]
+.B \f[I]FI_PSM3_PROG_AFFINITY\f[R]
 When set, specify the set of CPU cores to set the progress thread
 affinity to.
 The format is
-\f[C]<start>[:<end>[:<stride>]][,<start>[:<end>[:<stride>]]]*\f[], where
-each triplet \f[C]<start>:<end>:<stride>\f[] defines a block of
+\f[C]<start>[:<end>[:<stride>]][,<start>[:<end>[:<stride>]]]*\f[R],
+where each triplet \f[C]<start>:<end>:<stride>\f[R] defines a block of
 core_ids.
-Both \f[C]<start>\f[] and \f[C]<end>\f[] can be either the
-\f[C]core_id\f[] (when >=0) or \f[C]core_id\ \-\ num_cores\f[] (when
+Both \f[C]<start>\f[R] and \f[C]<end>\f[R] can be either the
+\f[C]core_id\f[R] (when >=0) or \f[C]core_id \- num_cores\f[R] (when
 <0).
-.RS
-.RE
 .PP
 By default affinity is not set.
 .TP
-.B \f[I]FI_PSM3_INJECT_SIZE\f[]
+.B \f[I]FI_PSM3_INJECT_SIZE\f[R]
 Maximum message size allowed for fi_inject and fi_tinject calls.
 This is an experimental feature to allow some applications to override
 default inject size limitation.
 When the inject size is larger than the default value, some inject calls
 might block.
-.RS
-.RE
 .PP
 The default setting is 64.
 .TP
-.B \f[I]FI_PSM3_LOCK_LEVEL\f[]
+.B \f[I]FI_PSM3_LOCK_LEVEL\f[R]
 When set, dictate the level of locking being used by the provider.
 Level 2 means all locks are enabled.
 Level 1 disables some locks and is suitable for runs that limit the
 access to each PSM3 context to a single thread.
 Level 0 disables all locks and thus is only suitable for single threaded
 runs.
-.RS
-.RE
 .PP
 To use level 0 or level 1, wait object and auto progress mode cannot be
 used because they introduce internal threads that may break the
@@ -268,7 +238,7 @@ conditions needed for these levels.
 .PP
 The default setting is 2.
 .TP
-.B \f[I]FI_PSM3_LAZY_CONN\f[]
+.B \f[I]FI_PSM3_LAZY_CONN\f[R]
 There are two strategies on when to establish connections between the
 PSM3 endpoints that OFI endpoints are built on top of.
 In eager connection mode, connections are established when addresses are
@@ -277,8 +247,6 @@ In lazy connection mode, connections are established when addresses are
 used the first time in communication.
 Eager connection mode has slightly lower critical path overhead but lazy
 connection mode scales better.
-.RS
-.RE
 .PP
 This option controls how the two connection modes are used.
 When set to 1, lazy connection mode is always used.
@@ -290,11 +258,11 @@ PSM3_MULTI_EP=0; and (2) the address vector type is FI_AV_MAP.
 .PP
 The default setting is 0.
 .TP
-.B \f[I]FI_PSM3_DISCONNECT\f[]
+.B \f[I]FI_PSM3_DISCONNECT\f[R]
 The provider has a mechanism to automatically send disconnection
 notifications to all connected peers before the local endpoint is
 closed.
-As the response, the peers call \f[I]psm3_ep_disconnect\f[] to clean up
+As the response, the peers call \f[I]psm3_ep_disconnect\f[R] to clean up
 the connection state at their side.
 This allows the same PSM3 epid be used by different dynamically started
 processes (clients) to communicate with the same peer (server).
@@ -302,8 +270,6 @@ This mechanism, however, introduce extra overhead to the finalization
 phase.
 For applications that never reuse epids within the same session such
 overhead is unnecessary.
-.RS
-.RE
 .PP
 This option controls whether the automatic disconnection notification
 mechanism should be enabled.
@@ -312,33 +278,31 @@ set this option to 1, but the server should set it to 0.
 .PP
 The default setting is 0.
 .TP
-.B \f[I]FI_PSM3_TAG_LAYOUT\f[]
+.B \f[I]FI_PSM3_TAG_LAYOUT\f[R]
 Select how the 96\-bit PSM3 tag bits are organized.
-Currently three choices are available: \f[I]tag60\f[] means 32\-4\-60
+Currently three choices are available: \f[I]tag60\f[R] means 32\-4\-60
 partitioning for CQ data, internal protocol flags, and application tag.
-\f[I]tag64\f[] means 4\-28\-64 partitioning for internal protocol flags,
-CQ data, and application tag.
-\f[I]auto\f[] means to choose either \f[I]tag60\f[] or \f[I]tag64\f[]
-based on the hints passed to fi_getinfo \-\- \f[I]tag60\f[] is used if
+\f[I]tag64\f[R] means 4\-28\-64 partitioning for internal protocol
+flags, CQ data, and application tag.
+\f[I]auto\f[R] means to choose either \f[I]tag60\f[R] or \f[I]tag64\f[R]
+based on the hints passed to fi_getinfo \[en] \f[I]tag60\f[R] is used if
 remote CQ data support is requested explicitly, either by passing
-non\-zero value via \f[I]hints\->domain_attr\->cq_data_size\f[] or by
-including \f[I]FI_REMOTE_CQ_DATA\f[] in \f[I]hints\->caps\f[], otherwise
-\f[I]tag64\f[] is used.
-If \f[I]tag64\f[] is the result of automatic selection,
-\f[I]fi_getinfo\f[] also returns a second instance of the provider with
-\f[I]tag60\f[] layout.
-.RS
-.RE
+non\-zero value via \f[I]hints\->domain_attr\->cq_data_size\f[R] or by
+including \f[I]FI_REMOTE_CQ_DATA\f[R] in \f[I]hints\->caps\f[R],
+otherwise \f[I]tag64\f[R] is used.
+If \f[I]tag64\f[R] is the result of automatic selection,
+\f[I]fi_getinfo\f[R] also returns a second instance of the provider with
+\f[I]tag60\f[R] layout.
 .PP
-The default setting is \f[I]auto\f[].
+The default setting is \f[I]auto\f[R].
 .PP
 Notice that if the provider is compiled with macro
-\f[I]PSMX3_TAG_LAYOUT\f[] defined to 1 (means \f[I]tag60\f[]) or 2
-(means \f[I]tag64\f[]), the choice is fixed at compile time and this
+\f[I]PSMX3_TAG_LAYOUT\f[R] defined to 1 (means \f[I]tag60\f[R]) or 2
+(means \f[I]tag64\f[R]), the choice is fixed at compile time and this
 runtime option will be disabled.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_psm\f[](7),
-\f[C]fi_psm2\f[](7),
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_psm\f[R](7),
+\f[C]fi_psm2\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_rstream.7 b/man/man7/fi_rstream.7
index ac3f94c..5622678 100644
--- a/man/man7/fi_rstream.7
+++ b/man/man7/fi_rstream.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_rstream" "7" "2020\-04\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_rstream" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -19,38 +19,28 @@ For messaging completions, use FI_PEEK on send/recv after poll to see
 what type of transaction has transpired.
 .SH SUPPORTED FEATURES
 .PP
-The rstream provider currently supports \f[I]FI_MSG\f[] capabilities.
+The rstream provider currently supports \f[I]FI_MSG\f[R] capabilities.
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports only endpoint type \f[I]FI_EP_SOCK_STREAM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports only endpoint type \f[I]FI_EP_SOCK_STREAM\f[R].
 .PP
-\f[I]Endpoint capabilities\f[] : The following data transfer interface
-is supported: \f[I]fi_msg\f[].
+\f[I]Endpoint capabilities\f[R] : The following data transfer interface
+is supported: \f[I]fi_msg\f[R].
 .TP
-.B \f[I]Modes\f[]
+.B \f[I]Modes\f[R]
 The provider does not require the use of any mode bits but supports core
 providers that require FI_CONTEXT and FI_RX_CQ_DATA.
-.RS
-.RE
 .TP
-.B \f[I]Progress\f[]
-The rstream provider only supports \f[I]FI_PROGRESS_MANUAL\f[].
-.RS
-.RE
+.B \f[I]Progress\f[R]
+The rstream provider only supports \f[I]FI_PROGRESS_MANUAL\f[R].
 .TP
-.B \f[I]Threading Model\f[]
+.B \f[I]Threading Model\f[R]
 The provider supports FI_THREAD_SAFE
-.RS
-.RE
 .TP
-.B \f[I]Verbs\-iWarp\f[]
+.B \f[I]Verbs\-iWarp\f[R]
 The provider has added features to enable iWarp.
 To use this feature, the ep protocol iWarp must be requested in an
 fi_getinfo call.
-.RS
-.RE
 .SH LIMITATIONS
 .PP
 The rstream provider is experimental and lacks performance validation
@@ -64,32 +54,24 @@ memory region size and CQ size).
 These can be modified by fi_setopt.
 .SH SETTINGS
 .PP
-The \f[I]rstream\f[] provider settings can be modified via fi_setopt on
+The \f[I]rstream\f[R] provider settings can be modified via fi_setopt on
 the endpoint (FI_OPT_ENDPOINT) along with the following parameters:
 .TP
-.B \f[I]FI_OPT_SEND_BUF_SIZE\f[]
+.B \f[I]FI_OPT_SEND_BUF_SIZE\f[R]
 Size of the send buffer.
 Default is 32KB.
-.RS
-.RE
 .TP
-.B \f[I]FI_OPT_RECV_BUF_SIZE\f[]
+.B \f[I]FI_OPT_RECV_BUF_SIZE\f[R]
 Size of the recv buffer.
 Default is 32KB.
-.RS
-.RE
 .TP
-.B \f[I]FI_OPT_TX_SIZE\f[]
+.B \f[I]FI_OPT_TX_SIZE\f[R]
 Size of the send queue.
 Default is 384.
-.RS
-.RE
 .TP
-.B \f[I]FI_OPT_RX_SIZE\f[]
+.B \f[I]FI_OPT_RX_SIZE\f[R]
 Size of the recv queue.
 Default is 384.
-.RS
-.RE
 .SH OFI EXTENSIONS
 .PP
 The rstream provider has extended the current OFI API set in order to
@@ -98,6 +80,6 @@ Specifically sendmsg(FI_PEEK) is supported which replicates the behavior
 of the recvmsg(FI_PEEK) feature.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_rxd.7 b/man/man7/fi_rxd.7
index d4723ec..2732693 100644
--- a/man/man7/fi_rxd.7
+++ b/man/man7/fi_rxd.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_rxd" "7" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_rxd" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -11,26 +11,20 @@ The RxD provider is a utility provider that supports RDM endpoints
 emulated over a base DGRAM provider.
 .SH SUPPORTED FEATURES
 .PP
-The RxD provider currently supports \f[I]FI_MSG\f[] capabilities.
+The RxD provider currently supports \f[I]FI_MSG\f[R] capabilities.
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports only endpoint type \f[I]FI_EP_RDM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports only endpoint type \f[I]FI_EP_RDM\f[R].
 .PP
-\f[I]Endpoint capabilities\f[] : The following data transfer interface
-is supported: \f[I]fi_msg\f[].
+\f[I]Endpoint capabilities\f[R] : The following data transfer interface
+is supported: \f[I]fi_msg\f[R].
 .TP
-.B \f[I]Modes\f[]
+.B \f[I]Modes\f[R]
 The provider does not require the use of any mode bits but supports core
 DGRAM providers that require FI_CONTEXT and FI_MSG_PREFIX.
-.RS
-.RE
 .TP
-.B \f[I]Progress\f[]
-The RxD provider only supports \f[I]FI_PROGRESS_MANUAL\f[].
-.RS
-.RE
+.B \f[I]Progress\f[R]
+The RxD provider only supports \f[I]FI_PROGRESS_MANUAL\f[R].
 .SH LIMITATIONS
 .PP
 The RxD provider has hard\-coded maximums for supported queue sizes and
@@ -45,36 +39,28 @@ The RxD provider is still under development and is not extensively
 tested.
 .SH RUNTIME PARAMETERS
 .PP
-The \f[I]rxd\f[] provider checks for the following environment
+The \f[I]rxd\f[R] provider checks for the following environment
 variables:
 .TP
-.B \f[I]FI_OFI_RXD_SPIN_COUNT\f[]
-Number of times to read the core provider\[aq]s CQ for a segment
+.B \f[I]FI_OFI_RXD_SPIN_COUNT\f[R]
+Number of times to read the core provider\[cq]s CQ for a segment
 completion before trying to progress sends.
 Default is 1000.
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXD_RETRY\f[]
+.B \f[I]FI_OFI_RXD_RETRY\f[R]
 Toggles retrying of packets and assumes reliability of individual
 packets and will reassemble all received packets.
 Retrying is turned on by default.
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXD_MAX_PEERS\f[]
+.B \f[I]FI_OFI_RXD_MAX_PEERS\f[R]
 Maximum number of peers the provider should prepare to track.
 Default: 1024
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXD_MAX_UNACKED\f[]
+.B \f[I]FI_OFI_RXD_MAX_UNACKED\f[R]
 Maximum number of packets (per peer) to send at a time.
 Default: 128
-.RS
-.RE
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_rxm.7 b/man/man7/fi_rxm.7
index 72bb391..209b644 100644
--- a/man/man7/fi_rxm.7
+++ b/man/man7/fi_rxm.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_rxm" "7" "2021\-01\-25" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_rxm" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -14,7 +14,7 @@ FI_EP_RDM endpoints have a reliable datagram interface and RxM emulates
 this by hiding the connection management of underlying FI_EP_MSG
 endpoints from the user.
 Additionally, RxM can hide memory registration requirement from a core
-provider like verbs if the apps don\[aq]t support it.
+provider like verbs if the apps don\[cq]t support it.
 .SH REQUIREMENTS
 .SS Requirements for core provider
 .PP
@@ -41,38 +41,29 @@ Not doing so would result in a stall.
 See also the ERRORS section in fi_msg(3).
 .SH SUPPORTED FEATURES
 .PP
-The RxM provider currently supports \f[I]FI_MSG\f[], \f[I]FI_TAGGED\f[],
-\f[I]FI_RMA\f[] and \f[I]FI_ATOMIC\f[] capabilities.
+The RxM provider currently supports \f[I]FI_MSG\f[R],
+\f[I]FI_TAGGED\f[R], \f[I]FI_RMA\f[R] and \f[I]FI_ATOMIC\f[R]
+capabilities.
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports only \f[I]FI_EP_RDM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports only \f[I]FI_EP_RDM\f[R].
 .TP
-.B \f[I]Endpoint capabilities\f[]
-The following data transfer interface is supported: \f[I]FI_MSG\f[],
-\f[I]FI_TAGGED\f[], \f[I]FI_RMA\f[], \f[I]FI_ATOMIC\f[].
-.RS
-.RE
+.B \f[I]Endpoint capabilities\f[R]
+The following data transfer interface is supported: \f[I]FI_MSG\f[R],
+\f[I]FI_TAGGED\f[R], \f[I]FI_RMA\f[R], \f[I]FI_ATOMIC\f[R].
 .TP
-.B \f[I]Progress\f[]
-The RxM provider supports both \f[I]FI_PROGRESS_MANUAL\f[] and
-\f[I]FI_PROGRESS_AUTO\f[].
+.B \f[I]Progress\f[R]
+The RxM provider supports both \f[I]FI_PROGRESS_MANUAL\f[R] and
+\f[I]FI_PROGRESS_AUTO\f[R].
 Manual progress in general has better connection scale\-up and lower CPU
-utilization since there\[aq]s no separate auto\-progress thread.
-.RS
-.RE
+utilization since there\[cq]s no separate auto\-progress thread.
 .TP
-.B \f[I]Addressing Formats\f[]
+.B \f[I]Addressing Formats\f[R]
 FI_SOCKADDR, FI_SOCKADDR_IN
-.RS
-.RE
 .TP
-.B \f[I]Memory Region\f[]
+.B \f[I]Memory Region\f[R]
 FI_MR_VIRT_ADDR, FI_MR_ALLOCATED, FI_MR_PROV_KEY MR mode bits would be
 required from the app in case the core provider requires it.
-.RS
-.RE
 .SH LIMITATIONS
 .PP
 When using RxM provider, some limitations from the underlying MSG
@@ -126,29 +117,25 @@ supported.
 .SS Miscellaneous limitations
 .IP \[bu] 2
 RxM protocol peers should have same endian\-ness otherwise connections
-won\[aq]t successfully complete.
+won\[cq]t successfully complete.
 This enables better performance at run\-time as byte order translations
 are avoided.
 .SH RUNTIME PARAMETERS
 .PP
 The ofi_rxm provider checks for the following environment variables.
 .TP
-.B \f[I]FI_OFI_RXM_BUFFER_SIZE\f[]
+.B \f[I]FI_OFI_RXM_BUFFER_SIZE\f[R]
 Defines the transmit buffer size / inject size.
 Messages of size less than this would be transmitted via an eager
 protocol and those above would be transmitted via a rendezvous or SAR
 (Segmentation And Reassembly) protocol.
-Transmit data would be copied up to this size (default: ~16k).
-.RS
-.RE
+Transmit data would be copied up to this size (default: \[ti]16k).
 .TP
-.B \f[I]FI_OFI_RXM_COMP_PER_PROGRESS\f[]
+.B \f[I]FI_OFI_RXM_COMP_PER_PROGRESS\f[R]
 Defines the maximum number of MSG provider CQ entries (default: 1) that
 would be read per progress (RxM CQ read).
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_ENABLE_DYN_RBUF\f[]
+.B \f[I]FI_OFI_RXM_ENABLE_DYN_RBUF\f[R]
 Enables support for dynamic receive buffering, if available by the
 message endpoint provider.
 This feature allows direct placement of received message data into
@@ -156,67 +143,47 @@ application buffers, bypassing RxM bounce buffers.
 This feature targets providers that provide internal network buffering,
 such as the tcp provider.
 (default: false)
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_SAR_LIMIT\f[]
+.B \f[I]FI_OFI_RXM_SAR_LIMIT\f[R]
 Set this environment variable to control the RxM SAR (Segmentation And
 Reassembly) protocol.
 Messages of size greater than this (default: 128 Kb) would be
 transmitted via rendezvous protocol.
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_USE_SRX\f[]
+.B \f[I]FI_OFI_RXM_USE_SRX\f[R]
 Set this to 1 to use shared receive context from MSG provider, or 0 to
 disable using shared receive context.
 Shared receive contexts reduce overall memory usage, but may increase in
 message latency.
 If not set, verbs will not use shared receive contexts by default, but
 the tcp provider will.
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_TX_SIZE\f[]
+.B \f[I]FI_OFI_RXM_TX_SIZE\f[R]
 Defines default TX context size (default: 1024)
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_RX_SIZE\f[]
+.B \f[I]FI_OFI_RXM_RX_SIZE\f[R]
 Defines default RX context size (default: 1024)
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_MSG_TX_SIZE\f[]
+.B \f[I]FI_OFI_RXM_MSG_TX_SIZE\f[R]
 Defines FI_EP_MSG TX size that would be requested (default: 128).
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_MSG_RX_SIZE\f[]
+.B \f[I]FI_OFI_RXM_MSG_RX_SIZE\f[R]
 Defines FI_EP_MSG RX size that would be requested (default: 128).
-.RS
-.RE
 .TP
-.B \f[I]FI_UNIVERSE_SIZE\f[]
+.B \f[I]FI_UNIVERSE_SIZE\f[R]
 Defines the expected number of ranks / peers an endpoint would
 communicate with (default: 256).
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_CM_PROGRESS_INTERVAL\f[]
+.B \f[I]FI_OFI_RXM_CM_PROGRESS_INTERVAL\f[R]
 Defines the duration of time in microseconds between calls to RxM CM
 progression functions when using manual progress.
 Higher values may provide less noise for calls to fi_cq read functions,
 but may increase connection setup time (default: 10000)
-.RS
-.RE
 .TP
-.B \f[I]FI_OFI_RXM_CQ_EQ_FAIRNESS\f[]
+.B \f[I]FI_OFI_RXM_CQ_EQ_FAIRNESS\f[R]
 Defines the maximum number of message provider CQ entries that can be
 consecutively read across progress calls without checking to see if the
 CM progress interval has been reached (default: 128)
-.RS
-.RE
 .SH Tuning
 .SS Bandwidth
 .PP
@@ -237,7 +204,8 @@ to only required values.
 .PP
 The data transfer API may return \-FI_EAGAIN during on\-demand
 connection setup of the core provider FI_MSG_EP.
-See \f[C]fi_msg\f[](3) for a detailed description of handling FI_EAGAIN.
+See \f[C]fi_msg\f[R](3) for a detailed description of handling
+FI_EAGAIN.
 .SH Troubleshooting / Known issues
 .PP
 If an RxM endpoint is expected to communicate with more peers than the
@@ -253,6 +221,6 @@ The workaround is to use shared receive contexts for the MSG provider
 (FI_OFI_RXM_MSG_TX_SIZE / FI_OFI_RXM_MSG_RX_SIZE).
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_shm.7 b/man/man7/fi_shm.7
index ed04e5a..43eede7 100644
--- a/man/man7/fi_shm.7
+++ b/man/man7/fi_shm.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_shm" "7" "2020\-04\-17" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_shm" "7" "2021\-04\-20" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -17,29 +17,23 @@ between processes on the same system.
 This release contains an initial implementation of the SHM provider that
 offers the following support:
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports only endpoint type \f[I]FI_EP_RDM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports only endpoint type \f[I]FI_EP_RDM\f[R].
 .TP
-.B \f[I]Endpoint capabilities\f[]
+.B \f[I]Endpoint capabilities\f[R]
 Endpoints cna support any combinations of the following data transfer
-capabilities: \f[I]FI_MSG\f[], \f[I]FI_TAGGED\f[], \f[I]FI_RMA\f[], amd
-\f[I]FI_ATOMICS\f[].
-These capabilities can be further defined by \f[I]FI_SEND\f[],
-\f[I]FI_RECV\f[], \f[I]FI_READ\f[], \f[I]FI_WRITE\f[],
-\f[I]FI_REMOTE_READ\f[], and \f[I]FI_REMOTE_WRITE\f[] to limit the
+capabilities: \f[I]FI_MSG\f[R], \f[I]FI_TAGGED\f[R], \f[I]FI_RMA\f[R],
+amd \f[I]FI_ATOMICS\f[R].
+These capabilities can be further defined by \f[I]FI_SEND\f[R],
+\f[I]FI_RECV\f[R], \f[I]FI_READ\f[R], \f[I]FI_WRITE\f[R],
+\f[I]FI_REMOTE_READ\f[R], and \f[I]FI_REMOTE_WRITE\f[R] to limit the
 direction of operations.
-.RS
-.RE
 .TP
-.B \f[I]Modes\f[]
+.B \f[I]Modes\f[R]
 The provider does not require the use of any mode bits.
-.RS
-.RE
 .TP
-.B \f[I]Progress\f[]
-The SHM provider supports \f[I]FI_PROGRESS_MANUAL\f[].
+.B \f[I]Progress\f[R]
+The SHM provider supports \f[I]FI_PROGRESS_MANUAL\f[R].
 Receive side data buffers are not modified outside of completion
 processing routines.
 The provider processes messages using three different methods, based on
@@ -48,70 +42,69 @@ For messages smaller than 4096 bytes, tx completions are generated
 immediately after the send.
 For larger messages, tx completions are not generated until the
 receiving side has processed the message.
-.RS
-.RE
 .TP
-.B \f[I]Address Format\f[]
+.B \f[I]Address Format\f[R]
 The SHM provider uses the address format FI_ADDR_STR, which follows the
-general format pattern "[prefix]://[addr]".
+general format pattern \[lq][prefix]://[addr]\[rq].
 The application can provide addresses through the node or hints
 parameter.
 As long as the address is in a valid FI_ADDR_STR format (contains
-"://"), the address will be used as is.
+\[lq]://\[rq]), the address will be used as is.
 If the application input is incorrectly formatted or no input was
 provided, the SHM provider will resolve it according to the following
 SHM provider standards:
-.RS
-.RE
 .PP
 (flags & FI_SOURCE) ?
-src_addr : dest_addr = \- if (node && service) : "fi_ns://node:service"
-\- if (service) : "fi_ns://service" \- if (node && !service) :
-"fi_shm://node" \- if (!node && !service) : "fi_shm://PID"
+src_addr : dest_addr = \- if (node && service) :
+\[lq]fi_ns://node:service\[rq] \- if (service) :
+\[lq]fi_ns://service\[rq] \- if (node && !service) :
+\[lq]fi_shm://node\[rq] \- if (!node && !service) :
+\[lq]fi_shm://PID\[rq]
 .PP
-!(flags & FI_SOURCE) \- src_addr = "fi_shm://PID"
+!(flags & FI_SOURCE) \- src_addr = \[lq]fi_shm://PID\[rq]
 .PP
 In other words, if the application provides a source and/or destination
-address in an acceptable FI_ADDR_STR format (contains "://"), the call
-to util_getinfo will successfully fill in src_addr and dest_addr with
-the provided input.
+address in an acceptable FI_ADDR_STR format (contains \[lq]://\[rq]),
+the call to util_getinfo will successfully fill in src_addr and
+dest_addr with the provided input.
 If the input is not in an ADDR_STR format, the shared memory provider
-will then create a proper FI_ADDR_STR address with either the "fi_ns://"
-(node/service format) or "fi_shm://" (shm format) prefixes signaling
-whether the addr is a "unique" address and does or does not need an
-extra endpoint name identifier appended in order to make it unique.
+will then create a proper FI_ADDR_STR address with either the
+\[lq]fi_ns://\[rq] (node/service format) or \[lq]fi_shm://\[rq] (shm
+format) prefixes signaling whether the addr is a \[lq]unique\[rq]
+address and does or does not need an extra endpoint name identifier
+appended in order to make it unique.
 For the shared memory provider, we assume that the service (with or
 without a node) is enough to make it unique, but a node alone is not
 sufficient.
-If only a node is provided, the "fi_shm://" prefix is used to signify
-that it is not a unique address.
+If only a node is provided, the \[lq]fi_shm://\[rq] prefix is used to
+signify that it is not a unique address.
 If no node or service are provided (and in the case of setting the src
 address without FI_SOURCE and no hints), the process ID will be used as
 a default address.
-On endpoint creation, if the src_addr has the "fi_shm://" prefix, the
-provider will append ":[uid]:[dom_idx]:[ep_idx]" as a unique endpoint
-name (essentially, in place of a service).
-In the case of the "fi_ns://" prefix (or any other prefix if one was
-provided by the application), no supplemental information is required to
-make it unique and it will remain with only the application\-defined
-address.
+On endpoint creation, if the src_addr has the \[lq]fi_shm://\[rq]
+prefix, the provider will append \[lq]:[uid]:[ep_idx]\[rq] as a unique
+endpoint name (essentially, in place of a service).
+In the case of the \[lq]fi_ns://\[rq] prefix (or any other prefix if one
+was provided by the application), no supplemental information is
+required to make it unique and it will remain with only the
+application\-defined address.
 Note that the actual endpoint name will not include the FI_ADDR_STR
-"*://" prefix since it cannot be included in any shared memory region
-names.
+\[dq]*://\[dq] prefix since it cannot be included in any shared memory
+region names.
 The provider will strip off the prefix before setting the endpoint name.
-As a result, the addresses "fi_prefix1://my_node:my_service" and
-"fi_prefix2://my_node:my_service" would result in endpoints and regions
-of the same name.
+As a result, the addresses \[lq]fi_prefix1://my_node:my_service\[rq] and
+\[lq]fi_prefix2://my_node:my_service\[rq] would result in endpoints and
+regions of the same name.
 The application can also override the endpoint name after creating an
 endpoint using setname() without any address format restrictions.
 .PP
-\f[I]Msg flags\f[] The provider currently only supports the
+\f[I]Msg flags\f[R] The provider currently only supports the
 FI_REMOTE_CQ_DATA msg flag.
 .PP
-\f[I]MR registration mode\f[] The provider implements FI_MR_VIRT_ADDR
+\f[I]MR registration mode\f[R] The provider implements FI_MR_VIRT_ADDR
 memory mode.
 .PP
-\f[I]Atomic operations\f[] The provider supports all combinations of
+\f[I]Atomic operations\f[R] The provider supports all combinations of
 datatype and operations as long as the message is less than 4096 bytes
 (or 2048 for compare operations).
 .SH LIMITATIONS
@@ -125,29 +118,27 @@ EPs must be bound to both RX and TX CQs.
 No support for counters.
 .SH RUNTIME PARAMETERS
 .PP
-The \f[I]shm\f[] provider checks for the following environment
+The \f[I]shm\f[R] provider checks for the following environment
 variables:
 .TP
-.B \f[I]FI_SHM_SAR_THRESHOLD\f[]
+.B \f[I]FI_SHM_SAR_THRESHOLD\f[R]
 Maximum message size to use segmentation protocol before switching to
 mmap (only valid when CMA is not available).
 Default: SIZE_MAX (18446744073709551615)
-.RS
-.RE
 .TP
-.B \f[I]FI_SHM_TX_SIZE\f[]
+.B \f[I]FI_SHM_TX_SIZE\f[R]
 Maximum number of outstanding tx operations.
 Default 1024
-.RS
-.RE
 .TP
-.B \f[I]FI_SHM_RX_SIZE\f[]
+.B \f[I]FI_SHM_RX_SIZE\f[R]
 Maximum number of outstanding rx operations.
 Default 1024
-.RS
-.RE
+.TP
+.B \f[I]FI_SHM_DISABLE_CMA\f[R]
+Manually disables CMA.
+Default false
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_sockets.7 b/man/man7/fi_sockets.7
index 68c4cb4..574e4f5 100644
--- a/man/man7/fi_sockets.7
+++ b/man/man7/fi_sockets.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_sockets" "7" "2019\-05\-30" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_sockets" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -26,34 +26,26 @@ The sockets provider supports all the features defined for the libfabric
 API.
 Key features include:
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports all endpoint types: \f[I]FI_EP_MSG\f[],
-\f[I]FI_EP_RDM\f[], and \f[I]FI_EP_DGRAM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports all endpoint types: \f[I]FI_EP_MSG\f[R],
+\f[I]FI_EP_RDM\f[R], and \f[I]FI_EP_DGRAM\f[R].
 .TP
-.B \f[I]Endpoint capabilities\f[]
+.B \f[I]Endpoint capabilities\f[R]
 The following data transfer interface is supported for a all endpoint
-types: \f[I]fi_msg\f[].
+types: \f[I]fi_msg\f[R].
 Additionally, these interfaces are supported for reliable endpoints
-(\f[I]FI_EP_MSG\f[] and \f[I]FI_EP_RDM\f[]): \f[I]fi_tagged\f[],
-\f[I]fi_atomic\f[], and \f[I]fi_rma\f[].
-.RS
-.RE
+(\f[I]FI_EP_MSG\f[R] and \f[I]FI_EP_RDM\f[R]): \f[I]fi_tagged\f[R],
+\f[I]fi_atomic\f[R], and \f[I]fi_rma\f[R].
 .TP
-.B \f[I]Modes\f[]
+.B \f[I]Modes\f[R]
 The sockets provider supports all operational modes including
-\f[I]FI_CONTEXT\f[] and \f[I]FI_MSG_PREFIX\f[].
-.RS
-.RE
+\f[I]FI_CONTEXT\f[R] and \f[I]FI_MSG_PREFIX\f[R].
 .TP
-.B \f[I]Progress\f[]
-Sockets provider supports both \f[I]FI_PROGRESS_AUTO\f[] and
-\f[I]FI_PROGRESS_MANUAL\f[], with a default set to auto.
+.B \f[I]Progress\f[R]
+Sockets provider supports both \f[I]FI_PROGRESS_AUTO\f[R] and
+\f[I]FI_PROGRESS_MANUAL\f[R], with a default set to auto.
 When progress is set to auto, a background thread runs to ensure that
 progress is made for asynchronous requests.
-.RS
-.RE
 .SH LIMITATIONS
 .PP
 Sockets provider attempts to emulate the entire API set, including all
@@ -69,102 +61,73 @@ Does not support FI_ADDR_STR address format.
 .PP
 The sockets provider checks for the following environment variables \-
 .TP
-.B \f[I]FI_SOCKETS_PE_WAITTIME\f[]
+.B \f[I]FI_SOCKETS_PE_WAITTIME\f[R]
 An integer value that specifies how many milliseconds to spin while
-waiting for progress in \f[I]FI_PROGRESS_AUTO\f[] mode.
-.RS
-.RE
+waiting for progress in \f[I]FI_PROGRESS_AUTO\f[R] mode.
 .TP
-.B \f[I]FI_SOCKETS_CONN_TIMEOUT\f[]
+.B \f[I]FI_SOCKETS_CONN_TIMEOUT\f[R]
 An integer value that specifies how many milliseconds to wait for one
 connection establishment.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_MAX_CONN_RETRY\f[]
+.B \f[I]FI_SOCKETS_MAX_CONN_RETRY\f[R]
 An integer value that specifies the number of socket connection retries
 before reporting as failure.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_DEF_CONN_MAP_SZ\f[]
+.B \f[I]FI_SOCKETS_DEF_CONN_MAP_SZ\f[R]
 An integer to specify the default connection map size.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_DEF_AV_SZ\f[]
+.B \f[I]FI_SOCKETS_DEF_AV_SZ\f[R]
 An integer to specify the default address vector size.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_DEF_CQ_SZ\f[]
+.B \f[I]FI_SOCKETS_DEF_CQ_SZ\f[R]
 An integer to specify the default completion queue size.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_DEF_EQ_SZ\f[]
+.B \f[I]FI_SOCKETS_DEF_EQ_SZ\f[R]
 An integer to specify the default event queue size.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_DGRAM_DROP_RATE\f[]
+.B \f[I]FI_SOCKETS_DGRAM_DROP_RATE\f[R]
 An integer value to specify the drop rate of dgram frame when endpoint
-is \f[I]FI_EP_DGRAM\f[].
+is \f[I]FI_EP_DGRAM\f[R].
 This is for debugging purpose only.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_PE_AFFINITY\f[]
+.B \f[I]FI_SOCKETS_PE_AFFINITY\f[R]
 If specified, progress thread is bound to the indicated range(s) of
 Linux virtual processor ID(s).
 This option is currently not supported on OS X.
 The usage is \- id_start[\-id_end[:stride]][,].
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[]
+.B \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[R]
 A boolean to enable the keepalive support.
-.RS
-.RE
 .TP
-.B \f[I]FI_SOCKETS_KEEPALIVE_TIME\f[]
+.B \f[I]FI_SOCKETS_KEEPALIVE_TIME\f[R]
 An integer to specify the idle time in seconds before sending the first
 keepalive probe.
-Only relevant if \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[] is enabled.
-.RS
-.RE
+Only relevant if \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[R] is enabled.
 .TP
-.B \f[I]FI_SOCKETS_KEEPALIVE_INTVL\f[]
+.B \f[I]FI_SOCKETS_KEEPALIVE_INTVL\f[R]
 An integer to specify the time in seconds between individual keepalive
 probes.
-Only relevant if \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[] is enabled.
-.RS
-.RE
+Only relevant if \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[R] is enabled.
 .TP
-.B \f[I]FI_SOCKETS_KEEPALIVE_PROBES\f[]
+.B \f[I]FI_SOCKETS_KEEPALIVE_PROBES\f[R]
 An integer to specify the maximum number of keepalive probes sent before
 dropping the connection.
-Only relevant if \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[] is enabled.
-.RS
-.RE
+Only relevant if \f[I]FI_SOCKETS_KEEPALIVE_ENABLE\f[R] is enabled.
 .TP
-.B \f[I]FI_SOCKETS_IFACE\f[]
+.B \f[I]FI_SOCKETS_IFACE\f[R]
 The prefix or the name of the network interface (default: any)
-.RS
-.RE
 .SH LARGE SCALE JOBS
 .PP
 For large scale runs one can use these environment variables to set the
-default parameters e.g.
-size of the address vector(AV), completion queue (CQ), connection map
-etc.
+default parameters e.g.\ size of the address vector(AV), completion
+queue (CQ), connection map etc.
 that satisfies the requirement of the particular benchmark.
 The recommended parameters for large scale runs are
-\f[I]FI_SOCKETS_MAX_CONN_RETRY\f[], \f[I]FI_SOCKETS_DEF_CONN_MAP_SZ\f[],
-\f[I]FI_SOCKETS_DEF_AV_SZ\f[], \f[I]FI_SOCKETS_DEF_CQ_SZ\f[],
-\f[I]FI_SOCKETS_DEF_EQ_SZ\f[].
+\f[I]FI_SOCKETS_MAX_CONN_RETRY\f[R],
+\f[I]FI_SOCKETS_DEF_CONN_MAP_SZ\f[R], \f[I]FI_SOCKETS_DEF_AV_SZ\f[R],
+\f[I]FI_SOCKETS_DEF_CQ_SZ\f[R], \f[I]FI_SOCKETS_DEF_EQ_SZ\f[R].
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_tcp.7 b/man/man7/fi_tcp.7
index cc8b072..b1d60b3 100644
--- a/man/man7/fi_tcp.7
+++ b/man/man7/fi_tcp.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_tcp" "7" "2020\-04\-14" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_tcp" "7" "2021\-05\-20" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -16,52 +16,42 @@ high\-performance fabric hardware.
 .PP
 The following features are supported
 .TP
-.B \f[I]Endpoint types\f[]
-\f[I]FI_EP_MSG\f[] is the only supported endpoint type.
+.B \f[I]Endpoint types\f[R]
+\f[I]FI_EP_MSG\f[R] is the only supported endpoint type.
 Reliable datagram endpoint over TCP sockets can be achieved by layering
 RxM over tcp provider.
-.RS
-.RE
-\f[I]FI_EP_RDM\f[] is supported by layering ofi_rxm provider on top of
+\f[I]FI_EP_RDM\f[R] is supported by layering ofi_rxm provider on top of
 the tcp provider.
-.RS
-.RE
 .TP
-.B \f[I]Endpoint capabilities\f[]
-The tcp provider currently supports \f[I]FI_MSG\f[], \f[I]FI_RMA\f[]
-.RS
-.RE
+.B \f[I]Endpoint capabilities\f[R]
+The tcp provider currently supports \f[I]FI_MSG\f[R], \f[I]FI_RMA\f[R]
 .TP
-.B \f[I]Progress\f[]
-Currently tcp provider supports only \f[I]FI_PROGRESS_MANUAL\f[]
-.RS
-.RE
+.B \f[I]Progress\f[R]
+Currently tcp provider supports only \f[I]FI_PROGRESS_MANUAL\f[R]
 .TP
-.B \f[I]Shared Rx Context\f[]
+.B \f[I]Shared Rx Context\f[R]
 The tcp provider supports shared receive context
-.RS
-.RE
 .TP
-.B \f[I]Multi recv buffers\f[]
+.B \f[I]Multi recv buffers\f[R]
 The tcp provider supports multi recv buffers
-.RS
-.RE
 .SH RUNTIME PARAMETERS
 .PP
 The tcp provider check for the following enviroment variables \-
 .TP
-.B \f[I]FI_TCP_IFACE\f[]
+.B \f[I]FI_TCP_IFACE\f[R]
 A specific can be requested with this variable
-.RS
-.RE
 .TP
-.B \f[I]FI_TCP_PORT_LOW_RANGE/FI_TCP_PORT_HIGH_RANGE\f[]
+.B \f[I]FI_TCP_PORT_LOW_RANGE/FI_TCP_PORT_HIGH_RANGE\f[R]
 These variables are used to set the range of ports to be used by the tcp
 provider for its passive endpoint creation.
 This is useful where only a range of ports are allowed by firewall for
 tcp connections.
-.RS
-.RE
+.TP
+.B \f[I]FI_TCP_TX_SIZE\f[R]
+Default tx context size (default: 256)
+.TP
+.B \f[I]FI_TCP_RX_SIZE\f[R]
+Default rx context size (default: 256)
 .SH LIMITATIONS
 .PP
 The tcp provider is implemented over TCP sockets to emulate libfabric
@@ -71,6 +61,6 @@ implementing to sockets directly, depending on the types of data
 transfers the application is trying to achieve.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_udp.7 b/man/man7/fi_udp.7
index 5da716a..d0a8fca 100644
--- a/man/man7/fi_udp.7
+++ b/man/man7/fi_udp.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_udp" "7" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_udp" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -19,30 +19,22 @@ the implementation of libfabric features over any hardware.
 The UDP provider supports a minimal set of features useful for sending
 and receiving datagram messages over an unreliable endpoint.
 .TP
-.B \f[I]Endpoint types\f[]
-The provider supports only endpoint type \f[I]FI_EP_DGRAM\f[].
-.RS
-.RE
+.B \f[I]Endpoint types\f[R]
+The provider supports only endpoint type \f[I]FI_EP_DGRAM\f[R].
 .TP
-.B \f[I]Endpoint capabilities\f[]
-The following data transfer interface is supported: \f[I]fi_msg\f[].
+.B \f[I]Endpoint capabilities\f[R]
+The following data transfer interface is supported: \f[I]fi_msg\f[R].
 The provider supports standard unicast datagram transfers, as well as
 multicast operations.
-.RS
-.RE
 .TP
-.B \f[I]Modes\f[]
+.B \f[I]Modes\f[R]
 The provider does not require the use of any mode bits.
-.RS
-.RE
 .TP
-.B \f[I]Progress\f[]
-The UDP provider supports both \f[I]FI_PROGRESS_AUTO\f[] and
-\f[I]FI_PROGRESS_MANUAL\f[], with a default set to auto.
+.B \f[I]Progress\f[R]
+The UDP provider supports both \f[I]FI_PROGRESS_AUTO\f[R] and
+\f[I]FI_PROGRESS_MANUAL\f[R], with a default set to auto.
 However, receive side data buffers are not modified outside of
 completion processing routines.
-.RS
-.RE
 .SH LIMITATIONS
 .PP
 The UDP provider has hard\-coded maximums for supported queue sizes and
@@ -59,6 +51,6 @@ No support for counters.
 No runtime parameters are currently defined.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7), \f[C]fi_getinfo\f[R](3)
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_usnic.7 b/man/man7/fi_usnic.7
index c75126f..ea1fdef 100644
--- a/man/man7/fi_usnic.7
+++ b/man/man7/fi_usnic.7
@@ -1,21 +1,21 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_usnic" "7" "2018\-10\-05" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_usnic" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
 fi_usnic \- The usNIC Fabric Provider
 .SH OVERVIEW
 .PP
-The \f[I]usnic\f[] provider is designed to run over the Cisco VIC
+The \f[I]usnic\f[R] provider is designed to run over the Cisco VIC
 (virtualized NIC) hardware on Cisco UCS servers.
 It utilizes the Cisco usNIC (userspace NIC) capabilities of the VIC to
 enable ultra low latency and other offload capabilities on Ethernet
 networks.
 .SH RELEASE NOTES
 .IP \[bu] 2
-The \f[I]usnic\f[] libfabric provider requires the use of the "libnl"
-library.
+The \f[I]usnic\f[R] libfabric provider requires the use of the
+\[lq]libnl\[rq] library.
 .RS 2
 .IP \[bu] 2
 There are two versions of libnl generally available: v1 and v3; the
@@ -24,12 +24,12 @@ usnic provider can use either version.
 If you are building libfabric/the usnic provider from source, you will
 need to have the libnl header files available (e.g., if you are
 installing libnl from RPM or other packaging system, install the
-"\-devel" versions of the package).
+\[lq]\-devel\[rq] versions of the package).
 .IP \[bu] 2
 If you have libnl (either v1 or v3) installed in a non\-standard
 location (e.g., not in /usr/lib or /usr/lib64), you may need to tell
-libfabric\[aq]s configure where to find libnl via the
-\f[C]\-\-with\-libnl=DIR\f[] command line option (where DIR is the
+libfabric\[cq]s configure where to find libnl via the
+\f[C]\-\-with\-libnl=DIR\f[R] command line option (where DIR is the
 installation prefix of the libnl package).
 .RE
 .IP \[bu] 2
@@ -37,51 +37,51 @@ The most common way to use the libfabric usnic provider is via an MPI
 implementation that uses libfabric (and the usnic provider) as a lower
 layer transport.
 MPI applications do not need to know anything about libfabric or usnic
-in this use case \-\- the MPI implementation hides all these details
+in this use case \[en] the MPI implementation hides all these details
 from the application.
 .IP \[bu] 2
 If you are writing applications directly to the libfabric API:
 .RS 2
 .IP \[bu] 2
-\f[I]FI_EP_DGRAM\f[] endpoints are the best supported method of
+\f[I]FI_EP_DGRAM\f[R] endpoints are the best supported method of
 utilizing the usNIC interface.
-Specifically, the \f[I]FI_EP_DGRAM\f[] endpoint type has been
-extensively tested as the underlying layer for Open MPI\[aq]s
-\f[I]usnic\f[] BTL.
+Specifically, the \f[I]FI_EP_DGRAM\f[R] endpoint type has been
+extensively tested as the underlying layer for Open MPI\[cq]s
+\f[I]usnic\f[R] BTL.
 .IP \[bu] 2
-\f[I]FI_EP_MSG\f[] and \f[I]FI_EP_RDM\f[] endpoints are implemented, but
-are only lightly tested.
+\f[I]FI_EP_MSG\f[R] and \f[I]FI_EP_RDM\f[R] endpoints are implemented,
+but are only lightly tested.
 It is likely that there are still some bugs in these endpoint types.
 In particular, there are known bugs in RDM support in the presence of
 congestion or packet loss (issue 1621).
 RMA is not yet supported.
 .IP \[bu] 2
-\f[C]fi_provider\f[](7) lists requirements for all providers.
-The following limitations exist in the \f[I]usnic\f[] provider:
+\f[C]fi_provider\f[R](7) lists requirements for all providers.
+The following limitations exist in the \f[I]usnic\f[R] provider:
 .RS 2
 .IP \[bu] 2
-multicast operations are not supported on \f[I]FI_EP_DGRAM\f[] and
-\f[I]FI_EP_RDM\f[] endpoints.
+multicast operations are not supported on \f[I]FI_EP_DGRAM\f[R] and
+\f[I]FI_EP_RDM\f[R] endpoints.
 .IP \[bu] 2
-\f[I]FI_EP_MSG\f[] endpoints only support connect, accept, and getname
+\f[I]FI_EP_MSG\f[R] endpoints only support connect, accept, and getname
 CM operations.
 .IP \[bu] 2
 Passive endpoints only support listen, setname, and getname CM
 operations.
 .IP \[bu] 2
-\f[I]FI_EP_DGRAM\f[] endpoints support \f[C]fi_sendmsg()\f[] and
-\f[C]fi_recvmsg()\f[], but some flags are ignored.
-\f[C]fi_sendmsg()\f[] supports \f[C]FI_INJECT\f[] and
-\f[C]FI_COMPLETION\f[].
-\f[C]fi_recvmsg()\f[] supports \f[C]FI_MORE\f[].
+\f[I]FI_EP_DGRAM\f[R] endpoints support \f[C]fi_sendmsg()\f[R] and
+\f[C]fi_recvmsg()\f[R], but some flags are ignored.
+\f[C]fi_sendmsg()\f[R] supports \f[C]FI_INJECT\f[R] and
+\f[C]FI_COMPLETION\f[R].
+\f[C]fi_recvmsg()\f[R] supports \f[C]FI_MORE\f[R].
 .IP \[bu] 2
-Address vectors only support \f[C]FI_AV_MAP\f[].
+Address vectors only support \f[C]FI_AV_MAP\f[R].
 .IP \[bu] 2
 No counters are supported.
 .IP \[bu] 2
 The tag matching interface is not supported.
 .IP \[bu] 2
-\f[I]FI_MSG_PREFIX\f[] is only supported on \f[I]FI_EP_DGRAM\f[] and
+\f[I]FI_MSG_PREFIX\f[R] is only supported on \f[I]FI_EP_DGRAM\f[R] and
 usage is limited to releases 1.1 and beyond.
 .IP \[bu] 2
 fi_control with FI_GETWAIT may only be used on CQs that have been bound
@@ -104,7 +104,7 @@ The application is responsible for resource protection.
 .IP \[bu] 2
 The usnic libfabric provider supports extensions that provide
 information and functionality beyond the standard libfabric interface.
-See the "USNIC EXTENSIONS" section, below.
+See the \[lq]USNIC EXTENSIONS\[rq] section, below.
 .RE
 .SH USNIC EXTENSIONS
 .PP
@@ -112,24 +112,26 @@ The usnic libfabric provider exports extensions for additional VIC,
 usNIC, and Ethernet capabilities not provided by the standard libfabric
 interface.
 .PP
-These extensions are available via the "fi_ext_usnic.h" header file.
+These extensions are available via the \[lq]fi_ext_usnic.h\[rq] header
+file.
 .SS Fabric Extension: getinfo
 .PP
-Version 2 of the "fabric getinfo" extension was introduced in Libfabric
-release v1.3.0 and can be used to retrieve IP and SR\-IOV information
-about a usNIC device obtained from the \f[C]fi_getinfo\f[](3) function.
+Version 2 of the \[lq]fabric getinfo\[rq] extension was introduced in
+Libfabric release v1.3.0 and can be used to retrieve IP and SR\-IOV
+information about a usNIC device obtained from the
+\f[C]fi_getinfo\f[R](3) function.
 .PP
-The "fabric getinfo" extension is obtained by calling
-\f[C]fi_open_ops\f[] and requesting \f[C]FI_USNIC_FABRIC_OPS_1\f[] to
+The \[lq]fabric getinfo\[rq] extension is obtained by calling
+\f[C]fi_open_ops\f[R] and requesting \f[C]FI_USNIC_FABRIC_OPS_1\f[R] to
 get the usNIC fabric extension operations.
-The \f[C]getinfo\f[] function accepts a version parameter that can be
+The \f[C]getinfo\f[R] function accepts a version parameter that can be
 used to select different versions of the extension.
-The information returned by the "fabric getinfo" extension is accessible
-through a \f[C]fi_usnic_info\f[] struct that uses a version tagged
-union.
+The information returned by the \[lq]fabric getinfo\[rq] extension is
+accessible through a \f[C]fi_usnic_info\f[R] struct that uses a version
+tagged union.
 The accessed union member must correspond with the requested version.
 It is recommended that applications explicitly request a version rather
-than using the header provided \f[C]FI_EXT_USNIC_INFO_VERSION\f[].
+than using the header provided \f[C]FI_EXT_USNIC_INFO_VERSION\f[R].
 Although there is a version 1 of the extension, its use is discouraged,
 and it may not be available in future releases.
 .SS Compatibility issues
@@ -149,213 +151,201 @@ patched version of an older release.
 .IP
 .nf
 \f[C]
-#include\ <rdma/fi_ext_usnic.h>
+#include <rdma/fi_ext_usnic.h>
 
-struct\ fi_usnic_info\ {
-\ \ \ \ uint32_t\ ui_version;
-\ \ \ \ uint8_t\ ui_pad0[4];
-\ \ \ \ union\ {
-\ \ \ \ \ \ \ \ struct\ fi_usnic_info_v1\ v1;
-\ \ \ \ \ \ \ \ struct\ fi_usnic_info_v2\ v2;
-\ \ \ \ }\ ui;
-}\ __attribute__((packed));
+struct fi_usnic_info {
+    uint32_t ui_version;
+    uint8_t ui_pad0[4];
+    union {
+        struct fi_usnic_info_v1 v1;
+        struct fi_usnic_info_v2 v2;
+    } ui;
+} __attribute__((packed));
 
-int\ getinfo(uint32_t\ version,\ struct\ fid_fabric\ *fabric,
-\ \ \ \ \ \ \ \ struct\ fi_usnic_info\ *info);
-\f[]
+int getinfo(uint32_t version, struct fid_fabric *fabric,
+        struct fi_usnic_info *info);
+\f[R]
 .fi
 .TP
-.B \f[I]version\f[]
+.B \f[I]version\f[R]
 Version of getinfo to be used
-.RS
-.RE
 .TP
-.B \f[I]fabric\f[]
+.B \f[I]fabric\f[R]
 Fabric descriptor
-.RS
-.RE
 .TP
-.B \f[I]info\f[]
+.B \f[I]info\f[R]
 Upon successful return, this parameter will contain information about
 the fabric.
-.RS
-.RE
 .IP \[bu] 2
 Version 2
 .IP
 .nf
 \f[C]
-struct\ fi_usnic_cap\ {
-\ \ \ \ const\ char\ *uc_capability;
-\ \ \ \ int\ uc_present;
-}\ __attribute__((packed));
+struct fi_usnic_cap {
+    const char *uc_capability;
+    int uc_present;
+} __attribute__((packed));
 
-struct\ fi_usnic_info_v2\ {
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_link_speed;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_netmask_be;
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ ui_ifname[IFNAMSIZ];
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_num_vf;
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_qp_per_vf;
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_cq_per_vf;
+struct fi_usnic_info_v2 {
+    uint32_t        ui_link_speed;
+    uint32_t        ui_netmask_be;
+    char            ui_ifname[IFNAMSIZ];
+    unsigned        ui_num_vf;
+    unsigned        ui_qp_per_vf;
+    unsigned        ui_cq_per_vf;
 
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ ui_devname[FI_EXT_USNIC_MAX_DEVNAME];
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ ui_mac_addr[6];
+    char            ui_devname[FI_EXT_USNIC_MAX_DEVNAME];
+    uint8_t         ui_mac_addr[6];
 
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ ui_pad0[2];
+    uint8_t         ui_pad0[2];
 
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_ipaddr_be;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_prefixlen;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_mtu;
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ ui_link_up;
+    uint32_t        ui_ipaddr_be;
+    uint32_t        ui_prefixlen;
+    uint32_t        ui_mtu;
+    uint8_t         ui_link_up;
 
-\ \ \ \ uint8_t\ \ \ \ \ \ \ \ \ ui_pad1[3];
+    uint8_t         ui_pad1[3];
 
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_vendor_id;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_vendor_part_id;
-\ \ \ \ uint32_t\ \ \ \ \ \ \ \ ui_device_id;
-\ \ \ \ char\ \ \ \ \ \ \ \ \ \ \ \ ui_firmware[64];
+    uint32_t        ui_vendor_id;
+    uint32_t        ui_vendor_part_id;
+    uint32_t        ui_device_id;
+    char            ui_firmware[64];
 
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_intr_per_vf;
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_max_cq;
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_max_qp;
+    unsigned        ui_intr_per_vf;
+    unsigned        ui_max_cq;
+    unsigned        ui_max_qp;
 
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_max_cqe;
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_max_send_credits;
-\ \ \ \ unsigned\ \ \ \ \ \ \ \ ui_max_recv_credits;
+    unsigned        ui_max_cqe;
+    unsigned        ui_max_send_credits;
+    unsigned        ui_max_recv_credits;
 
-\ \ \ \ const\ char\ \ \ \ \ \ *ui_nicname;
-\ \ \ \ const\ char\ \ \ \ \ \ *ui_pid;
+    const char      *ui_nicname;
+    const char      *ui_pid;
 
-\ \ \ \ struct\ fi_usnic_cap\ **ui_caps;
-}\ __attribute__((packed));
-\f[]
+    struct fi_usnic_cap **ui_caps;
+} __attribute__((packed));
+\f[R]
 .fi
 .IP \[bu] 2
 Version 1
 .IP
 .nf
 \f[C]
-struct\ fi_usnic_info_v1\ {
-\ \ \ \ uint32_t\ ui_link_speed;
-\ \ \ \ uint32_t\ ui_netmask_be;
-\ \ \ \ char\ ui_ifname[IFNAMSIZ];
+struct fi_usnic_info_v1 {
+    uint32_t ui_link_speed;
+    uint32_t ui_netmask_be;
+    char ui_ifname[IFNAMSIZ];
 
-\ \ \ \ uint32_t\ ui_num_vf;
-\ \ \ \ uint32_t\ ui_qp_per_vf;
-\ \ \ \ uint32_t\ ui_cq_per_vf;
-}\ __attribute__((packed));
-\f[]
+    uint32_t ui_num_vf;
+    uint32_t ui_qp_per_vf;
+    uint32_t ui_cq_per_vf;
+} __attribute__((packed));
+\f[R]
 .fi
 .PP
-Version 1 of the "fabric getinfo" extension can be used by explicitly
-requesting it in the call to \f[C]getinfo\f[] and accessing the
-\f[C]v1\f[] portion of the \f[C]fi_usnic_info.ui\f[] union.
+Version 1 of the \[lq]fabric getinfo\[rq] extension can be used by
+explicitly requesting it in the call to \f[C]getinfo\f[R] and accessing
+the \f[C]v1\f[R] portion of the \f[C]fi_usnic_info.ui\f[R] union.
 Use of version 1 is not recommended and it may be removed from future
 releases.
 .PP
 The following is an example of how to utilize version 2 of the usnic
-"fabric getinfo" extension.
+\[lq]fabric getinfo\[rq] extension.
 .IP
 .nf
 \f[C]
-#include\ <stdio.h>
-#include\ <rdma/fabric.h>
+#include <stdio.h>
+#include <rdma/fabric.h>
 
-/*\ The\ usNIC\ extensions\ are\ all\ in\ the
-\ \ \ rdma/fi_ext_usnic.h\ header\ */
-#include\ <rdma/fi_ext_usnic.h>
+/* The usNIC extensions are all in the
+   rdma/fi_ext_usnic.h header */
+#include <rdma/fi_ext_usnic.h>
 
-int\ main(int\ argc,\ char\ *argv[])\ {
-\ \ \ \ struct\ fi_info\ *info;
-\ \ \ \ struct\ fi_info\ *info_list;
-\ \ \ \ struct\ fi_info\ hints\ =\ {0};
-\ \ \ \ struct\ fi_ep_attr\ ep_attr\ =\ {0};
-\ \ \ \ struct\ fi_fabric_attr\ fabric_attr\ =\ {0};
+int main(int argc, char *argv[]) {
+    struct fi_info *info;
+    struct fi_info *info_list;
+    struct fi_info hints = {0};
+    struct fi_ep_attr ep_attr = {0};
+    struct fi_fabric_attr fabric_attr = {0};
 
-\ \ \ \ fabric_attr.prov_name\ =\ "usnic";
-\ \ \ \ ep_attr.type\ =\ FI_EP_DGRAM;
+    fabric_attr.prov_name = \[dq]usnic\[dq];
+    ep_attr.type = FI_EP_DGRAM;
 
-\ \ \ \ hints.caps\ =\ FI_MSG;
-\ \ \ \ hints.mode\ =\ FI_LOCAL_MR\ |\ FI_MSG_PREFIX;
-\ \ \ \ hints.addr_format\ =\ FI_SOCKADDR;
-\ \ \ \ hints.ep_attr\ =\ &ep_attr;
-\ \ \ \ hints.fabric_attr\ =\ &fabric_attr;
+    hints.caps = FI_MSG;
+    hints.mode = FI_LOCAL_MR | FI_MSG_PREFIX;
+    hints.addr_format = FI_SOCKADDR;
+    hints.ep_attr = &ep_attr;
+    hints.fabric_attr = &fabric_attr;
 
-\ \ \ \ /*\ Find\ all\ usnic\ providers\ */
-\ \ \ \ fi_getinfo(FI_VERSION(1,\ 0),\ NULL,\ 0,\ 0,\ &hints,\ &info_list);
+    /* Find all usnic providers */
+    fi_getinfo(FI_VERSION(1, 0), NULL, 0, 0, &hints, &info_list);
 
-\ \ \ \ for\ (info\ =\ info_list;\ NULL\ !=\ info;\ info\ =\ info\->next)\ {
-\ \ \ \ \ \ \ \ /*\ Open\ the\ fabric\ on\ the\ interface\ */
-\ \ \ \ \ \ \ \ struct\ fid_fabric\ *fabric;
-\ \ \ \ \ \ \ \ fi_fabric(info\->fabric_attr,\ &fabric,\ NULL);
+    for (info = info_list; NULL != info; info = info\->next) {
+        /* Open the fabric on the interface */
+        struct fid_fabric *fabric;
+        fi_fabric(info\->fabric_attr, &fabric, NULL);
 
-\ \ \ \ \ \ \ \ /*\ Pass\ FI_USNIC_FABRIC_OPS_1\ to\ get\ usnic\ ops
-\ \ \ \ \ \ \ \ \ \ \ on\ the\ fabric\ */
-\ \ \ \ \ \ \ \ struct\ fi_usnic_ops_fabric\ *usnic_fabric_ops;
-\ \ \ \ \ \ \ \ fi_open_ops(&fabric\->fid,\ FI_USNIC_FABRIC_OPS_1,\ 0,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (void\ **)\ &usnic_fabric_ops,\ NULL);
+        /* Pass FI_USNIC_FABRIC_OPS_1 to get usnic ops
+           on the fabric */
+        struct fi_usnic_ops_fabric *usnic_fabric_ops;
+        fi_open_ops(&fabric\->fid, FI_USNIC_FABRIC_OPS_1, 0,
+                (void **) &usnic_fabric_ops, NULL);
 
-\ \ \ \ \ \ \ \ /*\ Now\ use\ the\ returned\ usnic\ ops\ structure\ to\ call
-\ \ \ \ \ \ \ \ \ \ \ usnic\ extensions.\ \ The\ following\ extension\ queries
-\ \ \ \ \ \ \ \ \ \ \ some\ IP\ and\ SR\-IOV\ characteristics\ about\ the
-\ \ \ \ \ \ \ \ \ \ \ usNIC\ device.\ */
-\ \ \ \ \ \ \ \ struct\ fi_usnic_info\ usnic_info;
+        /* Now use the returned usnic ops structure to call
+           usnic extensions.  The following extension queries
+           some IP and SR\-IOV characteristics about the
+           usNIC device. */
+        struct fi_usnic_info usnic_info;
 
-\ \ \ \ \ \ \ \ /*\ Explicitly\ request\ version\ 2.\ */
-\ \ \ \ \ \ \ \ usnic_fabric_ops\->getinfo(2,\ fabric,\ &usnic_info);
+        /* Explicitly request version 2. */
+        usnic_fabric_ops\->getinfo(2, fabric, &usnic_info);
 
-\ \ \ \ \ \ \ \ printf("Fabric\ interface\ %s\ is\ %s:\\n"
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "\\tNetmask:\ \ 0x%08x\\n\\tLink\ speed:\ %d\\n"
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "\\tSR\-IOV\ VFs:\ %d\\n\\tQPs\ per\ SR\-IOV\ VF:\ %d\\n"
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ "\\tCQs\ per\ SR\-IOV\ VF:\ %d\\n",
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ info\->fabric_attr\->name,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ usnic_info.ui.v2.ui_ifname,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ usnic_info.ui.v2.ui_netmask_be,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ usnic_info.ui.v2.ui_link_speed,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ usnic_info.ui.v2.ui_num_vf,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ usnic_info.ui.v2.ui_qp_per_vf,
-\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ usnic_info.ui.v2.ui_cq_per_vf);
+        printf(\[dq]Fabric interface %s is %s:\[rs]n\[dq]
+               \[dq]\[rs]tNetmask:  0x%08x\[rs]n\[rs]tLink speed: %d\[rs]n\[dq]
+               \[dq]\[rs]tSR\-IOV VFs: %d\[rs]n\[rs]tQPs per SR\-IOV VF: %d\[rs]n\[dq]
+               \[dq]\[rs]tCQs per SR\-IOV VF: %d\[rs]n\[dq],
+               info\->fabric_attr\->name,
+               usnic_info.ui.v2.ui_ifname,
+               usnic_info.ui.v2.ui_netmask_be,
+               usnic_info.ui.v2.ui_link_speed,
+               usnic_info.ui.v2.ui_num_vf,
+               usnic_info.ui.v2.ui_qp_per_vf,
+               usnic_info.ui.v2.ui_cq_per_vf);
 
-\ \ \ \ \ \ \ \ fi_close(&fabric\->fid);
-\ \ \ \ }
+        fi_close(&fabric\->fid);
+    }
 
-\ \ \ \ fi_freeinfo(info_list);
-\ \ \ \ return\ 0;
+    fi_freeinfo(info_list);
+    return 0;
 }
-\f[]
+\f[R]
 .fi
 .SS Adress Vector Extension: get_distance
 .PP
-The "address vector get_distance" extension was introduced in Libfabric
-release v1.0.0 and can be used to retrieve the network distance of an
-address.
+The \[lq]address vector get_distance\[rq] extension was introduced in
+Libfabric release v1.0.0 and can be used to retrieve the network
+distance of an address.
 .PP
-The "get_distance" extension is obtained by calling \f[C]fi_open_ops\f[]
-and requesting \f[C]FI_USNIC_AV_OPS_1\f[] to get the usNIC address
-vector extension operations.
+The \[lq]get_distance\[rq] extension is obtained by calling
+\f[C]fi_open_ops\f[R] and requesting \f[C]FI_USNIC_AV_OPS_1\f[R] to get
+the usNIC address vector extension operations.
 .IP
 .nf
 \f[C]
-int\ get_distance(struct\ fid_av\ *av,\ void\ *addr,\ int\ *metric);
-\f[]
+int get_distance(struct fid_av *av, void *addr, int *metric);
+\f[R]
 .fi
 .TP
-.B \f[I]av\f[]
+.B \f[I]av\f[R]
 Address vector
-.RS
-.RE
 .TP
-.B \f[I]addr\f[]
+.B \f[I]addr\f[R]
 Destination address
-.RS
-.RE
 .TP
-.B \f[I]metric\f[]
-On output this will contain \f[C]\-1\f[] if the destination host is
-unreachable, \f[C]0\f[] is the destination host is locally connected,
-and \f[C]1\f[] otherwise.
-.RS
-.RE
+.B \f[I]metric\f[R]
+On output this will contain \f[C]\-1\f[R] if the destination host is
+unreachable, \f[C]0\f[R] is the destination host is locally connected,
+and \f[C]1\f[R] otherwise.
 .PP
 See fi_ext_usnic.h for more details.
 .SH VERSION DIFFERENCES
@@ -365,28 +355,28 @@ The release of libfabric v1.4 introduced a new naming convention for
 fabric and domain.
 However the usNIC provider remains backward compatible with applications
 supporting the old scheme and decides which one to use based on the
-version passed to \f[C]fi_getinfo\f[]:
+version passed to \f[C]fi_getinfo\f[R]:
 .IP \[bu] 2
-When \f[C]FI_VERSION(1,4)\f[] or higher is used:
+When \f[C]FI_VERSION(1,4)\f[R] or higher is used:
 .RS 2
 .IP \[bu] 2
 fabric name is the network address with the CIDR notation (i.e.,
-\f[C]a.b.c.d/e\f[])
+\f[C]a.b.c.d/e\f[R])
 .IP \[bu] 2
-domain name is the usNIC Linux interface name (i.e., \f[C]usnic_X\f[])
+domain name is the usNIC Linux interface name (i.e., \f[C]usnic_X\f[R])
 .RE
 .IP \[bu] 2
-When a lower version number is used, like \f[C]FI_VERSION(1,\ 3)\f[], it
+When a lower version number is used, like \f[C]FI_VERSION(1, 3)\f[R], it
 follows the same behavior the usNIC provider exhibited in libfabric <=
 v1.3:
 .RS 2
 .IP \[bu] 2
-fabric name is the usNIC Linux interface name (i.e., \f[C]usnic_X\f[])
+fabric name is the usNIC Linux interface name (i.e., \f[C]usnic_X\f[R])
 .IP \[bu] 2
-domain name is \f[C]NULL\f[]
+domain name is \f[C]NULL\f[R]
 .RE
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_open_ops\f[](3), \f[C]fi_provider\f[](7),
+\f[C]fabric\f[R](7), \f[C]fi_open_ops\f[R](3), \f[C]fi_provider\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/man/man7/fi_verbs.7 b/man/man7/fi_verbs.7
index 0793136..91954a3 100644
--- a/man/man7/fi_verbs.7
+++ b/man/man7/fi_verbs.7
@@ -1,6 +1,6 @@
-.\" Automatically generated by Pandoc 1.19.2.4
+.\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_verbs" "7" "2020\-11\-12" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_verbs" "7" "2021\-03\-22" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -21,8 +21,7 @@ librdmacm * librdmacm\-devel
 .PP
 You may also want to look into any OS specific instructions for enabling
 RDMA.
-e.g.
-RHEL has instructions on their documentation for enabling RDMA.
+e.g.\ RHEL has instructions on their documentation for enabling RDMA.
 .PP
 The IPoIB interface should be configured with a valid IP address.
 This is a requirement from librdmacm.
@@ -35,8 +34,8 @@ FI_EP_MSG, FI_EP_DGRAM (beta), FI_EP_RDM.
 .PP
 FI_EP_RDM is supported via OFI RxM and RxD utility providers which are
 layered on top of verbs.
-To the app, the provider name string would appear as "verbs;ofi_rxm" or
-"verbs;ofi_rxd".
+To the app, the provider name string would appear as
+\[lq]verbs;ofi_rxm\[rq] or \[lq]verbs;ofi_rxd\[rq].
 Please refer the man pages for RxM (fi_rxm.7) and RxD (fi_rxd.7) to know
 about the capabilities and limitations for the FI_EP_RDM endpoint.
 .SS Endpoint capabilities and features
@@ -103,7 +102,7 @@ See ibv_fork_init(3) for additional details.
 .SS Memory Registration Cache
 .PP
 The verbs provider uses the common memory registration cache
-functionality that\[aq]s part of libfabric utility code.
+functionality that\[cq]s part of libfabric utility code.
 This speeds up memory registration calls from applications by caching
 registrations of frequently used memory regions.
 Please refer to fi_mr(3): Memory Registration Cache section for more
@@ -154,7 +153,7 @@ to be re\-mapped when the process is forked (MADV_DONTFORK).
 .PP
 The XRC transport is intended to be used when layered with the RXM
 provider and requires the use of shared receive contexts.
-See \f[C]fi_rxm\f[](7).
+See \f[C]fi_rxm\f[R](7).
 To enable XRC, the following environment variables must usually be set:
 FI_VERBS_PREFER_XRC and FI_OFI_RXM_USE_SRX.
 .SH RUNTIME PARAMETERS
@@ -162,82 +161,56 @@ FI_VERBS_PREFER_XRC and FI_OFI_RXM_USE_SRX.
 The verbs provider checks for the following environment variables.
 .SS Common variables:
 .TP
-.B \f[I]FI_VERBS_TX_SIZE\f[]
+.B \f[I]FI_VERBS_TX_SIZE\f[R]
 Default maximum tx context size (default: 384)
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_RX_SIZE\f[]
+.B \f[I]FI_VERBS_RX_SIZE\f[R]
 Default maximum rx context size (default: 384)
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_TX_IOV_LIMIT\f[]
+.B \f[I]FI_VERBS_TX_IOV_LIMIT\f[R]
 Default maximum tx iov_limit (default: 4).
 Note: RDM (internal \- deprecated) EP type supports only 1
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_RX_IOV_LIMIT\f[]
+.B \f[I]FI_VERBS_RX_IOV_LIMIT\f[R]
 Default maximum rx iov_limit (default: 4).
 Note: RDM (internal \- deprecated) EP type supports only 1
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_INLINE_SIZE\f[]
+.B \f[I]FI_VERBS_INLINE_SIZE\f[R]
 Default maximum inline size.
 Actual inject size returned in fi_info may be greater (default: 64)
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_MIN_RNR_TIMER\f[]
+.B \f[I]FI_VERBS_MIN_RNR_TIMER\f[R]
 Set min_rnr_timer QP attribute (0 \- 31) (default: 12)
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_CQREAD_BUNCH_SIZE\f[]
+.B \f[I]FI_VERBS_CQREAD_BUNCH_SIZE\f[R]
 The number of entries to be read from the verbs completion queue at a
 time (default: 8).
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_PREFER_XRC\f[]
+.B \f[I]FI_VERBS_PREFER_XRC\f[R]
 Prioritize XRC transport fi_info before RC transport fi_info (default:
 0, RC fi_info will be before XRC fi_info)
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_GID_IDX\f[]
+.B \f[I]FI_VERBS_GID_IDX\f[R]
 The GID index to use (default: 0)
-.RS
-.RE
 .TP
-.B \f[I]FI_VERBS_DEVICE_NAME\f[]
+.B \f[I]FI_VERBS_DEVICE_NAME\f[R]
 Specify a specific verbs device to use by name
-.RS
-.RE
 .SS Variables specific to MSG endpoints
 .TP
-.B \f[I]FI_VERBS_IFACE\f[]
+.B \f[I]FI_VERBS_IFACE\f[R]
 The prefix or the full name of the network interface associated with the
 verbs device (default: ib)
-.RS
-.RE
 .SS Variables specific to DGRAM endpoints
 .TP
-.B \f[I]FI_VERBS_DGRAM_USE_NAME_SERVER\f[]
+.B \f[I]FI_VERBS_DGRAM_USE_NAME_SERVER\f[R]
 The option that enables/disables OFI Name Server thread.
 The NS thread is used to resolve IP\-addresses to provider specific
-addresses (default: 1, if "OMPI_COMM_WORLD_RANK" and "PMI_RANK"
-environment variables aren\[aq]t defined)
-.RS
-.RE
+addresses (default: 1, if \[lq]OMPI_COMM_WORLD_RANK\[rq] and
+\[lq]PMI_RANK\[rq] environment variables aren\[cq]t defined)
 .TP
-.B \f[I]FI_VERBS_NAME_SERVER_PORT\f[]
+.B \f[I]FI_VERBS_NAME_SERVER_PORT\f[R]
 The port on which Name Server thread listens incoming connections and
 requests (default: 5678)
-.RS
-.RE
 .SS Environment variables notes
 .PP
 The fi_info utility would give the up\-to\-date information on
@@ -249,18 +222,20 @@ Set FI_LOG_LEVEL=info or FI_LOG_LEVEL=debug (if debug build of libfabric
 is available) and check if there any errors because of incorrect input
 parameters to fi_getinfo.
 .IP \[bu] 2
-Check if "fi_info \-p verbs" is successful.
+Check if \[lq]fi_info \-p verbs\[rq] is successful.
 If that fails the following checklist may help in ensuring that the RDMA
 verbs stack is functional:
+.RS 2
 .IP \[bu] 2
 If libfabric was compiled, check if verbs provider was built.
 Building verbs provider would be skipped if its dependencies (listed in
-requirements) aren\[aq]t available on the system.
+requirements) aren\[cq]t available on the system.
 .IP \[bu] 2
 Verify verbs device is functional:
 .RS 2
 .IP \[bu] 2
 Does ibv_rc_pingpong (available in libibverbs) test work?
+.RS 2
 .IP \[bu] 2
 Does ibv_devinfo (available in libibverbs) show the device with
 PORT_ACTIVE status?
@@ -272,30 +247,32 @@ nodes in the cluster.
 Is the cable connected?
 .RE
 .RE
+.RE
 .IP \[bu] 2
 Verify librdmacm is functional:
 .RS 2
 .IP \[bu] 2
 Does ucmatose test (available in librdmacm) work?
 .IP \[bu] 2
-Is the IPoIB interface (e.g.
-ib0) up and configured with a valid IP address?
+Is the IPoIB interface (e.g.\ ib0) up and configured with a valid IP
+address?
+.RE
 .RE
 .SS Other issues
 .PP
 When running an app over verbs provider with Valgrind, there may be
-reports of memory leak in functions from dependent libraries (e.g.
-libibverbs, librdmacm).
+reports of memory leak in functions from dependent libraries
+(e.g.\ libibverbs, librdmacm).
 These leaks are safe to ignore.
 .PP
 The provider protects CQ overruns that may happen because more TX
 operations were posted to endpoints than CQ size.
-On the receive side, it isn\[aq]t expected to overrun the CQ.
+On the receive side, it isn\[cq]t expected to overrun the CQ.
 In case it happens the application developer should take care not to
 post excess receives without draining the CQ.
 CQ overruns can make the MSG endpoints unusable.
 .SH SEE ALSO
 .PP
-\f[C]fabric\f[](7), \f[C]fi_provider\f[](7),
+\f[C]fabric\f[R](7), \f[C]fi_provider\f[R](7),
 .SH AUTHORS
 OpenFabrics.
diff --git a/prov/bgq/configure.m4 b/prov/bgq/configure.m4
index 4a02059..5f88db6 100644
--- a/prov/bgq/configure.m4
+++ b/prov/bgq/configure.m4
@@ -72,7 +72,7 @@ AC_DEFUN([FI_BGQ_CONFIGURE],[
 				],
 				[bgq_external_source=$with_bgq_src])
 
-			AS_IF([test x"$bgq_external_source" == x"auto"], [
+			AS_IF([test x"$bgq_external_source" = x"auto"], [
 				for bgq_dir in `ls -r /bgsys/source`; do
 					AC_MSG_CHECKING([for bgq opensource distribution])
 					AS_IF([test -f /bgsys/source/$bgq_dir/spi/src/kernel/cnk/memory_impl.c],
@@ -80,7 +80,7 @@ AC_DEFUN([FI_BGQ_CONFIGURE],[
 						AC_MSG_RESULT([$bgq_external_source])
 						break)
 				done
-				AS_IF([test x"$bgq_external_source" == x"auto"], [
+				AS_IF([test x"$bgq_external_source" = x"auto"], [
 					bgq_happy=0
 					AC_MSG_RESULT([no])])
 			])
diff --git a/prov/efa/Makefile.include b/prov/efa/Makefile.include
index a4d027f..da3fa73 100644
--- a/prov/efa/Makefile.include
+++ b/prov/efa/Makefile.include
@@ -43,7 +43,6 @@ _efa_files = \
 	prov/efa/src/efa_rma.c \
 	prov/efa/src/rxr/rxr_attr.c	\
 	prov/efa/src/rxr/rxr_init.c	\
-	prov/efa/src/rxr/rxr_fabric.c	\
 	prov/efa/src/rxr/rxr_domain.c	\
 	prov/efa/src/rxr/rxr_cq.c	\
 	prov/efa/src/rxr/rxr_ep.c	\
@@ -52,6 +51,7 @@ _efa_files = \
 	prov/efa/src/rxr/rxr_msg.c	\
 	prov/efa/src/rxr/rxr_pkt_entry.c \
 	prov/efa/src/rxr/rxr_pkt_type_req.c \
+	prov/efa/src/rxr/rxr_pkt_type_base.c \
 	prov/efa/src/rxr/rxr_pkt_type_data.c \
 	prov/efa/src/rxr/rxr_pkt_type_misc.c \
 	prov/efa/src/rxr/rxr_pkt_cmd.c \
@@ -67,9 +67,11 @@ _efa_headers = \
 	prov/efa/src/rxr/rxr_pkt_entry.h \
 	prov/efa/src/rxr/rxr_pkt_type.h \
 	prov/efa/src/rxr/rxr_pkt_type_req.h \
+	prov/efa/src/rxr/rxr_pkt_type_base.h \
 	prov/efa/src/rxr/rxr_pkt_cmd.h \
 	prov/efa/src/rxr/rxr_read.h \
-	prov/efa/src/rxr/rxr_atomic.h
+	prov/efa/src/rxr/rxr_atomic.h \
+	prov/efa/src/rxr/rdm_proto_v4.h
 
 efa_CPPFLAGS += \
 	-I$(top_srcdir)/prov/efa/src/ \
diff --git a/prov/efa/configure.m4 b/prov/efa/configure.m4
index 8d1693f..6f309f8 100644
--- a/prov/efa/configure.m4
+++ b/prov/efa/configure.m4
@@ -29,7 +29,7 @@ AC_DEFUN([FI_EFA_CONFIGURE],[
 		],
 		[efa_h_enable_poisoning=$enableval],
 		[efa_h_enable_poisoning=no])
-	AS_IF([test x"$efa_h_enable_poisoning" == x"yes"],
+	AS_IF([test x"$efa_h_enable_poisoning" = x"yes"],
 		[AC_DEFINE([ENABLE_EFA_POISONING], [1],
 			[EFA memory poisoning support for debugging])],
 		[])
@@ -84,6 +84,21 @@ AC_DEFUN([FI_EFA_CONFIGURE],[
 	      ])
 	CPPFLAGS=$save_CPPFLAGS
 
+	dnl Check for ibv_is_fork_initialized() in libibverbs
+	have_ibv_is_fork_initialized=0
+	AS_IF([test $efa_happy -eq 1],
+		[AC_CHECK_DECL([ibv_is_fork_initialized],
+			[have_ibv_is_fork_initialized=1],
+			[],
+			[[#include <infiniband/verbs.h>]])
+		])
+
+	AC_DEFINE_UNQUOTED([HAVE_IBV_IS_FORK_INITIALIZED],
+		[$have_ibv_is_fork_initialized],
+		[Define to 1 if libibverbs has ibv_is_fork_initialized])
+
+	AS_IF([test "$enable_efa" = "no"], [efa_happy=0])
+
 	AS_IF([test $efa_happy -eq 1 ], [$1], [$2])
 
 	efa_CPPFLAGS="$efa_ibverbs_CPPFLAGS $efadv_CPPFLAGS"
diff --git a/prov/efa/docs/atomic_fetch_compare.drawio b/prov/efa/docs/atomic_fetch_compare.drawio
new file mode 100644
index 0000000..e6592ff
--- /dev/null
+++ b/prov/efa/docs/atomic_fetch_compare.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-25T17:06:42.692Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="w2Sv1vzeACmmFzqIVc2Y" version="12.4.8" type="device"><diagram id="y0qt14K1OZjQ2kAYbhKE" name="Page-1">3Zhfd5owGMY/jWdX2xGpgpdorb2Ytkfbs+7KE+EVchYIC1Fhn36JCULA9qw7tnW7Mu+TkD+/502IdOxxnE8ZSqMZDYB0et0g79jXnV7P6lqu+JFKoRTHcpQQMhzoRpWwxL+gfFKrWxxAZjTklBKOU1P0aZKAzw0NMUb3ZrMNJeaoKQqhJSx9RNrqNxzwSKluv1vpt4DDqBzZ6uqaGJWNtZBFKKD7mmRPOvaYUcpVKc7HQCS8kot67uaZ2uPEGCT8Tx6IZsPocT1y89XnJ/zorvNN9Pi5p3rZIbLVC/bSlGAfcUwTiRURkolfgtcbtMbM/ySjDXA/EiP5NJbjS9ScxtjXC+VFSY/RbRKAnEC3Y4/2EeawTJEva/ciX4QW8ZiIyBLF9oL0GnfAOOQ1SS9wCjQGzgrRRNeWrAsz3FfOWaUW1VwbaA3pZAmPHVc8RUEjfQVeu4V3AVlKBRN2AJkyGjLIZBGSECfw8QQvDeGghbDFCJLAk1tdRD5BWSYS0cASoCw6IJSBQMOKJ8nzS78Mv2u8h+A6N6JCR8+y5YiFwF9YwVC1g8A4adoO1BD3TxAuNQZE7M6deT6dwq5HuKdYzPhosDMU66477Dacy+iW+aCfqp8njY7cRqL0G/0oLq1+DklwXPXf54Xzz+dF+Yq7kMQ4X2a0U2P4rqnhnjk1nrX4Qpxr4La6g/P4Zg0G7+rbsOWbeRkRY1d3EYbVXUTeQQioFh/97myk/dVHvzvLE6ZGdE4vjNnlQbtqQRvfze69xWS1ePBElzeTh/GtKjfgiTVzk1DGGf0BY0ooE0pCxQ3PHm0wIQ0JERwm8jASJEHoI0lQZD7xdEWMg0AOc9IS0zRGudoxYgrOeVyyBo2TofwXV3Pp6oRJvTOYNJ+u5h5fFc7X6c/Ntrd7mi9vTlys/+8jvnU0n+vWZr3Zre2kb+295T3czRbL+8vfS3b/xeveKzZTv+Gl82abSYTVBwblYfWZxp78Bg==</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/atomic_write.drawio b/prov/efa/docs/atomic_write.drawio
new file mode 100644
index 0000000..a34ae32
--- /dev/null
+++ b/prov/efa/docs/atomic_write.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-25T17:00:30.288Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="9_luOnClqkntQMJL5NXL" version="12.4.8" type="device"><diagram id="ukeF6noGK0I00wAWyjMB" name="Page-1">5Zddk5owFIZ/jdOr7YCsiJfq2m0vttPqOru9ciIcIdNAmBAV+uubSCKEaKfbcVZneqOcN1+c5z2E0POmafnIUJ480QhIr+9EZc976PX7ruMG4k8qVa0M3WEtxAxHqlMjLPAv0COVusURFEZHTinhODfFkGYZhNzQEGN0b3bbUGKumqMYLGERImKrLzjiSa0GA6fRPwOOE72y66iWFOnOSigSFNF9S/JmPW/KKOX1VVpOgUh4mks97tOZ1uONMcj43wxInkbJcj0JytXdK14G63KTLO/69Sw7RLYq4XGeExwijmkmsaLDHASvN2jNcPihkBDxCnGa4lAlxitNi9FtFoFc0Ol5k32COSxyFMrWvagPoSU8JSJyxaWdgMppB4xD2ZJUQo9AU+CsEl1Uq2ZbmeG+ccrVWtJyyVcaUsURHydu+IkLhfANOD0L5xyKnAom7EAuZzRmUMhLyGKcwfUJ3hpC30JoMYIsGstHW0QhQUUhCtHAEqEiOSCUgUDDqlfJ8+NAhz8U3kPwUBpRpaKzbDliMfA/ZDCq+0Fk7Cy2Ay3EgxOEtcaAiKdxZ+5Hp7CrFb5RLO74aPBwJPJuOxx0nCvoloWgRrX3j85EQadQBp15ai7WPIciOGb973UxvHBdnPX3Rny7nHG2c6N3dS74z5zr4HYd/zK+ub7/rr6NLN/Ms4HcBbtHg5CmOYG6w7XfbJ2q96/9ZtNH2hbQr1TE0+/iZ5bpJW7qNHB9aPcWtJf5l+fZav48tnCJLLnJpOCM/oQpJZQJJaPixOVNNpiQjoQIjjO5+wh2IPSJZCZKnYxVQ4qjSC5z0gTTJkZ5/YiIWxhexhfX72wF+iuq5cv9CVv6b7dFhM03Sb2VNF923uw3</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/building.md b/prov/efa/docs/building.md
index 2a3f7ce..1eb0991 100644
--- a/prov/efa/docs/building.md
+++ b/prov/efa/docs/building.md
@@ -47,7 +47,7 @@ GPU memory.
 
 CFLAGS that might be useful:
 
-* `RXR_PERF_ENABLED`: enable the perf hooks to determine cycle/instruction count
+* `EFA_PERF_ENABLED`: enable the perf hooks to determine cycle/instruction count
 for functions in the send/receive/completion paths. See fi_hook(7) and the
 Linux perf documentation for more information.
 * `ENABLE_RXR_PKT_DUMP`: turn on packet dump prints, very verbose. These
diff --git a/prov/efa/docs/efa_rdm_protocol_v4.md b/prov/efa/docs/efa_rdm_protocol_v4.md
new file mode 100644
index 0000000..359cdc9
--- /dev/null
+++ b/prov/efa/docs/efa_rdm_protocol_v4.md
@@ -0,0 +1,1420 @@
+# EFA RDM Communication Protocol version 4
+
+## 0. Overview
+
+This document describes version 4 of EFA RDM communication protocol (protocol v4),
+which is adopted by libfabric EFA provider's RDM endpoint since libfabric 1.10.0 release.
+
+The purpose of this document is to provide a definition of the protocol that is
+not tied to a specific implementation. It is useful to distinguish protocol and
+implementation, because protocol change can cause backward compatibility issue,
+therefore needs to be handled with extra care.
+
+It is organized as the following:
+
+Chapter 1 "Basics" introduces some basic facts/concepts of EFA RDM protocol, including:
+
+ * Section 1.1 Why is EFA RDM protocol needed?
+
+ * Section 1.2 A list of features/sub-protocols.
+
+ * Section 1.3 packet, packet base header and a list of packet types.
+
+Chapter 2 "Handshake sub-protocol" describes the handshake sub-protocol, including:
+
+ * Section 2.1 "Handshake sub-protocol and backward compatibility" describes how to introduce
+   backward compatible changes to protocol v4, and how handshake sub-protocol is used to
+   facilitate the process.
+
+ * Section 2.2 "Handshake sub-protocol and raw address exchange" describes how handshake sub-protocol
+   impacts the behavior of including raw address in packet header.
+
+ * Section 2.3 "Implementation tips" include tips when implementing handshake sub-protocol.
+
+Chapter 3 "baseline features" describes the baseline features of protocol v4.
+
+ *  Section 3.1 "REQ packets" introduces the binary format of REQ packets, which all baseline features
+    use to initialize the communication.
+
+ *  Section 3.2 "baseline features for two-sided communications" describe 3 two-sided communication baseline features:
+
+    - eager message transfer,
+    - medium message transfer and
+    - long-cts message transfer.
+
+ *  Section 3.3 "baseline features for one-sided communications" describe 7 one-sided communication baseline features:
+
+    - emulated eager write,
+    - emulated long-cts write,
+    - emulated short read,
+    - emulated long-cts read,
+    - emulated write atomic,
+    - emulated fetch atomic and
+    - emulated compare atomic.
+
+Chapter 4 "extra features/requests" describes the extra features/requests defined in version 4.
+
+ *  Section 4.1 describe the extra feature: RDMA read based message transfer.
+
+ *  Section 4.2 describe the extra feature: delivery complete.
+
+ *  Section 4.3 describe the extra request: constant header length.
+
+ *  Section 4.4 describe the extra request: connid (connection ID) header.
+
+Chapter 5 "what's not covered?" describe the contents that are intentionally left out of
+this document because they are considered "implementation details".
+
+## 1. Basics
+
+EFA RDM communication protocol is for two lifabric endpoints to use EFA device to communication
+with each other.
+
+### 1.1 Why is EFA RDM communication protocol needed?
+
+The reason we need a EFA RDM communication protocol is to support features that
+EFA device does not directly support. Currently, EFA device supports the following
+two types of communications:
+
+ 1. send/receive a message up to EFA device's Maximum Transmission Unit (MTU) size.
+ 2. RDMA read of a memory buffer up to 1GB (if both endpoints' software stacks support RDMA read).
+
+Moreover, for send/receive, EFA device does not guarantee ordered delivery, e.g. when sender
+sends multiple messages to a receiver, the receiver may receive the packets in an order different
+from how they are sent.
+
+Protocol v4 defines how two endpoints can use EFA device's capability to achieve:
+
+ * send/receive up to 2^64-1 bytes,
+ * read up to 2^64-1 bytes,
+ * write up to 2^64-1 bytes,
+ * atomics up to MTU size.
+
+Moreover, protocol v4 provide mechanisms to meet extra requirements of application, which EFA device
+does not support, such as ordered send/receive (`FI_ORDER_SAS`) and delivery complete (DC).
+
+### 1.2 a list of sub-protocols
+
+To meet application's specific needs, protocol v4 defines a set of sub-protocols for
+as listed in table 1.1:
+
+Table: 1.1 a list of sub-protocols
+
+| Sub Protocol Name             | Used For  | Definition in |
+|-|-|-|
+| Eager message                 | Two sided | Section 3.2   |
+| Medium message                | Two sided | Section 3.2   |
+| Long-CTS message              | Two sided | Section 3.2   |
+| Long READ message             | Two sided | Section 4.1   |
+| DC Eager message              | Two sided | Section 4.2   |
+| DC Medium message             | Two sided | Section 4.2   |
+| DC Long-CTS message           | Two sided | Section 4.2   |
+| Emulated eager write          | One sided | Section 3.3   |
+| Emulated long-CTS write       | One sided | Section 3.3   |
+| Emulated long-read write      | One sided | Section 4.1   |
+| Emulated DC eager write       | One sided | Section 4.2   |
+| Emulated DC long-CTS write    | One sided | Section 4.2   |
+| Emulated short read           | One sided | Section 3.3   |
+| Emulated long-CTS read        | One sided | Section 3.3   |
+| Direct read                   | One sided | Section 4.1   |
+| Emulated atomic               | One sided | Section 3.3   |
+| Emulated fetch atomic         | One sided | Section 3.3   |
+| Emulated compare atomic       | One sided | Section 3.3   |
+| Handshake                     | Backward compatibility | Chapter 2 |
+
+### 1.3 packet, packet base header and a list of packets
+
+All the sub-protocols (except the Direct Read protocol) use packet(s) to exchange
+information between two endpoints.
+
+A packet is a message that does not exceed MTU size, and is exchanged between
+two endpoints using EFA device's send/receive capability.
+
+Protocol v4 defines a set of packet types. They can be split into two category:
+REQ packet types and non-REQ packet types.
+
+A REQ packet was the 1st packet sender/requester send to the receiver/responder
+in the workflow of a sub-protocol. Each sub-protocol is unique, thus each
+sub-protocol defines its own REQ packet type.
+
+A non-REQ packet is used to by some sub-protocols to transfer additional
+information that is not covered in the REQ packet.
+
+To distinguish various types of packets sent/received between two endpoints,
+each packet type was assigned an unique packet type ID. Table 1.2 lists
+all the packet types in protocol v4 and sub-protocol(s) that used it:
+
+Table: 1.2 a list of packet type IDs
+
+| Packet Type ID  | Nick Name         | Full Name                 | Category | Used by                       |
+|-|-|-|-|-|
+| 1               | RTS               | Request To Send           | non-REQ  | Deprecated                    |
+| 2               | CONNACK           | CONNection ACKnowlegement | non-REQ  | Deprecated                    |
+| 3               | CTS               | Clear To Send             | non-REQ  | long-CTS message/read/write |
+| 4               | DATA              | Data                      | non-REQ  | long-CTS message/read/write |
+| 5               | READRSP           | READ ReSPonse             | non-REQ  | emulated short/long-read      |
+| 6               | reserved          | N/A                       | non-REQ  | reserved for internal use      |
+| 7               | EOR               | End Of Read               | non-REQ  | long-read message/write     |
+| 8               | ATOMRSP           | ATOMic ResSPonse          | non-REQ  | emulated write/fetch/compare atomic |
+| 9               | HANDSHAKE         | Handshake                 | non-REQ  | handshake                     |
+| 10              | RECEIPT           | Receipt                   | non-REQ  | delivery complete (DC)         |
+| 64              | EAGER_MSGRTM      | Eager non-tagged Request To Message       | REQ  | eager message |
+| 65              | EAGER_TAGRTM      | Eager tagged Request To Message           | REQ  | eager message |
+| 66              | MEDIUM_MSGRTM     | Medium non-tagged Request To Message      | REQ  | medium message |
+| 67              | MEDIUM_TAGRTM     | Medium tagged Request To Message          | REQ  | medium message |
+| 68              | LONGCTS_MSGRTM    | Long-CTS non-tagged Request To Message    | REQ  | long-CTS message |
+| 69              | LONGCTS_TAGRTM    | Long-CTS tagged Request To Message        | REQ  | long-CTS message |
+| 70              | EAGER_RTW         | Eager Request To Write                    | REQ  | emulated eager write |
+| 71              | LONGCTS_RTW       | Long-CTS Request To Write                 | REQ  | emulated long-CTS write |
+| 72              | SHORT_RTR         | Eager Request To Read                     | REQ  | emulated short read |
+| 73              | LONGCTS_RTR       | Long-CTS Request To Read                  | REQ  | emulated long-CTS read |
+| 74              | WRITE_RTA         | Write Request To Atomic                   | REQ  | emulated write atomic |
+| 75              | FETCH_RTA         | Fetch Request To Atomic                   | REQ  | emulated fetch atomic |
+| 76              | COMPARE_RTA       | Compare Request To Atomic                 | REQ  | emulated compare atomic |
+| 128             | LONGREAD_MSGRTM   | Long-read non-tagged Request To Message   | REQ  | Long-read message |
+| 129             | LONGREAD_TAGRTM   | Long-read tagged Request To Message       | REQ  | Long-read message |
+| 130             | LONGREAD_RTW      | Long-read Request To Write                | REQ  | Long-read message |
+| 131             | reserved          | N/A                                       | N/A  | N/A               |
+| 132             | reserved          | N/A                                       | N/A  | N/A               |
+| 133             | DC_EAGER_MSGRTM   | DC Eager non-tagged Request To Message    | REQ  | DC eager message |
+| 134             | DC_EAGER_TAGRTM   | DC Eager tagged Request To Message        | REQ  | DC eager message |
+| 135             | DC_MEDIUM_MSGRTM  | DC Medium non-tagged Request To Message   | REQ  | DC medium message |
+| 136             | DC_MEDIUM_TAGRTM  | DC Medium tagged Request To Message       | REQ  | DC medium message |
+| 137             | DC_LONGCTS_MSGRTM | DC long-CTS non-tagged Request To Message | REQ  | DC long-CTS message |
+| 138             | DC_LONTCTS_TAGRTM | DC long-CTS tagged Request To Message     | REQ  | DC long-CTS message |
+| 139             | DC_EAGER_RTW      | DC Eager Request To Write                 | REQ  | DC emulated eager write |
+| 140             | DC_LONGCTS_RTW    | DC long-CTS Request To Write              | REQ  | DC emulated long-CTS write |
+| 141             | DC_WRITE_RTA      | DC Write Request To Atomic                | REQ  | DC emulated write atomic |
+
+The packet type ID is included in the 4 bytes EFA RDM base header, which every packet must be started
+with. The format of the EFA RDM base header is listed in table 1.3:
+
+Table: 1.3 format of EFA RDM base header
+
+| Name | Length (bytes) | type | C language type |
+|-|-|-|-|
+| `type`    | 1 | integer | `uint8_t` |
+| `version` | 1 | integer | `uint8_t` |
+| `flags`   | 2 | integer | `uint16_t` |
+
+In the table, `type` is the packet type ID.
+
+`version` is the EFA RDM protocol version, which is 4 for protocol v4.
+
+`flags` is a set of flags each packet type uses to customize its behavior. Typically, it is used
+to indicate the existence of optional header(s) in the packet header. Each packet type defines its own flags.
+
+Protocol v4 define the following universal flag, which every packet type should use:
+
+Table: 1.4 a list of universal flags
+
+| Bit ID | Value | Name | Description | Used by |
+|-|-|-|-|
+| 15     | 0x8000 | CONNID_HDR | This packet has "connid" in header | extra request "connid header" (section 4.4) |
+
+Note, the flag `CONNID_HDR` only indicate the presence of connid in the header. The exact location of connid
+would be different for each packet type.
+
+Other then the universal flags, each packet type defines its own flags.
+
+The format of each packet type is introduced in the sections where the sub-protocols are introduced.
+
+### 1.4 raw address
+
+raw address is the ID of an EFA RDM endpoint.
+
+To send message to an EFA endpoint, one need to know the endpoint's raw address. It the call
+`fi_av_insert` to insert the raw address to its address vector. `fi_av_insert` will return a libfabric
+internal address, the internal address is used to send message.
+(see [fi_av](https://ofiwg.github.io/libfabric/v1.1.1/man/fi_av.3.html) for more details)
+
+Interestingly, to receive message from an EFA endpoint, one does not need to know the endpoint's
+raw address. See section 2.3 for more discussion on this topic.
+
+Each provider defines its address format, the raw address of EFA RDM endpoint uses the
+format in the following table 1.5.
+
+Table: 1.5 binary format of EFA RDM raw address
+
+| Name | Lengths (bytes) | type | C language type | Notes |
+|-|-|-|-|-|
+| `gid`  | 16 | array   | `uint8_t[16]` | ipv6 format |
+| `qpn`  |  2 | integer | `uint16_t`    | queue pair number |
+| `pad`  |  2 | integer | `uint16_t`    | pad to 4 bytes |
+| `connid` | 4 | integer | `uint32_t`   | connection ID |
+| `reserved` | 8 | integer | `uint64_t` | reserved for internal use |
+
+The field `connid` warrants extra explanation: it is a 4-byte random integer generated
+during endpoint initialization, which can be used to identify the endpoint. When protocol v4
+was initially introduced, the field `connid` was named `qkey`, which is a concept of
+EFA device. Later it is realized that this is in fact a connection ID, which we happen
+to use a EFA device's Q-Key.
+
+Currently, the raw address of EFA is 32 bytes, but it can be expanded in the future without
+breaking backward compatibility.
+
+## 2. Handshake sub-protocol
+
+Handshake sub-protocol serves two purposes in protocol v4.
+
+First, it is used to exchange two endpoints' capability information, which allows to introduce
+changes to protocol v4 without breaking backward compatibility. (section 2.1)
+
+Second, it is used to adjust the behavior of including EFA raw address in REQ packet header
+(section 2.2)
+
+### 2.1 Handshake sub-protocol and backward compatibility
+
+The biggest problem when designing a communication protocol is how to maintain backward compatibility
+when introducing changes to the protocol. Imagine the following scenario: there are endpoints that are
+using protocol v4 in its current form. If a change is made to the protocol, how to make sure that
+the existing endpoints still be able to communicate with endpoints that have adopted changes?
+
+To tackle this issue, protocol v4 first introduced the concepts of "feature" and "request".
+
+- A feature is a functionality that an endpoint can support. Typically, a feature
+is the support of  a set of sub-protocols.
+
+- A request is an expectation an endpoint has on its peer. Typically, a request is
+for its peer to include some extra information in packet header.
+
+Protocol v4 defines the following 10 features as baseline features:
+
+- Eager message (send/receive)
+- Medium message (send/receive)
+- Long-CTS message (send/receive)
+- Emulated eager write
+- Emulated long-CTS write
+- Emulated short read
+- Emulated long-CTS read
+- Emulated write atomic
+- Emulated fetch atomic
+- Emulated compare atomic
+
+The definition of these baseline features are in chapter 3. Any endpoint that adopts protocol
+v4 must support these baseline features.
+
+Protocol v4 then allow changes to be introduced as "extra feature" and "extra request".
+Each extra feature/request will be assigned an ID, when it is introduce to protocol v4.
+The ID starts from 0, and increases by 1 for each extra feature/request. Typically,
+new extra feature/request is introduced with libfaric minor releases, and will NOT be
+back ported.
+
+Currently there are 4 such extra features/requests, as listed in table 2.1:
+
+Table: 2.1 a list of extra features/requests
+
+| ID | Name              |  Type    | Introduced since | Described in |
+|-|-|-|-|-|
+| 0  | RDMA read based data transfer    | extra feature | libfabric 1.10.0 | Section 4.1 |
+| 1  | delivery complete                | extra feature | libfabric 1.12.0 | Section 4.2 |
+| 2  | keep packet header length constant | extra request | libfabric 1.13.0 | Section 4.3 |
+| 3  | sender connection id in packet header  | extra request | libfabric 1.14.0 | Section 4.4 |
+
+How does protocol v4 maintain backward compatibility when extra features/requests are introduced?
+
+First, protocol v4 states that endpoint's support of an extra feature/request is optional,
+therefore cannot be assumed.
+
+Second, protocol v4 defines the handshake sub-protocol for two endpoint to exchange its extra
+feature/request status. Its workflow is:
+
+1. If an endpoint has never communicated with a peer, it does not know the peer's
+   extra/feature request status. Therefore, it can only use the baseline features to
+   communicate with a peer, which means it will send REQ packets (section 3.1) to the peer
+   to initialize a communication.
+2. Upon receiving the 1st packet from a peer, an endpoint must send back a handshake
+   packet, which contains the endpoint's capability information.
+3. Upon receiving the handshake packet, an endpoint will know the peer's extra feature/request
+   status.
+
+Regarding extra feature, if the peer support the extra feature the endpoint want to use,
+the endpoint can start using the extra feature. Otherwise, one of the following should happen:
+
+- a. the communication continues without using the extra feature/request, though
+   the performance may be sub-optimal. For example, if the peer does not support
+   the extra feature "RDMA read based data transfer", the endpoint can choose to
+   use baseline features to carry on the communication, though the performance will
+   be sub-optimal (section 4.1).
+
+- b. the requester of the communication aborts the communication and return an error
+   to the application. For example, if application requires delivery complete, but
+   the peer does not support it (this can happen when endpoint is using libfabric 1.12,
+   but the peer is using libfabric 1.10), the requester need to return an error to the
+   application (section 4.2)
+
+Regarding extra request, if an endpoint can support an extra request the peer has requested,
+it should comply the request. Otherwise, it can ignore the request. Peer should be do
+one of the following:
+
+- a. carry on the communication without using the extra request.
+
+- b. abort the communication and return an error to application. (see section 4.3
+     for example)
+
+For example, if sender is using libfabric 1.10, and receiver is using libfabric 1.13.
+If receiver is in zero copy receive mode, it will have the the extra request
+"constant header length", but sender does not support it. In this case, it is OK
+for sender to ignore the request, and send packets with different header length.
+It is receiver's responsibility to react accordingly. (section 4.3)
+
+This concludes the workflow of the handshake sub-protocol.
+
+The binary format of a HANDSHAKE packet is listed in table 2.2.
+
+Table: 2.2 binary format of the HANDSHAKE packet
+
+| Name      | Length (bytes) | type | C language type |
+|-|-|-|-|
+| `type`    | 1 | integer | `uint8_t`  |
+| `version` | 1 | integer | `uint8_t`  |
+| `flags`   | 2 | integer | `uint16_t` |
+| `nextra_p3`  | 4 | integer | `uint32_t` |
+| `extra_info`  | `8 * (nextra_p3 - 3)` | integer array | `uint64_t[]` |
+| `connid`  | 4 | integer | sender connection ID, optional, present when the CONNID_HDR flag is on `flags` |
+| `padding` | 4 | integer | padding for `connid`, optional, present when the CONNID_HDR flag is on `flags` |
+
+The first 4 bytes (3 fields: `type`, `version`, `flags`) is the EFA RDM base header (section 1.3).
+
+Immediately after the base header, there are 2 fields `nextra_p3` and `extra_info`.
+
+The field `extra_info` is an array of 8 byte integers, which stores the capability of an endpoint.
+
+As mentioned before, each extra feature/request was assigned an ID when it was introduced to protocol v4.
+When constructing the handshake packet, for each extra feature it supports (or an extra request it want to impose),
+an endpoint need to toggle on a corresponding bit in the `extra_info` array. Specifically, if an endpoint supports
+the extra feature with ID `i` (or want to impose extra request with ID `i`), it needs to toggle on the
+No. `i%64` bit of the No. `i/64` member of the `extra_info` array.
+
+For example, if an endpoint supports the extra feature "RDMA read based data transfer" (ID 0), it needs to
+toggle on the No. 0 (the first) bit of `extra_info[0]`. (section 4.1)
+
+If an endpoint wants to impose the "constant header length" extra request it need to toggle on bit No 2.
+in `extra_info[0]`. (section 4.3)
+
+Note, the field `extra_info` was named `features` when protocol v4 was initially introduced, at that time we
+only planned for extra features. Later, we discovered that the handshake sub-protocol can also be used to pass
+additional request information, thus introduced the concept of "extra request" and renamed this field `extra_info`.
+
+`nextra_p3` is number of `extra_info` flags of the endpoint plus 3. The "plus 3" is for historical reasons.
+When protocol v4 was initially introduced, this field is named `maxproto`. The original plan was that protocol
+v4 can only have 64 extra features/requests. If the number of extra feature/request ever exceeds 64, the next
+feature/request will be defined as version 5 feature/request, (version 6 if the number exceeds 128, so on so
+forth). The field `maxproto` means maximumly supported protocol version by an endpoint. The recipient of the
+HANDSHAKE packet use `maxproto` to calculate how many members `extra_info` has, which is `maxproto - 4 + 1`.
+(Starting from v4, each version has 1 flag, so if `maxproto` is 5, there are 2 members in `extra_info`. One
+for v4, the other for v5. Therefore the formula to compute number of number of members is `maxproto - 4 + 1`)
+
+However, it was later realized the original plan is overly complicated, and can cause a lot of confusion.
+For example, if an endpoint support a feature defined in version 5, what version number should it put in
+the base header? Given that the sole purpose of the field `maxproto` is to provide a way to calculate
+how many members the `extra_info` array has, the protocol would be much easier to understand if we re-interpret
+the field `maxproto` as `nextra_p3` and allow protocol v4 to have more than 64 extra feature/requests.
+
+After `extra_info`, there are two optional field `connid` and `padding`:
+
+`connid` is the sender's connection ID (4 bytes), `padding` is a 4 byte space to make the packet to align
+to 8 bytes boundary.
+
+These two fields were introduced with the extra request "connid in header". They are optional,
+therefore an implemenation is not required to set them. (section 4.4 for more details) If an implementation
+does set the connid, the implementation needs to toggle on the CONNID_HDR flag in `flags` (table 1.4).
+
+### 2.2 handshake sub-protocol and raw address exchange
+
+Another functionality of the handshake sub-protocol is to adjust behavior of including raw address in packet header.
+
+Currently, if an endpoint is communicating with a peer for the first time, it will include its raw address
+in the REQ packets it sends.
+
+After the endpoint received the HANDSHAKE packet from the peer, it will stop including its raw address
+in the header (see section 4.3 for an exception).
+
+This behavior is to compensate for a limitation of EFA device, which is EFA device cannot report
+the address of a packet of an unknown sender.
+
+The EFA device keeps an address book, which contains a list of raw addresses of its peer. Each address is assigned
+an address handle number (AHN). When EFA device received a message, it will report the AHN of the address.
+
+However, if the address of a received message is not in the address book, there is no AHN assigned to
+the address. In this case, EFA device will not be able to report the AHN.
+
+For the communication to proceed, an endpoint needs to have to the address of any received packet, because
+it need to send packets back.
+
+Therefore, if an endpoint is communicating with a peer for the 1st time, it will have to include its raw
+address in the header of REQ packets it sends to the peer. (see section 3.1 for the details of REQ packets).
+
+Upon receiving the packet, if the peer does not have the endpoint's address in peer's address book,
+the peer can get the endpoint's raw address from REQ packet header, then insert the raw address to its address book.
+
+This insertion only need to happen once, for the next packets from the endpoint, the EFA device will be able
+to report AHN. Therefore, it is desirable to have a mechanism for an endpoint to stop including raw address
+in packet header to reduce packet header length.
+
+As it turns out, the handshake sub-protocol is the perfect mechanism for that.
+
+In handshake sub-protocol, an endpoint will send a HANDSHAKE packet upon receiving 1st REQ packet from
+a peer. At that point, the peer's raw address must have been inserted to its address book.
+
+If an endpoint received a HANDSHAKE packet from a peer, the peer must know the endpoint's address, therefore
+the endpoint can stop including raw address in packet header.
+
+This concludes the discussion of the workflow.
+
+
+### 2.3 Implementation tips
+
+When implementing the handshake sub-protocol, keep in mind that the application
+does not know the existence of a HANDSHAKE packet, therefore will not wait
+for its completion.
+
+For example, it is normal that a HANDSHAKE packet encounter an send error
+because peer has already been closed, because application might just send
+1 message and close the endpoint.
+
+It is also possible to close an endpoint when there are inflight HANDSHAKE
+packet, because the application might just want to receive 1 message, then
+close the endpoint. However, the action of receiving a message, will cause
+a HANDSHAKE packet to be sent.
+
+## 3. Baseline features
+
+This part describes the 10 baseline features in protocol v4, which uses only the send/receive
+functionality of EFA device, and should be supported by any endpoint that implements protocol v4.
+
+### 3.1 REQ packet types
+
+Before getting into details of each baseline feature, we give a general introduction to
+the REQ packet types, which all these baseline features use to initialize the communication.
+
+REQ packets is not one but a category of packet types. In this chapter, 10 REQ packet types will be
+covered, as each baseline feature has its own REQ packet type.
+
+According to the type of communications it is used for, REQ packet types can be further divided into
+4 categories:
+
+RTM (Request To Message) is used by message sub-protocols (for two-sided communication). RTM can be
+further divided into MSGRTM and TAGRTM. TAGRTM is used when application calls libfabric's tagged
+send/receive API (such as `fi_tsend` and `fi_trecv`), MSGRTM is used by the non-tagged send/receive
+API (such as `fi_send` and `fi_recv`).
+
+RTW (Request To Write) is used by emulated write sub-protocols.
+
+RTR (Request To Read) is used by emulated read sub-protocols.
+
+RTA (Request To Atomic) is used by emulated atomic sub-protocols.
+
+Regardless, all REQ packets are consisted of 3 parts: REQ mandatory header, REQ optional header and
+application data (optional).
+
+**REQ mandatory header** is unique for each individual REQ packet type. However, they all must start with
+the same 4 bytes EFA RDM base header (section 1.3). Recall that a base header is consisted with 3 fields:
+`type`, `version` and `flags`. Among them, `flags` warrants more discussion here, as all REQ packets share
+the same set of flags, which is listed in table 3.1:
+
+Table: 3.1 a list of REQ packet flags
+
+| Bit Id | Value | Name | meaning |
+|-|-|-|-|
+|  0     | 0x1    | REQ_OPT_RAW_ADDR_HDR | This REQ packet has the optional raw address header |
+|  1     | 0x2    | REQ_OPT_CQ_DATA_HDR  | This REQ packet has the optional CQ data header |
+|  2     | 0x4    | REQ_MSG              | This REQ packet is used by two-sided communication |
+|  3     | 0x8    | REQ_TAGGED           | This REQ packet is used by tagged two-sided communication |
+|  4     | 0x10   | REQ_RMA              | This REQ packet is used by an emulated RMA (read or write) communication |
+|  5     | 0x20   | REQ_ATOMIC           | This REQ packet is used by an emulated atomic (write,fetch or compare) communication |
+| 15     | 0x8000 | CONNID_HDR           | This REQ packet has the optional connid header |
+
+Note, the CONNID_HDR flag is an universal flag (table 1.4), and is listed here for completeness.
+
+**REQ optional headers** contain additional information needed by the receiver of the REQ packets.
+As mentioned earlier, the existence of optional header in a REQ packet is indicated by bits in the `flags`
+field of the base header. There are currently 3 REQ optional headers defined:
+
+1. the raw address header, which has the following format:
+
+Table: 3.2 format of REQ optional raw address header
+
+| Field | type    | Length | C type |
+|-|-|-|-|
+| `size`  | integer | 4      | `uint32` |
+| `addr`  | array   | `size` | `uint8[]` |
+
+As can be seen, the optional raw address is consisted of two fields `size` and `addr`. The field `size` describes
+number of bytes in the `addr` array. The field `addr` contains the raw address. The `size` field is necessary because
+the raw address format of EFA can be expanded in the future.
+
+As mentioned before, an endpoint will include raw address in REQ packet before it receives a handshake packet back
+from a peer. This is because the peer might not have the endpoint's raw address in its address vector, thus cannot
+communicate with the endpoint.
+
+2. the CQ data header, which is an 8 byte integer. CQ data header is used when application called libfabric's
+CQ data send/write API (such as `fi_senddata`, `fi_tsenddata` and `fi_writedata`), which will include an extra
+data in the RX completion entry written to application.
+
+3. the connid (connection ID) header, which is a 4 byte integer. It is used when peer has the "connid header"
+extra request, and the endpoint can support it. More information about this header in section 4.4.
+
+Note, it is possible to have multiple optional REQ headers in one REQ packets. In this case, the order they appear
+in the REQ packets must be the same as their bit appear in the `flags` field. e.g. the raw address header
+must precede the CQ data header, and the CQ data header must precede the connid header.
+
+**Application data** follows immediately after the optional header. Note that not all REQ packet types contain
+application data. For example, the RTR (Request To Read) packet type does not contain application data.
+
+### 3.2 baseline features for two-sided communication
+
+This section describes the 3 baseline features for two sided communication: eager message, medium message, long-CTS message.
+Each of them correspond to the same named sub-protocol. When describing a sub-protocol, we always follow
+the same structure: workflow, packet format and implementation tips.
+
+#### Eager message feature/sub-protocol
+
+Eager message feature/sub-protocol is used when application's send buffer is small enough to be fit in one packet.
+This protocol works in the following order:
+
+1. On sender side, application call libfabric's send API, providing a send buffer.
+2. On receiver side, application call libfabric's receive API, providing a receive buffer.
+3. Sender sends an EAGER_RTM (EAGER_MSGRTM or EAGER_TAGRTM) packet, which contains the application's data.
+4. Upon receiving the packet, receiver will process the received packet, and make sure the received
+   data is in application's receive buffer.
+
+The following diagram illustrate the workflow:
+
+![eager message](message_eager.png)
+
+The mandatory header of an EAGER_RTM packet is described in table 3.3:
+
+Table: 3.3 format of an EAGER_RTM packet
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`      | 1 | integer | `uint8_t`  | part of base header |
+| `version`   | 1 | integer | `uint8_t`  | part of base header|
+| `flags`     | 2 | integer | `uint16_t` | part of base header |
+| `msg_id`    | 4 | integer | `uint32_t` | message ID |
+| `tag`       | 8 | integer | `uint64_t` | for eager TAGRTM only |
+
+The field `msg_id` records the sending order of all RTM packets between two endpoint.
+Receiver can use it to re-order the received RTM packet from the endpoint.
+
+When implementing the eager message sub-protocol, there are a few points worth attention:
+
+1. Noticing that `msg_id` is 4 bytes integer, which means its maximum value is 4,294,967,295.
+After it reaches the maximum value, next message's `msg_id` will became 0. This "wrap around" of
+message id can happen when two endpoints communicate for an extended period of time. Implementation
+must be able to handle it.
+
+2. receiver can either use application buffer to receive data directly (such an implementation is called zero copy receive),
+or it can use a bounce buffer to temporarily hold the application data and copy the data to application's receive buffer
+later. The difficulty of implementing zero copy receive is that EFA device does not guarantee ordered delivery (see Part 0),
+therefore if application want ordered send (`FI_ORDER_SAS`), using a bounce buffer might be the only choice.
+
+3. if a bounce buffer is to be used to receive packets, the receiver need to be able to handle an "unexpected message", which
+is the eager RTM packet arrived before application called libfabric's receive API.
+
+4. if application does not require ordered send, it would be possible to use application's receive buffer to receive data
+directly. In this case, receiver might need the sender to keep the packet header length constant through out the communication.
+The extra request "constant header length" is designed for this use case, see chapter 4.3 for more discussion on this topic.
+
+5. One might notice that there is no application data length in the header, so how can the receiver of an eager RTM packet
+   know how many application data is in the packet? The answer is to use the following formula:
+
+        application_data_length = total_packet_size - RTM mandatory header length - REQ optional header length
+
+   total packet size is reported by EFA device when a packet is received. REQ optional header length can be derived from
+   the `flags` field in the base header. The choice of not including data length in the header is because eager messages
+   are most sensitive to header length, and we want its header to be as compact as possible.
+
+#### Medium message feature/sub-protocol
+
+Medium message protocol split application data into multiple MEDIUM_RTM (either MEDIUM_MSGRTM or
+MEDIUM_TAGRTM) packets, and sender will try send them at once.
+
+In principal, medium message sub-protocol can be used on messages of any size. However, it is not
+recommended to use medium message sub-protocol for long messages, because it does not have flow
+control thus can overwhelm the receiver and cause network congestion. The exact size boundary for
+medium message protocol to be used is up to the implementation to decide.
+
+The following diagram illustrates its workflow:
+
+![medium message](message_medium.png)
+
+Table 3.4 describe the binary structure of a MEDIUM_RTM packet's mandatory header:
+
+Table: 3.4 the format of a MEDIUM_RTM packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`        | 1 | integer | `uint8_t`  | part of base header |
+| `version`     | 1 | integer | `uint8_t`  | part of base header|
+| `flags`       | 2 | integer | `uint16_t` | part of base header |
+| `msg_id`      | 4 | integer | `uint32_t` | message ID |
+| `seg_length` | 8 | integer | `uint64_t` | application data length |
+| `seg_offset` | 8 | integer | `uint64_t` | application data offset |
+| `tag`         | 8 | integer | `uint64_t  | for medium TAGRTM only |
+
+Most of the fields have been introduced before, and their meaning does not change.
+The two new fields are `seg_length` and `seg_offset`. (`seg` means segment, which
+refers to the segment of data in the packet)
+
+`seg_length` is the length of data segment in the medium RTM packet.
+
+`seg_offset` is the offset of data segment in the original send buffer.
+
+`seg_offset` seems redundant at the first glance, as it can be deduced
+from the `seg_length` of other packets.
+
+However, because EFA device does not guarantee ordered delivery, thus
+the MEDIUM_RTM packets of same message can arrive in different order.
+Therefore, the recipent of MEDIUM_RTM packets need `seg_offset` to
+put the data in the correct location in the receive buffer.
+
+When implementing the medium message protocol, please keep in mind
+that because EFA device has a limited TX queue (e.g. it can only send
+limited number of packets at a time), it is possible when
+sending multiple medium RTM packets, some of them were sent successfully,
+others were not sent due to temporary out of resource. Implementation needs
+to be able to handle this case.
+
+Note, this "partial send" situation is unique to medium message sub-protocol
+because medium message sub-protocol is the only one that sends multiple
+REQ packets. In all other protocol, only 1 REQ packet was sent to initialize
+the communication, if the REQ failed to send, the whole communication is
+cancelled.
+
+#### Long-CTS message feature/sub-protocol
+
+Long-CTS message protocol is designed for long messages, because it supports flow control.
+
+
+In long-CTS message protocol, the sender will send a LONGCTS_RTM (either LONGCTS_MSGRTM or LONGCTS_TAGRTM)
+packet to the receiver.
+
+Upon receiving the LONGCTS_RTM, receiver will match it with an application's call to
+libfabric's receive API. Receiver will then calculate how many data it can handle,
+and include that information in a CTS packet it sends back to the sender.
+
+Upon receiving the CTS packet, sender will send multiple DATA packets according to
+information in the CTS packet.
+
+After receiving all the DATA packets it was expecting, receiver will calculate and
+send a CTS packet again.
+
+The above process repeat until all data has been sent/received.
+
+The workflow of long-CTS protocol is demonstrated in the following diagram:
+
+![long-CTS message](message_longcts.png)
+
+There 3 packet types involved in the long-CTS message sub-protocol: LONGCTS_RTM, CTS
+and DATA.
+
+A LONGCTS_RTM packet, like any REQ packet, is consisted with 3 parts: LONGCTS RTM mandatory
+header, REQ optional header and application data.
+
+The format of the LONGCTS_RTM mandatory header is listed in table 3.5:
+
+Table: 3.5 The format of a LONGCTS_RTM packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `msg_id`         | 4 | integer | `uint32_t` | message ID |
+| `msg_length`     | 8 | integer | `uint64_t` | total length of the whole message |
+| `send_id`        | 4 | integer | `uint32_t` | ID of the ongoing TX operation |
+| `credit_request` | 4 | integer | `uint64_t` | number of data packets preferred to send |
+| `tag`            | 8 | integer | `uint64_t` | for LONGCTS TAGRTM only |
+
+There are 3 fields that is new:
+
+`msg_length` is the length of the whole application message.
+
+`send_id` is an ID the sending endpoint assigned to the send operation, and receive should include
+`send_id` in CTS packet. An endpoint will have multiple send operations at the same time, thus
+when processing a CTS packet from a receive, it needs a way to locate the send operation the
+CTS packet is referring to.
+
+Admittedly, the introduction of `send_id` is not absolute necessary, because receiver could have
+included `msg_id` in CTS header, and sender should be able to locate the send operation using
+the combination of receiver's address and message ID. However, that approach would require
+the sending endpoint set up a map between (address + `msg_id`) and send operation, and look up the map every time
+it received a CTS packet. We considered that approach too burdensome for an endpoint to implement
+and decided to introduce a 4 byte `send_id` in LONGCTS_RTM header to eliminate the cost.
+
+Another note about `send_id` is that it can be reused between messages. Because `send_id` is used to
+distinguish on-the-fly TX operations, so a send operation may have the same `send_id` as a previous
+one that has already finished.
+
+The field `send_id` was named `tx_id` when the protocol was initially introduced. It is renamed
+because the new name is clearer.
+
+The field `credit_request` is how many DATA packets the sender wish to receive from the receiver,
+the receiver will try to honor the request, but is not obligated to. However, receiver must allow
+the sender to send at least 1 DATA packet back, to keep the communication moving forward.
+
+Besides the LONGCTS_RTM packet, there are two other packet types used by the long-CTS message protocol:
+CTS and DATA.
+
+The binary format of a CTS packet is listed in table 3.6:
+
+Table: 3.6 the binary format a CTS packet
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `multiuse(connid/padding)`  | 4 | integer | `uint32_t` | `connid` if CONNID_HDR flag is set, otherwise `padding` |
+| `send_id`        | 4 | integer | `uint32_t` | send id from LONGCTS_RTM |
+| `recv_id`        | 4 | integer | `uint32_t` | receive id to be used in DATA packet |
+| `recv_length`    | 8 | integer | `uint64_t` | number of bytes the receiver is ready to receive |
+
+The 3 new fields in the header are `multiuse`, `recv_id` and `recv_length`.
+
+The field `multiuse` is 4 byte integer. As the name indicates, it is a multi-purpose field.
+Its exact usage is determined by the the `flags` field.
+
+If the CONNID_HDR universal flag is toggled in `flags`, this field is the sender's connection ID (connid).
+Otherwise, it is a padding space.
+
+An implementation is free to choose how to use this field.
+
+Note, when protocol v4 was originally introduced. This field was simply a 4-bytes padding space.
+Later, when we introduce the "connid header" extra feature, we re-purposed this field to to store
+connid. Because "connid header" is an extra request, an endpoint is not obligated to comply.
+In practice, if an endpoint is using libfabric 1.10 to 1.13, it uses this field as padding.
+If an endpoint is using libfabric 1.14 and above, it uses this field to store `connid`.
+
+The field `recv_id` is similar to `send_id` introduced earlier, but for an on-going receive operation.
+Sender should include `recv_id` in the DATA packet.
+
+The field `recv_length` is the number of bytes receiver is ready to receive for this operation,
+it must be > 0 to make the communication going forward.
+
+CTS packet header has 1 flag `CTS_EMULATED_READ` that can be set in `flags` field. This flags
+indicates the CTS packet is used by long-CTS emulated read protocol.
+The Bit ID for this flag is 7, and its value is 0x80.
+
+CTS packet does not contain application data.
+
+A DATA packet is consisted of two parts: DATA packet header and application data.
+Table 3.7 shows the binary format of DATA packet header:
+
+Table: 3.7 the binary format of DATA packet header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `recv_id`        | 4 | integer | `uint32_t` | `recv_id` from the CTS packet |
+| `seg_length`     | 8 | integer | `uint32_t` | length of the application data in the packet |
+| `seg_offset`     | 8 | integer | `uint64_t` | offset of the application data in the packet |
+| `connid`         | 4 | integer | `uint32_t` | sender connection id, optional, |
+| `padding`        | 4 | integer | `uint32_t` | padding for connid, optional |
+
+The last two fields `connid` and `padding` was introduced with the extra request "connid in header".
+They are optional, which means an implemenation was not required to include them in the DATA of the
+data packet. If an implementation does include them in DATA packet header, the implementation need
+to toggle on the CONNID_DHR flag in `flags` field (table 1.4).
+
+When implementing the long-CTS protocol, please keep in mind that although each implementation is allowed
+to choose its own flow control algorithm. They must allow some data to be sent in each CTS packet, e.g
+the `recv_length` field in CTS packet must be > 0. This is to avoid infinite loop.
+
+### 3.3 baseline features for one-sided communication
+
+This section explain the 7 baseline features for one-sided communication. These features/sub-procotols
+emulate one-sided operation by using send/receive functionality of the device. The 7 features are:
+emulated eager write, emulated long-CTS write, emulated short read, emulated long-CTS read, emulated write
+atomic, emulated fetch atomic and emulated compare atomic.
+
+Before getting into details of each feature, we want to discuss some topics related to one-sided operation.
+
+There are 3 types of one-sided operations: write, read and atomic.
+
+Like in two-sided communcation, there are also two endpoints involved in one-sided communcation.
+However, only on one side will application call libfabric's one-sided API (such as `fi_write`,
+`fi_read` and `fi_atomic`). In protocol v4, this side is called requester.
+
+On the other side (which is called responder), application does not make calls to lifabric API call,
+but the EFA provider requires application to keep the progress engine running on responder
+to facilitate the communication. This is because EFA provider only support `FI_PROGRESS_MANUAL`.
+
+Generally, in one-sided communication, only on the requester side will lifabric write a completion
+to notify the application that an one-sided communication is finished. Only exception to this
+rule is when application added the `FI_REMOTE_CQ_DATA` flag when calling libfabric's write API,
+in this case, the provider is required to write an CQ entry on responder with the CQ data in it.
+
+(In fact, there is another exception to this rule: which is if a provider claims support for
+the `FI_RMA_EVENT` capability, the provider will need to write CQ entry for any one-sided operation
+on the responder. However, this exception does not apply to EFA provider because the EFA provider
+does not support the `FI_RMA_EVENT` capability.)
+
+One key difference between one-sided and two-sided communication is that: in one-sided communication, the
+requester must know the remote buffer's information when submitting the request.
+
+In protocol v4, because one-sided operations are emulated, the remote buffer's information are stored
+in REQ packet header. For that, protocol v4 defines a data type `efa_rma_iov`, which is used by
+all REQ packets for one-side communication.
+
+A `efa_rma_iov` struct is consisted of 3 members: `addr`, `len` and `key`. Each member is a 8 byte integer.
+`addr` is the remote buffer address, `len` is the remote buffer length, and `key` is the memory registration
+key for the remote buffer, which is provided by the responder through prior communication.
+
+Another difference is that one-sided operation does not support tag matching, thus each one-sided
+sub-protocol only needs to define 1 REQ packet type.
+
+#### emulated eager write feature/sub-protocol
+
+Emulated eager write sub-protocol is used when the buffer size is small enough to fit in one
+packet.
+
+The workflow of the emulated eager write protocol is shown in the following diagram:
+
+![eager write](write_eager.png)
+
+Emulated eager write protocol is similar to eager message protocol, except an EAGER_RTW
+is used to initiate the communication. Like other REQ packets, an eager RTW packet is consisted of eager RTW mandatory header,
+REQ optional header and application data. The binary format of EAGER_RTW mandatory header is listed
+in table 3.8:
+
+Table: 3.8 the binary format of EAGER_RTW packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov structure |
+| `rma_iov`        | `rma_iov_count` * 24 | array of `efa_rma_iov` | `efa_rma_iov[]` | remote buffer information |
+
+One thing worth noting is that there is no `msg_id` in the eager RTW header, because EFA provider does not support
+ordered write operation.
+
+#### emulated long-CTS write feature/sub-protocol
+
+emulated long-CTS write sub-protocol is used when the buffer size is too big to fit in one packet.
+
+The workflow of emulated long-CTS write general follow the long-CTS message sub-protocol, as illustrated
+in the following diagram:
+
+![emulated long-CTS write](write_longcts.png)
+
+The main difference between the two protocol is that the LONGCTS_RTW packet is used instead of the
+LONGCTS_RTM packet. The binary format of LONGCTS_RTW packet's mandatory header is listed in table 3.9:
+
+Table: 3.9 the format of LONGCTS_RTW packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov structure |
+| `msg_length`     | 8 | integer | `uint64_t` | total length of the application buffer |
+| `send_id`        | 4 | integer | `uint32_t` | ID of send operation |
+| `credit_request` | 4 | integer | `uint32_t` | number of packets requester is ready to send |
+| `rma_iov`        | `rma_iov_count` * 24 | array of `efa_rma_iov` | `efa_rma_iov[]` | remote buffer information |
+
+All fields have been described before, but some explanation is warranted for the `send_id` field. It is not
+named `write_id` because this protocol is using send/receive to emulated write, therefore it is implied that
+the requester is treating this communication as a send operation internally, and this communication is subject
+to same flow control as a long-CTS message communication does.
+
+#### emulated read features/sub-protocols
+
+This section describes two emulated read sub-protocols: emulated short read and emulated long-CTS read. Both
+sub-protocols use send/receive to emulate read. The interesting part is, in an emulated read communication,
+the responder is the sender and the requester is the receiver.
+
+The workflow of emulated short read protocol is illustrated in the following diagram:
+
+![emulated short read](read_short.png)
+
+As can be seen, in this protocol, the requester send a short RTR packet to the responder and the responder send
+a READRSP packet back to the requester.
+
+The binary format of a SHORT_RTR mandatory header is listed in the table 3.10:
+
+Table: 3.10 the format of a SHORT_RTR packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov structure |
+| `msg_length`     | 8 | integer | `uint64_t` | total length of the application buffer |
+| `recv_id`        | 4 | integer | `uint32_t` | ID of the receive operation, to be included in READRSP packet |
+| `padding`	   | 4 | integer | `uint32_t` | alignment for 8 bytes |
+| `rma_iov`        | `rma_iov_count` * 24 | array of `efa_rma_iov` | `efa_rma_iov[]` | remote buffer information |
+
+Among the fields, the `recv_id` is most interesting. As mentioned before, in an emulated read protocol, the requester is the
+receiver, so it is necessary to include `recv_id` in the request. The responder needs to include this `recv_id` in
+the READRSP packet, for the requester to properly process it.
+
+A READRSP (READ ReSPonse) packet consists of two parts: READRSP header and application data. The binary format
+of the READRSP header is in table 3.11:
+
+Table: 3.11 the format of a READRSP packet's header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `multiuse(padding/connid)`         | 4 | integer | `uint32_t` | `connid` if CONNID_HDR flag is set, otherwise `padding` |
+| `send_id`        | 4 | integer | `uint64_t` | ID of the send operation, to be included in the CTS header |
+| `recv_id`        | 4 | integer | `uint32_t` | ID of the receive operation  |
+| `recv_length`    | 8 | integer | `uint64_t` | length of the application data in the packet |
+
+The field `multiuse` has been introduced before when introducing the CTS packet (table 3.6).
+It is a multi-purpose field, which can be used to store `connid` or as a padding
+space, depend on whether the CONNID_HDR universal flag is toggled in `flags`. See section 4.4
+for more information about the field `connid`.
+
+The workflow of the emulated long-CTS read sub-protocol is illustrated in the following diagram:
+
+![emulated long-CTS read](read_longcts.png)
+
+The protocol started by the requester send a LONGCTS_RTR packet. After that, the workflow generally follow that of the
+long-CTS message sub-protocol, except the responder is the sender and the requester is the receiver.
+
+The mandatory header of LONGCTS_RTR packet is listed in table 3.12:
+
+Table: 3.12 the format of a LONGCTS_RTR packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov structure |
+| `msg_length`     | 8 | integer | `uint64_t` | total length of the application buffer |
+| `recv_id`        | 4 | integer | `uint32_t` | ID of the receive operation, to be included in READRSP packet |
+| `recv_length`	   | 4 | integer | `uint32_t` | Number of bytes the responder is ready to receive |
+| `rma_iov`        | `rma_iov_count` * 24 | array of `efa_rma_iov` | `efa_rma_iov[]` | remote buffer information |
+
+The only difference between LONGCTS_RTR and SHORT_RTR is the field `padding` in SHORT_RTR is replaced by the field `recv_length`.
+Here, the LONGCTS_RTR packet serves the same functionality of the first CTS packet in long-CTS message sub-protocol. The reason
+is: when the endpoint is preparing the LONGCTS_RTR, it already knows it is going to receive some data, thus it should calculate
+how many bytes it is ready to receive using the flow control algorithm, and put the number in the packet.
+
+The short RTR protocol can only be used if the read buffer can fit in one READRSP packet, so the maximum size of a short emulated
+read protocol is (MTU size - READRSP header size). For messages whose size is larger, the emulated long-CTS read protocol has
+to be used.
+
+#### emulated atomic protocols
+
+This section describes the 3 emulated atomic protocols: emulated write atomic, emulate fetch atomic and emulated compare atomic.
+
+The workflow of emulated write atomic is illustrated in the following diagram:
+
+![atomic_write](atomic_write.png)
+
+It is similar to emulated eager write sub-protocol, except an WRITE_RTA packet was
+sent. Table 3.13 lists the binary structure of an WRITE_RTA packet's mandatory
+header:
+
+Table: 3.13 the format of an WRITE_RTA packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `msg_id`         | 4 | integer | `uint32_t` | message ID |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov structure |
+| `atomic_datatype`| 4 | integer | `uint32_t` | atomic data type |
+| `atomic_op`      | 4 | integer | `uint32_t` | atomic operation ID |
+| `pad`            | 4 | integer | `uint32_t` | atomic operation ID |
+| `rma_iov`        | `rma_iov_count` * 24 | array of `efa_rma_iov` | `efa_rma_iov[]` | remote buffer information |
+
+The two new fields introduced are `atomic_datatype` and `atomic_op`. There are atomic data type and atomic operations
+defined in libfabric standard. A list of atomic datatypes can be find in libfabric [fi_atomic](https://ofiwg.github.io/libfabric/v1.4.0/man/fi_atomic.3.html) man page.
+
+The field `msg_id` is provided as message ID. It is used to implement ordered atomic operations, which is supported by libfabric EFA provider,
+and is required by some application such as MPICH.
+
+The workflows of emulated fetch/compare atomic are the same, as illustrated in the following diagram:
+
+![atomic_fetch_compare](atomic_fetch_compare.png)
+
+Comparing to write atomic, the differences are:
+
+First, an FETCH_RTA/COMPARE_RTA is used to initiate the communication.
+second, that responder will send an ATOMRSP (atomic response) packet back.
+
+The binary format of FETCH_RTA and COMPARE_RTA are the same.
+Table 3.14 shows the format of mandatory header of a FETCH_RTA/COMPARE_RTA packet:
+
+Table: 3.14 the format of an FETCH_RTA/COMPARE_RTA packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `msg_id`         | 4 | integer | `uint32_t` | message ID |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov structure |
+| `atomic_datatype`| 4 | integer | `uint32_t` | atomic data type |
+| `atomic_op`      | 4 | integer | `uint32_t` | atomic operation ID |
+| `recv_id`        | 4 | integer | `uint32_t` | ID of the receive operation on the requester side |
+| `rma_iov`        | `rma_iov_count` * 24 | array of `efa_rma_iov` | `efa_rma_iov[]` | remote buffer information |
+
+The differences between a FETCH_RTA and a COMPARE_RTA are:
+
+First, The value of `atomic_op` is different between FETCH_RTA and COMPARE_RTA.
+
+Second, the application data part of a COMPARE_RTA packet contains two segments of data: `buf` and `compare`.
+(see [fi_atomic](https://ofiwg.github.io/libfabric/v1.4.0/man/fi_atomic.3.html))
+
+The difference between an WRITE_RTA and an FETCH_RTA/COMPARE_RTA is that the field `pad` was replaced by `recv_id`.
+Because we are using send/receive to emulate a fetch/compare atomic operation. The requester is going to receive
+data from the responder, the field `recv_id` is the ID of the receive operation the requester side, which is to
+be included in the header of ATOMRSP packet.
+
+An ATOMRSP packet is consisted of two parts: header and application data. Table 3.15 shows the format of the header of an ATOMRSP packet:
+
+Table: 3.15 the binary format of an ATOMRSP packet header.
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `multiuse(connid/padding)` | 4 | integer | `uint32_t` | `connid` if CONNID_HDR is set, otherwise `padding` |
+| `reserved`       | 4 | integer | `uint32_t` | reserved for future use |
+| `recv_id`        | 4 | integer | `uint32_t` | ID of the receive operation on the requester side |
+| `seg_length`     | 8 | integer | `uint64_t` | length of the application data in the packet |
+
+The field `multiuse` has been introduced before when introducing the CTS packet (table 3.6).
+It is a multi-purpose field, which can be used to store `connid` or as a padding
+space, depend on whether the CONNID_HDR universal flag is togged in `flags`. See section 4.4
+for more information about the field `connid`.
+
+## 4. Extra features and requests
+
+This chapter describes the extra features and requests of protocol v4.
+
+### 4.1 RDMA read based data transfer (RDMA read)
+
+The extra feature "RDMA read based data transfer" (RDMA read) was introduced together
+with protocol v4, when libfabric 1.10 was released. It was assigned ID 0.
+
+It is defined as an extra feature because there is a set of requirements (firmware,
+EFA kernel module and rdma-core) to be met before an endpoint can use the RDMA
+read capability, therefore an endpoint cannot assume the other party support RDMA read.
+
+The "RDMA read" extra feature corresponds to the following sub-protocols:
+long-read message, emulated long-read write, direct read.
+
+#### Long-read message sub-protocol
+
+The long-read message sub-protocol uses RDMA read to implement two-sided communication.
+
+The work flow of long-read message sub-protocol is illustrated in the following diagram:
+
+![long-read message](message_longread.png)
+
+There are two packet types involved in this protocol: LONGREAD_RTM and EOR (End Of Read).
+
+LONGREAD_RTM is sent by the sender to initiate the communication.
+
+Like all REQ packets, a LONGREAD_RTM consists of 3 parts: mandatory header, REQ optional
+header and the application data. However, the application data part of a LONGREAD_RTM is
+special: it is not the data in the application's send buffer, but information of the
+sender buffer.
+
+In long-read message sub-protocol, sender need to construct an `read_iov`, which is
+an array of `efa_rma_iov` of application's send buffer. The `read_iov` is used
+as the application data in the LONGREAD_RTM packet.
+
+The binary format of a LONGREAD_RTM packet's mandatory header is listed in table 4.1
+
+Table: 4.1 the binary format of a LONGREAD_RTM packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `msg_id`         | 4 | integer | `uint32_t` | message ID |
+| `msg_length`     | 4 | integer | `uint64_t` | total length of the message |
+| `send_id`        | 4 | integer | `uint32_t` | ID of the receive operation  |
+| `read_iov_count` | 4 | integer | `uint32_t` | number of iov to read |
+
+Noticing the new field `read_iov_count`, which is number of `struct efa_rma_iov` in `read_iov`.
+
+To construct `read_iov`, sender need to make sure the send buffer is registered with EFA device and fill
+the registration key in `read_iov`.
+
+There are two ways to achieve that:
+
+First, if the buffer has already been registered with device, application will provide
+a memory descriptor along with the sender buffer, registration key can be extracted from the descriptor;
+
+Second, if the buffer has not been registered with EFA device, sender need to register the buffer,
+and can get the key from the registration. Note because memory registration is a limited resource,
+it is possible for memory registration to fail and sender need to be able to handle the case.
+
+Upon receiving a long-read RTM, the receiver will use RDMA read to copy data from application's
+send buffer to application's receive buffer (avoiding copy). That is why this protocol is
+sometime referred as zero-copy.
+
+After all read is finished, the receiver will send an EOR packet to the sender to notify it
+the work is done.
+
+The binary format of the EOR packet is listed in table 4.2
+
+Table: 4.2 the format of an EOR packet
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `send_id`        | 4 | integer | `uint32_t` | ID of the send operation |
+| `recv_id`        | 4 | integer | `uint32_t` | ID of the receive operation |
+| `multiuse(connid/padding)`  | 4 | integer | `uint32_t` | `connid` if CONNID_HDR is set, otherwise `padding` |
+
+The field `multiuse` has been introduced before when introducing the CTS packet (table 3.6).
+It is a multi-purpose field, which can be used to store `connid` or as a padding
+space, depend on whether the CONNID_HDR universal flag is togged in `flags`. See section 4.4
+for more information about the field `connid`.
+
+#### emulated long-read write sub-protocol
+
+The emulated long-read write sub-protocol uses RDMA read to emulate an write operation.
+
+The workflow of this protocol is illustrated in the following diagram:
+
+![long-read write](write_longread.png)
+
+The workflow is similar to that of long-read message sub-protocol. One key difference is that
+a LONGREAD_RTW packet is used to initiate the communication. The binary format of the LONGREAD_RTW
+packet mandatory header is listed in table 4.3.
+
+Table: 4.3 the format of a LONGREAD_RTW packet's mandatory header
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `rma_iov_count`  | 4 | integer | `uint32_t` | number of RMA iov on the responder |
+| `msg_length`     | 8 | integer | `uint64_t` | total length of the message |
+| `send_id`        | 4 | integer | `uint32_t` | ID of the receive operation  |
+| `read_iov_count` | 4 | integer | `uint32_t` | number of iov on requester (to be read by responder) |
+| `rma_iov`        | `rma_iov_count` * 24 | array | `efa_rma_iov[]` | write iov information |
+
+One thing worth noting is the existence of both `rma_iov_count` and `read_iov_count`.
+
+Though both have been explained before, this is the first time they appear in same header, so it might
+be helpful to revisit them.
+
+The field `rma_iov_count` (and  `rma_iov`) are provided by application, which application called libfabric's write API.
+They contain information of the target buffer (of write) on the responder side.
+
+The field `read_iov_count` (and a `read_iov`) is constructed by the write requester,
+which contains information of the source buffer (of write) on the requester side.
+The `read_iov` is not part of the mandatory header, because it is considered
+application data, which is located right after the REQ optional header.
+
+#### direct read sub-protocol
+
+Direct read sub-protocol is the simplest sub-protocol in protocol v4. It does not involve a REQ packet.
+The workflow is just for read requester keep using RDMA read on the responder. For this protocol, it is
+not necessary that responder keep progress engine running.
+
+### 4.2 delivery complete
+
+The extra feature "delivery complete" was introduced with libfabric 1.12.0, and was assigned ID 1.
+
+Delivery complete is a requirement application can impose on an endpoint when opening the endpoint.
+It requires that when application gets the send/write completion, the application data must have
+been delivered to application's target buffer.
+
+The reason it is implemented as an extra feature is because, not all sub-protocols in the baseline
+features support delivery complete. Specifically, the following 6 sub-protocols do NOT:
+
+* eager message,
+* medium message,
+* long-CTS message,
+* eager write,
+* long-CTS write and
+* write atomic.
+
+These sub-protocols are designed to support a weaker completion model: transmit complete.
+Transmit complete requires that when the send/write completion was written, the data has been transmitted
+to the receiver/responder.
+
+The difference between transmit complete and delivery complete is transmit complete indicate
+that data has arrived at A buffer on the receive/responder, but the buffer is not necessary the application's
+target buffer. buffer. In fact, because of the limitation of the EFA device (no ordering guarantee) and the nature
+of the communications (emulated write), for some protocols, the implementation have to use a temporary
+buffer to receive data, and copy the data to application buffer later, and the time difference can be indefinite.
+
+The "delivery complete" extra feature was introduced to support applications with such requirements.
+It comes with 6 sub-protocols:
+
+* DC eager message,
+* DC medium message,
+* DC long-CTS message,
+* DC eager write,
+* DC long-CTS write and
+* DC write atomic.
+
+The workflow of these sub-protocols are same to that of there non-DC counterpart, with 3 differences changes:
+
+First, each DC capable sub-protocol defines its own REQ packet type.
+
+Second, after data was delivered to application buffer, the receiver/responder will send a RECEIPT
+packet back to the sender/requester.
+
+Third, sender/responder will not write completion until it received the RECEIPT packet.
+
+The binary format of a RECEIPT packet is as the following:
+
+| Name | Length (bytes) | type | C language type | Note |
+|-|-|-|-|-|
+| `type`           | 1 | integer | `uint8_t`  | part of base header |
+| `version`        | 1 | integer | `uint8_t`  | part of base header|
+| `flags`          | 2 | integer | `uint16_t` | part of base header |
+| `send_id`        | 4 | integer | `uint32_t` | ID of the send operation |
+| `msg_id`         | 4 | integer | `uint32_t` | message ID |
+| `multiuse(connid/padding)`  | 4 | integer | `uint32_t` | `connid` if CONNID_HDR is set in `flags`, otherwise `padding` |
+
+The field `multiuse` has been introduced before when introducing the CTS packet (table 3.6).
+It is a multi-purpose field, which can be used to store `connid` or as a padding
+space, depend on whether the CONNID_HDR universal flag is togged in `flags`. See section 4.4
+for more information about the field `connid`.
+
+### 4.3 keep packet header length constant (constant header length) and zero-copy receive
+
+The extra request "keep packet header length constant" (constant header length) was introduced in libfabric 1.13.0
+release, and was assigned the ID 2.
+
+This extra request would be useful if endpoint want to implement the "zero copy receive" optimization.
+
+As can be seen from previous discussion, because EFA device does not support ordered delivery, an endpoint
+usually needs to use a temporary buffer to receive incoming packets, and copy data to application's receive buffer
+later. However, if an application has the following set of requirement:
+
+   1. does not need ordered send/receive (`FI_ORDER_SAS`).
+   2. only send/receive eager messages.
+   3. does not use tagged send.
+   4. does not require FI_DIRECTED_RECV (the ability to receive only from certain address).
+
+it should be possible to receive data directly using application buffer directly. Because under such condition,
+receiver does not have special requirement on the data it is going to receive, thus will accept any message from
+the sender.
+
+However, there is one more hurdle to overcome in implementing "zero copy receive", which is the packet header
+length.
+
+Under the condition that endpoints "will only send eager messages" and "do not use tag matching", a sender will
+send data in an EAGER_MSGRTM packet. An EAGER_MSGRTM packet is consisted of packet header and application data.
+However, we cannot put packet header in application's receive buffer. Therefore, for "zero copy receive" to work,
+the receiver need to:
+
+   a. be able to estimate the packer header length of an incoming EAGER_MSGRTM packet, and the
+   b. the packet header length of EAGER_MSGRTM cannot change throughout the communication.
+
+However, there is no guarantee in the base protocol that the packet header length of EAGER_MSGRTM will not
+change.
+
+In fact, because of the existance of the handshake sub-protocol, the packet header length of an EAGER_MSGRTM
+will definitely change. Recall handshake sub-protocol's workflow is:
+
+Before receiving handshake packet, an endpoint will always include the optional raw address header in REQ packets.
+
+After receiving handshake packet, an endpoint will stop including the optional raw address header in REQ packets.
+
+The extra feature "keep packet header length constant" (constant header length) is designed to solve this problem.
+
+When an endpoint toggle on this extra request, its peer will try to satisfy it by keep the header length constant.
+Exactly how to achieve that is up to the implementation to decide, the easiest way to do that is to keep including
+raw address header in the EAGER_MSGRTM even after receiving the handshake packet.
+
+Note, because this is an extra request, an endpoint cannot assume its peer will comply with the request. Therefore,
+the receiving endpoint must be able to handle the situation that a received packet does not have the expected header length.
+
+In that case, implementation will have two choices:
+
+1. write a truncated error completion entry
+2. move the application data to right place.
+
+Note this extra request was initially introduced as an extra feature named "zero copy receive", but it is later realized
+that is not an feature because the peer does not do anything different. Rather, it is an expectation the receiving
+endpoint has for the sender. Therefore, it was re-interpreted as an extra request named "constant header length".
+This re-interpretation does not change the implementation, thus does not cause backward incompatibility.
+
+### 4.4 have connection ID in packet header (connid header)
+
+The "have connection ID in packet header" extra request was introduced with libfabric 1.14.0 release, and was
+assigned the ID 3.
+
+This extra feature is designed to solve the "QP collision" problem, which is commonly experienced in
+client-server type of application.
+
+The "QP collision" problem arise from fact that the EFA device uses the Device ID (GID)
++ QP number (QPN) as the unique
+identifier of a peer. Recall that raw address of EFA endpoint is consisted of 3 parts:
+GID + QPN + Connection ID (CONNID). EFA device only recognizes GID and QPN.
+The connection ID was generated by the endpoint itself during its initialization.
+
+Because of that, it is possible for an endpoint to receive packets from a destroyed QP, which was used
+by a previous process that used the same QPN. As can be seen throughout the document, each packet in the
+EFA RDM communication protocol is not indepenedent. The correct processing of a packet need prior knowledge.
+
+For example, there is a `recv_id` in the header of a DATA packet (section 3.2), which assumes
+the receiver to maintain a list of receive operations and can find the operation correspond to
+the message using `recv_id`.
+
+To solve this problem, receiver need to know the full address of the sender. As shown in table
+1.5, EFA's full address consists of: GID, QPN and CONNID. Currently, EFA device will report
+Address Handle Number (AHN) and QPN of a received packet. Because GID can be obtained from AHN,
+the only unknown part is CONNID.
+
+The extra request "connid header" was introduced to address the issue. An endpoint can flag
+the bit correspond to 
+Also because this is an extra request, an endpoint cannot assume that the peer support it, thus need to be able
+to handle the case that incoming packets does not have sender connection ID in it. It is up to the
+implementation to decide whether in this case the endpoint should abort the communication or continue without
+using the extra request.
+
+A universal flag CONNID_HDR (table 1.4) was designated for CONNID in packet header. An implementation is not required to
+set connid. However, when it does include connid in packet header, it need to toggle on the CONNID_HDR flag in
+the `flags` field of the base header. The exact location of connid is different for each packet type.
+
+## 5. What's not covered?
+
+The purpose of this document is to define the communication protocol, therefore it is intentionally written
+as "implementation neutral". It should be possible to rewrite a libfabric EFA provider from scratch by
+following this document, and the newly written EFA provider should be able to interoperate with any
+libfabric EFA provider since libfabric 1.10.0.
+
+Because of this, in various places, the document provides tips about how to implementation certain
+aspects of the protocol, but did not give specific instruction about what implementation should do.
+
+There are a few things that this document consider as implementation specific thus does not cover.
+
+For example, this document does not specify the selection logic of various protocols. For example, there are
+three message sub-protocols medium message, long-CTS message and long-read message that can be used for
+long message (it is not recommended), and an implementation can choose switch point without breaking
+protocol.
+
+On similar note, this document does not describe the shared memory (SHM) implementation of EFA provider, which is an optimization technique
+an endpoint can use to speed up intra-instance communication. The idea is that if two endpoints are on
+same instance, and opened by same user, they can use the SHM mechanism on the instance to communicate,
+which has higher bandwidth and lower latency.
+
+It is not covered in this document because EFA device is not used, and because two endpoints are opened
+on same instance by the same user, they should be using the same libfabric library, so the concern of backward
+compatibility does not apply to this case.
+
diff --git a/prov/efa/docs/handshake.drawio b/prov/efa/docs/handshake.drawio
new file mode 100644
index 0000000..826b1b0
--- /dev/null
+++ b/prov/efa/docs/handshake.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2020-02-28T22:28:00.796Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:68.0) Gecko/20100101 Firefox/68.0" version="12.4.8" etag="AZ6Ufx_XuPE9K7quufH2" pages="2"><diagram id="Sn-nw_Uit1KFLxTq_Swt" name="Page-1">7Vpdb6M4FP010e4+ZIQxEHhsPraV+qFts9lp92XkgANWHZwxTgP769duTBIwI3Uq0kRl8hJ8MTY+x+fea5seHC3zS45WyS2LMO3ZVpT34Lhn23bgePJPWYqtBVi+v7XEnETatjdMyX+4rKitaxLhrFJRMEYFWVWNIUtTHIqKDXHONtVqC0arva5QjA3DNETUtH4lkUi2Vt+19vYrTOKk7BlY+s4SlZW1IUtQxDYHJjjpwRFnTGyvlvkIU4Veicv2uT9/cHf3Yhyn4i0PJLdBMpsP/fxb/5HM/Hm+SGZ9e9vKC6JrPeCH/KFne1Q2OZxzeRWrqwf8fY0zgbkeiihKfDhbpxFWXViy3iYhAk9XKFR3N3JKSFsillSWgLw0X1mP4gVzgfMDkx7CJWZLLHghq5RTyh5sH9ETCmp0N3tyQIl4ckCMp21Iz4d41/IeMnmhUfsJBOFbEcxWTCJ1egSdc0PQMxA0MMJpdKHELEshRVlGwiosEcqSVwhVQULDi0eF5xe3LD5peF8L47xSKnTph9jiqOIjTGQPoHMbkCttHFMkyEvVszTBqXv4ixH5Jvup73tyQIfUBTVKMrbmIdaPHboCo6XqFHBhrSGBeIyF0dArvbtxv5/xQcuMnztzTmvMqZYCz7d8F1ge9O3gpDz6HeOxrhtQx/vtNNYaGnwsccAxo9bkXhp+3xCJowQKbdTbRBHHWfaHwaoMNaLKYyY4e8YjRhmXlpSlsuZwQSitmRAlcaomg6RYBkM4VIGLyHzrQt9YkihS3TRGwmqs5ExINpl6CrQUHCG0mxk+jI6wYYbZLUTHdHrZn/4zGi2L0B9/8xy6jhb9oGMaM6ThtxTkDJ/bnsYaiStXMAfMXV3cjadXF9eT8xcUhLAlRQU1GoCpKKdhOoFjCQqYa57PrSgjarlBS1ErqDXUnqImNzfLGOZ39B7c8WeneP7X+7tJUZ+dOa+qHei+l7laQwP3WMzdD8aWfe3e387610+Dm9C+fIwbEv4vr7/z94OtJRZOXYQDww16R8orGikxc/euUyKTv9NS0kH/VmPAamk9tcsoPsi/AWAw13U1geDEDq7zeR5sa3cCHm93opk512BuiDJMidTBeeqpBf3UDzZ2e/An04+5L99BFpxTezEzde4gC/6pWTCz5e6xYJ/cI5l7oXdM7VM/p+qcX/VbP3bl5bHrb/o+Wiqs0nmm/kK0QnNCiSg+L4ugzC5KFhu0BJqykqPRaJvh/eDwga1VJtLZ84cySa3ncKdj69dGAbRqpAzeSAqARyIFdn2nwFiSvHdt49TPGdpb24zBQ0y+zhZPxfd5kaf31Ov3G3ZCu5BI1PTzgYlEIwlmHtEFEqpfoEB3901K+zTI4v7Dxq109t+Hwsn/</diagram><diagram id="w6Jld2eqyOy6yJ187mOw" name="Page-2">ldFPD4IgFADwT8OxTaU2PZvVxbVlf1w3JiRs6HOI0/z06cSMdakT8OPx4D0QDotur0jFY6BMIs+hHcJb5HlusA6GYZTnJL7vT5ArQU3QAonomUHHaCMoq61ADSC1qGzMoCxZpi0jSkFrhz1A2rdWJGdfkGREfutNUM1NFRtn8QMTOZ9vdh2zU5A52EDNCYX2g3CEcKgA9DQrupDJsXlzX3SV9peUtCeZXe9eEx/PBaymZLt/jrxLUKzUv6YeJsvThoX1wTh6AQ==</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/message_dc_eager.drawio b/prov/efa/docs/message_dc_eager.drawio
new file mode 100644
index 0000000..d843f02
--- /dev/null
+++ b/prov/efa/docs/message_dc_eager.drawio
@@ -0,0 +1,2 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-25T18:20:04.067Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15" etag="J5Nr3i0nlfrDectmQK_h" version="12.4.8" type="device"><diagram id="aD31mCK-H1K8Hn6eNp0I" name="Page-1">5Zhbb5swGIZ/TbSrTgESklwmlGXV0i1Lj+tN5GAHvBmMjFOgv352MIRTq66iTaRdxX5tbL7n/XwgPcPykzkDoXdJISI9vQ+TnnHe03VNn5jiRypppow0PRNchqHqdBCu8BNSYl+pOwxRVOnIKSUch1XRoUGAHF7RAGM0rnbbUlKdNQQuaghXDiBN9Q5D7mXqeNg/6F8Rdr18Zq2vWnyQd1ZC5AFI45Jk2D3DYpTyrOQnFiISXs4le+7LM63FizEU8Nc84F1OvJvNbJysz+7xzXiTbL2bM2XGIyA7FfA0DAl2AMc0kFjBfgyCN1uwYdj5FMlIUADFz3R5oSLjaY6L0V0AkZyx3zNmsYc5ugqBI1tjkSBC87hPRE0TxWYEKqhHxDhKSpKKaI6ojzhLRRfVmsNNq9X4YJWWa17JJlNpQGWHWwx8ACgKiuE/8DQaPFfIQVgEtCcXMuoyFMkiClwcoOMDPDWCZoNgg5FIv6lc2qLmEBBF2KligSDy9ghlRaBh6b3k+XmYV38pvPvKeVKppar2LFsOmIv4CxFMsn4IVnaWpgMlxMMWwrnGEBGr8bG6H7VhVzMsKRZvXBg8moi4yw6Pa85FdMccpJ4q7x+1gca1RBnWxsm4NMbZJ0ER9dvzYtRxXjzr74n41p1xTecmH+rc+D9zroZb65vd+KaZ5of6Nmn4Vr0buJQXlwGTCIazDRMlV5Ys6ocEZR2PfcLVst889gmXX21fBlsnyrJ7xGmjPj3Wgwbrc2ttT+f2ar26vhQMgS85BJsoLBiUCIrAeRVTxBn9gyxKKBNKQMUdzphtMSE1CRDsBnI/EziR0GcSo/CYTFWDjyGU07T6UnWOUZ7lhniFUTdWaWZtc9FGDasGLU7pHTi10G3j5zw1sbm43U7tuzgePLztS6RYFCfxMWKY1QNbM5pM3y39l/CbtV5Q42F7u/j9/cfonl6sWqB2cfTWztkWake7NNWPzK4uu/W07+7obfWt5TPStuyL5fXpb1BnmvzMglgsTaXEKOJF7jRW06uyJ39gOKjZq7/XpiWqh79mMlsPf3AZ9l8=</diagram></mxfile>
diff --git a/prov/efa/docs/message_dc_longcts.drawio b/prov/efa/docs/message_dc_longcts.drawio
new file mode 100644
index 0000000..34fe6b4
--- /dev/null
+++ b/prov/efa/docs/message_dc_longcts.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-25T18:29:53.714Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="9H__y9q8GqsaccNC4onW" version="12.4.8" type="device"><diagram id="OzKOerL5p1M4QQdfBGPd" name="Page-1">5ZpRd5owFMc/jWdP7QECiI8Wnds5bddVu3V76UGIkNNAXMCK+/QLEqok6dZ5wHLqk+QSgfx/917ujfaAG+cT6i2jKxJA3DO0IO+BUc8wdE132Edh2ZSWvt4vDSFFAZ+0M0zRb1h9k1tXKIBpbWJGCM7Qsm70SZJAP6vZPErJuj5tQXD9rksvhJJh6ntYtn5HQRaVVsfSdvZPEIVRdWdd42dir5rMDWnkBWS9ZwLjHnApIVl5FOcuxIV4lS7l9z6+cPb5wShMstd8IboaRHfzCyd/OLtHd848X0R3Z0Z5lScPr/iCh8slRr6XIZIUsnrba1yi+cKbU+R/SIuVwCRgH8Obz3xl2aaSi5JVEsDijloPXKwjlMHp0vOLs2vmIMwWZTFmI50dyivgi3qCNIP5nomvaAJJDDO6YVOqs5W63L0AH653rJ6nRHucbG7zuHuEz1feKcgOuIj/ISiQBL2FPkRsRVvplpSEFKbFIUxClMA3VxB0TUFbUlDSiPnfsIhtNvKxl6bIr8sSeGm0lbAYMGno5r7Q89yqhj+4vNvBKK+NNnz0oraZR0OY/WUFg3IeDGqpRSawJ7GlULiyUYhZOD7VE5JKdn6HG4LYE+9CpD9gC99HPBDQpWRFfci/tp9BxCs5dVeRfKBURrrQ1g2e1324Z/Qb9ozDCVdvp44gBo0hBhLiwVERO8dC3BFyYkjpoCFwev+4sTmQwNULiZAUD4uLOkIoI3wSLzEsp711NaGb545R09EavPX7sMo1/5C2XqLRsujolrxiqdEBbU1J25H7cPnleuLOpg+3sytJM7bUrC5MmlHyCF2CCWWWhLCaDlwsEMaCycMoTIqkxQSEzH5RCMco4iE/EaMgKG6jJFFnRUlW0meP0BAcA0i+r6tqQU07r3q6fURGA4iS6eRs+s11443vjB5sE6+ChSKxvO83gpTInaaqNRFRc28EJThF3mJB1f2AAuagoYiyHIGkLYWTqXAkva1Q0nWJyGg4G3YfSWM5zqhXycpmV1cgaS276fIGzPtOb1JWaqrgNdoreMeXl3EI8mv8Vb+mj+bm8ac9U+ycnRg4qylwYq/aMjh5h+7EwImBcig4IL6oWgYn1+onBq6pTR0g5tyWwVkSuFf8yLDrYFW/M3SjRHnRg/6jJxbLfVtRktgtlSRKWk3vv3c9zKToMA8MMzFegdi5tRxm8t7pifVbtgiyL4VSW/2WEoi8dXHa7RZQ7fe11W4piSj2JN53cpNyUlM1hHnkGkKxd3Fa5EwxTR1MTmzc2iZ36p2yFCoHkzvyFod+6q2yFCoHk2tvjyO3J97ihwYef83vJzOAZ3dJcNZOshQoKUh2pYa3NHAYONEDLBMcFZzcfN2O3fHnm5nEr3NVY1FuV54i1Yiv8pWqbNRFmPLvJk2VjWy4+wdmCXH3P1Yw/gM=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/message_dc_medium.drawio b/prov/efa/docs/message_dc_medium.drawio
new file mode 100644
index 0000000..6f5d928
--- /dev/null
+++ b/prov/efa/docs/message_dc_medium.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-25T18:25:50.342Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="2xACA87PeaCZUWKGTXLx" version="12.4.8" type="device"><diagram id="dTPj6XVY3x3ysBFnl7aE" name="Page-1">5VnbbuIwEP0atE+tciXJI7de1KJladltnyqTmMSqiVPHFNivX4c4QOxQ0W4KSPCC5/jGnDP2TELD7EwX1xQkUZ8EEDcMLVg0zG7DMHRNd/lXhixzxNGdHAgpCsSgDfCA/sJipkBnKIBpaSAjBDOUlEGfxDH0WQkDlJJ5ediE4PKuCQihAjz4AKvoHxSwKEddW9vgNxCFUbGzromeKSgGCyCNQEDmW5DZa5gdSgjLW9NFB+KMvIKXfN7Vjt71D6MwZvtMiPpeNBq33cXLxRMauePFJBpdGPkq7wDPhMOtJMHIBwyROKMVrNbAaDwBY4r8H2nmCYwD/tUa3ArP2LKgi5JZHMBsR61htucRYvAhAX7WO+cBwrGITTG3dN5UPRBOvUPK4GILEh5dQzKFjC75ENFbkLssm/ONVHqBRVsyNQUGRHSE64U3BPKG4PATfJoKn0PoQ8QdWjGXUBJSmGZNGIcohscn8NQYbCoMKhzx8GtlR5tbPgZpivwyLQFIoxWFmcGpocunjM9LuzCfBb0ro7soWUth7eSWARpC9oEHXj4OBqWbRVVgi2K7guECoxDz0/hevo+qaBc7DAjiv3gtsONxv7cVdiXlUjKjPhSztu8PaSFXChRbWifnRVlnFQRrr78eF86px4Wk9x6BomvHDIz6IkMNDe+goeHWHBr7Snws5SS6da1Zj266d9gj7Sm6lYuPkLAdtUcTc0bbY8pbYdbyyTTBMJ927IQqnQXv2Am1qKQ/Q7PML82LmNMm/vSYtxTmu52Xfq97O+q/DB/7CmXcU1bmJWWUvMIOwYRyJCa8YjTbE4SxBAGMwji73Dh/kOPtjDcuMW6JjikKgmybSiHKUlHC8tAwu8aHGW9/bfSmdNMYjqKNVSGNUYM0zE7eru/mOrOMeHgzvB+kDr7QFWXOK2UU/P93qpcVqi9lVOqmPq+el25y0f1l3bTD6qY+F5+Xbk5dupnfphu9e30z9Yth8jM1W7HXuR+7dsVT1+Xqc/qpS3dqSl22W5bS1Q+WuiolUZ929nhlty7gTuKtnWnvOB6HKNUqSdXVxHJ2gd6UAt06XI327P3q/7zx8NC1mo7V+x358VXtbwIrE0QFaUd7ISPf9dYXk4a8jnxq6ssZlbqpF9Sw1+ndDh5P/yyZtrUOFOXk7BUqO7KGYX5b1uDm5k+kXMPNX3Fm7x8=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/message_eager.drawio b/prov/efa/docs/message_eager.drawio
new file mode 100644
index 0000000..56d85d5
--- /dev/null
+++ b/prov/efa/docs/message_eager.drawio
@@ -0,0 +1,47 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-19T13:47:34.573Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15" etag="WZxVpdtQeOrHKn9q1XDl" version="12.4.8" type="browser">
+  <diagram id="aD31mCK-H1K8Hn6eNp0I" name="Page-1">
+    <mxGraphModel dx="935" dy="627" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="850" pageHeight="1100" math="0" shadow="0">
+      <root>
+        <mxCell id="0"/>
+        <mxCell id="1" parent="0"/>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-2" value="Application call libfabric&#39;s send API" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="20" y="20" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-3" value="Receiver&#39;s progress engine" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="220" y="20" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-6" value="" style="endArrow=classic;html=1;dashed=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" target="hM9hUbB8x_-XiU8bxfhU-9" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="79.5" y="80" as="sourcePoint"/>
+            <mxPoint x="80" y="250" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-7" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="279.5" y="80" as="sourcePoint"/>
+            <mxPoint x="280" y="259" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-8" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="80" y="106" as="sourcePoint"/>
+            <mxPoint x="280" y="166" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-9" value="Application got send&lt;br&gt;Completion" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="20" y="260" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-10" value="Application got&lt;br&gt;receive&lt;br&gt;Completion" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="220" y="260" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-14" value="EAGER_RTM&amp;nbsp;" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;rotation=17;" parent="1" vertex="1">
+          <mxGeometry x="160" y="117" width="40" height="20" as="geometry"/>
+        </mxCell>
+        <mxCell id="L2E3QGy6i6LVfAEWww4Z-2" value="Application call libfabric&#39;s receive API" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="365" y="137" width="120" height="60" as="geometry"/>
+        </mxCell>
+      </root>
+    </mxGraphModel>
+  </diagram>
+</mxfile>
diff --git a/prov/efa/docs/message_longcts.drawio b/prov/efa/docs/message_longcts.drawio
new file mode 100644
index 0000000..0da67a9
--- /dev/null
+++ b/prov/efa/docs/message_longcts.drawio
@@ -0,0 +1,119 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-20T15:06:47.205Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15" etag="2F3kZEbBkw2vVyTIGyXy" version="12.4.8" type="browser">
+  <diagram id="OzKOerL5p1M4QQdfBGPd" name="Page-1">
+    <mxGraphModel dx="1418" dy="627" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="850" pageHeight="1100" math="0" shadow="0">
+      <root>
+        <mxCell id="0"/>
+        <mxCell id="1" parent="0"/>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-2" value="Application call Libfabric&#39;s send API" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="120" y="30" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-3" value="Receiver&#39;s progress engine" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="320" y="30" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-6" value="" style="endArrow=classic;html=1;dashed=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" target="hM9hUbB8x_-XiU8bxfhU-9" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="179.5" y="90" as="sourcePoint"/>
+            <mxPoint x="180" y="260" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-7" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" target="hM9hUbB8x_-XiU8bxfhU-10" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="379.5" y="90" as="sourcePoint"/>
+            <mxPoint x="380" y="269" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-8" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="130" as="sourcePoint"/>
+            <mxPoint x="380" y="170" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-9" value="Application got libfaric&#39;s send completion" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="120" y="530" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-10" value="Application got Libfabric&#39;s receive completion" style="rounded=0;whiteSpace=wrap;html=1;" parent="1" vertex="1">
+          <mxGeometry x="320" y="530" width="120" height="60" as="geometry"/>
+        </mxCell>
+        <mxCell id="hM9hUbB8x_-XiU8bxfhU-14" value="LONGCTS RTM" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;rotation=11;" parent="1" vertex="1">
+          <mxGeometry x="234.82" y="130" width="100.18" height="20" as="geometry"/>
+        </mxCell>
+        <mxCell id="nSG-SVCCmyc8D_64ludf-9" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="380" y="180" as="sourcePoint"/>
+            <mxPoint x="180" y="220" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="nSG-SVCCmyc8D_64ludf-10" value="CTS" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;rotation=349;" parent="1" vertex="1">
+          <mxGeometry x="258" y="186" width="40" height="10" as="geometry"/>
+        </mxCell>
+        <mxCell id="nSG-SVCCmyc8D_64ludf-11" value="DATA" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;rotation=11;" parent="1" vertex="1">
+          <mxGeometry x="225" y="230" width="110" height="20" as="geometry"/>
+        </mxCell>
+        <mxCell id="nSG-SVCCmyc8D_64ludf-12" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="230" as="sourcePoint"/>
+            <mxPoint x="380" y="270" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-2" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="250" as="sourcePoint"/>
+            <mxPoint x="380" y="290" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-3" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="270" as="sourcePoint"/>
+            <mxPoint x="380" y="310" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-4" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="290" as="sourcePoint"/>
+            <mxPoint x="380" y="330" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-5" value="Application call Libfabric&#39;s receive API" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;" parent="1" vertex="1">
+          <mxGeometry x="380" y="160" width="160" height="20" as="geometry"/>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-6" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="380" y="340" as="sourcePoint"/>
+            <mxPoint x="180" y="380" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-8" value="CTS" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;rotation=349;" parent="1" vertex="1">
+          <mxGeometry x="260" y="347" width="40" height="10" as="geometry"/>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-9" value="DATA" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;rotation=11;" parent="1" vertex="1">
+          <mxGeometry x="225" y="390" width="110" height="20" as="geometry"/>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-10" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="390" as="sourcePoint"/>
+            <mxPoint x="380" y="430" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-11" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="410" as="sourcePoint"/>
+            <mxPoint x="380" y="450" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-12" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="430" as="sourcePoint"/>
+            <mxPoint x="380" y="470" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+        <mxCell id="ELLmg3xNlQ1Nrk4ykZ6T-13" value="" style="endArrow=classic;html=1;dashed=1;" parent="1" edge="1">
+          <mxGeometry width="50" height="50" relative="1" as="geometry">
+            <mxPoint x="180" y="450" as="sourcePoint"/>
+            <mxPoint x="380" y="490" as="targetPoint"/>
+          </mxGeometry>
+        </mxCell>
+      </root>
+    </mxGraphModel>
+  </diagram>
+</mxfile>
diff --git a/prov/efa/docs/message_longread.drawio b/prov/efa/docs/message_longread.drawio
new file mode 100644
index 0000000..a6994ac
--- /dev/null
+++ b/prov/efa/docs/message_longread.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-21T03:58:26.253Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="bT_koqUHzDPkJrmW13qh" version="12.4.8" type="device"><diagram id="APAEDZxGAzosg-hluIWG" name="Page-1">3Vhdc6IwFP01zj51B0QUH/3a7s6s046tu21fnAgBMhsIG6Li/vpNICAQbF1rrdsnkpOQ5J5zc3OTljEKkmsKIn9KHIhbbc1JWsa41W7rnXaXfwSyzZCebmWAR5EjO+2AO/QHSlCT6Ao5MK50ZIRghqIqaJMwhDarYIBSsql2cwmuzhoBDyrAnQ2wiv5EDvMz1DK1Hf4VIs/PZ9Y12RKAvLMEYh84ZFOCjEnLGFFCWFYKkhHEgrycl+y/L3tai4VRGLJDfvCnfX++HFrJ4uoBza1l4vrzq3Y2yhrglTR4EEUY2YAhEgpaQToGRksXLCmyP8XCEhg6/DO4/SYtY9ucLkpWoQPFjFrLGG58xOBdBGzRuuEOwjGfBZjXdF5ULZBGrSFlMClB0qJrSALI6JZ3ka05udtqdbOTSs8xvyRTV2JAeodXDLwjkBckh//Ap6HwOYM2RNyglLmIEo/CWBRh6KEQvj+Bl8ZgV2FQ4Yi730BsbV6zMYhjZFdpcUDspxSKCqeGbh8En5/NvPoo6U0r46RS28raXm4ZoB5kz1jQz/pBpxJZVAVKFJsNDOcYhZjvxnU1HjXRLme4JYivuBC41+d2lxW2asrFZEVtKP8qx4/aQFbNUczaOBkvyjipExRWH+8XvRP7xfH65kfThQh8OoVViftnldg6l8QXolyN7iLOvlY3vR6c31i3vqJbNYnwCCvnEGm2FkQYZs3vfQDWfN567wMwDy+H0pkmFpdEqN55mVHjrIx2FEZnk8F4MbufKmxxI1mVkphR8guOCCaUIyHhSZsxdBHGNQhg5IUiLnHqIMeHgjKuGh7IhgA5jpimUYOqSpSwTG2+hBPJUgSFItroiiydBlXqQel0qbKqyjzmCbE2G08HaXAFTnrbS8uMIrgWrQ5gQFzkKAnkVYSTfZkqnkI1raZar2Ezdc8pW0e94nzsU1rJivT/M4HuqPttcjO70K1TCoBGp/82EbCtny8CPoVPv0BMxu7NqPd4f6u5G/K94fFFUcNe0XXpKvuBt1UR2V59a9EO21ecSrAtdYtEh/iZBZvNC967rlr3/gvdzWYzdg6WLfjYmHAzjqNkgYfd+8XGv47dH7+Bd2UqHnjA8x/NXrRazS+AlxE/9sSGhg2xP4+tO6jZcPQ2bZAj4gWv7h6CM3F3z+nG5C8=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/message_medium.drawio b/prov/efa/docs/message_medium.drawio
new file mode 100644
index 0000000..267daff
--- /dev/null
+++ b/prov/efa/docs/message_medium.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-20T12:12:56.946Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="jeIVPnwt3zu9YySMpD0F" version="12.4.8" type="device"><diagram id="dTPj6XVY3x3ysBFnl7aE" name="Page-1">5VjbbuIwEP0atE9UuZDbI5duW7VoK7pou0+VSUxi1YlTx5SwX782cYDEoaJtSpHgBc/xLXPO2DNJxxzG+RUFaTQmAcQdQwvyjjnqGIbeM2z+J5BVgTi6WwAhRYEctAUe0D8oQU2iCxTArDKQEYIZSqugT5IE+qyCAUrJsjpsTnB11xSEUAEefIBV9A8KWFSgrqVt8WuIwqjcWddkTwzKwRLIIhCQ5Q5kXnbMISWEFa04H0IsyCt5Keb93NO7eTAKE3bIhGjsRdPZwM2fuo9o6s7yeTTtGsUqrwAvpMP9NMXIBwyRRNAK1mtgNJuDGUX+j0x4ApOA//Xvb6RnbFXSRckiCaDYUeuYg2WEGHxIgS96lzxAOBaxGHNL503VA+nUK6QM5juQ9OgKkhgyuuJDZG9J7qpqLrdS6SUW7chkSwzI6Ag3C28J5A3J4Tv4NBU+J9CHiDu0Zi6lJKQwE02YhCiB30/gqTFoKwwqHPHw64ujzS0fgyxDfpWWAGTRmkJhcGro6lHweWGV5l9J79oY5RVrJa293DJAQ8je8MArxsGgcrOoCuxQbDUwXGIUYn4aX6v3URPtcod7gvgTbwR2PO73rsJuTbmMLKgP5azd+6O2kFsLFKu2TsGLss46CDZefzwunJbjYq++J6Jbe8KpynlHVc49M+VqdOua3Y5uunfcE+cpulVrg5CwPaWBjTmjgxnlrVC0fBKnGBbTvjvf1c6C/d35rix030NznV9a1BinTfzpMd9TmB9fjm6m46fJ77HCF3eTVUnJGCXPcEgwoRxJCK/mzMEcYVyDAEZhIm42Th7k+ECQxvXFfdkRoyAQ2zSqUNWJElbEhTky3qxSDhdGt2vXjOEowvQadDFa0IVZ6cvV7VJnPSOZXE/u7jMHd3VFlvPKFyX/n87zdYXayxeNuqnvkuelW70g/rBu2nF1U99Zz0s3py3dzC/Tjd4+v5h6d5L+ysx+4g3vZq7V8EZ0sf6dfurSnU2YKHmqIXL2py7LrUrp6kdLXY2SqK86B3xO21RvbX1R+xSnprXneByjTmskVVcTy9kFul0L9N6X1Wjc3H6TLq6q7Zd98/I/</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/qp_collision.drawio b/prov/efa/docs/qp_collision.drawio
new file mode 100644
index 0000000..1e10097
--- /dev/null
+++ b/prov/efa/docs/qp_collision.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-08-03T20:04:57.121Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="jaKwibttJulnFHG_Ro7a" version="12.4.8" type="device"><diagram id="VktbdoG0Y93yvyAxLsci" name="Page-1">7Vpbb9owFP41qE+bcuXyWCjbKm1SJaS1e5q85EC8OjZyzK2/fjZxrg5toBR62QvEJ/aJ833fOfYxdNxRvP7K0Tz6wUIgHccK1x33quM4tud05ZeybFJLz+6nhhnHoe5UGCb4AbTR0tYFDiGpdBSMEYHnVWPAKIVAVGyIc7aqdpsyUn3qHM3AMEwCREzrLQ5FlFr7vlXYvwGeRdmTbUvfiVHWWRuSCIVsVTK544474oyJ9Cpej4Ao8DJc0nFfdtzNJ8aBijYD7sa9h4jDxBqPAeLfd7fJFH3SXpaILPQL68mKTYYA0PBSASlblFFpHIYoiUB5tWUjEjHRl+lQCA1Eiyna+YtLxQCLQfCN7LIqoM2QjUqoZjYOBAm8rLpHmuFZ7i5/wg3D8sGOpdXoZNxoLXpZO3ORsAUPQI8qQ/mEI9upORKIz0AYjuRF6bUL05apPVhzDNauaSIQDUBaLw0GOVvQMCdsFWEBkzkK1N2VjNomEpfABaz3pTGP+io8Xd0ssZwjVqa5a+1mtALhvni5j+E1PDtenvXK8PI+UlbwulX0ffvArOD5g8/+WfOC38BblwhFDl7Ky5m6HNNwriduK5Dl3HSnPzzrk1nkLEojG5wFHJCQrLf2cOZQ69USt3/uUOvuSZnz4SjzXh1nvb3SY0BQkuDgbSVFY4fT9w9MirXsKrPkSVOiPTh5TpT1iuBs84ZCrBZhjnvuCHOa6pLHWHM/XFqscebaZ+fMrErefVp0vUP3irW0aJSiL11B7rfFfxdkOQdv7OsVwqnJ8k1uJOoT3WRcRGzGKCLjwjos8pElW0Wf74zNNYF/QYiNPnJDC8Gq9MIaizs1XBY1aetX6c7VWnveNja60VYS8l222LdYuFNsn+xoiqy1ep53tGAZgXSDgntQ8plyFis4qpsLiXPFZgaeXBRElQu1obiHESOMd/KCe4oJqZkQwTOq4lXCDdI+VEsMDhC51DdiHIZbeTQtUFXJcCYkfkyNst3jrFlO7WDIHjjmotVvyAj1Ovp49JmL1vujz/OPRJ/rVRPqoHtu+syTvUfoc98mfUeLPre2eemdPfrMXQimiXzHWthdJA0bdhSGHJIk4/Ty58Fb99fB+BEY9moMO01HJb2TMtzqSLKJdPfiOdWckkUhELz92D5EVXiFYEatHXKI2RJ26vLJ8cVkdF76r1d1FlQ9L/ecBr02VQgvp9dWx0UjRClTCogQVXBJRUWwffN07WmpiQUlWp+pL3WZSH0BV8La/qQtv66v/gtlZwVWFsrgOEKRzeJH+bSEK/7a4I7/AQ==</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/read_longcts.drawio b/prov/efa/docs/read_longcts.drawio
new file mode 100644
index 0000000..9490fb4
--- /dev/null
+++ b/prov/efa/docs/read_longcts.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-21T03:39:40.468Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="WTTV_khaAKcNuogEBubj" version="12.4.8" type="device"><diagram id="FbJiT4IxuIQ_kcybeP4A" name="Page-1">5Zpdb5swFIZ/TbSrVoAhkMuEdNmktauaduuuKhccsOZgZJyv/fqZYBqM6dRFhKImN8EHY+z38Tk+GAbAX25nDKbxNQ0RGVhGuB2A6cCyTNsair/csissrukVhojhUFY6GOb4D5JGQ1pXOESZUpFTSjhOVWNAkwQFXLFBxuhGrbagRL1rCiOkGeYBJLr1Jw55XFg9xzjYvyAcxeWdTUOeWcKysjRkMQzppmICVwPgM0p5cbTc+ojk4pW6FNd9fuXsS8cYSvhbLoivR/HD88TbPl084gfvebuIHy6sopU1JCs54HGaEhxAjmmSywr3bRD8vIDPDAefMlFiCIbib3z7VY6M70q5GF0lIcrvaAzAZBNjjuYpDPKzGzFBhC3mSyJKpjjURyAHtUaMo23FJEc0Q3SJONuJKuXZUl05vYAsbg6sXqrEFU5DaYNyekQvLR8UFAdSxP8QFGiC3qEspUIUttcuZTRiKMsPURLhBL27hKBvEg41CTWNUBKOc+cWpYDALMOBKksIs3gvYV4Q0rDdY67npVMWf0l594XpVintZOlVbTlkEeL/GMGoqIdCJbboBCoSOw0KlzaGiPDHtRqRmmSXd7ilWPT44CPuSAy8inhUQ5fRFQuQvKwaQuoteepU0eZAoYzW0H4avIz7+JnhtjwzjidcLk89QQxaQww0xKNOEXtdIe4JubpLmaAlcKbbrW+ONHBqJhFRriUSAV2mBBUVepZIOO++DJYhpqLoDe2XaPXUoQei2Zpo377fzPz7uTDe3d9pkomRclWXjDP6G/mUUCYsCRUpGpgsMCE1EyQ4SvIYJPRDwj7JdRPznYzliSUOw/w2jSBUVIzywk9EF1piY9nOpVH9gVqccS5dV6M1aoBlnQpWGduryfLVeHo3v+0/JuA4LXEClgrGs3QfMrvEAkwNy8deg4HrqQhay4/NbtdgoDuUv2J72XMtk3z/Yf/4HsTiX/TNz/tGCBWrtHgQNRK0qVTspQu24XE1SF7DojU8kcMl89nF/IfvL3eBN30a2mQVLpo2YQ5MGAoQXuMkOj8yZtnCu5HRd3POKxIKBC1FQlBrqL1I2AhOTwM/ODhNb/tYcGo6Yjl2p+CccwfnOC2Bc51Owem7ttPx/bina1UXibxlNKxep0rkG5HoWzIf25fqiw4wjszj604J7JPl8Y3gGrZ+9jsYffel1vYuhq+ArMwju0tPMvVH4nOLbpb6fgE4DU9NnUa3MjU9m/CmRSWn9mrm2FQBuCd7x9NM7tyzc+CBdsjZBuiW3Nml56VrlILXd/TeTs5UG6q/5zs1uba/qug9uZrP2fUc7mifa+2jB1E8fP5VVD98RAeu/gI=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/read_short.drawio b/prov/efa/docs/read_short.drawio
new file mode 100644
index 0000000..25971a1
--- /dev/null
+++ b/prov/efa/docs/read_short.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-21T03:42:10.864Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="IdlnvQ-IAYbCGyDJeS47" version="12.4.8" type="device"><diagram id="FbJiT4IxuIQ_kcybeP4A" name="Page-1">3ZjRbpswFIafJtpVq4BDgcsk7dpddKuSVeuuKgefgDWDkXEasqefCSbgOJ22iqVouQnnt7E5339sLEZonpa3AufJPSfARu6YlCN0PXJdZ+Jeqb9K2dWK7wS1EAtKdKdWWNKfoMWxVjeUQGF0lJwzSXNTjHiWQSQNDQvBt2a3NWfmrDmOwRKWEWa2+o0SmdRq4I1b/Q5onDQzO2PdkuKmsxaKBBO+7UjoZoTmgnNZX6XlHFgFr+FS3/fxldbDgwnI5J/ckNyHyeNqFpTPF0/0MViV6+Txwq1HecFsoxOe5jmjEZaUZxVWvB+D0dUarwSNPhQqEoCJ+ps+fNKZyV2DS/BNRqCacTxCs21CJSxzHFWtW1UgSktkylTkqEs7A53UCwgJZUfSGd0CT0GKnerStDZ0dXkhHW5brw5dko5PV1rDujziw8gtQXWhIf4FUGQBXUCRcwVF7NnlgscCiuoSsphm8O4I0dAQXlkILUaQkWm1uFUUMVwUNDKxEFwke4RVoNCI3VPF89Jrwu8a7z64Lo1op6NX2UosYpC/ySCs+wEx9hbbgQ5i7wThRhPA1Hp8MXekU9j1DA+cqidu14gfqsS7FodH1hV8IyLQt3W3kOORArNUrBqoyVgD7cvgkPfbK8PvuTLe7nDzehqIxag3i5FlcXhWi4NzWTwQ546XlIN6Ms7xz7s2Q8s48yQRc2kdJCKe5gzqDgM7SLiT934NNltMh+hnPixox0eHAUCbWNCWd18WX58XXxcWLpWlNJkUUvAfMOeMC6VkXB3P0GxNGTuSMKNxVu0/ih0ofVYxU7XOprohpYRU05w0wbRJcFmvEfUIPfniTrzLcfeHjvYY79L3LafCE0a5/8qoZl/vHpRvpteL5cPwbUKe15NPyDWNCVx7/TjntAU5li3/9/sX+YFpQW9nY6ev968K268Edff2Wwu6+QU=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/write_dc_eager.drawio b/prov/efa/docs/write_dc_eager.drawio
new file mode 100644
index 0000000..0df3b62
--- /dev/null
+++ b/prov/efa/docs/write_dc_eager.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-25T18:35:32.678Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="kGBGikaECE3CBg2lTirp" version="12.4.8" type="device"><diagram id="lewDNAd5vCCO2mxxK25Q" name="Page-1">5Zhdc6IwFIZ/jbNX3QFRxEtE7PbCbdfqtN0bJ8IRMhsIG2KF/voNEhSI7nQ7tnZmr8x5ExLO8+YLO4YTZdcMJeGU+kA6Xc3POsa40+3qmm6Jn0LJS2WgD0ohYNiXjQ7CPX6B6kmpbrAPaaMhp5RwnDRFj8YxeLyhIcbottlsTUlz1AQFoAj3HiKq+oB9Hpaq1dcO+jfAQViNrGuyJkJVYymkIfLptiYZbsdwGKW8LEWZA6SAV3Epn5ucqN2/GIOYv+aBcDoMF6uRlS2vHvHCWmXrcHHVLXt5RmQjE7aThGAPcUzjAiva9UHwao1WDHtfUhFtGeYgfu27G5kazytejG5iH4ohtY4x2oai5X2CvKJ2K2aI0EIeERHpoqimILN6BsYhq0kypWugEXCWiyaytqKbN8PtwSu90sKaT6bUkJwewb7jA0FRkBD/AaihAJ1BmlDBhO3YJYwGDNKiCHGAY7g8wc+G0FQQKowg9u1icYvIIyhNsdfE4qM03CEsAoGG5Y8Fz6/9KnySeHfBOGtEuYxOsuWIBcD/ksGwbAd+Y29RHagh7h8hXGkMiFiPz80d6Rh2OcIdxeKN9wYPhiLvusNWy7mUbpgH8qn6DtLqyGpNlH6rn5KL0s9uEuyzfvu8GJx5Xpz095P4dj7jVOeGH+qc9Z8518Kta+Z5fNNN80N9Gyq+NW8HAeXK5cCjUUKgbHDpk601681Ln2zVpbYG9DttMTOJeIvRiolSUJQWMSmvC2rV5GY5c6e3c3fp/FiO7bl9eeL6K5D3PhR5T0E+dpaufe3OlrP5g0JMJMqbWFLO6C9wKKFMKDEVVzZjtMaEtCREcBAX25fAB0IfFdjEWiG2rIiw7xfDHPWh6RSjvFxj4hUG57FGN1t7SfUhVrPmmDPdMziDfwN6yD3/xbz96WZTbT7h0ytdMeYch0LrBDgC7WLHeXszH7zxVGgf5sa7HQpHfVM/GWeu497czT//WjJ6xn6iKCvnVVPl1GKy+u+1mER4+I+g9PDwT4vh/gE=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/write_eager.drawio b/prov/efa/docs/write_eager.drawio
new file mode 100644
index 0000000..7d4e2e0
--- /dev/null
+++ b/prov/efa/docs/write_eager.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-21T03:33:35.090Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="VSjpR7l5KNzOPeatHdgu" version="12.4.8" type="device"><diagram id="lewDNAd5vCCO2mxxK25Q" name="Page-1">5ZfRbtowFIafBu2qEyElhMtAadeLbh0tanuFTHJIrDlx5JgS9vQ7Jg7EMUzthEqlXcXnt2P7fL/tOB13nJY3guTJHY+AdXrdqOy4V51ez7nsefhQyqZSBo5fCbGgkW60Fx7ob9BiV6srGkFhNJScM0lzUwx5lkEoDY0IwddmsyVn5qg5icESHkLCbPWJRjKpVL/f3evfgMZJPbLT1TUpqRtroUhIxNcNyZ103LHgXFaltBwDU/BqLtV710dqdxMTkMm3vJDcDZPZYuSX84tnOvMX5TKZXfSqXl4JW+mEgzxnNCSS8kxhJds+GF0syULQ8EuB0VpQCfgM7m91anJT8xJ8lUWghux23NE6wZYPOQlV7RpXCGqJTBlGDhbtFHRWryAklA1Jp3QDPAUpNthE19Z0N2a43nvl1FrS8MnTGtHLI951vCeIBQ3xHUBdC+gUipwjE7FllwseCyhUEbKYZnB+gp8NoWchtBhBFgVqc2MUMlIUNDSxRKRItghVgGjE5lnx/NqvwxeNdxtclUa00dFRtpKIGORfMhhW7SAyzhbbgQbi/gHCtSaA4X58NU+kQ9j1CPec4ox3Bg+GmHfTYb/lXMFXIgT9VvMEaXXktxZKv9VPxcXqZ7sIdln/+7oYnHhdHPX3k/h2OuNs54Yf6pz/nznXwu10vdP45njeh/o2tHwzbwcxl9blIORpzqBqcO4vW2vVe+f+stWX2gbQ77zFzGM4i9FCYClWpVnGquuCXXV9O59O7n48Tubjn/Or4DE4P3HnDcgvPxT5pYV8EtxMpvPp45OFC7OUJpNCCv4LxpxxgUrG8b7mjpaUsZZEGI0zdXYhO0B9pJjhRmGBrkhpFKlhDppg2iS4rDYYTmFwGl8cr3WQOAPLl0O29N5vC4b7f5rqINr/GbqTPw==</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/write_longcts.drawio b/prov/efa/docs/write_longcts.drawio
new file mode 100644
index 0000000..7307e2c
--- /dev/null
+++ b/prov/efa/docs/write_longcts.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-21T04:01:36.429Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="PEkWYDq-Nzlm6bRhR7C_" version="12.4.8" type="device"><diagram id="HXWCCChorXi73rZmTyBJ" name="Page-1">5VrRkpowFP0ap0/bEYKIj8rabWd2t+3qdtu+OAgRMhsIE3DVfn0TCQpJ2mkdcJ3lieQQIDkn997cqz3gxtsb6qXRHQkg7pn9YNsD1z3TNCzTZheO7ApkaDgFEFIUiEFHYIZ+QQH2BbpGAcxqA3NCcI7SOuiTJIF+XsM8SsmmPmxFcP2rqRdCBZj5HlbRJxTkUYE6g/4R/whRGJVfNvriTuyVgwWQRV5ANhUITHvApYTkRSveuhBz8kpeiuc+/OHuYWIUJvm/PBDdjaLH5cTZLq6+o0dnuV1Fj1dm8ZYXD6/FgsdpipHv5YgknFZv/w6MlitvSZH/LmO9DUU5ZNfxl09iafmu5IuSdRJA/sl+D0w2ERs5Sz2f392wHcKwKI8x6xmsqS5BrOoF0hxuK5BY0g0kMczpjg0p75b0iv0FRHdzFOswJKoIZQvME/sjPLz5SCFrCBb/g1GgMPoAs5QwUuievJSSkMKMN2ESogS+OoXg0ii0FQoVjmASjLl1s56PvSxDfp2WwMuiPYW8w6ihu++cz/eDsvtD0LvvXG9rvZ3o/ZHb3KMhzP+yglExDgY156IqUKF4oGG4xCjEzCBf6i5JR7v4wheC2IyPNjIcsYVXJR5J0mVkTX0oHqv6EPlNTn2rKHugYEZ50X4bHNZ9+s4YNrwzTle4jE8XIjFoTGKgSDw6q8TOuSS+EOVkkzJAQ8IZw/Pa5kgRrn6U4PYknyR8EqcYFgMu7CAxePUwWLqYCqP3RCKtv07w/jBhYzadyZKyVshbHz4tHqZ3n+fThft1cT2ej1+dYGD8A8HWWQm2FIJvP9/fuPMZAx/mTwplbKV5nZcsp+QZugQTypCEsOMcmKwQxhLkYRQm3F8x/iDDJ5w3Zht4LG7EKAj4Z7RC1KWiJC9sik2hIW1MUA8ehk6bvkYbs7VjtJqZuGu6XyKfVcJTKe57PT9iVzYLl88CY8L8Dc9NEripDLxIHZuwKcnnA51udku6JbObq9k31413vnO9sC28DlaaKPC2w7cSdZ0Tw7dytJYlai58a4XTRJu9I7xI06m4QGCNGvKBA0dS0lZsSReejLZMyTAURbSB/OIkaSwqmfWopK1MGOf0boYald62e1O8UlPZidledjK9vY1DsL3HX417+mztnn/ac02hs2PCDZoSTi4stCycWk/tmHCyoZwqHJADVcvCqclVx4RrqgIHZJ/bsnADRbjxMa+i0IfoBSVhB7OrQyZ1juxKK03Tv4xcuk0ppmCdaFOycQI5TWvZptSqdseSK1sWcqiYUlvJlVYQtU7R7dwKjDS+ra3cSquIpgDxtp2b4pOaOjBYZz4waAoV3VLOkt3UycrJWVrbynU9LVZM5WTlzlzPMLqeFyumcrJyjRU0WPf4d75i+PFPkWD6Gw==</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/docs/write_longread.drawio b/prov/efa/docs/write_longread.drawio
new file mode 100644
index 0000000..1a26218
--- /dev/null
+++ b/prov/efa/docs/write_longread.drawio
@@ -0,0 +1 @@
+<mxfile host="drawio.corp.amazon.com" modified="2021-07-21T04:03:55.374Z" agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0" etag="p7vFC4OL6i7cI91cCkVy" version="12.4.8" type="device"><diagram id="ZDJ_sdb5zI1M9X0xq4Zh" name="Page-1">3Vhbc6IwFP41zj7tjogXfES03c7U2lqdXl6cCBEyjYQJUWF//SYQEAi9bGut2xfN+XIIOd+XnJzQ0K11dE5B4I2JA3Gj1XSihj5stFpau9XlfwKJU6SnGSngUuRIpz1wi/5ACTYlukEODEuOjBDMUFAGbeL70GYlDFBKdmW3FcHltwbAhQpwawOsonfIYV6KGp3mHv8Nketlb9aasmcNMmcJhB5wyK4A6aOGblFCWNpaRxbEgryMl/S5s2d684lR6LO3POCN+958OTCixc97NDeW0cqb/2ylo2wB3siAzSDAyAYMEV/QCpIxMFquwJIi+0fIrR1FDPJ/8/pChsbijC9KNr4DxSubDX2w87jnbQBs0bvjK4RjHltjbmm8qYYgo9pCymBUgGRI55CsIaMxd5G9Gbtx2dzttdIyzCvo1JUYkMvDzQfeM8gbksR/IFRXCJ3CMCCcE5pwF1DiUhiKJvRd5MOvZ/DUKOwqFCocQd8xxebmlo1BGCK7TIsDQi+hUBicGhrfCz5/dTLzQdKbGMOoZMXSepZbBqgL2QsR9FM/6JRyi6pAgeJODcMZRiHm+3Fbzkh1tMs3XBPEZ5wL3OvzuIsKGxXlQrKhNpRPFTNIZSCjslA6lXFSXpRxkkWQR/3+ddE78Lp4v77Z4XQiAh9OYVXi/lElNo4l8YkoV6E7z7Mf1U2rJudP1q2vnnrRtNHqYk7XYEl5yxWtu7RwqMIWWQcYphXHVx+GlfVvfPVhmKWaArVXhNvWDf8Z+enkNz5OSooqsWcXi+loPJmNFtbNYmjOzBPgt/c6we3uMQluKwRfTq7OpyNzuJjO7hTGeKCsTEvIKHmCFsGEcsQnvKbTByuEcQUCGLm+SFucPsjxgaCNl9rYlB1r5DjiNbU6lJWihKUlOp/CgaTJc0aejDRFmnbN0q/mrMOV0qoy81DcO6bDsZnkXuAk18GkzSiCW9HrAAZOVLVDqKRXVOrVbaBjytRWrzzf+9BWiiTt/6yn2+r+Gk2mJ7p1CglPb/c/J+O1tONlvEf/8QmEZLiaWL2H2XVztSOXNV9jFDXsDd0WbrbfeFvlme3Dl5jm2/YVpxLEBbdAOIQvTLhTP+Fn51VxN15x79SHsV9g6YTfmBO4uf/WmLrvv9jqo78=</diagram></mxfile>
\ No newline at end of file
diff --git a/prov/efa/src/efa.h b/prov/efa/src/efa.h
index 7e6e673..924b5a0 100644
--- a/prov/efa/src/efa.h
+++ b/prov/efa/src/efa.h
@@ -83,6 +83,7 @@
 #define EFA_EP_TYPE_IS_RDM(_info) \
 	(_info && _info->ep_attr && (_info->ep_attr->type == FI_EP_RDM))
 
+#define EFA_DEF_POOL_ALIGNMENT (8)
 #define EFA_MEM_ALIGNMENT (64)
 
 #define EFA_DEF_CQ_SIZE 1024
@@ -90,6 +91,12 @@
 #define EFA_MR_SUPPORTED_PERMISSIONS (FI_SEND | FI_RECV | FI_REMOTE_READ)
 
 /*
+ * Setting ibv_qp_attr.rnr_retry to this number when modifying qp
+ * to cause firmware to retry indefininetly.
+ */
+#define EFA_RNR_INFINITE_RETRY 7
+
+/*
  * Multiplier to give some room in the device memory registration limits
  * to allow processes added to a running job to bootstrap.
  */
@@ -101,6 +108,8 @@
  * Specific flags and attributes for shm provider
  */
 #define EFA_SHM_MAX_AV_COUNT       (256)
+/* maximum name length for shm endpoint */
+#define EFA_SHM_NAME_MAX	   (256)
 
 extern int efa_mr_cache_enable;
 extern size_t efa_mr_max_cached_count;
@@ -111,26 +120,29 @@ extern struct util_prov efa_util_prov;
 
 struct efa_fabric {
 	struct util_fabric	util_fabric;
+	struct fid_fabric *shm_fabric;
+#ifdef EFA_PERF_ENABLED
+	struct ofi_perfset perf_set;
+#endif
 };
 
-struct efa_ep_addr {
-	uint8_t			raw[16];
-	uint16_t		qpn;
-	uint16_t		pad;
-	uint32_t		qkey;
-	struct efa_ep_addr	*next;
-};
-
-#define EFA_EP_ADDR_LEN sizeof(struct efa_ep_addr)
 
 struct efa_ah {
-	struct ibv_ah	*ibv_ah;
-	uint16_t	ahn;
+	uint8_t		gid[EFA_GID_LEN]; /* efa device GID */
+	struct ibv_ah	*ibv_ah; /* created by ibv_create_ah() using GID */
+	uint16_t	ahn; /* adress handle number */
+	int		refcnt; /* reference counter. Multiple efa_conn can share an efa_ah */
+	UT_hash_handle	hh; /* hash map handle, link all efa_ah with efa_ep->ah_map */
 };
 
 struct efa_conn {
-	struct efa_ah		ah;
-	struct efa_ep_addr	ep_addr;
+	struct efa_ah		*ah;
+	struct efa_ep_addr	*ep_addr;
+	/* for FI_AV_TABLE, fi_addr is same as util_av_fi_addr,
+	 * for FI_AV_MAP, fi_addr is pointer to efa_conn; */
+	fi_addr_t		fi_addr;
+	fi_addr_t		util_av_fi_addr;
+	struct rdm_peer		rdm_peer;
 };
 
 /*
@@ -156,6 +168,62 @@ struct efa_domain {
 	size_t			qp_table_sz_m1;
 };
 
+/**
+ * @brief get a pointer to struct efa_domain from a domain_fid
+ *
+ * @param[in]	domain_fid	a fid to a domain
+ * @return	return the pointer to struct efa_domain
+ */
+static inline
+struct efa_domain *efa_domain_from_fid(struct fid_domain *domain_fid)
+{
+	struct util_domain *util_domain;
+	struct efa_domain_base *efa_domain_base;
+	struct rxr_domain *rxr_domain;
+	struct efa_domain *efa_domain;
+
+	util_domain = container_of(domain_fid, struct util_domain,
+				   domain_fid);
+	efa_domain_base = container_of(util_domain, struct efa_domain_base,
+				       util_domain.domain_fid);
+
+	/*
+	 * An rxr_domain fid was passed to the user if this is an RDM
+	 * endpoint, otherwise it is an efa_domain fid.  This will be
+	 * removed once the rxr and efa domain structures are combined.
+	 */
+	if (efa_domain_base->type == EFA_DOMAIN_RDM) {
+		rxr_domain = (struct rxr_domain *)efa_domain_base;
+		efa_domain = container_of(rxr_domain->rdm_domain, struct efa_domain,
+					  util_domain.domain_fid);
+	} else {
+		assert(efa_domain_base->type == EFA_DOMAIN_DGRAM);
+		efa_domain = (struct efa_domain *)efa_domain_base;
+	}
+
+	return efa_domain;
+}
+
+/**
+ * @brief get efa domain type from domain fid
+ *
+ * @param[in]	domain_fid	a fid to a domain
+ * @return	efa domain type, either EFA_DOMAIN_DGRAM or EFA_DOMAIN_RDM
+ */
+static inline
+enum efa_domain_type efa_domain_get_type(struct fid_domain *domain_fid)
+{
+	struct util_domain *util_domain;
+	struct efa_domain_base *efa_domain_base;
+
+	util_domain = container_of(domain_fid, struct util_domain,
+				   domain_fid);
+	efa_domain_base = container_of(util_domain, struct efa_domain_base,
+				       util_domain.domain_fid);
+
+	return efa_domain_base->type;
+}
+
 extern struct fi_ops_mr efa_domain_mr_ops;
 extern struct fi_ops_mr efa_domain_mr_cache_ops;
 int efa_mr_cache_entry_reg(struct ofi_mr_cache *cache,
@@ -244,6 +312,7 @@ struct efa_ep {
 	struct efa_cq		*scq;
 	struct efa_av		*av;
 	struct fi_info		*info;
+	size_t			rnr_retry;
 	void			*src_addr;
 	struct ibv_send_wr	xmit_more_wr_head;
 	struct ibv_send_wr	*xmit_more_wr_tail;
@@ -264,43 +333,54 @@ struct efa_recv_wr {
 	struct ibv_sge sge[];
 };
 
-typedef struct efa_conn *
-	(*efa_addr_to_conn_func)
-	(struct efa_av *av, fi_addr_t addr);
-
 struct efa_av {
 	struct fid_av		*shm_rdm_av;
 	fi_addr_t		shm_rdm_addr_map[EFA_SHM_MAX_AV_COUNT];
 	struct efa_domain       *domain;
 	struct efa_ep           *ep;
 	size_t			used;
-	size_t			next;
 	size_t			shm_used;
 	enum fi_av_type		type;
-	efa_addr_to_conn_func	addr_to_conn;
+	/* connid_map is a map from (ahn + qpn) to latest connid.
+	 * it is used when a peer does not support the "connid header"
+	 * extra request
+	 */
+	struct efa_connid_map   *connid_map;
+	/* reverse_av is a map from (ahn + qpn + connid) to efa_conn */
 	struct efa_reverse_av	*reverse_av;
+	struct efa_ah		*ah_map;
 	struct util_av		util_av;
-	size_t			count;
 	enum fi_ep_type         ep_type;
-	/* Used only for FI_AV_TABLE */
-	struct efa_conn         **conn_table;
 };
 
 struct efa_av_entry {
 	uint8_t			ep_addr[EFA_EP_ADDR_LEN];
-	fi_addr_t		rdm_addr;
-	fi_addr_t		shm_rdm_addr;
-	bool			local_mapping;
+	struct efa_conn		conn;
+};
+
+struct efa_connid_map_key {
+	uint16_t ahn;
+	uint16_t qpn;
+	uint32_t connid;
+};
+
+struct efa_connid_map {
+	struct efa_connid_map_key key;
+	uint32_t connid;
+	UT_hash_handle hh;
 };
 
-struct efa_ah_qpn {
+struct efa_reverse_av_key {
 	uint16_t ahn;
 	uint16_t qpn;
+	uint32_t connid;
 };
 
+#define EFA_DGRAM_CONNID (0x0)
+
 struct efa_reverse_av {
-	struct efa_ah_qpn key;
-	fi_addr_t fi_addr;
+	struct efa_reverse_av_key key;
+	struct efa_conn *conn;
 	UT_hash_handle hh;
 };
 
@@ -361,19 +441,28 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 		struct fid_av **av_fid, void *context);
 int efa_cq_open(struct fid_domain *domain_fid, struct fi_cq_attr *attr,
 		struct fid_cq **cq_fid, void *context);
+int efa_fabric(struct fi_fabric_attr *attr, struct fid_fabric **fabric_fid,
+	       void *context);
+int efa_getinfo(uint32_t version, const char *node, const char *service,
+		uint64_t flags, const struct fi_info *hints, struct fi_info **info);
+void efa_finalize_prov(void);
 
 /* AV sub-functions */
-int efa_av_insert_addr(struct efa_av *av, struct efa_ep_addr *addr,
-		       fi_addr_t *fi_addr, uint64_t flags, void *context);
+int efa_av_insert_one(struct efa_av *av, struct efa_ep_addr *addr,
+		      fi_addr_t *fi_addr, uint64_t flags, void *context);
+
+struct efa_conn *efa_av_addr_to_conn(struct efa_av *av, fi_addr_t fi_addr);
 
 /* Caller must hold cq->inner_lock. */
 void efa_cq_inc_ref_cnt(struct efa_cq *cq, uint8_t sub_cq_idx);
 /* Caller must hold cq->inner_lock. */
 void efa_cq_dec_ref_cnt(struct efa_cq *cq, uint8_t sub_cq_idx);
 
-fi_addr_t efa_ahn_qpn_to_addr(struct efa_av *av, uint16_t ahn, uint16_t qpn);
+uint32_t *efa_av_lookup_latest_connid(struct efa_av *av, uint16_t ahn, uint16_t qpn);
+
+fi_addr_t efa_av_reverse_lookup(struct efa_av *av, uint16_t ahn, uint16_t qpn, uint32_t connid);
 
-struct fi_provider *init_lower_efa_prov();
+int efa_init_prov(void);
 
 ssize_t efa_post_flush(struct efa_ep *ep, struct ibv_send_wr **bad_wr);
 
@@ -381,6 +470,17 @@ ssize_t efa_cq_readfrom(struct fid_cq *cq_fid, void *buf, size_t count, fi_addr_
 
 ssize_t efa_cq_readerr(struct fid_cq *cq_fid, struct fi_cq_err_entry *entry, uint64_t flags);
 
+/*
+ * ON will avoid using huge pages for bounce buffers, so that the libibverbs
+ * fork support can be used safely.
+ */
+enum efa_fork_support_status {
+	EFA_FORK_SUPPORT_OFF = 0,
+	EFA_FORK_SUPPORT_ON,
+	EFA_FORK_SUPPORT_UNNEEDED,
+};
+extern enum efa_fork_support_status efa_fork_status;
+
 bool efa_device_support_rdma_read(void);
 
 static inline
@@ -405,19 +505,40 @@ bool efa_ep_support_rnr_retry_modify(struct fid_ep *ep_fid)
 #endif
 }
 
+/**
+ * @brief return whether this endpoint should write error cq entry for RNR.
+ *
+ * For an endpoint to write RNR completion, two conditions must be met:
+ *
+ * First, the end point must be able to receive RNR completion from rdma-core,
+ * which means rnr_etry must be less then EFA_RNR_INFINITE_RETRY.
+ *
+ * Second, the app need to request this feature when opening endpoint
+ * (by setting info->domain_attr->resource_mgmt to FI_RM_DISABLED).
+ * The setting was saved as rxr_ep->handle_resource_management.
+ *
+ * @param[in]	ep	endpoint
+ */
+static inline
+bool rxr_ep_should_write_rnr_completion(struct rxr_ep *ep)
+{
+	return (rxr_env.rnr_retry < EFA_RNR_INFINITE_RETRY) &&
+		(ep->handle_resource_management == FI_RM_DISABLED);
+}
+
 static inline
-bool efa_peer_support_rdma_read(struct rxr_peer *peer)
+bool efa_peer_support_rdma_read(struct rdm_peer *peer)
 {
 	/* RDMA READ is an extra feature defined in version 4 (the base version).
 	 * Because it is an extra feature, an EP will assume the peer does not support
 	 * it before a handshake packet was received.
 	 */
 	return (peer->flags & RXR_PEER_HANDSHAKE_RECEIVED) &&
-	       (peer->features[0] & RXR_REQ_FEATURE_RDMA_READ);
+	       (peer->extra_info[0] & RXR_EXTRA_FEATURE_RDMA_READ);
 }
 
 static inline
-bool rxr_peer_support_delivery_complete(struct rxr_peer *peer)
+bool rxr_peer_support_delivery_complete(struct rdm_peer *peer)
 {
 	/* FI_DELIVERY_COMPLETE is an extra feature defined
 	 * in version 4 (the base version).
@@ -426,11 +547,11 @@ bool rxr_peer_support_delivery_complete(struct rxr_peer *peer)
 	 * it before a handshake packet was received.
 	 */
 	return (peer->flags & RXR_PEER_HANDSHAKE_RECEIVED) &&
-	       (peer->features[0] & RXR_REQ_FEATURE_DELIVERY_COMPLETE);
+	       (peer->extra_info[0] & RXR_EXTRA_FEATURE_DELIVERY_COMPLETE);
 }
 
 static inline
-bool efa_both_support_rdma_read(struct rxr_ep *ep, struct rxr_peer *peer)
+bool efa_both_support_rdma_read(struct rxr_ep *ep, struct rdm_peer *peer)
 {
 	if (!rxr_env.use_device_rdma)
 		return 0;
@@ -439,58 +560,82 @@ bool efa_both_support_rdma_read(struct rxr_ep *ep, struct rxr_peer *peer)
 	       (peer->is_self || efa_peer_support_rdma_read(peer));
 }
 
+/**
+ * @brief determines whether a peer needs the endpoint to include
+ * raw address int the req packet header.
+ *
+ * There are two cases a peer need the raw address in REQ packet header:
+ *
+ * 1. the initial packets to a peer should include the raw address,
+ * because the peer might not have ep's address in its address vector
+ * causing the peer to be unable to send packet back. Normally, after
+ * an endpoint received a hanshake packet from a peer, it can stop
+ * including raw address in packet header.
+ *
+ * 2. If the peer requested to keep the header length constant through
+ * out the communiciton, endpoint will include the raw address in the
+ * header even afer received handshake from a header to conform to the
+ * request. Usually, peer has this request because they are in zero
+ * copy receive mode, which requires the packet header size to remain
+ * the same.
+ *
+ * @params[in]	peer	pointer to rdm_peer
+ * @return	a boolean indicating whether the peer needs the raw address header
+ */
 static inline
-size_t efa_max_rdma_size(struct fid_ep *ep_fid)
+bool rxr_peer_need_raw_addr_hdr(struct rdm_peer *peer)
 {
-	struct efa_ep *efa_ep;
+	if (OFI_UNLIKELY(!(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED)))
+		return true;
 
-	efa_ep = container_of(ep_fid, struct efa_ep, util_ep.ep_fid);
-	return efa_ep->domain->ctx->max_rdma_size;
+	return peer->extra_info[0] & RXR_EXTRA_REQUEST_CONSTANT_HEADER_LENGTH;
 }
 
-static inline
-struct rxr_peer *efa_ep_get_peer(struct dlist_entry *ep_list_entry,
-				 fi_addr_t addr)
-{
-	struct util_ep *util_ep;
-	struct rxr_ep *rxr_ep;
-
-	util_ep = container_of(ep_list_entry, struct util_ep,
-			       av_entry);
-	rxr_ep = container_of(util_ep, struct rxr_ep, util_ep);
-	return rxr_ep_get_peer(rxr_ep, addr);
-}
 
+/**
+ * @brief determines whether a peer needs the endpoint to include
+ * connection ID (connid) in packet header.
+ *
+ * Connection ID is a 4 bytes random integer identifies an endpoint.
+ * Including connection ID in a packet's header allows peer to
+ * identify sender of the packet. It is necessary because device
+ * only report GID+QPN of a received packet, while QPN may be reused
+ * accross device endpoint teardown and initialization.
+ *
+ * EFA uses qkey as connection ID.
+ *
+ * @params[in]	peer	pointer to rdm_peer
+ * @return	a boolean indicating whether the peer needs connection ID
+ */
 static inline
-int efa_peer_in_use(struct rxr_peer *peer)
+bool rxr_peer_need_connid(struct rdm_peer *peer)
 {
-	struct rxr_pkt_entry *pending_pkt;
-
-	if ((peer->tx_pending) || (peer->flags & RXR_PEER_IN_BACKOFF))
-		return -FI_EBUSY;
-	if (peer->rx_init) {
-		pending_pkt = *ofi_recvwin_peek(peer->robuf);
-		if (pending_pkt && pending_pkt->pkt)
-			return -FI_EBUSY;
-	}
-	return 0;
+	return (peer->flags & RXR_PEER_HANDSHAKE_RECEIVED) &&
+	       (peer->extra_info[0] & RXR_EXTRA_REQUEST_CONNID_HEADER);
 }
+
 static inline
-void efa_free_robuf(struct rxr_peer *peer)
+size_t efa_max_rdma_size(struct fid_ep *ep_fid)
 {
-	ofi_recvwin_free(peer->robuf);
-	ofi_buf_free(peer->robuf);
+	struct efa_ep *efa_ep;
+
+	efa_ep = container_of(ep_fid, struct efa_ep, util_ep.ep_fid);
+	return efa_ep->domain->ctx->max_rdma_size;
 }
 
 static inline
-void efa_peer_reset(struct rxr_peer *peer)
+struct rdm_peer *rxr_ep_get_peer(struct rxr_ep *ep, fi_addr_t addr)
 {
-	efa_free_robuf(peer);
-#ifdef ENABLE_EFA_POISONING
-	rxr_poison_mem_region((uint32_t *)peer, sizeof(struct rxr_peer));
-#endif
-	memset(peer, 0, sizeof(struct rxr_peer));
-	dlist_init(&peer->rnr_entry);
+	struct util_av_entry *util_av_entry;
+	struct efa_av_entry *av_entry;
+
+	if (OFI_UNLIKELY(addr == FI_ADDR_NOTAVAIL))
+		return NULL;
+
+	util_av_entry = ofi_bufpool_get_ibuf(ep->util_ep.av->av_entry_pool,
+	                                     addr);
+	av_entry = (struct efa_av_entry *)util_av_entry->data;
+	return av_entry->conn.ep_addr ? &av_entry->conn.rdm_peer : NULL;
 }
 
 static inline bool efa_ep_is_cuda_mr(struct efa_mr *efa_mr)
@@ -511,4 +656,51 @@ static inline bool efa_is_cache_available(struct efa_domain *efa_domain)
 	return efa_domain->cache;
 }
 
+#define RXR_REQ_OPT_HDR_ALIGNMENT 8
+#define RXR_REQ_OPT_RAW_ADDR_HDR_SIZE (((sizeof(struct rxr_req_opt_raw_addr_hdr) + EFA_EP_ADDR_LEN - 1)/RXR_REQ_OPT_HDR_ALIGNMENT + 1) * RXR_REQ_OPT_HDR_ALIGNMENT)
+
+/*
+ * Per libfabric standard, the prefix must be a multiple of 8, hence the static assert
+ */
+#define RXR_MSG_PREFIX_SIZE (sizeof(struct rxr_pkt_entry) + sizeof(struct rxr_eager_msgrtm_hdr) + RXR_REQ_OPT_RAW_ADDR_HDR_SIZE)
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(RXR_MSG_PREFIX_SIZE % 8 == 0, "message prefix size alignment check");
+#endif
+
+/* Performance counter declarations */
+#ifdef EFA_PERF_ENABLED
+#define EFA_PERF_FOREACH(DECL)	\
+	DECL(perf_efa_tx),	\
+	DECL(perf_efa_recv),	\
+	DECL(efa_perf_size)	\
+
+enum efa_perf_counters {
+	EFA_PERF_FOREACH(OFI_ENUM_VAL)
+};
+
+extern const char *efa_perf_counters_str[];
+
+static inline void efa_perfset_start(struct rxr_ep *ep, size_t index)
+{
+	struct rxr_domain *domain = rxr_ep_domain(ep);
+	struct efa_fabric *fabric = container_of(domain->util_domain.fabric,
+						 struct efa_fabric,
+						 util_fabric);
+	ofi_perfset_start(&fabric->perf_set, index);
+}
+
+static inline void efa_perfset_end(struct rxr_ep *ep, size_t index)
+{
+	struct rxr_domain *domain = rxr_ep_domain(ep);
+	struct efa_fabric *fabric = container_of(domain->util_domain.fabric,
+						 struct efa_fabric,
+						 util_fabric);
+	ofi_perfset_end(&fabric->perf_set, index);
+}
+#else
+#define efa_perfset_start(ep, index) do {} while (0)
+#define efa_perfset_end(ep, index) do {} while (0)
+#endif
+
 #endif /* EFA_H */
diff --git a/prov/efa/src/efa_av.c b/prov/efa/src/efa_av.c
index 74192f1..39c7326 100644
--- a/prov/efa/src/efa_av.c
+++ b/prov/efa/src/efa_av.c
@@ -36,8 +36,8 @@
 #include <stdio.h>
 
 #include <infiniband/efadv.h>
-
 #include <ofi_enosys.h>
+
 #include "efa.h"
 #include "rxr.h"
 
@@ -74,31 +74,177 @@ static bool efa_is_same_addr(struct efa_ep_addr *lhs, struct efa_ep_addr *rhs)
 	       lhs->qpn == rhs->qpn && lhs->qkey == rhs->qkey;
 }
 
-static inline struct efa_conn *efa_av_tbl_idx_to_conn(struct efa_av *av, fi_addr_t addr)
+/**
+ * @brief initialize a rdm peer
+ *
+ * @param[in,out]	peer	rdm peer
+ * @param[in]		ep	rdm endpoint
+ * @param[in]		conn	efa conn object
+ */
+static inline
+void efa_rdm_peer_init(struct rdm_peer *peer, struct rxr_ep *ep, struct efa_conn *conn)
 {
-	if (OFI_UNLIKELY(addr == FI_ADDR_UNSPEC))
-		return NULL;
-	return av->conn_table[addr];
+	memset(peer, 0, sizeof(struct rdm_peer));
+
+	peer->efa_fiaddr = conn->fi_addr;
+	peer->is_self = efa_is_same_addr((struct efa_ep_addr *)ep->core_addr,
+					 conn->ep_addr);
+
+	ofi_recvwin_buf_alloc(&peer->robuf, rxr_env.recvwin_size);
+	peer->rx_credits = rxr_env.rx_window_size;
+	peer->tx_credits = rxr_env.tx_max_credits;
+	dlist_init(&peer->outstanding_tx_pkts);
+	dlist_init(&peer->rx_unexp_list);
+	dlist_init(&peer->rx_unexp_tagged_list);
+	dlist_init(&peer->tx_entry_list);
+	dlist_init(&peer->rx_entry_list);
+}
+
+/**
+ * @brief clear resources accociated with a peer
+ *
+ * release reorder buffer, tx_entry list and rx_entry list of a peer
+ *
+ * @param[in,out]	peer 	rdm peer
+ */
+void efa_rdm_peer_clear(struct rxr_ep *ep, struct rdm_peer *peer)
+{
+	struct dlist_entry *tmp;
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_pkt_entry *pkt_entry;
+	/*
+	 * TODO: Add support for wait/signal until all pending messages have
+	 * been sent/received so we do not attempt to complete a data transfer
+	 * or internal transfer after the EP is shutdown.
+	 */
+	if ((peer->flags & RXR_PEER_REQ_SENT) &&
+	    !(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED))
+		FI_WARN_ONCE(&rxr_prov, FI_LOG_EP_CTRL, "Closing EP with unacked CONNREQs in flight\n");
+
+	if (peer->robuf.pending)
+		ofi_recvwin_free(&peer->robuf);
+
+	if (!ep) {
+		/* ep is NULL means the endpoint has been closed.
+		 * In this case there is no need to proceed because
+		 * all the tx_entry, rx_entry, pkt_entry has been released.
+		 */
+		return;
+	}
+
+	/* we cannot release outstanding TX packets because device
+	 * will report completion of these packets later. Setting
+	 * the address to FI_ADDR_NOTAVAIL, so rxr_ep_get_peer()
+	 * will return NULL for the address, so the completion will
+	 * be ignored.
+	 */
+	dlist_foreach_container(&peer->outstanding_tx_pkts,
+				struct rxr_pkt_entry,
+				pkt_entry, entry) {
+		pkt_entry->addr = FI_ADDR_NOTAVAIL;
+	}
+
+	dlist_foreach_container_safe(&peer->tx_entry_list,
+				     struct rxr_tx_entry,
+				     tx_entry, peer_entry, tmp) {
+		rxr_release_tx_entry(ep, tx_entry);
+	}
+
+	dlist_foreach_container_safe(&peer->rx_entry_list,
+				     struct rxr_rx_entry,
+				     rx_entry, peer_entry, tmp) {
+		rxr_release_rx_entry(ep, rx_entry);
+	}
+
+	if (peer->flags & RXR_PEER_HANDSHAKE_QUEUED)
+		dlist_remove(&peer->handshake_queued_entry);
+
+	if (peer->flags & RXR_PEER_IN_BACKOFF)
+		dlist_remove(&peer->rnr_backoff_entry);
+
+#ifdef ENABLE_EFA_POISONING
+	rxr_poison_mem_region((uint32_t *)peer, sizeof(struct rdm_peer));
+#endif
 }
 
-static inline struct efa_conn *efa_av_map_addr_to_conn(struct efa_av *av, fi_addr_t addr)
+/**
+ * @brief find efa_conn struct using fi_addr
+ *
+ * @param[in]	av	efa av
+ * @param[in]	addr	fi_addr
+ * @return	if address is valid, return pointer to efa_conn struct
+ * 		otherwise, return NULL
+ */
+struct efa_conn *efa_av_addr_to_conn(struct efa_av *av, fi_addr_t fi_addr)
 {
-	if (OFI_UNLIKELY(addr == FI_ADDR_UNSPEC))
+	struct util_av_entry *util_av_entry;
+	struct efa_av_entry *efa_av_entry;
+
+	if (OFI_UNLIKELY(fi_addr == FI_ADDR_UNSPEC))
 		return NULL;
-	return (struct efa_conn *)(void *)addr;
+
+	if (av->type == FI_AV_MAP) {
+		return (struct efa_conn *)fi_addr;
+	}
+
+	assert(av->type == FI_AV_TABLE);
+	util_av_entry = ofi_bufpool_get_ibuf(av->util_av.av_entry_pool, fi_addr);
+	if (!util_av_entry)
+		return NULL;
+
+	efa_av_entry = (struct efa_av_entry *)util_av_entry->data;
+	return efa_av_entry->conn.ep_addr ? &efa_av_entry->conn : NULL;
 }
 
-fi_addr_t efa_ahn_qpn_to_addr(struct efa_av *av, uint16_t ahn, uint16_t qpn)
+/**
+ * @brief find connid by address handle number (ahn) and QP number (qpn)
+ *
+ * This function is used when a received packet does not not connid
+ * in its header (which means the packet is from a peer that is using
+ * an older version of libfabric that does not support the "connid
+ * header" extra request)
+ *
+ * @param[in]	av	address vector
+ * @param[in]	ahn	address handle number
+ * @param[in]	qpn	QP number
+ * @return	On success, return the pointer to connid
+ * 		If no such connid exist, return NULL
+ */
+uint32_t *efa_av_lookup_latest_connid(struct efa_av *av, uint16_t ahn, uint16_t qpn)
+{
+	struct efa_connid_map *connid_map_entry;
+	struct efa_connid_map_key key;
+
+	memset(&key, 0, sizeof(key));
+	key.ahn = ahn;
+	key.qpn = qpn;
+	HASH_FIND(hh, av->connid_map, &key, sizeof(key), connid_map_entry);
+
+	return OFI_LIKELY(!!connid_map_entry) ? &(connid_map_entry->connid) : NULL;
+}
+/**
+ * @brief find fi_addr by address handle number (ahn), QP number (qpn) and connid
+ *
+ * @param[in]	av	address vector
+ * @param[in]	ahn	address handle number
+ * @param[in]	qpn	QP number
+ * @param[in]   connid	connection ID.
+ * @return	On success, return pointer to rdm_peer
+ * 		If no such peer exist, return FI_ADDR_NOTAVAIL
+ */
+fi_addr_t efa_av_reverse_lookup(struct efa_av *av, uint16_t ahn, uint16_t qpn, uint32_t connid)
 {
-	struct efa_reverse_av *reverse_av;
-	struct efa_ah_qpn key = {
-		.ahn = ahn,
-		.qpn = qpn,
-	};
+	struct efa_reverse_av *reverse_av_entry;
+	struct efa_reverse_av_key key;
 
-	HASH_FIND(hh, av->reverse_av, &key, sizeof(key), reverse_av);
+	memset(&key, 0, sizeof(key));
+	key.ahn = ahn;
+	key.qpn = qpn;
+	key.connid = connid;
+	HASH_FIND(hh, av->reverse_av, &key, sizeof(key), reverse_av_entry);
 
-	return OFI_LIKELY(!!reverse_av) ? reverse_av->fi_addr : FI_ADDR_NOTAVAIL;
+	return OFI_LIKELY(!!reverse_av_entry) ? reverse_av_entry->conn->fi_addr : FI_ADDR_NOTAVAIL;
 }
 
 static inline int efa_av_is_valid_address(struct efa_ep_addr *addr)
@@ -108,304 +254,477 @@ static inline int efa_av_is_valid_address(struct efa_ep_addr *addr)
 	return memcmp(addr->raw, all_zeros.raw, sizeof(addr->raw));
 }
 
-/* Returns the first NULL index in av connection table, starting from @hint */
-static size_t efa_av_tbl_find_first_empty(struct efa_av *av, size_t hint)
+/**
+ * @brief allocate an ibv_ah object from GID.
+ * This function use a hash map to store GID to ibv_ah map,
+ * and re-use ibv_ah for same GID
+ *
+ * @param[in]	av	address vector
+ * @param[in]	gid	GID
+ */
+static
+struct efa_ah *efa_ah_alloc(struct efa_av *av, const uint8_t *gid)
 {
-	struct efa_conn **conn_table;
+	struct ibv_pd *ibv_pd = av->domain->ibv_pd;
+	struct efa_ah *efa_ah;
+	struct ibv_ah_attr ibv_ah_attr = { 0 };
+	struct efadv_ah_attr efa_ah_attr = { 0 };
+	int err;
 
-	assert(av->type == FI_AV_TABLE);
+	efa_ah = NULL;
+	HASH_FIND(hh, av->ah_map, gid, EFA_GID_LEN, efa_ah);
+	if (efa_ah) {
+		efa_ah->refcnt += 1;
+		return efa_ah;
+	}
 
-	conn_table = av->conn_table;
-	for (; hint < av->count; hint++) {
-		if (!conn_table[hint])
-			return hint;
+	efa_ah = malloc(sizeof(struct efa_ah));
+	if (!efa_ah) {
+		errno = FI_ENOMEM;
+		EFA_WARN(FI_LOG_AV, "cannot allocate memory for efa_ah");
+		return NULL;
+	}
+
+	ibv_ah_attr.port_num = 1;
+	ibv_ah_attr.is_global = 1;
+	memcpy(ibv_ah_attr.grh.dgid.raw, gid, EFA_GID_LEN);
+	efa_ah->ibv_ah = ibv_create_ah(ibv_pd, &ibv_ah_attr);
+	if (!efa_ah->ibv_ah) {
+		EFA_WARN(FI_LOG_AV, "ibv_create_ah failed! errno: %d\n", errno);
+		goto err_free_efa_ah;
+	}
+
+	err = efadv_query_ah(efa_ah->ibv_ah, &efa_ah_attr, sizeof(efa_ah_attr));
+	if (err) {
+		errno = err;
+		EFA_WARN(FI_LOG_AV, "efadv_query_ah failed! err: %d\n", err);
+		goto err_destroy_ibv_ah;
 	}
 
-	return -1;
+	efa_ah->refcnt = 1;
+	efa_ah->ahn = efa_ah_attr.ahn;
+	memcpy(efa_ah->gid, gid, EFA_GID_LEN);
+	HASH_ADD(hh, av->ah_map, gid, EFA_GID_LEN, efa_ah);
+	return efa_ah;
+
+err_destroy_ibv_ah:
+	ibv_destroy_ah(efa_ah->ibv_ah);
+err_free_efa_ah:
+	free(efa_ah);
+	return NULL;
 }
 
-static int efa_peer_resize(struct rxr_ep *ep, size_t current_count,
-			   size_t new_count)
+/**
+ * @brief release an efa_ah object
+ *
+ * @param[in]	av	address vector
+ * @param[in]	ah	efa_ah object pointer
+ */
+static
+void efa_ah_release(struct efa_av *av, struct efa_ah *ah)
 {
-	void *p = realloc(&ep->peer[0], (new_count * sizeof(struct rxr_peer)));
+	int err;
+#if ENABLE_DEBUG
+	struct efa_ah *tmp;
 
-	if (p)
-		ep->peer = p;
-	else
-		return -FI_ENOMEM;
-#ifdef ENABLE_EFA_POISONING
-	rxr_poison_mem_region((uint32_t *)&ep->peer[current_count], (new_count -
-			      current_count) * sizeof(struct rxr_peer));
+	HASH_FIND(hh, av->ah_map, ah->gid, EFA_GID_LEN, tmp);
+	assert(tmp == ah);
 #endif
-	memset(&ep->peer[current_count], 0,
-		(new_count - current_count) * sizeof(struct rxr_peer));
-	return 0;
+	assert(ah->refcnt > 0);
+	ah->refcnt -= 1;
+	if (ah->refcnt == 0) {
+		HASH_DEL(av->ah_map, ah);
+		err = ibv_destroy_ah(ah->ibv_ah);
+		if (err)
+			EFA_WARN(FI_LOG_AV, "ibv_destroy_ah failed! err=%d\n", err);
+		free(ah);
+	}
 }
 
-static int efa_av_resize(struct efa_av *av, size_t new_av_count)
+static
+void efa_conn_release(struct efa_av *av, struct efa_conn *conn);
+
+/**
+ * @brief initialize the rdm related resources of an efa_conn object
+ *
+ * This function setup rdm_peer and shm address for an efa_conn.
+ * If shm transfer is enabled and the addr comes from local peer,
+ *  1. convert addr to format 'gid_qpn', which will be set as shm's ep name later.
+ *  2. insert gid_qpn into shm's av
+ *  3. store returned fi_addr from shm into the hash table
+ *
+ * @param[in]	av	address vector
+ * @param[in]	conn	efa_conn object
+ * @return	On success return 0, otherwise return a negative error code
+ */
+static
+int efa_conn_rdm_init(struct efa_av *av, struct efa_conn *conn)
 {
-	if (av->type == FI_AV_TABLE) {
-		void *p = realloc(av->conn_table,
-				  (new_av_count *
-				  sizeof(*av->conn_table)));
-
-		if (p)
-			av->conn_table = p;
-		else
+	int err, ret;
+	char smr_name[EFA_SHM_NAME_MAX];
+	struct rxr_ep *rxr_ep;
+	struct rdm_peer *peer;
+
+	assert(av->ep_type == FI_EP_RDM);
+	assert(conn->ep_addr);
+
+	/* currently multiple EP bind to same av is not supported */
+	assert(!dlist_empty(&av->util_av.ep_list));
+	rxr_ep = container_of(av->util_av.ep_list.next, struct rxr_ep, util_ep.av_entry);
+
+	peer = &conn->rdm_peer;
+	efa_rdm_peer_init(peer, rxr_ep, conn);
+
+	/* If peer is local, insert the address into shm provider's av */
+	if (rxr_ep->use_shm && efa_is_local_peer(av, conn->ep_addr)) {
+		if (av->shm_used >= rxr_env.shm_av_size) {
+			EFA_WARN(FI_LOG_AV,
+				 "Max number of shm AV entry (%d) has been reached.\n",
+				 rxr_env.shm_av_size);
 			return -FI_ENOMEM;
+		}
 
-#ifdef ENABLE_EFA_POISONING
-	rxr_poison_mem_region((uint32_t *)av->conn_table + av->count,
-			      (new_av_count - av->count) *
-			      sizeof(*av->conn_table));
-#endif
+		err = rxr_raw_addr_to_smr_name(conn->ep_addr, smr_name);
+		if (err != FI_SUCCESS) {
+			EFA_WARN(FI_LOG_AV,
+				 "rxr_ep_efa_addr_to_str() failed! err=%d\n", err);
+			return err;
+		}
+
+		ret = fi_av_insert(av->shm_rdm_av, smr_name, 1, &peer->shm_fiaddr, 0, NULL);
+		if (OFI_UNLIKELY(ret != 1)) {
+			EFA_WARN(FI_LOG_AV,
+				 "Failed to insert address to shm provider's av: %s\n",
+				 fi_strerror(-ret));
+			return ret;
+		}
+
+		EFA_INFO(FI_LOG_AV,
+			"Successfully inserted %s to shm provider's av. efa_fiaddr: %ld shm_fiaddr = %ld\n",
+			smr_name, conn->fi_addr, peer->shm_fiaddr);
+
+		assert(peer->shm_fiaddr < rxr_env.shm_av_size);
+		av->shm_used++;
+		av->shm_rdm_addr_map[peer->shm_fiaddr] = conn->fi_addr;
+		peer->is_local = 1;
+	}
+
+	return 0;
+}
+
+/**
+ * @brief release the rdm related resources of an efa_conn object
+ *
+ * this function release the shm av entry and rdm peer;
+ *
+ * @param[in]	av	address vector
+ * @param[in]	conn	efa_conn object
+ */
+static
+void efa_conn_rdm_deinit(struct efa_av *av, struct efa_conn *conn)
+{
+	int err;
+	struct rdm_peer *peer;
+	struct rxr_ep *ep;
+
+	assert(av->ep_type == FI_EP_RDM);
 
-		memset(av->conn_table + av->count, 0,
-		       (new_av_count - av->count) * sizeof(*av->conn_table));
+	peer = &conn->rdm_peer;
+	if (peer->is_local) {
+		err = fi_av_remove(av->shm_rdm_av, &peer->shm_fiaddr, 1, 0);
+		if (err) {
+			EFA_WARN(FI_LOG_AV, "remove address from shm av failed! err=%d\n", err);
+		} else {
+			av->shm_used--;
+			assert(peer->shm_fiaddr < rxr_env.shm_av_size);
+			av->shm_rdm_addr_map[peer->shm_fiaddr] = FI_ADDR_UNSPEC;
+		}
+	}
+
+	/*
+	 * We need peer->shm_fiaddr to remove shm address from shm av table,
+	 * so efa_rdm_peer_clear must be after removing shm av table.
+	 */
+	ep = dlist_empty(&av->util_av.ep_list) ? NULL : container_of(av->util_av.ep_list.next, struct rxr_ep, util_ep.av_entry);
+	efa_rdm_peer_clear(ep, peer);
+}
+
+/*
+ * @brief update av->connid_map when inserting an new address to AV
+ *
+ * @param[in,out]	av		efa AV
+ * @param[in]		raw_addr	raw address
+ * @param[in]		conn		efa_conn object
+ * @return		On success, return 0.
+ * 			Otherwise, return a negative libfabric error code
+ */
+static
+int efa_av_update_connid_map(struct efa_av *av, struct efa_ep_addr *raw_addr,
+			      struct efa_conn *conn)
+{
+	struct efa_connid_map *connid_map_entry = NULL;
+	struct efa_connid_map_key connid_map_key;
+
+	assert(av->ep_type == FI_EP_RDM);
+
+	memset(&connid_map_key, 0, sizeof(connid_map_key));
+	connid_map_key.ahn = conn->ah->ahn;
+	connid_map_key.qpn = raw_addr->qpn;
+
+	connid_map_entry = NULL;
+	HASH_FIND(hh, av->connid_map, &connid_map_key, sizeof(connid_map_key), connid_map_entry);
+	if (connid_map_entry) {
+		connid_map_entry->connid = raw_addr->qkey;
+		return 0;
 	}
 
-	av->count = new_av_count;
+	connid_map_entry = malloc(sizeof(*connid_map_entry));
+	if (!connid_map_entry) {
+		FI_WARN(&rxr_prov, FI_LOG_AV, "Cannot allocate memory for connid_map entry");
+		return -FI_ENOMEM;
+	}
 
+	memcpy(&connid_map_entry->key, &connid_map_key, sizeof(connid_map_key));
+	connid_map_entry->connid = raw_addr->qkey;
+	HASH_ADD(hh, av->connid_map, key,
+		 sizeof(connid_map_entry->key),
+		 connid_map_entry);
 	return 0;
 }
 
-/* Inserts a single AH to AV. */
-static int efa_av_insert_ah(struct efa_av *av, struct efa_ep_addr *addr,
-				fi_addr_t *fi_addr, uint64_t flags, void *context)
+/*
+ * @brief update av->reverse when inserting an new address to AV
+ *
+ * @param[in,out]	av		efa AV
+ * @param[in]		raw_addr	raw address
+ * @param[in]		conn		efa_conn object
+ * @return		On success, return 0.
+ * 			Otherwise, return a negative libfabric error code
+ */
+static
+int efa_av_update_reverse_av(struct efa_av *av, struct efa_ep_addr *raw_addr,
+			     struct efa_conn *conn)
 {
-	struct ibv_pd *ibv_pd = av->domain->ibv_pd;
-	struct ibv_ah_attr ah_attr = { 0 };
+	struct efa_reverse_av *reverse_av_entry = NULL;
+	struct efa_reverse_av_key reverse_av_key;
+
+	assert(av->ep_type == FI_EP_RDM || raw_addr->qkey == EFA_DGRAM_CONNID);
+
+	memset(&reverse_av_key, 0, sizeof(reverse_av_key));
+	reverse_av_key.ahn = conn->ah->ahn;
+	reverse_av_key.qpn = raw_addr->qpn;
+	reverse_av_key.connid = raw_addr->qkey;
+#if ENABLE_DEBUG
+	reverse_av_entry = NULL;
+	HASH_FIND(hh, av->reverse_av, &reverse_av_key, sizeof(reverse_av_key), reverse_av_entry);
+	assert(!reverse_av_entry);
+#endif
+	reverse_av_entry = malloc(sizeof(*reverse_av_entry));
+	if (!reverse_av_entry)
+		return -FI_ENOMEM;
 
-	char str[INET6_ADDRSTRLEN] = { 0 };
-	struct efadv_ah_attr attr = { 0 };
-	struct efa_reverse_av *reverse_av;
-	struct efa_ah_qpn key;
+	memcpy(&reverse_av_entry->key, &reverse_av_key, sizeof(reverse_av_key));
+	reverse_av_entry->conn = conn;
+	HASH_ADD(hh, av->reverse_av, key,
+		 sizeof(reverse_av_entry->key),
+		 reverse_av_entry);
+	return 0;
+}
+
+/**
+ * @brief allocate an efa_conn object
+ * caller of this function must obtain av->util_av.lock
+ *
+ * @param[in]	av		efa address vector
+ * @param[in]	raw_addr	raw efa address
+ * @param[in]	flags		flags application passed to fi_av_insert
+ * @param[in]	context		context application passed to fi_av_insert
+ * @return	on success, return a pointer to an efa_conn object
+ *		otherwise, return NULL. errno will be set to a positive error code.
+ */
+static
+struct efa_conn *efa_conn_alloc(struct efa_av *av, struct efa_ep_addr *raw_addr,
+				uint64_t flags, void *context)
+{
+	struct util_av_entry *util_av_entry = NULL;
+	struct efa_av_entry *efa_av_entry = NULL;
 	struct efa_conn *conn;
+	fi_addr_t util_av_fi_addr;
 	int err;
 
-	if (av->util_av.flags & FI_EVENT)
-		return -FI_ENOEQ;
-	if ((flags & FI_SYNC_ERR) && (!context || (flags & FI_EVENT)))
-		return -FI_EINVAL;
-	else if (flags & FI_SYNC_ERR)
+	if (flags & FI_SYNC_ERR)
 		memset(context, 0, sizeof(int));
 
-	memset(&ah_attr, 0, sizeof(struct ibv_ah_attr));
-	inet_ntop(AF_INET6, addr->raw, str, INET6_ADDRSTRLEN);
-	EFA_INFO(FI_LOG_AV, "Insert address: GID[%s] QP[%u] QKEY[%u]\n", str, addr->qpn, addr->qkey);
-	if (!efa_av_is_valid_address(addr)) {
+	if (!efa_av_is_valid_address(raw_addr)) {
 		EFA_WARN(FI_LOG_AV, "Failed to insert bad addr");
-		err = -FI_EADDRNOTAVAIL;
-		goto err_invalid;
+		errno = FI_EINVAL;
+		return NULL;
 	}
 
-	err = ofi_memalign((void **)&conn, EFA_MEM_ALIGNMENT, sizeof(*conn));
+	err = ofi_av_insert_addr(&av->util_av, raw_addr, &util_av_fi_addr);
 	if (err) {
-		err = -FI_ENOMEM;
-		goto err_invalid;
-	}
-
-	ah_attr.port_num = 1;
-	ah_attr.is_global = 1;
-	memcpy(ah_attr.grh.dgid.raw, addr->raw, sizeof(addr->raw));
-	conn->ah.ibv_ah = ibv_create_ah(ibv_pd, &ah_attr);
-	if (!conn->ah.ibv_ah) {
-		err = -FI_EINVAL;
-		goto err_free_conn;
-	}
-	memcpy((void *)&conn->ep_addr, addr, sizeof(*addr));
-
-	switch (av->type) {
-	case FI_AV_MAP:
-		*fi_addr = (uintptr_t)(void *)conn;
-
-		break;
-	case FI_AV_TABLE:
-		if (av->ep_type == FI_EP_DGRAM) {
-			av->next = efa_av_tbl_find_first_empty(av, av->next);
-			assert(av->next != -1);
-			*fi_addr = av->next;
-		}
-
-		av->conn_table[*fi_addr] = conn;
-		av->next++;
-		break;
-	default:
-		assert(0);
-		break;
+		EFA_WARN(FI_LOG_AV, "ofi_av_insert_addr failed! Error message: %s\n",
+			 fi_strerror(err));
+		return NULL;
 	}
 
-	err = -efadv_query_ah(conn->ah.ibv_ah, &attr, sizeof(attr));
-	if (err)
-		goto err_destroy_ah;
-
-	conn->ah.ahn = attr.ahn;
-	key.ahn = conn->ah.ahn;
-	key.qpn = addr->qpn;
-	/* This is correct since the same address should be mapped to the same ah. */
-	HASH_FIND(hh, av->reverse_av, &key, sizeof(key), reverse_av);
-	if (!reverse_av) {
-		reverse_av = malloc(sizeof(*reverse_av));
-		if (!reverse_av) {
-			err = -FI_ENOMEM;
-			goto err_destroy_ah;
+	util_av_entry = ofi_bufpool_get_ibuf(av->util_av.av_entry_pool,
+					     util_av_fi_addr);
+	efa_av_entry = (struct efa_av_entry *)util_av_entry->data;
+	assert(efa_is_same_addr(raw_addr, (struct efa_ep_addr *)efa_av_entry->ep_addr));
+
+	conn = &efa_av_entry->conn;
+	memset(conn, 0, sizeof(*conn));
+	conn->ep_addr = (struct efa_ep_addr *)efa_av_entry->ep_addr;
+	assert(av->type == FI_AV_MAP || av->type == FI_AV_TABLE);
+	conn->fi_addr = (av->type == FI_AV_MAP) ? (uintptr_t)(void *)conn : util_av_fi_addr;
+	conn->util_av_fi_addr = util_av_fi_addr;
+
+	conn->ah = efa_ah_alloc(av, raw_addr->raw);
+	if (!conn->ah)
+		goto err_release;
+
+	if (av->ep_type == FI_EP_RDM) {
+		err = efa_conn_rdm_init(av, conn);
+		if (err) {
+			errno = -err;
+			goto err_release;
 		}
+	}
 
-		memcpy(&reverse_av->key, &key, sizeof(key));
-		reverse_av->fi_addr = *fi_addr;
-		HASH_ADD(hh, av->reverse_av, key,
-			 sizeof(reverse_av->key), reverse_av);
+	err = efa_av_update_reverse_av(av, raw_addr, conn);
+	if (err) {
+		if (av->ep_type == FI_EP_RDM)
+			efa_conn_rdm_deinit(av, conn);
+		goto err_release;
 	}
 
-	EFA_INFO(FI_LOG_AV, "av successfully inserted conn[%p] fi_addr[%" PRIu64 "]\n",
-		 conn, *fi_addr);
+	if (av->ep_type == FI_EP_RDM) {
+		err = efa_av_update_connid_map(av, raw_addr, conn);
+		if (err) {
+			efa_conn_rdm_deinit(av, conn);
+			goto err_release;
+		}
+	}
 
 	av->used++;
-	return FI_SUCCESS;
-
-err_destroy_ah:
-	ibv_destroy_ah(conn->ah.ibv_ah);
-err_free_conn:
-	ofi_freealign(conn);
-err_invalid:
-	*fi_addr = FI_ADDR_NOTAVAIL;
-	return err;
+	return conn;
+
+err_release:
+	if (conn->ah)
+		efa_ah_release(av, conn->ah);
+
+	conn->ep_addr = NULL;
+	err = ofi_av_remove_addr(&av->util_av, util_av_fi_addr);
+	if (err)
+		EFA_WARN(FI_LOG_AV, "While processing previous failure, ofi_av_remove_addr failed! err=%d\n",
+			 err);
+
+	return NULL;
 }
 
-/*
- * Insert address translation in core av & in hash.
+/**
+ * @brief release an efa conn object
+ * Caller of this function must obtain av->util_av.lock
  *
- * If shm transfer is enabled and the addr comes from local peer,
- * 1. convert addr to format 'gid_qpn', which will be set as shm's ep name later.
- * 2. insert gid_qpn into shm's av
- * 3. store returned fi_addr from shm into the hash table
+ * @param[in]	av	address vector
+ * @param[in]	conn	efa_conn object pointer
  */
-int efa_av_insert_addr(struct efa_av *av, struct efa_ep_addr *addr,
-			   fi_addr_t *fi_addr, uint64_t flags,
-			   void *context)
+static
+void efa_conn_release(struct efa_av *av, struct efa_conn *conn)
 {
-	struct efa_av_entry *av_entry;
+	struct efa_reverse_av *reverse_av_entry;
 	struct util_av_entry *util_av_entry;
+	struct efa_av_entry *efa_av_entry;
+	struct efa_reverse_av_key key;
+	char gidstr[INET6_ADDRSTRLEN];
+
+	if (av->ep_type == FI_EP_RDM)
+		efa_conn_rdm_deinit(av, conn);
+
+	memset(&key, 0, sizeof(key));
+	key.ahn = conn->ah->ahn;
+	key.qpn = conn->ep_addr->qpn;
+	key.connid = conn->ep_addr->qkey;
+	HASH_FIND(hh, av->reverse_av, &key, sizeof(key), reverse_av_entry);
+	assert(reverse_av_entry);
+	HASH_DEL(av->reverse_av, reverse_av_entry);
+	free(reverse_av_entry);
+
+	efa_ah_release(av, conn->ah);
+
+	util_av_entry = ofi_bufpool_get_ibuf(av->util_av.av_entry_pool, conn->util_av_fi_addr);
+	assert(util_av_entry);
+	efa_av_entry = (struct efa_av_entry *)util_av_entry->data;
+
+	ofi_av_remove_addr(&av->util_av, conn->util_av_fi_addr);
+
+	inet_ntop(AF_INET6, conn->ep_addr->raw, gidstr, INET6_ADDRSTRLEN);
+	EFA_INFO(FI_LOG_AV, "efa_conn released! conn[%p] GID[%s] QP[%u]\n",
+		 conn, gidstr, conn->ep_addr->qpn);
+
+	conn->ep_addr = NULL;
+	memset(efa_av_entry->ep_addr, 0, EFA_EP_ADDR_LEN);
+
+	av->used--;
+}
+
+/**
+ * @brief insert one address into address vector (AV)
+ *
+ * @param[in]	av	address vector
+ * @param[in]	addr	raw address, in the format of gid:qpn:qkey
+ * @param[out]	fi_addr pointer the output fi address. This addres is used by fi_send
+ * @param[in]	flags	flags user passed to fi_av_insert.
+ * @param[in]	context	context user passed to fi_av_insert
+ * @return	0 on success, a negative error code on failure
+ */
+int efa_av_insert_one(struct efa_av *av, struct efa_ep_addr *addr,
+		      fi_addr_t *fi_addr, uint64_t flags, void *context)
+{
+	struct efa_conn *conn;
+	char raw_gid_str[INET6_ADDRSTRLEN];
+	fi_addr_t efa_fiaddr;
 	int ret = 0;
-	struct rxr_peer *peer;
-	struct rxr_ep *rxr_ep;
-	struct util_ep *util_ep;
-	struct dlist_entry *ep_list_entry;
-	fi_addr_t shm_fiaddr;
-	char smr_name[NAME_MAX];
 
-	fastlock_acquire(&av->util_av.lock);
-	ret = ofi_av_insert_addr(&av->util_av, addr, fi_addr);
+	if (av->ep_type == FI_EP_DGRAM)
+		addr->qkey = EFA_DGRAM_CONNID;
 
-	if (ret) {
-		EFA_WARN(FI_LOG_AV, "Error in inserting address: %s\n",
-			 fi_strerror(ret));
+	fastlock_acquire(&av->util_av.lock);
+	memset(raw_gid_str, 0, sizeof(raw_gid_str));
+	if (!inet_ntop(AF_INET6, addr->raw, raw_gid_str, INET6_ADDRSTRLEN)) {
+		EFA_WARN(FI_LOG_AV, "cannot convert address to string. errno: %d", errno);
+		ret = -FI_EINVAL;
+		*fi_addr = FI_ADDR_NOTAVAIL;
 		goto out;
 	}
-	util_av_entry = ofi_bufpool_get_ibuf(av->util_av.av_entry_pool,
-					     *fi_addr);
-	/*
-	 * If the entry already exists then calling ofi_av_insert_addr would
-	 * increase the use_cnt by 1. For a new entry use_cnt will be 1, whereas
-	 * for a duplicate entry, use_cnt will be more that 1.
-	 */
-	if (ofi_atomic_get32(&util_av_entry->use_cnt) > 1)
-		goto find_out;
-
-	av_entry = (struct efa_av_entry *)util_av_entry->data;
-	av_entry->rdm_addr = *fi_addr;
-	av_entry->local_mapping = 0;
-
-	if (av->used + 1 > av->count) {
-		ret = efa_av_resize(av, av->count * 2);
-		if (ret)
-			goto out;
-		dlist_foreach(&av->util_av.ep_list, ep_list_entry) {
-			util_ep = container_of(ep_list_entry, struct util_ep,
-					       av_entry);
-			rxr_ep = container_of(util_ep, struct rxr_ep, util_ep);
-			ret = efa_peer_resize(rxr_ep, av->used,
-					      av->count);
-			if (ret)
-				goto out;
-		}
-	}
+
+	EFA_INFO(FI_LOG_AV, "Inserting address GID[%s] QP[%u] QKEY[%u] to AV ....\n",
+		 raw_gid_str, addr->qpn, addr->qkey);
 
 	/*
-	 * Walk through all the EPs that bound to the AV,
-	 * update is_self flag corresponding peer structure
+	 * Check if this address already has been inserted, if so set *fi_addr to existing address,
+	 * and return 0 for success.
 	 */
-	dlist_foreach(&av->util_av.ep_list, ep_list_entry) {
-		util_ep = container_of(ep_list_entry, struct util_ep, av_entry);
-		rxr_ep = container_of(util_ep, struct rxr_ep, util_ep);
-		peer = rxr_ep_get_peer(rxr_ep, *fi_addr);
-		peer->is_self = efa_is_same_addr((struct efa_ep_addr *)rxr_ep->core_addr,
-						 addr);
+	efa_fiaddr = ofi_av_lookup_fi_addr_unsafe(&av->util_av, addr);
+	if (efa_fiaddr != FI_ADDR_NOTAVAIL) {
+		*fi_addr = efa_fiaddr;
+		EFA_INFO(FI_LOG_AV, "Found existing AV entry pointing to this address! fi_addr: %ld\n", *fi_addr);
+		ret = 0;
+		goto out;
 	}
 
-	/* If peer is local, insert the address into shm provider's av */
-	if (rxr_env.enable_shm_transfer && efa_is_local_peer(av, addr)) {
-		if (av->shm_used >= rxr_env.shm_av_size) {
-			ret = -FI_ENOMEM;
-			EFA_WARN(FI_LOG_AV,
-				 "Max number of shm AV entry %d has been reached.\n",
-				 rxr_env.shm_av_size);
-			goto err_free_av_entry;
-		}
-		ret = rxr_ep_efa_addr_to_str(addr, smr_name);
-		if (ret != FI_SUCCESS)
-			goto err_free_av_entry;
-
-		ret = fi_av_insert(av->shm_rdm_av, smr_name, 1, &shm_fiaddr,
-					flags, context);
-		if (OFI_UNLIKELY(ret != 1)) {
-			EFA_WARN(FI_LOG_AV,
-				 "Failed to insert address to shm provider's av: %s\n",
-				 fi_strerror(-ret));
-			goto err_free_av_entry;
-		} else {
-			ret = 0;
-		}
-		EFA_INFO(FI_LOG_AV,
-			"Insert %s to shm provider's av. addr = %" PRIu64
-			" rdm_fiaddr = %" PRIu64 " shm_rdm_fiaddr = %" PRIu64
-			"\n", smr_name, *(uint64_t *)addr, *fi_addr, shm_fiaddr);
-
-		assert(shm_fiaddr < rxr_env.shm_av_size);
-		av->shm_used++;
-		av_entry->local_mapping = 1;
-		av_entry->shm_rdm_addr = shm_fiaddr;
-		av->shm_rdm_addr_map[shm_fiaddr] = av_entry->rdm_addr;
-
-		/*
-		 * Walk through all the EPs that bound to the AV,
-		 * update is_local flag and shm fi_addr_t in corresponding peer structure
-		 */
-		dlist_foreach(&av->util_av.ep_list, ep_list_entry) {
-			util_ep = container_of(ep_list_entry, struct util_ep, av_entry);
-			rxr_ep = container_of(util_ep, struct rxr_ep, util_ep);
-			if (rxr_ep->use_shm) {
-				peer = rxr_ep_get_peer(rxr_ep, *fi_addr);
-				peer->shm_fiaddr = shm_fiaddr;
-				peer->is_local = 1;
-			}
-		}
+	conn = efa_conn_alloc(av, addr, flags, context);
+	if (!conn) {
+		*fi_addr = FI_ADDR_NOTAVAIL;
+		ret = -FI_EADDRNOTAVAIL;
+		goto out;
 	}
-	ret = efa_av_insert_ah(av, addr, fi_addr,
-			       flags, context);
-	if (ret) {
-		EFA_WARN(FI_LOG_AV, "Error in inserting address: %s\n",
-			 fi_strerror(ret));
-		goto err_free_av_entry;
-	}
-
-find_out:
-	EFA_INFO(FI_LOG_AV,
-			"addr = %" PRIu64 " rdm_fiaddr =  %" PRIu64 "\n",
-			*(uint64_t *)addr, *fi_addr);
-	goto out;
-err_free_av_entry:
-	ofi_ibuf_free(util_av_entry);
+
+	*fi_addr = conn->fi_addr;
+	EFA_INFO(FI_LOG_AV, "Successfully inserted address GID[%s] QP[%u] QKEY[%u] to AV. fi_addr: %ld\n",
+		 raw_gid_str, addr->qpn, addr->qkey, *fi_addr);
+	ret = 0;
 out:
 	fastlock_release(&av->util_av.lock);
 	return ret;
@@ -421,43 +740,34 @@ int efa_av_insert(struct fid_av *av_fid, const void *addr,
 	struct efa_ep_addr *addr_i;
 	fi_addr_t fi_addr_res;
 
+	if (av->util_av.flags & FI_EVENT)
+		return -FI_ENOEQ;
+
+	if ((flags & FI_SYNC_ERR) && (!context || (flags & FI_EVENT)))
+		return -FI_EINVAL;
+
 	/*
 	 * Providers are allowed to ignore FI_MORE.
 	 */
-
 	flags &= ~FI_MORE;
 	if (flags)
 		return -FI_ENOSYS;
 
-	if (av->ep_type == FI_EP_RDM) {
-		for (i = 0; i < count; i++) {
-			addr_i = (struct efa_ep_addr *) ((uint8_t *)addr + i * EFA_EP_ADDR_LEN);
-			ret = efa_av_insert_addr(av, addr_i, &fi_addr_res,
-					flags, context);
-			if (ret)
-				break;
-			if (fi_addr)
-				fi_addr[i] = fi_addr_res;
-			success_cnt++;
-		}
-	} else {
-		if (av->used + count > av->count) {
-			ret = efa_av_resize(av, av->used + count);
-			if (ret)
-				goto out;
-		}
-		for (i = 0; i < count; i++) {
-			addr_i = (struct efa_ep_addr *) ((uint8_t *)addr + i * EFA_EP_ADDR_LEN);
-			ret = efa_av_insert_ah(av, addr_i, &fi_addr_res,
-					     flags, context);
-			if (ret)
-				break;
-			if (fi_addr)
-				fi_addr[i] = fi_addr_res;
-			success_cnt++;
+	for (i = 0; i < count; i++) {
+		addr_i = (struct efa_ep_addr *) ((uint8_t *)addr + i * EFA_EP_ADDR_LEN);
+
+		ret = efa_av_insert_one(av, addr_i, &fi_addr_res, flags, context);
+		if (ret) {
+			EFA_WARN(FI_LOG_AV, "insert raw_addr to av failed! ret=%d\n",
+				 ret);
+			break;
 		}
+
+		if (fi_addr)
+			fi_addr[i] = fi_addr_res;
+		success_cnt++;
 	}
-out:
+
 	/* cancel remaining request and log to event queue */
 	for (; i < count ; i++) {
 		if (av->util_av.eq)
@@ -475,7 +785,6 @@ out:
 }
 
 static int efa_av_lookup(struct fid_av *av_fid, fi_addr_t fi_addr,
-
 			 void *addr, size_t *addrlen)
 {
 	struct efa_av *av = container_of(av_fid, struct efa_av, util_av.av_fid);
@@ -487,156 +796,78 @@ static int efa_av_lookup(struct fid_av *av_fid, fi_addr_t fi_addr,
 	if (fi_addr == FI_ADDR_NOTAVAIL)
 		return -FI_EINVAL;
 
-	if (av->type == FI_AV_MAP) {
-		conn = (struct efa_conn *)fi_addr;
-	} else { /* (av->type == FI_AV_TABLE) */
-		if (fi_addr >= av->count)
-			return -FI_EINVAL;
-
-		conn = av->conn_table[fi_addr];
-	}
+	conn = efa_av_addr_to_conn(av, fi_addr);
 	if (!conn)
 		return -FI_EINVAL;
 
-	memcpy(addr, (void *)&conn->ep_addr, MIN(sizeof(conn->ep_addr), *addrlen));
-	*addrlen = sizeof(conn->ep_addr);
+	memcpy(addr, (void *)conn->ep_addr, MIN(EFA_EP_ADDR_LEN, *addrlen));
+	if (*addrlen > EFA_EP_ADDR_LEN)
+		*addrlen = EFA_EP_ADDR_LEN;
 	return 0;
 }
 
-static int efa_av_remove_ah(struct fid_av *av_fid, fi_addr_t *fi_addr,
-			    size_t count, uint64_t flags)
-{
-	struct efa_av *av = container_of(av_fid, struct efa_av, util_av.av_fid);
-	struct efa_conn *conn = NULL;
-	struct efa_reverse_av *reverse_av;
-	struct efa_ah_qpn key;
-	char str[INET6_ADDRSTRLEN];
-	int ret = 0;
-
-	if (!fi_addr || (av->type != FI_AV_MAP && av->type != FI_AV_TABLE))
-		return -FI_EINVAL;
-
-	if (*fi_addr == FI_ADDR_NOTAVAIL)
-		return ret;
-
-	if (av->type == FI_AV_MAP) {
-		conn = (struct efa_conn *)fi_addr;
-	} else { /* (av->type == FI_AV_TABLE) */
-		conn = av->conn_table[*fi_addr];
-		av->conn_table[*fi_addr] = NULL;
-		av->next = MIN(av->next, *fi_addr);
-	}
-	if (!conn)
-		return ret;
-
-	key.ahn = conn->ah.ahn;
-	key.qpn = conn->ep_addr.qpn;
-	HASH_FIND(hh, av->reverse_av, &key, sizeof(key), reverse_av);
-	if (OFI_LIKELY(!!reverse_av)) {
-		HASH_DEL(av->reverse_av, reverse_av);
-		free(reverse_av);
-	}
-
-	ret = -ibv_destroy_ah(conn->ah.ibv_ah);
-	if (ret)
-		goto err_free_conn;
-
-	memset(str, 0, sizeof(str));
-	inet_ntop(AF_INET6, conn->ep_addr.raw, str, INET6_ADDRSTRLEN);
-	EFA_INFO(FI_LOG_AV, "av_remove conn[%p] with GID[%s] QP[%u]\n", conn,
-			str, conn->ep_addr.qpn);
-	av->used--;
-
-err_free_conn:
-	ofi_freealign(conn);
-	return ret;
-}
-
+/*
+ * @brief remove a set of addresses from AV and release its resources
+ *
+ * This function implements fi_av_remove() for EFA provider.
+ *
+ * Note that even after an address was removed from AV, it is still
+ * possible to get TX and RX completion for the address. Per libfabric
+ * standard, these completions should be ignored.
+ *
+ * To help TX completion handler to identify such a TX completion,
+ * when removing an address, all its outstanding TX packet's addr
+ * was set to FI_ADDR_NOTAVAIL. The TX completion handler will
+ * ignore TX packet whose address is FI_ADDR_NOTAVAIL.
+ *
+ * Meanwhile, lower provider  will set a packet's address to
+ * FI_ADDR_NOTAVAIL from it is from a removed address. RX completion
+ * handler will ignore such packets.
+ *
+ * @param[in]	av_fid	fid of AV (address vector)
+ * @param[in]	fi_addr pointer to an array of libfabric addresses
+ * @param[in]	counter	number of libfabric addresses in the array
+ * @param[in]	flags	flags
+ * @return	0 if all addresses have been removed successfully,
+ * 		negative libfabric error code if error was encoutnered.
+ */
 static int efa_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
 			 size_t count, uint64_t flags)
 {
-	int ret = 0;
+	int err = 0;
 	size_t i;
 	struct efa_av *av;
-	struct util_av_entry *util_av_entry;
-	struct efa_av_entry *av_entry;
-	struct rxr_peer *peer;
-	struct dlist_entry *ep_list_entry;
+	struct efa_conn *conn;
+
+	if (!fi_addr)
+		return -FI_EINVAL;
 
 	av = container_of(av_fid, struct efa_av, util_av.av_fid);
-	if (av->ep_type == FI_EP_DGRAM) {
-		for (i = 0; i < count; i++) {
-			ret = efa_av_remove_ah(&av->util_av.av_fid, &fi_addr[i],
-					       1, flags);
-			if (ret)
-				goto out;
-		}
-		goto out;
-	}
+	if (av->type != FI_AV_MAP && av->type != FI_AV_TABLE)
+		return -FI_EINVAL;
+
 	fastlock_acquire(&av->util_av.lock);
 	for (i = 0; i < count; i++) {
-		if (fi_addr[i] == FI_ADDR_NOTAVAIL ||
-		    fi_addr[i] > av->count) {
-			ret = -FI_ENOENT;
-			goto release_lock;
-		}
-		util_av_entry = ofi_bufpool_get_ibuf(
-						av->util_av.av_entry_pool,
-						fi_addr[i]);
-		if (!util_av_entry) {
-			ret = -FI_ENOENT;
-			goto release_lock;
-		}
-		/*
-		 * If use_cnt is greater than 1, then just decrement
-		 * the count by 1, without removing the entry.
-		 */
-		if (ofi_atomic_get32(&util_av_entry->use_cnt) > 1) {
-			ret = ofi_av_remove_addr(&av->util_av, fi_addr[i]);
-			goto release_lock;
+		conn = efa_av_addr_to_conn(av, fi_addr[i]);
+		if (!conn) {
+			err = -FI_EINVAL;
+			break;
 		}
-		av_entry = (struct efa_av_entry *)util_av_entry->data;
 
-		/* Check if the peer is in use if it is then return */
-		dlist_foreach(&av->util_av.ep_list, ep_list_entry) {
-			peer = efa_ep_get_peer(ep_list_entry, fi_addr[i]);
-			ret = efa_peer_in_use(peer);
-			if (ret)
-				goto release_lock;
-		}
-
-		/* Only if the peer is not in use reset the peer */
-		dlist_foreach(&av->util_av.ep_list, ep_list_entry) {
-			peer = efa_ep_get_peer(ep_list_entry, fi_addr[i]);
-			if (peer->rx_init)
-				efa_peer_reset(peer);
-		}
-		ret = efa_av_remove_ah(&av->util_av.av_fid, &fi_addr[i], 1,
-				       flags);
-		if (ret)
-			goto release_lock;
-		/* remove an address from shm provider's av */
-		if (rxr_env.enable_shm_transfer && av_entry->local_mapping) {
-			ret = fi_av_remove(av->shm_rdm_av, &av_entry->shm_rdm_addr, 1, flags);
-			if (ret)
-				goto err_free_av_entry;
+		efa_conn_release(av, conn);
+	}
 
-			av->shm_used--;
-			assert(av_entry->shm_rdm_addr < rxr_env.shm_av_size);
-			av->shm_rdm_addr_map[av_entry->shm_rdm_addr] = FI_ADDR_UNSPEC;
+	if (i < count) {
+		/* something went wrong, so err cannot be zero */
+		assert(err);
+		if (av->util_av.eq) {
+			for (; i < count; ++i)
+				ofi_av_write_event(&av->util_av, i, FI_ECANCELED, NULL);
 		}
-		ret = ofi_av_remove_addr(&av->util_av, *fi_addr);
-		if (ret)
-			goto err_free_av_entry;
 	}
+
 	fastlock_release(&av->util_av.lock);
-	goto out;
-err_free_av_entry:
-	ofi_ibuf_free(util_av_entry);
-release_lock:
-	fastlock_release(&av->util_av.lock);
-out:
-	return ret;
+	return err;
 }
 
 static const char *efa_av_straddr(struct fid_av *av_fid, const void *addr,
@@ -655,28 +886,54 @@ static struct fi_ops_av efa_av_ops = {
 	.straddr = efa_av_straddr
 };
 
+static void efa_av_close_reverse_av(struct efa_av *av)
+{
+	struct efa_reverse_av *reverse_av_entry, *tmp;
+
+	fastlock_acquire(&av->util_av.lock);
+
+	HASH_ITER(hh, av->reverse_av, reverse_av_entry, tmp) {
+		efa_conn_release(av, reverse_av_entry->conn);
+	}
+
+	fastlock_release(&av->util_av.lock);
+}
+
+static void efa_av_close_connid_map(struct efa_av *av)
+{
+	struct efa_connid_map *connid_map_entry, *tmp;
+
+	fastlock_acquire(&av->util_av.lock);
+
+	HASH_ITER(hh, av->connid_map, connid_map_entry, tmp) {
+		free(connid_map_entry);
+	}
+
+	fastlock_release(&av->util_av.lock);
+}
+
 static int efa_av_close(struct fid *fid)
 {
 	struct efa_av *av;
 	int ret = 0;
 	int err = 0;
-	int i;
 
 	av = container_of(fid, struct efa_av, util_av.av_fid.fid);
-	for (i = 0; i < av->count; i++) {
-		fi_addr_t addr = i;
 
-		ret = efa_av_remove_ah(&av->util_av.av_fid, &addr, 1, 0);
-		if (ret) {
-			err = ret;
-			EFA_WARN(FI_LOG_AV, "Failed to remove ah: %s\n",
-				fi_strerror(ret));
-		}
+	efa_av_close_connid_map(av);
+
+	efa_av_close_reverse_av(av);
+
+	ret = ofi_av_close(&av->util_av);
+	if (ret) {
+		err = ret;
+		EFA_WARN(FI_LOG_AV, "Failed to close av: %s\n",
+			fi_strerror(ret));
 	}
-	free(av->conn_table);
+
 	if (av->ep_type == FI_EP_RDM) {
-		if (rxr_env.enable_shm_transfer && av->shm_rdm_av &&
-		    &av->shm_rdm_av->fid) {
+		if (av->shm_rdm_av) {
+			assert(rxr_env.enable_shm_transfer);
 			ret = fi_close(&av->shm_rdm_av->fid);
 			if (ret) {
 				err = ret;
@@ -684,12 +941,6 @@ static int efa_av_close(struct fid *fid)
 					fi_strerror(ret));
 			}
 		}
-		ret = ofi_av_close(&av->util_av);
-		if (ret) {
-			err = ret;
-			EFA_WARN(FI_LOG_AV, "Failed to close av: %s\n",
-				fi_strerror(ret));
-		}
 	}
 	free(av);
 	return err;
@@ -708,16 +959,40 @@ static struct fi_ops efa_av_fi_ops = {
 	.ops_open = fi_no_ops_open,
 };
 
+/**
+ * @brief initialize the util_av field in efa_av
+ *
+ * @param[in]	util_domain	util_domain which is part of efa_domain_base
+ * @param[in]	attr		AV attr application passed to fi_av_open
+ * @param[out]	util_av		util_av field in efa_av
+ * @param[in]	context		contexted application passed to fi_av_open
+ * @return	On success, return 0.
+ *		On failure, return a negative libfabric error code.
+ */
+int efa_av_init_util_av(struct efa_domain *efa_domain,
+			struct fi_av_attr *attr,
+			struct util_av *util_av,
+			void *context)
+{
+	struct util_av_attr util_attr;
+	size_t universe_size;
+
+	if (fi_param_get_size_t(NULL, "universe_size",
+				&universe_size) == FI_SUCCESS)
+		attr->count = MAX(attr->count, universe_size);
+
+	util_attr.addrlen = EFA_EP_ADDR_LEN;
+	util_attr.context_len = sizeof(struct efa_av_entry) - EFA_EP_ADDR_LEN;
+	util_attr.flags = 0;
+	return ofi_av_init(&efa_domain->util_domain, attr, &util_attr,
+			   util_av, context);
+}
+
 int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 		struct fid_av **av_fid, void *context)
 {
 	struct efa_domain *efa_domain;
-	struct util_domain *util_domain;
-	struct rxr_domain *rxr_domain;
-	struct efa_domain_base *efa_domain_base;
 	struct efa_av *av;
-	struct util_av_attr util_attr;
-	size_t universe_size;
 	struct fi_av_attr av_attr;
 	int i, ret, retv;
 
@@ -744,35 +1019,19 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 	if (!av)
 		return -FI_ENOMEM;
 
-	util_domain = container_of(domain_fid, struct util_domain,
-				   domain_fid);
-	efa_domain_base = container_of(util_domain, struct efa_domain_base,
-				       util_domain.domain_fid);
 	attr->type = FI_AV_TABLE;
-	/*
-	 * An rxr_domain fid was passed to the user if this is an RDM
-	 * endpoint, otherwise it is an efa_domain fid.  This will be
-	 * removed once the rxr and efa domain structures are combined.
-	 */
-	if (efa_domain_base->type == EFA_DOMAIN_RDM) {
-		rxr_domain = (struct rxr_domain *)efa_domain_base;
-		efa_domain = container_of(rxr_domain->rdm_domain, struct efa_domain,
-						util_domain.domain_fid);
+
+	efa_domain = efa_domain_from_fid(domain_fid);
+
+	ret = efa_av_init_util_av(efa_domain, attr, &av->util_av, context);
+	if (ret)
+		goto err;
+
+	if (efa_domain_get_type(domain_fid) == EFA_DOMAIN_RDM) {
 		av->ep_type = FI_EP_RDM;
 
-		if (fi_param_get_size_t(NULL, "universe_size",
-					&universe_size) == FI_SUCCESS)
-			attr->count = MAX(attr->count, universe_size);
-
-		util_attr.addrlen = EFA_EP_ADDR_LEN;
-		util_attr.context_len = sizeof(struct efa_av_entry) - EFA_EP_ADDR_LEN;
-		util_attr.flags = 0;
-		ret = ofi_av_init(&efa_domain->util_domain, attr, &util_attr,
-					&av->util_av, context);
-		if (ret)
-			goto err;
 		av_attr = *attr;
-		if (rxr_env.enable_shm_transfer) {
+		if (efa_domain->fab && efa_domain->fab->shm_fabric) {
 			/*
 			 * shm av supports maximum 256 entries
 			 * Reset the count to 128 to reduce memory footprint and satisfy
@@ -796,7 +1055,6 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 				av->shm_rdm_addr_map[i] = FI_ADDR_UNSPEC;
 		}
 	} else {
-		efa_domain = (struct efa_domain *)efa_domain_base;
 		av->ep_type = FI_EP_DGRAM;
 	}
 
@@ -806,25 +1064,7 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 	av->domain = efa_domain;
 	av->type = attr->type;
 	av->used = 0;
-	av->next = 0;
 	av->shm_used = 0;
-	av->count = attr->count;
-
-	if (av->type == FI_AV_TABLE && av->count > 0) {
-		av->conn_table = calloc(av->count, sizeof(*av->conn_table));
-		if (!av->conn_table) {
-			ret = -FI_ENOMEM;
-			if (av->ep_type == FI_EP_DGRAM)
-				goto err_close_util_av;
-			else
-				goto err_close_shm_av;
-		}
-	}
-
-	if (av->type == FI_AV_MAP)
-		av->addr_to_conn = efa_av_map_addr_to_conn;
-	else /* if (av->type == FI_AV_TABLE) */
-		av->addr_to_conn = efa_av_tbl_idx_to_conn;
 
 	*av_fid = &av->util_av.av_fid;
 	(*av_fid)->fid.fclass = FI_CLASS_AV;
@@ -834,13 +1074,6 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 
 	return 0;
 
-err_close_shm_av:
-	if (rxr_env.enable_shm_transfer) {
-		retv = fi_close(&av->shm_rdm_av->fid);
-		if (retv)
-			EFA_WARN(FI_LOG_AV, "Unable to close shm av: %s\n",
-				fi_strerror(ret));
-	}
 err_close_util_av:
 	retv = ofi_av_close(&av->util_av);
 	if (retv)
diff --git a/prov/efa/src/efa_cq.c b/prov/efa/src/efa_cq.c
index 2b8fe67..06b99e4 100644
--- a/prov/efa/src/efa_cq.c
+++ b/prov/efa/src/efa_cq.c
@@ -171,9 +171,10 @@ ssize_t efa_cq_readfrom(struct fid_cq *cq_fid, void *buf, size_t count,
 			av = cq->domain->qp_table[wc.ibv_wc.qp_num &
 			     cq->domain->qp_table_sz_m1]->ep->av;
 
-			src_addr[i] = efa_ahn_qpn_to_addr(av,
-							  wc.ibv_wc.slid,
-							  wc.ibv_wc.src_qp);
+			src_addr[i] = efa_av_reverse_lookup(av,
+							    wc.ibv_wc.slid,
+							    wc.ibv_wc.src_qp,
+							    EFA_DGRAM_CONNID);
 		}
 		cq->read_entry(&wc, i, buf);
 	}
diff --git a/prov/efa/src/efa_domain.c b/prov/efa/src/efa_domain.c
index 2250411..ce18894 100644
--- a/prov/efa/src/efa_domain.c
+++ b/prov/efa/src/efa_domain.c
@@ -40,6 +40,8 @@
 fastlock_t pd_list_lock;
 struct efa_pd *pd_list = NULL;
 
+enum efa_fork_support_status efa_fork_status = EFA_FORK_SUPPORT_OFF;
+
 static int efa_domain_close(fid_t fid)
 {
 	struct efa_domain *domain;
@@ -139,23 +141,49 @@ static int efa_open_device_by_name(struct efa_domain *domain, const char *name)
 	return ret;
 }
 
-/*
- * Register a temporary buffer and call ibv_fork_init() to determine if fork
- * support is enabled.
+/* @brief Check if rdma-core fork support is enabled and prevent fork
+ * support from being enabled later.
+ *
+ * Register a temporary buffer and call ibv_fork_init() to determine
+ * if fork support is enabled. Registering a buffer prevents future
+ * calls to ibv_fork_init() from completing successfully.
  *
  * This relies on internal behavior in rdma-core and is a temporary workaround.
+ *
+ * @param domain_fid domain fid so we can register memory
+ * @return 1 if fork support is enabled, 0 otherwise
  */
 static int efa_check_fork_enabled(struct fid_domain *domain_fid)
 {
 	struct fid_mr *mr;
 	char *buf;
 	int ret;
+	long page_size;
 
-	buf = malloc(ofi_get_page_size());
+	/* If ibv_is_fork_initialized is availble, check if the function
+	 * can exit early.
+	 */
+#if HAVE_IBV_IS_FORK_INITIALIZED == 1
+	enum ibv_fork_status fork_status = ibv_is_fork_initialized();
+
+	/* If fork support is enabled or unneeded, return. */
+	if (fork_status != IBV_FORK_DISABLED)
+		return fork_status == IBV_FORK_ENABLED;
+
+#endif /* HAVE_IBV_IS_FORK_INITIALIZED */
+
+	page_size = ofi_get_page_size();
+	if (page_size <= 0) {
+		EFA_WARN(FI_LOG_DOMAIN, "Unable to determine page size %ld\n",
+			 page_size);
+		return -FI_EINVAL;
+	}
+
+	buf = malloc(page_size);
 	if (!buf)
 		return -FI_ENOMEM;
 
-	ret = fi_mr_reg(domain_fid, buf, ofi_get_page_size(),
+	ret = fi_mr_reg(domain_fid, buf, page_size,
 			FI_SEND, 0, 0, 0, &mr, NULL);
 	if (ret) {
 		free(buf);
@@ -202,18 +230,142 @@ static struct fi_ops_domain efa_domain_ops = {
 	.query_collective = fi_no_query_collective,
 };
 
+/* @brief Fork handler that is installed when EFA is loaded
+ *
+ * We register this fork handler so that users do not inadvertently trip over
+ * memory corruption when fork is called. Calling fork() without enabling fork
+ * support in rdma-core can cause corruption, even if the registered pages are
+ * not used in the child process.
+ *
+ * It is critical that this fork handler is only installed once an EFA device
+ * is present and selected. We don't want this to trigger when Libfabric is not
+ * running on an EC2 instance.
+ */
+static
+void efa_atfork_callback()
+{
+	static int visited = 0;
+
+	if (visited)
+		return;
+	visited = 1;
+
+	fprintf(stderr,
+		"A process has executed an operation involving a call\n"
+		"to the fork() system call to create a child process.\n"
+		"\n"
+		"As a result, the Libfabric EFA provider is operating in\n"
+		"a condition that could result in memory corruption or\n"
+		"other system errors.\n"
+		"\n"
+		"For the Libfabric EFA provider to work safely when fork()\n"
+		"is called please do one of the following:\n"
+		"1) Set the environment variable:\n"
+		"          FI_EFA_FORK_SAFE=1\n"
+		"and verify you are using rdma-core v31.1 or later.\n"
+		"\n"
+		"OR\n"
+		"2) Use Linux Kernel 5.13+ with rdma-core v35.0+\n"
+		"\n"
+		"Please note that enabling fork support may cause a\n"
+		"small performance impact.\n"
+		"\n"
+		"You may want to check with your application vendor to see\n"
+		"if an application-level alternative (of not using fork)\n"
+		"exists.\n"
+		"\n"
+		"Your job will now abort.\n");
+	abort();
+}
+
+/* @brief Setup the MR cache.
+ *
+ * This function enables the MR cache using the util MR cache code. Note that
+ * if the call to ofi_mr_cache_init fails, we continue but disable the cache.
+ *
+ * @param efa_domain The EFA domain where cache ops should be set
+ * @param info Validated info struct selected by the user
+ * @return 0 on success, fi_errno on failure.
+ */
+static int efa_mr_cache_init(struct efa_domain *domain, struct fi_info *info)
+{
+	struct ofi_mem_monitor *memory_monitors[OFI_HMEM_MAX] = {
+		[FI_HMEM_SYSTEM] = uffd_monitor,
+		[FI_HMEM_CUDA] = cuda_monitor,
+	};
+	int ret;
+
+	/* If FI_MR_CACHE_MONITOR env is set, this check will override our
+	 * default monitor with the user specified monitor which is stored
+	 * as default_monitor
+	 */
+	if (cache_params.monitor) {
+		memory_monitors[FI_HMEM_SYSTEM] = default_monitor;
+	}
+
+	domain->cache = (struct ofi_mr_cache *)calloc(1, sizeof(struct ofi_mr_cache));
+	if (!domain->cache)
+		return -FI_ENOMEM;
+
+	if (!efa_mr_max_cached_count)
+		efa_mr_max_cached_count = info->domain_attr->mr_cnt *
+					  EFA_MR_CACHE_LIMIT_MULT;
+	if (!efa_mr_max_cached_size)
+		efa_mr_max_cached_size = domain->ctx->max_mr_size *
+					 EFA_MR_CACHE_LIMIT_MULT;
+	/*
+	 * XXX: we're modifying a global in the util mr cache? do we need an
+	 * API here instead?
+	 */
+	cache_params.max_cnt = efa_mr_max_cached_count;
+	cache_params.max_size = efa_mr_max_cached_size;
+	domain->cache->entry_data_size = sizeof(struct efa_mr);
+	domain->cache->add_region = efa_mr_cache_entry_reg;
+	domain->cache->delete_region = efa_mr_cache_entry_dereg;
+	ret = ofi_mr_cache_init(&domain->util_domain, memory_monitors,
+				domain->cache);
+	if (!ret) {
+		domain->util_domain.domain_fid.mr = &efa_domain_mr_cache_ops;
+		EFA_INFO(FI_LOG_DOMAIN, "EFA MR cache enabled, max_cnt: %zu max_size: %zu\n",
+			 cache_params.max_cnt, cache_params.max_size);
+	} else {
+		EFA_WARN(FI_LOG_DOMAIN, "EFA MR cache init failed: %s\n",
+		         fi_strerror(ret));
+		free(domain->cache);
+		domain->cache = NULL;
+	}
+
+	return 0;
+}
+
+/* @brief Allocate a domain, open the device, and set it up based on the hints.
+ *
+ * This function creates a domain and uses the info struct to configure the
+ * domain based on what capabilities are set. Fork support is checked here and
+ * the MR cache is also set up here.
+ *
+ * Note the trickery with rxr_domain where detect whether this endpoint is RDM
+ * or DGRAM to set some state in rxr_domain. We can do this as the type field
+ * is at the beginning of efa_domain and rxr_domain, and we know efa_domain
+ * stored within rxr_domain. This will be removed when rxr_domain_open and
+ * efa_domain_open are combined.
+ *
+ * @param fabric_fid fabric that the domain should be tied to
+ * @param info info struct that was validated and returned by fi_getinfo
+ * @param domain_fid pointer where newly domain fid should be stored
+ * @param context void pointer stored with the domain fid
+ * @return 0 on success, fi_errno on error
+ */
 int efa_domain_open(struct fid_fabric *fabric_fid, struct fi_info *info,
 		    struct fid_domain **domain_fid, void *context)
 {
+	static int fork_handler_installed = 0;
 	struct efa_domain *domain;
 	struct efa_fabric *fabric;
 	const struct fi_info *fi;
 	size_t qp_table_size;
 	bool app_mr_local;
-	int ret;
-	struct ofi_mem_monitor *memory_monitors[OFI_HMEM_MAX] = {
-		[FI_HMEM_SYSTEM] = uffd_monitor,
-	};
+	int ret, err;
 
 	fi = efa_get_efa_info(info->domain_attr->name);
 	if (!fi)
@@ -287,59 +439,70 @@ int efa_domain_open(struct fid_fabric *fabric_fid, struct fi_info *info,
 	domain->cache = NULL;
 
 	/*
-	 * Check whether fork support is enabled when app does not request
-	 * FI_MR_LOCAL even if the cache is disabled.
+	 * Call ibv_fork_init if the user asked for fork support.
+	 */
+	if (efa_fork_status == EFA_FORK_SUPPORT_ON) {
+		ret = -ibv_fork_init();
+		if (ret) {
+			EFA_WARN(FI_LOG_DOMAIN,
+			         "Fork support requested but ibv_fork_init failed: %s\n",
+			         strerror(-ret));
+			goto err_free_info;
+		}
+	}
+
+	/*
+	 * Run check to see if fork support was enabled by another library. If
+	 * one of the environment variables was set to enable fork support,
+	 * this variable was set to ON during provider init.  Huge pages for
+	 * bounce buffers will not be used if fork support is on.
 	 */
-	if (!app_mr_local && efa_check_fork_enabled(*domain_fid)) {
-		fprintf(stderr,
-		         "\nlibibverbs fork support is not supported by the EFA Libfabric\n"
-			 "provider when memory registrations are handled by the provider.\n"
-			 "\nFork support may currently be enabled via the RDMAV_FORK_SAFE\n"
-			 "or IBV_FORK_SAFE environment variable or another library in your\n"
-			 "application may be calling ibv_fork_init().\n"
-			 "\nPlease refer to https://github.com/ofiwg/libfabric/issues/6332\n"
-			 "for more information. Your job will now abort.\n");
-		abort();
+	if (efa_fork_status == EFA_FORK_SUPPORT_OFF &&
+	    efa_check_fork_enabled(*domain_fid))
+		efa_fork_status = EFA_FORK_SUPPORT_ON;
+
+	if (efa_fork_status == EFA_FORK_SUPPORT_ON &&
+	    getenv("RDMAV_HUGEPAGES_SAFE")) {
+		EFA_WARN(FI_LOG_DOMAIN,
+			 "Using libibverbs fork support and huge pages is not supported by the EFA provider.\n");
+		ret = -FI_EINVAL;
+		goto err_free_info;
 	}
 
 	/*
+	 * It'd be better to install this during provider init (since that's
+	 * only invoked once) but we need to do a memory registration for the
+	 * fork check above. This can move to the provider init once that check
+	 * is gone.
+	 */
+	if (!fork_handler_installed && efa_fork_status == EFA_FORK_SUPPORT_OFF) {
+		ret = pthread_atfork(efa_atfork_callback, NULL, NULL);
+		if (ret) {
+			EFA_WARN(FI_LOG_DOMAIN,
+				 "Unable to register atfork callback: %s\n",
+				 strerror(-ret));
+			goto err_free_info;
+		}
+		fork_handler_installed = 1;
+	}
+	/*
 	 * If FI_MR_LOCAL is set, we do not want to use the MR cache.
 	 */
 	if (!app_mr_local && efa_mr_cache_enable) {
-		domain->cache = (struct ofi_mr_cache *)calloc(1, sizeof(struct ofi_mr_cache));
-		if (!domain->cache) {
-			ret = -FI_ENOMEM;
+		ret = efa_mr_cache_init(domain, info);
+		if (ret)
 			goto err_free_info;
-		}
-
-		if (!efa_mr_max_cached_count)
-			efa_mr_max_cached_count = info->domain_attr->mr_cnt *
-			                          EFA_MR_CACHE_LIMIT_MULT;
-		if (!efa_mr_max_cached_size)
-			efa_mr_max_cached_size = domain->ctx->max_mr_size *
-			                         EFA_MR_CACHE_LIMIT_MULT;
-		cache_params.max_cnt = efa_mr_max_cached_count;
-		cache_params.max_size = efa_mr_max_cached_size;
-		domain->cache->entry_data_size = sizeof(struct efa_mr);
-		domain->cache->add_region = efa_mr_cache_entry_reg;
-		domain->cache->delete_region = efa_mr_cache_entry_dereg;
-		ret = ofi_mr_cache_init(&domain->util_domain, memory_monitors,
-					domain->cache);
-		if (!ret) {
-			domain->util_domain.domain_fid.mr = &efa_domain_mr_cache_ops;
-			EFA_INFO(FI_LOG_DOMAIN, "EFA MR cache enabled, max_cnt: %zu max_size: %zu\n",
-			         cache_params.max_cnt, cache_params.max_size);
-		} else {
-			free(domain->cache);
-			domain->cache = NULL;
-		}
 	}
 
 	return 0;
 err_free_info:
 	fi_freeinfo(domain->info);
 err_close_domain:
-	ofi_domain_close(&domain->util_domain);
+	err = ofi_domain_close(&domain->util_domain);
+	if (err) {
+		EFA_WARN(FI_LOG_DOMAIN,
+			   "ofi_domain_close fails: %d", err);
+	}
 err_free_qp_table:
 	free(domain->qp_table);
 err_free_domain:
diff --git a/prov/efa/src/efa_ep.c b/prov/efa/src/efa_ep.c
index 76ae322..865a145 100644
--- a/prov/efa/src/efa_ep.c
+++ b/prov/efa/src/efa_ep.c
@@ -38,7 +38,7 @@
 #include <infiniband/efadv.h>
 #define EFA_CQ_PROGRESS_ENTRIES 500
 
-static int efa_generate_qkey()
+static int efa_generate_rdm_connid()
 {
 	struct timeval tv;
 	struct timezone tz;
@@ -80,8 +80,8 @@ static int efa_ep_destroy_qp(struct efa_qp *qp)
 	return err;
 }
 
-static int efa_ep_modify_qp_state(struct efa_qp *qp, enum ibv_qp_state qp_state,
-				  int attr_mask)
+static int efa_ep_modify_qp_state(struct efa_ep *ep, struct efa_qp *qp,
+				  enum ibv_qp_state qp_state, int attr_mask)
 {
 	struct ibv_qp_attr attr = {};
 
@@ -93,14 +93,8 @@ static int efa_ep_modify_qp_state(struct efa_qp *qp, enum ibv_qp_state qp_state,
 	if (attr_mask & IBV_QP_QKEY)
 		attr.qkey = qp->qkey;
 
-	/*
-	 * You can set how many times the firmware retries here.
-	 * Valid values are from 0(included) to 7(included).
-	 * 0 stands for no firmware level retries.
-	 * 7 means firmware retries infinitely.
-	 */
 	if (attr_mask & IBV_QP_RNR_RETRY)
-		attr.rnr_retry = 7;
+		attr.rnr_retry = ep->rnr_retry;
 
 	return -ibv_modify_qp(qp->ibv_qp, &attr, attr_mask);
 
@@ -110,22 +104,22 @@ static int efa_ep_modify_qp_rst2rts(struct efa_ep *ep, struct efa_qp *qp)
 {
 	int err;
 
-	err = efa_ep_modify_qp_state(qp, IBV_QPS_INIT,
+	err = efa_ep_modify_qp_state(ep, qp, IBV_QPS_INIT,
 				     IBV_QP_STATE | IBV_QP_PKEY_INDEX |
 				     IBV_QP_PORT | IBV_QP_QKEY);
 	if (err)
 		return err;
 
-	err = efa_ep_modify_qp_state(qp, IBV_QPS_RTR, IBV_QP_STATE);
+	err = efa_ep_modify_qp_state(ep, qp, IBV_QPS_RTR, IBV_QP_STATE);
 	if (err)
 		return err;
 
 	if (ep->util_ep.type != FI_EP_DGRAM &&
 	    efa_ep_support_rnr_retry_modify(&ep->util_ep.ep_fid))
-		return efa_ep_modify_qp_state(qp, IBV_QPS_RTS,
+		return efa_ep_modify_qp_state(ep, qp, IBV_QPS_RTS,
 			IBV_QP_STATE | IBV_QP_SQ_PSN | IBV_QP_RNR_RETRY);
 
-	return efa_ep_modify_qp_state(qp, IBV_QPS_RTS,
+	return efa_ep_modify_qp_state(ep, qp, IBV_QPS_RTS,
 				      IBV_QP_STATE | IBV_QP_SQ_PSN);
 }
 
@@ -159,7 +153,7 @@ static int efa_ep_create_qp_ex(struct efa_ep *ep,
 	}
 
 	qp->ibv_qp_ex = ibv_qp_to_qp_ex(qp->ibv_qp);
-	qp->qkey = efa_generate_qkey();
+	qp->qkey = (init_attr_ex->qp_type == IBV_QPT_UD) ? EFA_DGRAM_CONNID: efa_generate_rdm_connid();
 	err = efa_ep_modify_qp_rst2rts(ep, qp);
 	if (err)
 		goto err_destroy_qp;
@@ -306,6 +300,15 @@ static int efa_ep_bind(struct fid *fid, struct fid *bfid, uint64_t flags)
 		break;
 	case FI_CLASS_AV:
 		av = container_of(bfid, struct efa_av, util_av.av_fid.fid);
+		/*
+		 * Binding multiple endpoints to a single AV is currently not
+		 * supported.
+		 */
+		if (av->ep) {
+			EFA_WARN(FI_LOG_EP_CTRL,
+				 "Address vector already has endpoint bound to it.\n");
+			return -FI_ENOSYS;
+		}
 		if (ep->domain != av->domain) {
 			EFA_WARN(FI_LOG_EP_CTRL,
 				 "Address vector doesn't belong to same domain as EP.\n");
@@ -688,6 +691,7 @@ int efa_ep_open(struct fid_domain *domain_fid, struct fi_info *info,
 	ep->domain = domain;
 	ep->xmit_more_wr_tail = &ep->xmit_more_wr_head;
 	ep->recv_more_wr_tail = &ep->recv_more_wr_head;
+	ep->rnr_retry = rxr_env.rnr_retry;
 
 	if (info->src_addr) {
 		ep->src_addr = (void *)calloc(1, EFA_EP_ADDR_LEN);
diff --git a/prov/efa/src/efa_fabric.c b/prov/efa/src/efa_fabric.c
index b823334..1ea1b69 100644
--- a/prov/efa/src/efa_fabric.c
+++ b/prov/efa/src/efa_fabric.c
@@ -80,10 +80,15 @@
 
 #define EFA_DEF_MR_CACHE_ENABLE 1
 
+#ifdef EFA_PERF_ENABLED
+const char *efa_perf_counters_str[] = {
+	EFA_PERF_FOREACH(OFI_STR)
+};
+#endif
+
 int efa_mr_cache_enable		= EFA_DEF_MR_CACHE_ENABLE;
 size_t efa_mr_max_cached_count;
 size_t efa_mr_max_cached_size;
-int efa_set_rdmav_hugepages_safe = 0;
 
 static void efa_addr_to_str(const uint8_t *raw_addr, char *str);
 static int efa_get_addr(struct efa_context *ctx, void *src_addr);
@@ -741,7 +746,7 @@ static int efa_alloc_info(struct efa_context *ctx, struct fi_info **info,
 	fi->domain_attr->name = malloc(name_len + 1);
 	if (!fi->domain_attr->name) {
 		ret = -FI_ENOMEM;
-		goto err_free_fab_name;
+		goto err_free_info;
 	}
 
 	snprintf(fi->domain_attr->name, name_len + 1, "%s%s",
@@ -752,24 +757,18 @@ static int efa_alloc_info(struct efa_context *ctx, struct fi_info **info,
 	fi->src_addr = calloc(1, EFA_EP_ADDR_LEN);
 	if (!fi->src_addr) {
 		ret = -FI_ENOMEM;
-		goto err_free_dom_name;
+		goto err_free_info;
 	}
 	fi->src_addrlen = EFA_EP_ADDR_LEN;
 	ret = efa_get_addr(ctx, fi->src_addr);
 	if (ret)
-		goto err_free_src;
+		goto err_free_info;
 
 	fi->domain_attr->av_type = FI_AV_TABLE;
 
 	*info = fi;
 	return 0;
 
-err_free_src:
-	free(fi->src_addr);
-err_free_dom_name:
-	free(fi->domain_attr->name);
-err_free_fab_name:
-	free(fi->fabric_attr->name);
 err_free_info:
 	fi_freeinfo(fi);
 	return ret;
@@ -921,14 +920,33 @@ out:
 
 static int efa_fabric_close(fid_t fid)
 {
-	struct efa_fabric *fab;
+	struct efa_fabric *efa_fabric;
 	int ret;
 
-	fab = container_of(fid, struct efa_fabric, util_fabric.fabric_fid.fid);
-	ret = ofi_fabric_close(&fab->util_fabric);
-	if (ret)
+	efa_fabric = container_of(fid, struct efa_fabric, util_fabric.fabric_fid.fid);
+	ret = ofi_fabric_close(&efa_fabric->util_fabric);
+	if (ret) {
+		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
+			"Unable to close fabric: %s\n",
+			fi_strerror(-ret));
 		return ret;
-	free(fab);
+	}
+
+	if (efa_fabric->shm_fabric) {
+		ret = fi_close(&efa_fabric->shm_fabric->fid);
+		if (ret) {
+			FI_WARN(&rxr_prov, FI_LOG_FABRIC,
+				"Unable to close fabric: %s\n",
+				fi_strerror(-ret));
+			return ret;
+		}
+	}
+
+#ifdef EFA_PERF_ENABLED
+	ofi_perfset_log(&efa_fabric->perf_set, efa_perf_counters_str);
+	ofi_perfset_close(&efa_fabric->perf_set);
+#endif
+	free(efa_fabric);
 
 	return 0;
 }
@@ -943,102 +961,87 @@ static struct fi_ops efa_fi_ops = {
 
 static struct fi_ops_fabric efa_ops_fabric = {
 	.size = sizeof(struct fi_ops_fabric),
-	.domain = efa_domain_open,
+	/*
+	 * The reason we use rxr_domain_open() here is because it actually handles
+	 * both RDM and DGRAM.
+	 */
+	.domain = rxr_domain_open,
 	.passive_ep = fi_no_passive_ep,
 	.eq_open = ofi_eq_create,
 	.wait_open = ofi_wait_fd_open,
 	.trywait = ofi_trywait
 };
 
-static
-void efa_atfork_callback()
-{
-	static int visited = 0;
-
-	if (visited)
-		return;
-
-	visited = 1;
-	if (getenv("RDMAV_FORK_SAFE") || getenv("IBV_FORK_SAFE") )
-		return;
-
-	fprintf(stderr,
-		"A process has executed an operation involving a call\n"
-		"to the fork() system call to create a child process.\n"
-		"\n"
-		"As a result, the libfabric EFA provider is operating in\n"
-		"a condition that could result in memory corruption or\n"
-		"other system errors.\n"
-		"\n"
-		"For the libfabric EFA provider to work safely when fork()\n"
-		"is called, the application must handle memory registrations\n"
-		"(FI_MR_LOCAL) and you will need to set the following environment\n"
-		"variables:\n"
-		"          RDMAV_FORK_SAFE=1\n"
-		"MPI applications do not support this mode.\n"
-		"\n"
-		"However, this setting can result in signficant performance\n"
-		"impact to your application due to increased cost of memory\n"
-		"registration.\n"
-		"\n"
-		"You may want to check with your application vendor to see\n"
-		"if an application-level alternative (of not using fork)\n"
-		"exists.\n"
-		"\n"
-		"Please refer to https://github.com/ofiwg/libfabric/issues/6332\n"
-		"for more information.\n"
-		"\n"
-		"Your job will now abort.\n");
-	abort();
-}
-
 int efa_fabric(struct fi_fabric_attr *attr, struct fid_fabric **fabric_fid,
 	       void *context)
 {
 	const struct fi_info *info;
-	struct efa_fabric *fab;
-	int ret = 0;
+	struct efa_fabric *efa_fabric;
+	int ret = 0, retv;
 
-	ret = pthread_atfork(efa_atfork_callback, NULL, NULL);
-	if (ret) {
-		EFA_WARN(FI_LOG_FABRIC,
-			 "Unable to register atfork callback: %s\n",
-			 strerror(-ret));
-		return -ret;
-	}
-
-	fab = calloc(1, sizeof(*fab));
-	if (!fab)
+	efa_fabric = calloc(1, sizeof(*efa_fabric));
+	if (!efa_fabric)
 		return -FI_ENOMEM;
 
 	for (info = efa_util_prov.info; info; info = info->next) {
 		ret = ofi_fabric_init(&efa_prov, info->fabric_attr, attr,
-				      &fab->util_fabric, context);
+				      &efa_fabric->util_fabric, context);
 		if (ret != -FI_ENODATA)
 			break;
 	}
-	if (ret) {
-		free(fab);
-		return ret;
+
+	if (ret)
+		goto err_free_fabric;
+
+	/* Open shm provider's fabric domain */
+	if (rxr_env.enable_shm_transfer) {
+		assert(!strcmp(shm_info->fabric_attr->name, "shm"));
+		ret = fi_fabric(shm_info->fabric_attr,
+				    &efa_fabric->shm_fabric, context);
+		if (ret)
+			goto err_close_util_fabric;
+	} else {
+		efa_fabric->shm_fabric = NULL;
 	}
 
-	*fabric_fid = &fab->util_fabric.fabric_fid;
+
+#ifdef EFA_PERF_ENABLED
+	ret = ofi_perfset_create(&rxr_prov, &efa_fabric->perf_set,
+				 efa_perf_size, perf_domain, perf_cntr,
+				 perf_flags);
+
+	if (ret)
+		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
+			"Error initializing EFA perfset: %s\n",
+			fi_strerror(-ret));
+#endif
+
+
+	*fabric_fid = &efa_fabric->util_fabric.fabric_fid;
 	(*fabric_fid)->fid.fclass = FI_CLASS_FABRIC;
 	(*fabric_fid)->fid.ops = &efa_fi_ops;
 	(*fabric_fid)->ops = &efa_ops_fabric;
 	(*fabric_fid)->api_version = attr->api_version;
 
 	return 0;
+
+err_close_util_fabric:
+	retv = ofi_fabric_close(&efa_fabric->util_fabric);
+	if (retv)
+		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
+			"Unable to close fabric: %s\n",
+			fi_strerror(-retv));
+err_free_fabric:
+	free(efa_fabric);
+
+	return ret;
 }
 
-static void fi_efa_fini(void)
+void efa_finalize_prov(void)
 {
 	struct efa_context **ctx_list;
 	int num_devices;
 
-	if (efa_set_rdmav_hugepages_safe)
-		unsetenv("RDMAV_HUGEPAGES_SAFE");
-
 	fi_freeinfo((void *)efa_util_prov.info);
 	efa_util_prov.info = NULL;
 
@@ -1056,7 +1059,7 @@ struct fi_provider efa_prov = {
 	.fi_version = OFI_VERSION_LATEST,
 	.getinfo = efa_getinfo,
 	.fabric = efa_fabric,
-	.cleanup = fi_efa_fini
+	.cleanup = efa_finalize_prov
 };
 
 struct util_prov efa_util_prov = {
@@ -1077,17 +1080,23 @@ static int efa_init_info(const struct fi_info **all_infos)
 		return ret;
 
 	ctx_list = efa_device_get_context_list(&num_devices);
-	if (!num_devices)
+	if (!num_devices) {
+		if (ctx_list) {
+			free(ctx_list);
+		}
 		return -FI_ENODEV;
+	}
 
 	*all_infos = NULL;
 	for (i = 0; i < num_devices; i++) {
 		ret = efa_alloc_info(ctx_list[i], &fi, &efa_rdm_domain);
 		if (!ret) {
-			if (!*all_infos)
+			if (!*all_infos) {
 				*all_infos = fi;
-			else
+			} else {
+				assert(tail);
 				tail->next = fi;
+			}
 			tail = fi;
 			ret = efa_alloc_info(ctx_list[i], &fi, &efa_dgrm_domain);
 			if (!ret) {
@@ -1107,29 +1116,7 @@ static int efa_init_info(const struct fi_info **all_infos)
 	return retv;
 }
 
-struct fi_provider *init_lower_efa_prov()
+int efa_init_prov(void)
 {
-	int err;
-
-	if (!getenv("RDMAV_HUGEPAGES_SAFE")) {
-		/*
-		 * Setting RDMAV_HUGEPAGES_SAFE alone will not impact
-		 * application performance, because rdma-core will only
-		 * check this environment variable when either
-		 * RDMAV_FORK_SAFE or IBV_FORK_SAFE is set.
-		 */
-		err = setenv("RDMAV_HUGEPAGES_SAFE", "1", 1);
-		if (err) {
-			EFA_WARN(FI_LOG_FABRIC,
-				 "Unable to set environment variable RDMAV_HUGEPAGES_SAFE\n");
-			return NULL;
-		}
-
-		efa_set_rdmav_hugepages_safe = 1;
-	}
-
-	if (efa_init_info(&efa_util_prov.info))
-		return NULL;
-
-	return &efa_prov;
+	return efa_init_info(&efa_util_prov.info);
 }
diff --git a/prov/efa/src/efa_mr.c b/prov/efa/src/efa_mr.c
index f89eb34..02c80f8 100644
--- a/prov/efa/src/efa_mr.c
+++ b/prov/efa/src/efa_mr.c
@@ -151,7 +151,6 @@ static int efa_mr_cache_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 	struct efa_mr *efa_mr;
 	struct ofi_mr_entry *entry;
 	int ret;
-	static const int EFA_MR_CACHE_FLUSH_CHECK = 512;
 
 	if (flags & OFI_MR_NOCACHE) {
 		ret = efa_mr_regattr(fid, attr, flags, mr_fid);
@@ -167,10 +166,6 @@ static int efa_mr_cache_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 	domain = container_of(fid, struct efa_domain,
 			      util_domain.domain_fid.fid);
 
-	if (domain->cache->cached_cnt > 0 && domain->cache->cached_cnt % EFA_MR_CACHE_FLUSH_CHECK==0) {
-		ofi_mr_cache_flush(domain->cache, false);
-	}
-
 	ret = ofi_mr_cache_search(domain->cache, attr, &entry);
 	if (OFI_UNLIKELY(ret))
 		return ret;
@@ -248,7 +243,8 @@ static int efa_mr_dereg_impl(struct efa_mr *efa_mr)
 			fi_strerror(-ret));
 		ret = err;
 	}
-	if (rxr_env.enable_shm_transfer && efa_mr->shm_mr) {
+	if (efa_mr->shm_mr) {
+		assert(rxr_env.enable_shm_transfer);
 		err = fi_close(&efa_mr->shm_mr->fid);
 		if (err) {
 			EFA_WARN(FI_LOG_MR,
@@ -307,6 +303,9 @@ static int efa_mr_reg_impl(struct efa_mr *efa_mr, uint64_t flags, void *attr)
 	if (efa_mr->domain->ctx->device_caps & EFADV_DEVICE_ATTR_CAPS_RDMA_READ)
 		fi_ibv_access |= IBV_ACCESS_REMOTE_READ;
 
+	if (efa_mr->domain->cache)
+		ofi_mr_cache_flush(efa_mr->domain->cache, false);
+
 	efa_mr->ibv_mr = ibv_reg_mr(efa_mr->domain->ibv_pd, 
 				    (void *)mr_attr->mr_iov->iov_base,
 				    mr_attr->mr_iov->iov_len, fi_ibv_access);
@@ -341,10 +340,11 @@ static int efa_mr_reg_impl(struct efa_mr *efa_mr, uint64_t flags, void *attr)
 			mr_attr->mr_iov->iov_len);
 		return ret;
 	}
-	if (efa_mr->domain->shm_domain && rxr_env.enable_shm_transfer) {
+	if (efa_mr->domain->shm_domain) {
 		/* We need to add FI_REMOTE_READ to allow for Read implemented
 		* message protocols.
 		*/
+		assert(rxr_env.enable_shm_transfer);
 		original_access = mr_attr->access;
 		mr_attr->access |= FI_REMOTE_READ;
 		ret = fi_mr_regattr(efa_mr->domain->shm_domain, attr,
diff --git a/prov/efa/src/efa_msg.c b/prov/efa/src/efa_msg.c
index b0c5385..75a11dd 100644
--- a/prov/efa/src/efa_msg.c
+++ b/prov/efa/src/efa_msg.c
@@ -123,6 +123,15 @@ static ssize_t efa_post_recv_validate(struct efa_ep *ep, const struct fi_msg *ms
 	return 0;
 }
 
+/**
+ * @brief post receive buffer to EFA device via ibv_post_recv
+ *
+ * @param[in]	ep	endpoint
+ * @param[in]	msg	libfabric message
+ * @param[in]	flags	libfabric flags, currently only FI_MORE is supported.
+ * @reutrn	On Success, return 0
+ * 		On failure, return negative libfabric error code
+ */
 static ssize_t efa_post_recv(struct efa_ep *ep, const struct fi_msg *msg, uint64_t flags)
 {
 	struct efa_mr *efa_mr;
@@ -170,6 +179,13 @@ static ssize_t efa_post_recv(struct efa_ep *ep, const struct fi_msg *msg, uint64
 		return 0;
 
 	err = ibv_post_recv(qp->ibv_qp, ep->recv_more_wr_head.next, &bad_wr);
+	if (OFI_UNLIKELY(err)) {
+		/* On failure, ibv_post_recv() return positive errno.
+		 * Meanwhile, this function return a negative errno.
+		 * So, we do the conversion here.
+		 */
+		err = (err == ENOMEM) ? -FI_EAGAIN : -err;
+	}
 
 	free_recv_wr_list(ep->recv_more_wr_head.next);
 	ep->recv_more_wr_tail = &ep->recv_more_wr_head;
@@ -315,7 +331,8 @@ static ssize_t efa_post_send(struct efa_ep *ep, const struct fi_msg *msg, uint64
 
 	memset(ewr, 0, sizeof(*ewr) + sizeof(*ewr->sge) * msg->iov_count);
 	wr = &ewr->wr;
-	conn = ep->av->addr_to_conn(ep->av, msg->addr);
+	conn = efa_av_addr_to_conn(ep->av, msg->addr);
+	assert(conn && conn->ep_addr);
 
 	ret = efa_post_send_validate(ep, msg, conn, flags, &len);
 	if (OFI_UNLIKELY(ret)) {
@@ -325,14 +342,17 @@ static ssize_t efa_post_send(struct efa_ep *ep, const struct fi_msg *msg, uint64
 
 	efa_post_send_sgl(ep, msg, ewr);
 
-	if (flags & FI_INJECT)
+	/* TODO: uncomment this code block when the underlying
+	 * issue with INLINE send is fixed.
+	if (len <= ep->domain->ctx->inline_buf_size)
 		wr->send_flags |= IBV_SEND_INLINE;
+	*/
 
 	wr->opcode = IBV_WR_SEND;
 	wr->wr_id = (uintptr_t)msg->context;
-	wr->wr.ud.ah = conn->ah.ibv_ah;
-	wr->wr.ud.remote_qpn = conn->ep_addr.qpn;
-	wr->wr.ud.remote_qkey = conn->ep_addr.qkey;
+	wr->wr.ud.ah = conn->ah->ibv_ah;
+	wr->wr.ud.remote_qpn = conn->ep_addr->qpn;
+	wr->wr.ud.remote_qkey = conn->ep_addr->qkey;
 
 	ep->xmit_more_wr_tail->next = wr;
 	ep->xmit_more_wr_tail = wr;
diff --git a/prov/efa/src/efa_rma.c b/prov/efa/src/efa_rma.c
index 97c6813..32ee572 100644
--- a/prov/efa/src/efa_rma.c
+++ b/prov/efa/src/efa_rma.c
@@ -101,9 +101,10 @@ ssize_t efa_rma_post_read(struct efa_ep *ep, const struct fi_msg_rma *msg,
 		ibv_wr_set_ud_addr(qp->ibv_qp_ex, ep->self_ah,
 				   qp->qp_num, qp->qkey);
 	} else {
-		conn = ep->av->addr_to_conn(ep->av, msg->addr);
-		ibv_wr_set_ud_addr(qp->ibv_qp_ex, conn->ah.ibv_ah,
-				   conn->ep_addr.qpn, conn->ep_addr.qkey);
+		conn = efa_av_addr_to_conn(ep->av, msg->addr);
+		assert(conn && conn->ep_addr);
+		ibv_wr_set_ud_addr(qp->ibv_qp_ex, conn->ah->ibv_ah,
+				   conn->ep_addr->qpn, conn->ep_addr->qkey);
 	}
 
 	return ibv_wr_complete(qp->ibv_qp_ex);
diff --git a/prov/efa/src/rxr/rdm_proto_v4.h b/prov/efa/src/rxr/rdm_proto_v4.h
new file mode 100644
index 0000000..d0f7373
--- /dev/null
+++ b/prov/efa/src/rxr/rdm_proto_v4.h
@@ -0,0 +1,713 @@
+/*
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _RXR_PROTO_V4_H
+#define _RXR_PROTO_V4_H
+
+/*
+ * This header file contains constants, flags and data structures
+ * that are defined in EFA RDM protocol v4. Any change to this
+ * header file can potentially break backward compatibility, thus
+ * need to be reviewed with extra care.
+ *
+ * The section number in this file refers to the sections
+ * in EFA RDM protocol version 4.
+ */
+
+#define RXR_PROTOCOL_VERSION	(4)
+
+/* raw address format. (section 1.4) */
+#define EFA_GID_LEN	16
+
+struct efa_ep_addr {
+	uint8_t			raw[EFA_GID_LEN];
+	uint16_t		qpn;
+	uint16_t		pad;
+	uint32_t		qkey;
+	struct efa_ep_addr	*next;
+};
+
+#define EFA_EP_ADDR_LEN sizeof(struct efa_ep_addr)
+
+/*
+ * Extra Feature/Request Flags (section 2.1)
+ */
+#define RXR_EXTRA_FEATURE_RDMA_READ			BIT_ULL(0)
+#define RXR_EXTRA_FEATURE_DELIVERY_COMPLETE 		BIT_ULL(1)
+#define RXR_EXTRA_REQUEST_CONSTANT_HEADER_LENGTH	BIT_ULL(2)
+#define RXR_EXTRA_REQUEST_CONNID_HEADER			BIT_ULL(3)
+#define RXR_NUM_EXTRA_FEATURE_OR_REQUEST		4
+#define RXR_MAX_NUM_EXINFO	(256)
+
+/*
+ * Packet type ID of each packet type (section 1.3)
+ *
+ * Changing packet type ID would break backward compatiblity thus is strictly
+ * prohibited.
+ *
+ * New packet types can be added with introduction of an extra feature
+ * (section 2.1)
+ */
+#define RXR_RETIRED_RTS_PKT		1
+#define RXR_RETIRED_CONNACK_PKT		2
+#define RXR_CTS_PKT			3
+#define RXR_DATA_PKT			4
+#define RXR_READRSP_PKT			5
+#define RXR_RMA_CONTEXT_PKT		6
+#define RXR_EOR_PKT			7
+#define RXR_ATOMRSP_PKT 	        8
+#define RXR_HANDSHAKE_PKT		9
+#define RXR_RECEIPT_PKT 		10
+
+#define RXR_REQ_PKT_BEGIN		64
+#define RXR_BASELINE_REQ_PKT_BEGIN	64
+#define RXR_EAGER_MSGRTM_PKT		64
+#define RXR_EAGER_TAGRTM_PKT		65
+#define RXR_MEDIUM_MSGRTM_PKT		66
+#define RXR_MEDIUM_TAGRTM_PKT		67
+#define RXR_LONGCTS_MSGRTM_PKT		68
+#define RXR_LONGCTS_TAGRTM_PKT		69
+#define RXR_EAGER_RTW_PKT		70
+#define RXR_LONGCTS_RTW_PKT		71
+#define RXR_SHORT_RTR_PKT		72
+#define RXR_LONGCTS_RTR_PKT		73
+#define RXR_WRITE_RTA_PKT		74
+#define RXR_FETCH_RTA_PKT		75
+#define RXR_COMPARE_RTA_PKT		76
+#define RXR_BASELINE_REQ_PKT_END	77
+
+#define RXR_EXTRA_REQ_PKT_BEGIN		128
+#define RXR_LONGREAD_MSGRTM_PKT		128
+#define RXR_LONGREAD_TAGRTM_PKT		129
+#define RXR_LONGREAD_RTW_PKT		130
+#define RXR_READ_RTR_PKT		131
+
+#define RXR_DC_REQ_PKT_BEGIN		132
+#define RXR_DC_EAGER_MSGRTM_PKT 	133
+#define RXR_DC_EAGER_TAGRTM_PKT 	134
+#define RXR_DC_MEDIUM_MSGRTM_PKT 	135
+#define RXR_DC_MEDIUM_TAGRTM_PKT 	136
+#define RXR_DC_LONGCTS_MSGRTM_PKT  	137
+#define RXR_DC_LONGCTS_TAGRTM_PKT  	138
+#define RXR_DC_EAGER_RTW_PKT    	139
+#define RXR_DC_LONGCTS_RTW_PKT     	140
+#define RXR_DC_WRITE_RTA_PKT    	141
+#define RXR_DC_REQ_PKT_END		142
+#define RXR_EXTRA_REQ_PKT_END   	142
+
+/*
+ *  Packet fields common to all rxr packets. The other packet headers below must
+ *  be changed if this is updated.
+ */
+struct rxr_base_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_base_hdr check");
+#endif
+
+/* Universal flags that can be applied on "rxr_base_hdr.flags".
+ *
+ * Universal flags start from the last bit and goes backwards.
+ * Because "rxr_base_hdr.flags" is a 16-bits integer, the
+ * last bit is the 15th bit.
+ * Other than universal flags, each packet type defines its
+ * own set of flags, which generally starts from the 0th bit
+ * in "rxr_base_hdr.flags".
+ */
+
+/* indicate this packet has the sender connid */
+#define RXR_PKT_CONNID_HDR		BIT_ULL(15)
+
+struct efa_rma_iov {
+	uint64_t		addr;
+	size_t			len;
+	uint64_t		key;
+};
+
+/*
+ * @breif header format of CTS packet (Packet Type ID 3)
+ *
+ * CTS is used in long-CTS sub-protocols for flow control.
+ *
+ * It is sent from receiver to sender, and contains number of bytes
+ * receiver is ready to receive.
+ *
+ * long-CTS is used not only by two-sided communication but also
+ * by emulated write and emulated read protocols.
+ *
+ * In emulated write, requester is sender, and responder is receiver.
+ *
+ * In emulated read, requester is receiver, and responder is sender.
+ */
+struct rxr_cts_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	union {
+		uint32_t connid; /* sender connection ID, set when RXR_PKT_CONNID_HDR is on */
+		uint32_t padding; /* otherwise, a padding space to 8 bytes */
+	};
+	uint32_t send_id; /* ID of the send opertaion on sender side */
+	uint32_t recv_id; /* ID of the receive operatin on receive side */
+	uint64_t recv_length; /* number of bytes receiver is ready to receive */
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_cts_hdr) == 24, "rxr_cts_hdr check");
+#endif
+
+/* this flag is to indicated the CTS is the response of a RTR packet */
+#define RXR_CTS_READ_REQ		BIT_ULL(7)
+
+
+/*
+ * @brief optional connid header for DATA packet
+ */
+struct rxr_data_opt_connid_hdr {
+	uint32_t connid;
+	uint32_t padding;
+};
+
+/*
+ * @brief header format of DATA packet header (Packet Type ID 4)
+ *
+ * DATA is used in long-CTS sub-protocols.
+ *
+ * It is sent from sender to receiver, and contains a segment
+ * of application data.
+ *
+ * long-CTS is used not only by two-sided communication but also
+ * by emulated write and emulated read protocols.
+ *
+ * In emulated write, requester is sender, and responder is receiver.
+ *
+ * In emulated read, requester is receiver, and responder is sender.
+ */
+struct rxr_data_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t recv_id; /* ID of the receive operation on receiver */
+	uint64_t seg_length;
+	uint64_t seg_offset;
+	/* optional connid header, present when RXR_PKT_CONNID_HDR is on */
+	struct rxr_data_opt_connid_hdr connid_hdr[0];
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_data_hdr) == 24, "rxr_data_hdr check");
+#endif
+
+/*
+ *  @brief READRSP packet header (Packet Type ID 5)
+ *
+ *  READRSP is sent from read responder to read requester, and it contains
+ *  application data.
+ */
+struct rxr_readrsp_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	union {
+		uint32_t connid; /* sender connection ID, set when RXR_PKT_CONNID_HDR is on */
+		uint32_t padding; /* otherwise, a padding space to 8 bytes boundary */
+	};
+	uint32_t recv_id; /* ID of the receive operation on the read requester, from rtr packet */
+	uint32_t send_id; /* ID of the send operation on the read responder, will be included in CTS packet */
+	uint64_t seg_length;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_readrsp_hdr) == sizeof(struct rxr_data_hdr), "rxr_readrsp_hdr check");
+#endif
+
+struct rxr_readrsp_pkt {
+	struct rxr_readrsp_hdr hdr;
+	char data[];
+};
+
+/*
+ * RMA Context pkt (Packe Type ID 6) is a special type
+ * of packet. It is used as the context of an RMA
+ * operatation, thus is not sent over wire. Therefore
+ * its header format is not part of protocol. In doc,
+ * the packet type ID 6 is marked as reserved
+ */
+
+/*
+ * @brief format of the EOR packet. (Packet Type ID 7)
+ *
+ * EOR packet is used in long-read sub-protocols, which is
+ * part of the extra request: RDMA read based data transfer.
+ *
+ * It is sent from receiver to sender, to notify
+ * the finish of data transfer.
+ *
+ * long-read is used not only by two-sided communication but also
+ * by emulated write.
+ *
+ * In emulated write, requester is sender, and responder is receiver.
+ */
+struct rxr_eor_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t send_id; /* ID of the send operation on sender */
+	uint32_t recv_id; /* ID of the receive operation on receiver */
+	union {
+		uint32_t connid; /* sender connection ID, optional, set whne RXR_PKT_CONNID_HDR is on */
+		uint32_t padding; /* otherwise, a padding space to 8 bytes boundary */
+	};
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_eor_hdr) == 16, "rxr_eor_hdr check");
+#endif
+
+/**
+ * @brief header format of ATOMRSP packet. (Packet Type ID 8)
+ * ATOMRSP packet is used in emulated fetch/compare atomic sub-protocol.
+ * 
+ * It is sent from responder to requester, which contains the response
+ * to a fetch/compare atomic request
+ */
+struct rxr_atomrsp_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	union {
+		uint32_t connid; /* sender connid. set when RXR_PKT_CONNID_HDR is on in flags */
+		uint32_t padding; /* otherwise, a padding space to 8 bytes boundary */
+	};
+	uint32_t reserved;
+	uint32_t recv_id;
+	uint64_t seg_length;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_atomrsp_hdr) == 24, "rxr_atomrsp_hdr check");
+#endif
+
+struct rxr_atomrsp_pkt {
+	struct rxr_atomrsp_hdr hdr;
+	char data[];
+};
+
+/**
+ * @breif header format of a HANDSHAKE packet
+ *
+ * HANDSHAKE packet is used in the handshake sub-protocol.
+ *
+ * Upon receiving 1st packet from a peer, an endpoint will
+ * send a HANDSHAKE packet back, which contains its capablity bits
+ */
+struct rxr_handshake_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	/* nextra_p3 is number of members in extra_info plus 3.
+	 * The "p3" part was introduced for backward compatibility.
+	 * See protocol v4 document section 2.1 for detail.
+	 */
+	uint32_t nextra_p3;
+	uint64_t extra_info[0];
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_handshake_hdr) == 8, "rxr_handshake_hdr check");
+#endif
+
+struct rxr_handshake_opt_connid_hdr {
+	uint32_t connid;
+	uint32_t padding; /* padding to 8 bytes boundary */
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_handshake_opt_connid_hdr) == 8, "rxr_handshake_opt_connid_hdr check");
+#endif
+
+/* @brief header format of RECEIPT packet */
+struct rxr_receipt_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t tx_id;
+	uint32_t msg_id;
+	union {
+		uint32_t connid; /* sender connection ID, set when RXR_PKT_CONNID_HDR is on */
+		uint32_t padding; /* otherwise, a padding space to 8 bytes */
+	};
+};
+
+/*
+ * The following are REQ packets related constants, flags
+ * and data structures.
+ *
+ * REQ packets can be classifed into 4 categories (section 3.1):
+ *    RTM (Request To Message) is used by message
+ *    RTW (Request To Write) is used by RMA write
+ *    RTR (Request To Read) is used by RMA read
+ *    RTA (Request To Atomic) is used by Atomic
+ */
+
+
+/*
+ * REQ Packets common Header Flags (section 3.1)
+ */
+#define RXR_REQ_OPT_RAW_ADDR_HDR	BIT_ULL(0)
+#define RXR_REQ_OPT_CQ_DATA_HDR		BIT_ULL(1)
+#define RXR_REQ_MSG			BIT_ULL(2)
+#define RXR_REQ_TAGGED			BIT_ULL(3)
+#define RXR_REQ_RMA			BIT_ULL(4)
+#define RXR_REQ_ATOMIC			BIT_ULL(5)
+
+/*
+ * optional headers for REQ packets
+ */
+struct rxr_req_opt_raw_addr_hdr {
+	uint32_t addr_len;
+	char raw_addr[0];
+};
+
+struct rxr_req_opt_cq_data_hdr {
+	int64_t cq_data;
+};
+
+struct rxr_req_opt_connid_hdr {
+	uint32_t connid; /* sender's connection ID */
+};
+
+#define RXR_REQ_OPT_HDR_ALIGNMENT 8
+#define RXR_REQ_OPT_RAW_ADDR_HDR_SIZE (((sizeof(struct rxr_req_opt_raw_addr_hdr) + EFA_EP_ADDR_LEN - 1)/RXR_REQ_OPT_HDR_ALIGNMENT + 1) * RXR_REQ_OPT_HDR_ALIGNMENT)
+
+/*
+ * Base header for all RTM packets
+ */
+struct rxr_rtm_base_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	uint32_t msg_id;
+};
+
+/**
+ * @brief header format of EAGER_MSGRTM packet (Packet Type ID 64)
+ */
+struct rxr_eager_msgrtm_hdr {
+	struct rxr_rtm_base_hdr hdr;
+};
+
+
+/**
+ * @brief header format of EAGER_TAGRTM packet (Packet Type ID 65)
+ */
+struct rxr_eager_tagrtm_hdr {
+	struct rxr_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+struct rxr_medium_rtm_base_hdr {
+	struct rxr_rtm_base_hdr hdr;
+	uint64_t msg_length;
+	uint64_t seg_offset;
+};
+
+/**
+ * @brief header format of MEDIUM_MSGRTM packet (Packet Type ID 66)
+ */
+struct rxr_medium_msgrtm_hdr {
+	struct rxr_medium_rtm_base_hdr hdr;
+};
+
+/**
+ * @brief header format of MEDIUM_TAGRTM packet (Packet Type ID 67)
+ */
+struct rxr_medium_tagrtm_hdr {
+	struct rxr_medium_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+struct rxr_longcts_rtm_base_hdr {
+	struct rxr_rtm_base_hdr hdr;
+	uint64_t msg_length;
+	uint32_t send_id;
+	uint32_t credit_request;
+};
+
+/**
+ * @brief header format of LONGCTS_MSGRTM packet (Packet Type ID 68)
+ */
+struct rxr_longcts_msgrtm_hdr {
+	struct rxr_longcts_rtm_base_hdr hdr;
+};
+
+/**
+ * @brief header format of LONGCTS_TAGRTM packet (Packet Type ID 69)
+ */
+struct rxr_longcts_tagrtm_hdr {
+	struct rxr_longcts_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+struct rxr_rtw_base_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+};
+
+/**
+ * @brief header format of EAGER_RTW packet (Packet Type ID 70)
+ */
+struct rxr_eager_rtw_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+	struct efa_rma_iov rma_iov[0];
+};
+
+/**
+ * @brief header format of LONGCTS_RTW packet (Packet Type ID 71)
+ */
+struct rxr_longcts_rtw_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+	uint64_t msg_length;
+	uint32_t send_id;
+	uint32_t credit_request;
+	struct efa_rma_iov rma_iov[0];
+};
+
+/*
+ * rxr_rtr_hdr is used by both SHORT_RTR (Packet Type ID 72)
+ * and LONGCTS_RTR (Packet Type ID 73)
+ */
+struct rxr_rtr_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+	uint64_t msg_length;
+	uint32_t recv_id; /* ID of the receive operation of the read requester, will be included in DATA/READRSP header */
+	uint32_t recv_length; /* number of bytes that the read requester is ready to receive */
+	struct efa_rma_iov rma_iov[0];
+};
+
+/* @brief rxr_rta_hdr are shared by 4 types of RTA:
+ *    WRITE_RTA (Packet Type ID 74),
+ *    FETCH_RTA (Packet Type ID 75),
+ *    COMPARE_RTA (Packet Type ID 76) and
+ *    DC_WRTIE_RTA (Packe Type ID 141)
+ */
+struct rxr_rta_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	uint32_t msg_id;
+	/* end of rtm_base_hdr, atomic packet need msg_id for reordering */
+	uint32_t rma_iov_count;
+	uint32_t atomic_datatype;
+	uint32_t atomic_op;
+	union {
+		/* padding is used by WRITE_RTA, align to 8 bytes */
+		uint32_t padding;
+		/* recv_id is used by FETCH_RTA and COMPARE_RTA. It is the ID of the receive operation on atomic requester,
+		 * it will be included in ATOMRSP packet header.
+		 */
+		uint32_t recv_id;
+		/* send_id is used by DC_WRITE_RTA. It is ID of the send operation on the atomic requester.
+		 * It will be included in RECEIPT packet header.
+		 */
+		uint32_t send_id;
+	};
+
+	struct efa_rma_iov rma_iov[0];
+};
+
+/*
+ * Extra request: RDMA read based data transfer (section 4.1)
+ */
+struct rxr_longread_rtm_base_hdr {
+	struct rxr_rtm_base_hdr hdr;
+	uint64_t msg_length;
+	uint32_t send_id;
+	uint32_t read_iov_count;
+};
+
+/**
+ * @brief header format of LONGREAD_MSGRTM (Packet Type ID 128)
+ */
+struct rxr_longread_msgrtm_hdr {
+	struct rxr_longread_rtm_base_hdr hdr;
+};
+
+/**
+ * @brief header format of LONGREAD_MSGRTM (Packet Type ID 129)
+ */
+struct rxr_longread_tagrtm_hdr {
+	struct rxr_longread_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+/**
+ * @brief header format of LONGREAD_MSGRTM (Packet Type ID 130)
+ */
+struct rxr_longread_rtw_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+	uint64_t msg_length;
+	uint32_t send_id;
+	uint32_t read_iov_count;
+	struct efa_rma_iov rma_iov[0];
+};
+
+/*
+ * Extra requester: delivery complete (section 4.2)
+ */
+
+struct rxr_dc_eager_rtm_base_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	uint32_t msg_id;
+	uint32_t send_id;
+	uint32_t padding;
+};
+
+/**
+ * @brief header format of a DC_EAGER_MSGRTM packet
+ */
+struct rxr_dc_eager_msgrtm_hdr {
+	struct rxr_dc_eager_rtm_base_hdr hdr;
+};
+
+/**
+ * @brief header format of a DC_EAGER_TAGRTM packet
+ */
+struct rxr_dc_eager_tagrtm_hdr {
+	struct rxr_dc_eager_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+struct rxr_dc_medium_rtm_base_hdr {
+	struct rxr_rtm_base_hdr hdr;
+	uint32_t send_id;
+	uint32_t padding;
+	uint64_t msg_length;
+	uint64_t seg_offset;
+};
+
+/**
+ * @brief header format of a DC_MEDIUM_MSGRTM packet
+ */
+struct rxr_dc_medium_msgrtm_hdr {
+	struct rxr_dc_medium_rtm_base_hdr hdr;
+};
+
+/**
+ * @brief header format of a DC_MEDIUM_TAGRTM packet
+ */
+struct rxr_dc_medium_tagrtm_hdr {
+	struct rxr_dc_medium_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+/**
+ * @brief header format of a DC_LONGCTS_MSGRTM packet
+ */
+struct rxr_dc_longcts_msgrtm_hdr {
+	struct rxr_longcts_rtm_base_hdr hdr;
+};
+
+/**
+ * @brief header format of a DC_LONGCTS_TAGRTM packet
+ */
+struct rxr_dc_longcts_tagrtm_hdr {
+	struct rxr_longcts_rtm_base_hdr hdr;
+	uint64_t tag;
+};
+
+/**
+ * @brief header format of a DC_EAGER_RTW packet
+ */
+struct rxr_dc_eager_rtw_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+	/* end of rxr_rtw_base_hdr */
+	uint32_t send_id;
+	uint32_t padding;
+	struct efa_rma_iov rma_iov[0];
+};
+
+/**
+ * @brief header format of a DC_LONGCTS_RTW packet
+ */
+struct rxr_dc_longcts_rtw_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t rma_iov_count;
+	uint64_t msg_length;
+	uint32_t send_id;
+	uint32_t credit_request;
+	struct efa_rma_iov rma_iov[0];
+};
+
+/* DC_WRITE_RTA header format is merged into rxr_rta_hdr */
+
+#endif
diff --git a/prov/efa/src/rxr/rxr.h b/prov/efa/src/rxr/rxr.h
index 32873a0..1b08ca8 100644
--- a/prov/efa/src/rxr/rxr.h
+++ b/prov/efa/src/rxr/rxr.h
@@ -49,6 +49,7 @@
 #include <rdma/fi_rma.h>
 #include <rdma/fi_tagged.h>
 #include <rdma/fi_trigger.h>
+#include <rdma/fi_ext.h>
 
 #include <ofi.h>
 #include <ofi_iov.h>
@@ -66,15 +67,6 @@
 #include "rxr_pkt_entry.h"
 #include "rxr_pkt_type.h"
 
-/*
- * EFA support interoperability between protocol version 4 and above,
- * and version 4 is considered the base version.
- */
-#define RXR_BASE_PROTOCOL_VERSION	(4)
-#define RXR_CUR_PROTOCOL_VERSION	(4)
-#define RXR_NUM_PROTOCOL_VERSION	(RXR_CUR_PROTOCOL_VERSION - RXR_BASE_PROTOCOL_VERSION + 1)
-#define RXR_MAX_PROTOCOL_VERSION	(100)
-
 #define RXR_FI_VERSION		OFI_VERSION_LATEST
 
 #define RXR_IOV_LIMIT		(4)
@@ -102,8 +94,17 @@ static inline void rxr_poison_mem_region(uint32_t *ptr, size_t size)
 #define RXR_DEF_CQ_SIZE			(8192)
 #define RXR_REMOTE_CQ_DATA_LEN		(8)
 
-/* maximum timeout for RNR backoff (microseconds) */
-#define RXR_DEF_RNR_MAX_TIMEOUT		(1000000)
+/* the default value for rxr_env.rnr_backoff_wait_time_cap */
+#define RXR_DEFAULT_RNR_BACKOFF_WAIT_TIME_CAP	(1000000)
+
+/*
+ * the maximum value for rxr_env.rnr_backoff_wait_time_cap
+ * Because the backoff wait time is multiplied by 2 when
+ * RNR is encountered, its value must be < INT_MAX/2.
+ * Therefore, its cap must be < INT_MAX/2 too.
+ */
+#define RXR_MAX_RNR_BACKOFF_WAIT_TIME_CAP	(INT_MAX/2 - 1)
+
 /* bounds for random RNR backoff timeout */
 #define RXR_RAND_MIN_TIMEOUT		(40)
 #define RXR_RAND_MAX_TIMEOUT		(120)
@@ -175,6 +176,10 @@ static inline void rxr_poison_mem_region(uint32_t *ptr, size_t size)
  */
 #define RXR_LONGCTS_PROTOCOL BIT_ULL(8)
 
+#define RXR_TX_ENTRY_QUEUED_RNR BIT_ULL(9)
+
+#define RXR_RX_ENTRY_QUEUED_RNR BIT_ULL(9)
+
 /*
  * OFI flags
  * The 64-bit flag field is used as follows:
@@ -197,7 +202,6 @@ static inline void rxr_poison_mem_region(uint32_t *ptr, size_t size)
 
 extern struct fi_info *shm_info;
 
-extern struct fi_provider *lower_efa_prov;
 extern struct fi_provider rxr_prov;
 extern struct fi_info rxr_info;
 extern struct rxr_env rxr_env;
@@ -217,6 +221,8 @@ struct rxr_env {
 	int shm_av_size;
 	int shm_max_medium_size;
 	int recvwin_size;
+	int ooo_pool_chunk_size;
+	int unexp_pool_chunk_size;
 	int readcopy_pool_size;
 	int atomrsp_pool_size;
 	int cq_size;
@@ -228,14 +234,25 @@ struct rxr_env {
 	size_t rx_iov_limit;
 	int rx_copy_unexp;
 	int rx_copy_ooo;
-	int max_timeout;
-	int timeout_interval;
+	int rnr_backoff_wait_time_cap; /* unit is us */
+	int rnr_backoff_initial_wait_time; /* unit is us */
 	size_t efa_cq_read_size;
 	size_t shm_cq_read_size;
 	size_t efa_max_medium_msg_size;
 	size_t efa_min_read_msg_size;
 	size_t efa_min_read_write_size;
 	size_t efa_read_segment_size;
+	/* If first attempt to send a packet failed,
+	 * this value controls how many times firmware
+	 * retries the send before it report an RNR error
+	 * (via rdma-core error cq entry).
+	 *
+	 * The valid number is from
+	 *      0 (no retry)
+	 * to
+	 *      EFA_RNR_INFINITY_RETRY (retry infinitely)
+	 */
+	int rnr_retry;
 };
 
 enum rxr_lower_ep_type {
@@ -255,12 +272,6 @@ enum rxr_tx_comm_type {
 	RXR_TX_SEND,		/* tx_entry sending data in progress */
 	RXR_TX_QUEUED_SHM_RMA,	/* tx_entry was unable to send RMA operations over shm provider */
 	RXR_TX_QUEUED_CTRL,	/* tx_entry was unable to send ctrl packet */
-	RXR_TX_QUEUED_REQ_RNR,  /* tx_entry RNR sending REQ packet */
-	RXR_TX_QUEUED_DATA_RNR,	/* tx_entry RNR sending data packets */
-	RXR_TX_WAIT_READ_FINISH, /* tx_entry (on initiating EP) wait
-				  * for rx_entry to finish receiving
-				  * (FI_READ only)
-				  */
 };
 
 enum rxr_rx_comm_type {
@@ -269,54 +280,50 @@ enum rxr_rx_comm_type {
 	RXR_RX_UNEXP,		/* rx_entry unexp msg waiting for post recv */
 	RXR_RX_MATCHED,		/* rx_entry matched with RTM */
 	RXR_RX_RECV,		/* rx_entry large msg recv data pkts */
-	RXR_RX_QUEUED_CTRL,	/* rx_entry was unable to send ctrl packet */
-	RXR_RX_QUEUED_EOR,	/* rx_entry was unable to send EOR over shm */
-	RXR_RX_QUEUED_CTS_RNR,	/* rx_entry RNR sending CTS */
+	RXR_RX_QUEUED_CTRL,	/* rx_entry encountered error when sending control
+				   it is in rxr_ep->rx_queued_entry_list, progress
+				   engine will resend the ctrl packet */
 	RXR_RX_WAIT_READ_FINISH, /* rx_entry wait for send to finish, FI_READ */
 	RXR_RX_WAIT_ATOMRSP_SENT, /* rx_entry wait for atomrsp packet sent completion */
 };
 
-enum rxr_rx_buf_owner {
-	RXR_RX_PROV_BUF = 0,	 /* Bounce buffers allocated and owned by provider */
-	RXR_RX_USER_BUF,	 /* Recv buffers posted by applications */
-};
-
 #define RXR_PEER_REQ_SENT BIT_ULL(0) /* sent a REQ to the peer, peer should send a handshake back */
-#define RXR_PEER_HANDSHAKE_SENT BIT_ULL(1)
+#define RXR_PEER_HANDSHAKE_SENT BIT_ULL(1) /* a handshake packet has been sent to a peer */
 #define RXR_PEER_HANDSHAKE_RECEIVED BIT_ULL(2)
 #define RXR_PEER_IN_BACKOFF BIT_ULL(3) /* peer is in backoff, not allowed to send */
-#define RXR_PEER_BACKED_OFF BIT_ULL(4) /* peer backoff was increased during this loop of the progress engine */
-
-struct rxr_fabric {
-	struct util_fabric util_fabric;
-	struct fid_fabric *lower_fabric;
-	struct fid_fabric *shm_fabric;
-#ifdef RXR_PERF_ENABLED
-	struct ofi_perfset perf_set;
-#endif
-};
-
+/*
+ * FI_EAGAIN error was encountered when sending handsahke to this peer,
+ * the peer was put in rxr_ep->handshake_queued_peer_list.
+ * Progress engine will retry sending handshake.
+ */
+#define RXR_PEER_HANDSHAKE_QUEUED      BIT_ULL(5)
 #define RXR_MAX_NUM_PROTOCOLS (RXR_MAX_PROTOCOL_VERSION - RXR_BASE_PROTOCOL_VERSION + 1)
 
-struct rxr_peer {
-	bool tx_init;			/* tracks initialization of tx state */
-	bool rx_init;			/* tracks initialization of rx state */
+struct rdm_peer {
 	bool is_self;			/* self flag */
 	bool is_local;			/* local/remote peer flag */
+	fi_addr_t efa_fiaddr;		/* fi_addr_t addr from efa provider */
 	fi_addr_t shm_fiaddr;		/* fi_addr_t addr from shm provider */
-	struct rxr_robuf *robuf;	/* tracks expected msg_id on rx */
+	struct rxr_robuf robuf;		/* tracks expected msg_id on rx */
+	uint32_t prev_qkey;		/* each peer has unique gid+qpn. the qkey can change */
 	uint32_t next_msg_id;		/* sender's view of msg_id */
 	uint32_t flags;
-	uint32_t maxproto;		/* maximum supported protocol version by this peer */
-	uint64_t features[RXR_MAX_NUM_PROTOCOLS]; /* the feature flag for each version */
-	size_t tx_pending;		/* tracks pending tx ops to this peer */
+	uint32_t nextra_p3;		/* number of members in extra_info plus 3 */
+	uint64_t extra_info[RXR_MAX_NUM_EXINFO]; /* the feature/request flag for each version */
+	size_t efa_outstanding_tx_ops;	/* tracks outstanding tx ops to this peer on EFA device */
+	size_t shm_outstanding_tx_ops;  /* tracks outstanding tx ops to this peer on SHM */
+	struct dlist_entry outstanding_tx_pkts; /* a list of outstanding tx pkts to the peer */
 	uint16_t tx_credits;		/* available send credits */
 	uint16_t rx_credits;		/* available credits to allocate */
-	uint64_t rnr_ts;		/* timestamp for RNR backoff tracking */
+	uint64_t rnr_backoff_begin_ts;	/* timestamp for RNR backoff period begin */
+	uint64_t rnr_backoff_wait_time;	/* how long the RNR backoff period last */
 	int rnr_queued_pkt_cnt;		/* queued RNR packet count */
-	int timeout_interval;		/* initial RNR timeout value */
-	int rnr_timeout_exp;		/* RNR timeout exponentation calc val */
-	struct dlist_entry rnr_entry;	/* linked to rxr_ep peer_backoff_list */
+	struct dlist_entry rnr_backoff_entry;	/* linked to rxr_ep peer_backoff_list */
+	struct dlist_entry handshake_queued_entry; /* linked with rxr_ep->handshake_queued_peer_list */
+	struct dlist_entry rx_unexp_list; /* a list of unexpected untagged rx_entry for this peer */
+	struct dlist_entry rx_unexp_tagged_list; /* a list of unexpected tagged rx_entry for this peer */
+	struct dlist_entry tx_entry_list; /* a list of tx_entry related to this peer */
+	struct dlist_entry rx_entry_list; /* a list of rx_entry relased to this peer */
 };
 
 struct rxr_queued_ctrl_info {
@@ -347,6 +354,7 @@ struct rxr_rx_entry {
 	enum rxr_x_entry_type type;
 
 	fi_addr_t addr;
+	struct rdm_peer *peer;
 
 	/*
 	 * freestack ids used to lookup rx_entry during pkt recv
@@ -387,7 +395,6 @@ struct rxr_rx_entry {
 
 	/* App-provided buffers and descriptors */
 	void *desc[RXR_IOV_LIMIT];
-	enum rxr_rx_buf_owner owner;
 	struct fi_msg *posted_recv;
 
 	/* iov_count on sender side, used for large message READ over shm */
@@ -399,8 +406,13 @@ struct rxr_rx_entry {
 	/* entry is linked with rx entry lists in rxr_ep */
 	struct dlist_entry entry;
 
-	/* queued_entry is linked with rx_queued_ctrl_list in rxr_ep */
-	struct dlist_entry queued_entry;
+	struct dlist_entry peer_unexp_entry; /* linked to peer->rx_unexp_list or peer->rx_unexp_tagged_list */
+
+	/* queued_ctrl_entry is linked with rx_queued_ctrl_list in rxr_ep */
+	struct dlist_entry queued_ctrl_entry;
+
+	/* queued_rnr_entry is linked with rx_queued_rnr_list in rxr_ep */
+	struct dlist_entry queued_rnr_entry;
 
 	/* Queued packets due to TX queue full or RNR backoff */
 	struct dlist_entry queued_pkts;
@@ -419,11 +431,14 @@ struct rxr_rx_entry {
 	struct rxr_pkt_entry *unexp_pkt;
 	char *atomrsp_data;
 
+	/* linked with rx_entry_list in rdm_peer */
+	struct dlist_entry peer_entry;
+
+	/* linked with rx_entry_list in rxr_ep */
+	struct dlist_entry ep_entry;
 #if ENABLE_DEBUG
 	/* linked with rx_pending_list in rxr_ep */
 	struct dlist_entry rx_pending_entry;
-	/* linked with rx_entry_list in rxr_ep */
-	struct dlist_entry rx_entry_entry;
 #endif
 };
 
@@ -433,6 +448,7 @@ struct rxr_tx_entry {
 
 	uint32_t op;
 	fi_addr_t addr;
+	struct rdm_peer *peer;
 
 	/*
 	 * freestack ids used to lookup tx_entry during ctrl pkt recv
@@ -458,7 +474,6 @@ struct rxr_tx_entry {
 	uint64_t fi_flags;
 	uint64_t rxr_flags;
 
-	uint64_t send_flags;
 	size_t iov_count;
 	size_t iov_index;
 	size_t iov_offset;
@@ -485,16 +500,20 @@ struct rxr_tx_entry {
 	/* entry is linked with tx_pending_list in rxr_ep */
 	struct dlist_entry entry;
 
-	/* queued_entry is linked with tx_queued_ctrl_list in rxr_ep */
-	struct dlist_entry queued_entry;
+	/* queued_ctrl_entry is linked with tx_queued_ctrl_list in rxr_ep */
+	struct dlist_entry queued_ctrl_entry;
+
+	/* queued_rnr_entry is linked with tx_queued_rnr_list in rxr_ep */
+	struct dlist_entry queued_rnr_entry;
 
 	/* Queued packets due to TX queue full or RNR backoff */
 	struct dlist_entry queued_pkts;
 
-#if ENABLE_DEBUG
+	/* peer_entry is linked with tx_entry_list in rdm_peer */
+	struct dlist_entry peer_entry;
+
 	/* linked with tx_entry_list in rxr_ep */
-	struct dlist_entry tx_entry_entry;
-#endif
+	struct dlist_entry ep_entry;
 };
 
 #define RXR_GET_X_ENTRY_TYPE(pkt_entry)	\
@@ -524,14 +543,8 @@ struct rxr_ep {
 	uint8_t core_addr[RXR_MAX_NAME_LENGTH];
 	size_t core_addrlen;
 
-	/* per-version feature flag */
-	uint64_t features[RXR_NUM_PROTOCOL_VERSION];
-
-	/* per-peer information */
-	struct rxr_peer *peer;
-
-	/* bufpool for reorder buffer */
-	struct ofi_bufpool *robuf_pool;
+	/* per-version extra feature/request flag */
+	uint64_t extra_info[RXR_MAX_NUM_EXINFO];
 
 	/* core provider fid */
 	struct fid_ep *rdm_ep;
@@ -552,6 +565,7 @@ struct rxr_ep {
 	size_t mtu_size;
 	size_t rx_iov_limit;
 	size_t tx_iov_limit;
+	size_t inject_size;
 
 	/* core's capabilities */
 	uint64_t core_caps;
@@ -564,7 +578,7 @@ struct rxr_ep {
 
 	/* rx/tx queue size of core provider */
 	size_t core_rx_size;
-	size_t max_outstanding_tx;
+	size_t efa_max_outstanding_tx_ops;
 	size_t core_inject_size;
 	size_t max_data_payload_size;
 
@@ -579,6 +593,9 @@ struct rxr_ep {
 	/* Application's maximum msg size hint */
 	size_t max_msg_size;
 
+	/* Applicaiton's message prefix size. */
+	size_t msg_prefix_size;
+
 	/* RxR protocol's max header size */
 	size_t max_proto_hdr_size;
 
@@ -589,15 +606,21 @@ struct rxr_ep {
 	size_t min_multi_recv_size;
 
 	/* buffer pool for send & recv */
-	struct ofi_bufpool *tx_pkt_efa_pool;
-	struct ofi_bufpool *rx_pkt_efa_pool;
+	struct ofi_bufpool *efa_tx_pkt_pool;
+	struct ofi_bufpool *efa_rx_pkt_pool;
+
+	/*
+	 * buffer pool for rxr_pkt_sendv struct, which is used
+	 * to store iovec related information
+	 */
+	struct ofi_bufpool *pkt_sendv_pool;
 
 	/*
 	 * buffer pool for send & recv for shm as mtu size is different from
 	 * the one of efa, and do not require local memory registration
 	 */
-	struct ofi_bufpool *tx_pkt_shm_pool;
-	struct ofi_bufpool *rx_pkt_shm_pool;
+	struct ofi_bufpool *shm_tx_pkt_pool;
+	struct ofi_bufpool *shm_rx_pkt_pool;
 
 	/* staging area for unexpected and out-of-order packets */
 	struct ofi_bufpool *rx_unexp_pkt_pool;
@@ -641,16 +664,22 @@ struct rxr_ep {
 	struct dlist_entry rx_posted_buf_list;
 	/* list of pre-posted recv buffers for shm */
 	struct dlist_entry rx_posted_buf_shm_list;
-	/* tx entries with queued messages */
-	struct dlist_entry tx_entry_queued_list;
-	/* rx entries with queued messages */
-	struct dlist_entry rx_entry_queued_list;
+	/* tx entries with queued ctrl packets */
+	struct dlist_entry tx_entry_queued_ctrl_list;
+	/* tx entries with queued rnr packets */
+	struct dlist_entry tx_entry_queued_rnr_list;
+	/* rx entries with queued ctrl packets */
+	struct dlist_entry rx_entry_queued_ctrl_list;
+	/* rx entries with queued rnr packets */
+	struct dlist_entry rx_entry_queued_rnr_list;
 	/* tx_entries with data to be sent (large messages) */
 	struct dlist_entry tx_pending_list;
 	/* read entries with data to be read */
 	struct dlist_entry read_pending_list;
 	/* rxr_peer entries that are in backoff due to RNR */
 	struct dlist_entry peer_backoff_list;
+	/* rxr_peer entries that will retry posting handshake pkt */
+	struct dlist_entry handshake_queued_peer_list;
 
 #if ENABLE_DEBUG
 	/* rx_entries waiting for data to arrive (large messages) */
@@ -664,29 +693,43 @@ struct rxr_ep {
 	/* tx packets waiting for send completion */
 	struct dlist_entry tx_pkt_list;
 
-	/* track allocated rx_entries and tx_entries for endpoint cleanup */
-	struct dlist_entry rx_entry_list;
-	struct dlist_entry tx_entry_list;
-
-	size_t sends;
+	size_t efa_total_posted_tx_ops;
+	size_t shm_total_posted_tx_ops;
 	size_t send_comps;
 	size_t failed_send_comps;
 	size_t recv_comps;
 #endif
-	/* number of posted buffer for shm */
-	size_t posted_bufs_shm;
-	size_t rx_bufs_shm_to_post;
+	/* track allocated rx_entries and tx_entries for endpoint cleanup */
+	struct dlist_entry rx_entry_list;
+	struct dlist_entry tx_entry_list;
 
-	/* number of posted buffers */
-	size_t posted_bufs_efa;
-	size_t rx_bufs_efa_to_post;
+	/*
+	 * number of posted RX packets for shm
+	 */
+	size_t shm_rx_pkts_posted;
+	/*
+	 * number of RX packets to be posted by progress engine for shm.
+	 * It exists because posting RX packets by bulk is more efficient.
+	 */
+	size_t shm_rx_pkts_to_post;
+	/*
+	 * number of posted RX packets for EFA device
+	 */
+	size_t efa_rx_pkts_posted;
+	/*
+	 * Number of RX packets to be posted by progress engine for EFA device.
+	 * It exists because posting RX packets by bulk is more efficient.
+	 */
+	size_t efa_rx_pkts_to_post;
 	/* number of buffers available for large messages */
 	size_t available_data_bufs;
 	/* Timestamp of when available_data_bufs was exhausted. */
 	uint64_t available_data_bufs_ts;
 
-	/* number of outstanding sends */
-	size_t tx_pending;
+	/* number of outstanding tx ops on efa device */
+	size_t efa_outstanding_tx_ops;
+	/* number of outstanding tx ops on shm */
+	size_t shm_outstanding_tx_ops;
 };
 
 #define rxr_rx_flags(rxr_ep) ((rxr_ep)->util_ep.rx_op_flags)
@@ -708,11 +751,6 @@ static inline void rxr_copy_shm_cq_entry(struct fi_cq_tagged_entry *cq_tagged_en
 	cq_tagged_entry->tag = 0; // No tag for RMA;
 
 }
-static inline struct rxr_peer *rxr_ep_get_peer(struct rxr_ep *ep,
-					       fi_addr_t addr)
-{
-	return &ep->peer[addr];
-}
 
 static inline void rxr_setup_msg(struct fi_msg *msg, const struct iovec *iov, void **desc,
 				 size_t count, fi_addr_t addr, void *context, uint32_t data)
@@ -725,25 +763,6 @@ static inline void rxr_setup_msg(struct fi_msg *msg, const struct iovec *iov, vo
 	msg->data = data;
 }
 
-static inline void rxr_ep_peer_init_rx(struct rxr_ep *ep, struct rxr_peer *peer)
-{
-	assert(!peer->rx_init);
-
-	peer->robuf = ofi_buf_alloc(ep->robuf_pool);
-	assert(peer->robuf);
-	peer->robuf = ofi_recvwin_buf_alloc(peer->robuf,
-					    rxr_env.recvwin_size);
-	peer->rx_credits = rxr_env.rx_window_size;
-	peer->rx_init = 1;
-}
-
-static inline void rxr_ep_peer_init_tx(struct rxr_peer *peer)
-{
-	assert(!peer->tx_init);
-	peer->tx_credits = rxr_env.tx_max_credits;
-	peer->tx_init = 1;
-}
-
 struct efa_ep_addr *rxr_ep_raw_addr(struct rxr_ep *ep);
 
 const char *rxr_ep_raw_addr_str(struct rxr_ep *ep, char *buf, size_t *buflen);
@@ -778,13 +797,31 @@ struct rxr_tx_entry *rxr_ep_alloc_tx_entry(struct rxr_ep *rxr_ep,
 
 void rxr_release_tx_entry(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry);
 
+struct rxr_rx_entry *rxr_ep_alloc_rx_entry(struct rxr_ep *ep,
+					   fi_addr_t addr, uint32_t op);
+
 static inline void rxr_release_rx_entry(struct rxr_ep *ep,
 					struct rxr_rx_entry *rx_entry)
 {
-#if ENABLE_DEBUG
-	dlist_remove(&rx_entry->rx_entry_entry);
-#endif
-	assert(dlist_empty(&rx_entry->queued_pkts));
+	struct rxr_pkt_entry *pkt_entry;
+	struct dlist_entry *tmp;
+
+	if (rx_entry->peer)
+		dlist_remove(&rx_entry->peer_entry);
+
+	dlist_remove(&rx_entry->ep_entry);
+
+	if (!dlist_empty(&rx_entry->queued_pkts)) {
+		dlist_foreach_container_safe(&rx_entry->queued_pkts,
+					     struct rxr_pkt_entry,
+					     pkt_entry, entry, tmp) {
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		}
+		dlist_remove(&rx_entry->queued_rnr_entry);
+	} else if (rx_entry->state == RXR_RX_QUEUED_CTRL) {
+		dlist_remove(&rx_entry->queued_ctrl_entry);
+	}
+
 #ifdef ENABLE_EFA_POISONING
 	rxr_poison_mem_region((uint32_t *)rx_entry,
 			      sizeof(struct rxr_rx_entry));
@@ -804,27 +841,9 @@ static inline int rxr_match_tag(uint64_t tag, uint64_t ignore,
 	return ((tag | ignore) == (match_tag | ignore));
 }
 
-static inline void rxr_ep_inc_tx_pending(struct rxr_ep *ep,
-					 struct rxr_peer *peer)
-{
-	ep->tx_pending++;
-	peer->tx_pending++;
-#if ENABLE_DEBUG
-	ep->sends++;
-#endif
-}
+void rxr_ep_record_tx_op_submitted(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
-static inline void rxr_ep_dec_tx_pending(struct rxr_ep *ep,
-					 struct rxr_peer *peer,
-					 int failed)
-{
-	ep->tx_pending--;
-	peer->tx_pending--;
-#if ENABLE_DEBUG
-	if (failed)
-		ep->failed_send_comps++;
-#endif
-}
+void rxr_ep_record_tx_op_completed(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
 static inline size_t rxr_get_rx_pool_chunk_cnt(struct rxr_ep *ep)
 {
@@ -833,7 +852,7 @@ static inline size_t rxr_get_rx_pool_chunk_cnt(struct rxr_ep *ep)
 
 static inline size_t rxr_get_tx_pool_chunk_cnt(struct rxr_ep *ep)
 {
-	return MIN(ep->max_outstanding_tx, ep->tx_size);
+	return MIN(ep->efa_max_outstanding_tx_ops, ep->tx_size);
 }
 
 static inline int rxr_need_sas_ordering(struct rxr_ep *ep)
@@ -859,8 +878,6 @@ int rxr_get_lower_rdm_info(uint32_t version, const char *node, const char *servi
 			   uint64_t flags, const struct util_prov *util_prov,
 			   const struct fi_info *util_hints,
 			   struct fi_info **core_info);
-int rxr_fabric(struct fi_fabric_attr *attr,
-	       struct fid_fabric **fabric, void *context);
 int rxr_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 		    struct fid_domain **dom, void *context);
 int rxr_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
@@ -871,8 +888,9 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 /* EP sub-functions */
 void rxr_ep_progress(struct util_ep *util_ep);
 void rxr_ep_progress_internal(struct rxr_ep *rxr_ep);
-int rxr_ep_post_buf(struct rxr_ep *ep, const struct fi_msg *posted_recv,
-		    uint64_t flags, enum rxr_lower_ep_type lower_ep);
+
+int rxr_ep_post_user_recv_buf(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+			      uint64_t flags);
 
 int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep,
 				 struct rxr_tx_entry *tx_entry);
@@ -881,6 +899,8 @@ int rxr_ep_tx_init_mr_desc(struct rxr_domain *rxr_domain,
 			   struct rxr_tx_entry *tx_entry,
 			   int mr_iov_start, uint64_t access);
 
+void rxr_convert_desc_for_shm(int numdesc, void **desc);
+
 void rxr_prepare_desc_send(struct rxr_domain *rxr_domain,
 			   struct rxr_tx_entry *tx_entry);
 
@@ -901,20 +921,23 @@ struct rxr_rx_entry *rxr_ep_split_rx_entry(struct rxr_ep *ep,
 					   struct rxr_rx_entry *posted_entry,
 					   struct rxr_rx_entry *consumer_entry,
 					   struct rxr_pkt_entry *pkt_entry);
-int rxr_ep_efa_addr_to_str(const void *addr, char *temp_name);
+int rxr_raw_addr_to_smr_name(void *addr, char *smr_name);
 
 /* CQ sub-functions */
-int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-			   ssize_t prov_errno);
-int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
-			   ssize_t prov_errno);
-int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err);
+void rxr_cq_write_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+			   int err, int prov_errno);
+
+void rxr_cq_write_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
+			   int err, int prov_errno);
+
+void rxr_cq_queue_rnr_pkt(struct rxr_ep *ep,
+			  struct dlist_entry *list,
+			  struct rxr_pkt_entry *pkt_entry);
 
 void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 				struct rxr_rx_entry *rx_entry);
 
 void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
-				 struct rxr_pkt_entry *pkt_entry,
 				 struct rxr_rx_entry *rx_entry);
 
 void rxr_cq_write_tx_completion(struct rxr_ep *ep,
@@ -928,11 +951,11 @@ void rxr_cq_handle_shm_completion(struct rxr_ep *ep,
 				  fi_addr_t src_addr);
 
 int rxr_cq_reorder_msg(struct rxr_ep *ep,
-		       struct rxr_peer *peer,
+		       struct rdm_peer *peer,
 		       struct rxr_pkt_entry *pkt_entry);
 
 void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
-					  struct rxr_peer *peer);
+					  struct rdm_peer *peer);
 
 void rxr_cq_handle_shm_rma_write_data(struct rxr_ep *ep,
 				      struct fi_cq_data_entry *shm_comp,
@@ -1009,47 +1032,4 @@ static inline void rxr_rm_tx_cq_check(struct rxr_ep *ep, struct util_cq *tx_cq)
 	fastlock_release(&tx_cq->cq_lock);
 }
 
-static inline bool rxr_peer_timeout_expired(struct rxr_ep *ep,
-					    struct rxr_peer *peer,
-					    uint64_t ts)
-{
-	return (ts >= (peer->rnr_ts + MIN(rxr_env.max_timeout,
-					  peer->timeout_interval *
-					  (1 << peer->rnr_timeout_exp))));
-}
-
-/* Performance counter declarations */
-#ifdef RXR_PERF_ENABLED
-#define RXR_PERF_FOREACH(DECL)	\
-	DECL(perf_rxr_tx),	\
-	DECL(perf_rxr_recv),	\
-	DECL(rxr_perf_size)	\
-
-enum rxr_perf_counters {
-	RXR_PERF_FOREACH(OFI_ENUM_VAL)
-};
-
-extern const char *rxr_perf_counters_str[];
-
-static inline void rxr_perfset_start(struct rxr_ep *ep, size_t index)
-{
-	struct rxr_domain *domain = rxr_ep_domain(ep);
-	struct rxr_fabric *fabric = container_of(domain->util_domain.fabric,
-						 struct rxr_fabric,
-						 util_fabric);
-	ofi_perfset_start(&fabric->perf_set, index);
-}
-
-static inline void rxr_perfset_end(struct rxr_ep *ep, size_t index)
-{
-	struct rxr_domain *domain = rxr_ep_domain(ep);
-	struct rxr_fabric *fabric = container_of(domain->util_domain.fabric,
-						 struct rxr_fabric,
-						 util_fabric);
-	ofi_perfset_end(&fabric->perf_set, index);
-}
-#else
-#define rxr_perfset_start(ep, index) do {} while (0)
-#define rxr_perfset_end(ep, index) do {} while (0)
-#endif
 #endif
diff --git a/prov/efa/src/rxr/rxr_atomic.c b/prov/efa/src/rxr/rxr_atomic.c
index c1c16ee..4fcf520 100644
--- a/prov/efa/src/rxr/rxr_atomic.c
+++ b/prov/efa/src/rxr/rxr_atomic.c
@@ -39,9 +39,10 @@
 #include "rxr_atomic.h"
 #include "rxr_pkt_cmd.h"
 
-static void rxr_atomic_copy_shm_msg(struct fi_msg_atomic *shm_msg,
+static void rxr_atomic_init_shm_msg(struct fi_msg_atomic *shm_msg,
 				    const struct fi_msg_atomic *msg,
-				    struct fi_rma_ioc *rma_iov)
+				    struct fi_rma_ioc *rma_iov,
+				    void **shm_desc)
 {
 	int i;
 
@@ -54,6 +55,14 @@ static void rxr_atomic_copy_shm_msg(struct fi_msg_atomic *shm_msg,
 			rma_iov[i].addr = 0;
 		shm_msg->rma_iov = rma_iov;
 	}
+
+	if (msg->desc) {
+		memcpy(shm_desc, msg->desc, msg->iov_count * sizeof(void *));
+		rxr_convert_desc_for_shm(msg->iov_count, shm_desc);
+		shm_msg->desc = shm_desc;
+	} else {
+		shm_msg->desc = NULL;
+	}
 }
 
 static
@@ -66,7 +75,12 @@ rxr_atomic_alloc_tx_entry(struct rxr_ep *rxr_ep,
 	struct rxr_tx_entry *tx_entry;
 	struct fi_msg msg;
 	struct iovec iov[RXR_IOV_LIMIT];
-	size_t datatype_size = ofi_datatype_size(msg_atomic->datatype);
+	size_t datatype_size;
+
+	datatype_size = ofi_datatype_size(msg_atomic->datatype);
+	if (OFI_UNLIKELY(!datatype_size)) {
+		return NULL;
+	}
 
 	tx_entry = ofi_buf_alloc(rxr_ep->tx_entry_pool);
 	if (OFI_UNLIKELY(!tx_entry)) {
@@ -74,9 +88,8 @@ rxr_atomic_alloc_tx_entry(struct rxr_ep *rxr_ep,
 		return NULL;
 	}
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
+
 	ofi_ioc_to_iov(msg_atomic->msg_iov, iov, msg_atomic->iov_count, datatype_size);
 	msg.addr = msg_atomic->addr;
 	msg.msg_iov = iov;
@@ -112,7 +125,7 @@ ssize_t rxr_atomic_generic_efa(struct rxr_ep *rxr_ep,
 			       uint32_t op, uint64_t flags)
 {
 	struct rxr_tx_entry *tx_entry;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	bool delivery_complete_requested;
 	ssize_t err;
 	static int req_pkt_type_list[] = {
@@ -122,7 +135,7 @@ ssize_t rxr_atomic_generic_efa(struct rxr_ep *rxr_ep,
 	};
 
 	assert(msg->iov_count <= rxr_ep->tx_iov_limit);
-	rxr_perfset_start(rxr_ep, perf_rxr_tx);
+	efa_perfset_start(rxr_ep, perf_efa_tx);
 	fastlock_acquire(&rxr_ep->util_ep.lock);
 
 	if (OFI_UNLIKELY(is_tx_res_full(rxr_ep))) {
@@ -131,6 +144,7 @@ ssize_t rxr_atomic_generic_efa(struct rxr_ep *rxr_ep,
 	}
 
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 
 	if (peer->flags & RXR_PEER_IN_BACKOFF) {
 		err = -FI_EAGAIN;
@@ -182,6 +196,7 @@ ssize_t rxr_atomic_generic_efa(struct rxr_ep *rxr_ep,
 		err = rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY,
 					tx_entry,
 					RXR_DC_WRITE_RTA_PKT,
+					0,
 					0);
 	} else {
 		/*
@@ -192,6 +207,7 @@ ssize_t rxr_atomic_generic_efa(struct rxr_ep *rxr_ep,
 		err = rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY,
 					tx_entry,
 					req_pkt_type_list[op],
+					0,
 					0);
 	}
 
@@ -202,7 +218,7 @@ ssize_t rxr_atomic_generic_efa(struct rxr_ep *rxr_ep,
 
 out:
 	fastlock_release(&rxr_ep->util_ep.lock);
-	rxr_perfset_end(rxr_ep, perf_rxr_tx);
+	efa_perfset_end(rxr_ep, perf_efa_tx);
 	return err;
 }
 
@@ -217,14 +233,16 @@ rxr_atomic_inject(struct fid_ep *ep,
 	struct fi_msg_atomic msg;
 
 	struct rxr_ep *rxr_ep;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	peer = rxr_ep_get_peer(rxr_ep, dest_addr);
+	assert(peer);
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
 		if (!(shm_info->domain_attr->mr_mode & FI_MR_VIRT_ADDR))
 			remote_addr = 0;
+
 		return fi_inject_atomic(rxr_ep->shm_ep, buf, count, peer->shm_fiaddr,
 					remote_addr, remote_key, datatype, op);
 	}
@@ -258,8 +276,9 @@ rxr_atomic_writemsg(struct fid_ep *ep,
 {
 	struct fi_msg_atomic shm_msg;
 	struct rxr_ep *rxr_ep;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct fi_rma_ioc rma_iov[RXR_IOV_LIMIT];
+	void *shm_desc[RXR_IOV_LIMIT];
 
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 	       "%s: iov_len: %lu flags: %lx\n",
@@ -267,9 +286,10 @@ rxr_atomic_writemsg(struct fid_ep *ep,
 
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
-		rxr_atomic_copy_shm_msg(&shm_msg, msg, rma_iov);
+		rxr_atomic_init_shm_msg(&shm_msg, msg, rma_iov, shm_desc);
 		shm_msg.addr = peer->shm_fiaddr;
 		return fi_atomicmsg(rxr_ep->shm_ep, &shm_msg, flags);
 	}
@@ -329,20 +349,27 @@ rxr_atomic_readwritemsg(struct fid_ep *ep,
 			uint64_t flags)
 {
 	struct rxr_ep *rxr_ep;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct fi_msg_atomic shm_msg;
-	struct fi_rma_ioc rma_iov[RXR_IOV_LIMIT];
+	struct fi_rma_ioc shm_rma_iov[RXR_IOV_LIMIT];
+	void *shm_desc[RXR_IOV_LIMIT];
 	struct rxr_atomic_ex atomic_ex;
-	size_t datatype_size = ofi_datatype_size(msg->datatype);
+	size_t datatype_size;
+
+	datatype_size = ofi_datatype_size(msg->datatype);
+	if (OFI_UNLIKELY(!datatype_size)) {
+		return -errno;
+	}
 
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s total_len=%ld atomic_op=%d\n", __func__,
 	       ofi_total_ioc_cnt(msg->msg_iov, msg->iov_count), msg->op);
 
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
-		rxr_atomic_copy_shm_msg(&shm_msg, msg, rma_iov);
+		rxr_atomic_init_shm_msg(&shm_msg, msg, shm_rma_iov, shm_desc);
 		shm_msg.addr = peer->shm_fiaddr;
 		return fi_fetch_atomicmsg(rxr_ep->shm_ep, &shm_msg,
 					  resultv, result_desc, result_count,
@@ -410,11 +437,17 @@ rxr_atomic_compwritemsg(struct fid_ep *ep,
 			uint64_t flags)
 {
 	struct rxr_ep *rxr_ep;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct fi_msg_atomic shm_msg;
-	struct fi_rma_ioc rma_iov[RXR_IOV_LIMIT];
+	struct fi_rma_ioc shm_rma_iov[RXR_IOV_LIMIT];
+	void *shm_desc[RXR_IOV_LIMIT];
 	struct rxr_atomic_ex atomic_ex;
-	size_t datatype_size = ofi_datatype_size(msg->datatype);
+	size_t datatype_size;
+
+	datatype_size = ofi_datatype_size(msg->datatype);
+	if (OFI_UNLIKELY(!datatype_size)) {
+		return -errno;
+	}
 
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 	       "%s: iov_len: %lu flags: %lx\n",
@@ -422,9 +455,10 @@ rxr_atomic_compwritemsg(struct fid_ep *ep,
 
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
-		rxr_atomic_copy_shm_msg(&shm_msg, msg, rma_iov);
+		rxr_atomic_init_shm_msg(&shm_msg, msg, shm_rma_iov, shm_desc);
 		shm_msg.addr = peer->shm_fiaddr;
 		return fi_compare_atomicmsg(rxr_ep->shm_ep, &shm_msg,
 					    comparev, compare_desc, compare_count,
@@ -526,6 +560,9 @@ int rxr_query_atomic(struct fid_domain *domain,
 		max_atomic_size /= 2;
 
 	attr->size = ofi_datatype_size(datatype);
+	if (OFI_UNLIKELY(!attr->size)) {
+		return -errno;
+	}
 	attr->count = max_atomic_size / attr->size;
 	return 0;
 }
diff --git a/prov/efa/src/rxr/rxr_attr.c b/prov/efa/src/rxr/rxr_attr.c
index 77b2a9e..9934a63 100644
--- a/prov/efa/src/rxr/rxr_attr.c
+++ b/prov/efa/src/rxr/rxr_attr.c
@@ -82,7 +82,7 @@ struct fi_ep_attr rxr_ep_attr = {
 	.type = FI_EP_RDM,
 	.protocol = FI_PROTO_EFA,
 	.mem_tag_format = FI_TAG_GENERIC,
-	.protocol_version = RXR_CUR_PROTOCOL_VERSION,
+	.protocol_version = RXR_PROTOCOL_VERSION,
 	.max_msg_size = UINT64_MAX,
 	.msg_prefix_size = 0,
 	.tx_ctx_cnt = 1,
diff --git a/prov/efa/src/rxr/rxr_cq.c b/prov/efa/src/rxr/rxr_cq.c
index 0e03c02..74167aa 100644
--- a/prov/efa/src/rxr/rxr_cq.c
+++ b/prov/efa/src/rxr/rxr_cq.c
@@ -67,32 +67,42 @@ static const char *rxr_cq_strerror(struct fid_cq *cq_fid, int prov_errno,
 	return str;
 }
 
-/*
- * Teardown rx_entry and write an error cq entry. With our current protocol we
- * will only encounter an RX error when sending a queued REQ or CTS packet or
- * if we are sending a CTS message. Because of this, the sender will not send
- * any additional data packets if the receiver encounters an error. If there is
- * a scenario in the future where the sender will continue to send data packets
- * we need to prevent rx_id mismatch. Ideally, we should add a NACK message and
- * tear down both RX and TX entires although whatever caused the error may
- * prevent that.
+/**
+ * @brief handle error happened to an RX (receive) operation
+ *
+ * This function will write an error cq entry to notify application the rx
+ * operation failed. If write failed, it will write an eq entry.
+ *
+ * It will also release resources owned by the RX entry, such as unexpected
+ * packet entry, because the RX operation is aborted.
+ *
+ * It will remove the rx_entry from queued rx_entry list for the same reason.
+ *
+ * It will NOT release the rx_entry because it is still possible to receive
+ * packet for this rx_entry.
  *
  * TODO: add a NACK message to tear down state on sender side
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	rx_entry	rx_entry that contains information of the tx operation
+ * @param[in]	err		positive libfabric error code
+ * @param[in]	prov_errno	positive provider specific error code
  */
-int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-			   ssize_t prov_errno)
+void rxr_cq_write_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+			   int err, int prov_errno)
 {
 	struct fi_cq_err_entry err_entry;
 	struct util_cq *util_cq;
 	struct dlist_entry *tmp;
 	struct rxr_pkt_entry *pkt_entry;
+	int write_cq_err;
 
 	memset(&err_entry, 0, sizeof(err_entry));
 
 	util_cq = ep->util_ep.rx_cq;
 
-	err_entry.err = FI_EIO;
-	err_entry.prov_errno = (int)prov_errno;
+	err_entry.err = err;
+	err_entry.prov_errno = prov_errno;
 
 	switch (rx_entry->state) {
 	case RXR_RX_INIT:
@@ -107,9 +117,7 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 #endif
 		break;
 	case RXR_RX_QUEUED_CTRL:
-	case RXR_RX_QUEUED_CTS_RNR:
-	case RXR_RX_QUEUED_EOR:
-		dlist_remove(&rx_entry->queued_entry);
+		dlist_remove(&rx_entry->queued_ctrl_entry);
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_CQ, "rx_entry unknown state %d\n",
@@ -117,10 +125,13 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 		assert(0 && "rx_entry unknown state");
 	}
 
-	dlist_foreach_container_safe(&rx_entry->queued_pkts,
-				     struct rxr_pkt_entry,
-				     pkt_entry, entry, tmp)
-		rxr_pkt_entry_release_tx(ep, pkt_entry);
+	if (rx_entry->rxr_flags & RXR_RX_ENTRY_QUEUED_RNR) {
+		dlist_foreach_container_safe(&rx_entry->queued_pkts,
+					     struct rxr_pkt_entry,
+					     pkt_entry, entry, tmp)
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		dlist_remove(&rx_entry->queued_rnr_entry);
+	}
 
 	if (rx_entry->unexp_pkt) {
 		rxr_pkt_entry_release_rx(ep, rx_entry->unexp_pkt);
@@ -140,7 +151,7 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 	rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
 
         FI_WARN(&rxr_prov, FI_LOG_CQ,
-		"rxr_cq_handle_rx_error: err: %d, prov_err: %s (%d)\n",
+		"rxr_cq_write_rx_error: err: %d, prov_err: %s (%d)\n",
 		err_entry.err, fi_strerror(-err_entry.prov_errno),
 		err_entry.prov_errno);
 
@@ -152,35 +163,50 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 	//rxr_release_rx_entry(ep, rx_entry);
 
 	efa_cntr_report_error(&ep->util_ep, err_entry.flags);
-	return ofi_cq_write_error(util_cq, &err_entry);
+	write_cq_err = ofi_cq_write_error(util_cq, &err_entry);
+	if (write_cq_err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Error writing error cq entry when handling RX error");
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	}
 }
 
-/*
- * Teardown tx_entry and write an error cq entry. With our current protocol the
- * receiver will only send a CTS once the window is exhausted, meaning that all
- * data packets for that window will have been received successfully. This
- * means that the receiver will not send any CTS packets if the sender
- * encounters and error sending data packets. If that changes in the future we
- * will need to be careful to prevent tx_id mismatch.
+/**
+ * @brief write error CQ entry for a TX operation.
+ *
+ * This function write an error cq entry for a TX operation, if writing
+ * CQ error entry failed, it will write eq entry.
+ *
+ * If also remote the TX entry from ep->tx_queued_list and ep->tx_pending_list
+ * if the tx_entry is on it.
+ *
+ * It does NOT release tx entry because it is still possible to receive
+ * send completion for this TX entry
  *
  * TODO: add NACK message to tear down receive side state
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	tx_entry	tx_entry that contains information of the tx operation
+ * @param[in]	err		positive libfabric error code
+ * @param[in]	prov_errno	positive EFA provider specific error code
  */
-int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
-			   ssize_t prov_errno)
+void rxr_cq_write_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
+			   int err, int prov_errno)
 {
 	struct fi_cq_err_entry err_entry;
 	struct util_cq *util_cq;
 	uint32_t api_version;
 	struct dlist_entry *tmp;
 	struct rxr_pkt_entry *pkt_entry;
+	int write_cq_err;
 
 	memset(&err_entry, 0, sizeof(err_entry));
 
 	util_cq = ep->util_ep.tx_cq;
 	api_version = util_cq->domain->fabric->fabric_fid.api_version;
 
-	err_entry.err = FI_EIO;
-	err_entry.prov_errno = (int)prov_errno;
+	err_entry.err = err;
+	err_entry.prov_errno = prov_errno;
 
 	switch (tx_entry->state) {
 	case RXR_TX_REQ:
@@ -190,11 +216,7 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 		break;
 	case RXR_TX_QUEUED_CTRL:
 	case RXR_TX_QUEUED_SHM_RMA:
-	case RXR_TX_QUEUED_REQ_RNR:
-	case RXR_TX_QUEUED_DATA_RNR:
-		dlist_remove(&tx_entry->queued_entry);
-		break;
-	case RXR_TX_WAIT_READ_FINISH:
+		dlist_remove(&tx_entry->queued_ctrl_entry);
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_CQ, "tx_entry unknown state %d\n",
@@ -202,6 +224,9 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 		assert(0 && "tx_entry unknown state");
 	}
 
+	if (tx_entry->rxr_flags & RXR_TX_ENTRY_QUEUED_RNR)
+		dlist_remove(&tx_entry->queued_rnr_entry);
+
 	dlist_foreach_container_safe(&tx_entry->queued_pkts,
 				     struct rxr_pkt_entry,
 				     pkt_entry, entry, tmp)
@@ -216,7 +241,7 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 		err_entry.err_data_size = 0;
 
 	FI_WARN(&rxr_prov, FI_LOG_CQ,
-		"rxr_cq_handle_tx_error: err: %d, prov_err: %s (%d)\n",
+		"rxr_cq_write_tx_error: err: %d, prov_err: %s (%d)\n",
 		err_entry.err, fi_strerror(-err_entry.prov_errno),
 		err_entry.prov_errno);
 
@@ -228,223 +253,122 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	//rxr_release_tx_entry(ep, tx_entry);
 
 	efa_cntr_report_error(&ep->util_ep, tx_entry->cq_entry.flags);
-	return ofi_cq_write_error(util_cq, &err_entry);
+	write_cq_err = ofi_cq_write_error(util_cq, &err_entry);
+	if (write_cq_err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Error writing error cq entry when handling TX error");
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	}
 }
 
-/*
- * Queue a packet on the appropriate list when an RNR error is received.
+/* @brief Queue a packet that encountered RNR error and setup RNR backoff
+ *
+ * We uses an exponential backoff strategy to handle RNR errors.
+ *
+ * `Backoff` means if a peer encountered RNR, an endpoint will
+ * wait a period of time before sending packets to the peer again
+ *
+ * `Exponential` means the more RNR encountered, the longer the
+ * backoff wait time will be.
+ *
+ * To quantify how long a peer stay in backoff mode, two parameters
+ * are defined:
+ *
+ *    rnr_backoff_begin_ts (ts is timestamp) and rnr_backoff_wait_time.
+ *
+ * A peer stays in backoff mode until:
+ *
+ * current_timestamp >= (rnr_backoff_begin_ts + rnr_backoff_wait_time),
+ *
+ * with one exception: a peer can got out of backoff mode early if a
+ * packet's send completion to this peer was reported by the device.
+ *
+ * Specifically, the implementation of RNR backoff is:
+ *
+ * For a peer, the first time RNR is encountered, the packet will
+ * be resent immediately.
+ *
+ * The second time RNR is encountered, the endpoint will put the
+ * peer in backoff mode, and initialize rnr_backoff_begin_timestamp
+ * and rnr_backoff_wait_time.
+ *
+ * The 3rd and following time RNR is encounter, the RNR will be handled
+ * like this:
+ *
+ *     If peer is already in backoff mode, rnr_backoff_begin_ts
+ *     will be updated
+ *
+ *     Otherwise, peer will be put in backoff mode again,
+ *     rnr_backoff_begin_ts will be updated and rnr_backoff_wait_time
+ *     will be doubled until it reached maximum wait time.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	list		queued RNR packet list
+ * @param[in]	pkt_entry	packet entry that encounter RNR
  */
-static inline void rxr_cq_queue_pkt(struct rxr_ep *ep,
-				    struct dlist_entry *list,
-				    struct rxr_pkt_entry *pkt_entry)
+void rxr_cq_queue_rnr_pkt(struct rxr_ep *ep,
+			  struct dlist_entry *list,
+			  struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+#if ENABLE_DEBUG
+	dlist_remove(&pkt_entry->dbg_entry);
+#endif
+	dlist_insert_tail(&pkt_entry->entry, list);
 
-	/*
-	 * Queue the packet if it has not been retransmitted yet.
-	 */
-	if (pkt_entry->state != RXR_PKT_ENTRY_RNR_RETRANSMIT) {
-		pkt_entry->state = RXR_PKT_ENTRY_RNR_RETRANSMIT;
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
+	if (!(pkt_entry->flags & RXR_PKT_ENTRY_RNR_RETRANSMIT)) {
+		/* This is the first time this packet encountered RNR,
+		 * we are NOT going to put the peer in backoff mode just yet.
+		 */
+		pkt_entry->flags |= RXR_PKT_ENTRY_RNR_RETRANSMIT;
 		peer->rnr_queued_pkt_cnt++;
-		goto queue_pkt;
+		return;
 	}
 
-	/*
-	 * Otherwise, increase the backoff if the peer is already not in
-	 * backoff. Reset the timer when starting backoff or if another RNR for
-	 * a retransmitted packet is received while waiting for the timer to
-	 * expire.
+	/* This packet has encountered RNR multiple times, therefore the peer
+	 * need to be in backoff mode.
+	 *
+	 * If the peer is already in backoff mode, we just need to update the
+	 * RNR backoff begin time.
+	 *
+	 * Otherwise, we need to put the peer in backoff mode and set up backoff
+	 * begin time and wait time.
 	 */
-	peer->rnr_ts = ofi_gettime_us();
-	if (peer->flags & RXR_PEER_IN_BACKOFF)
-		goto queue_pkt;
+	if (peer->flags & RXR_PEER_IN_BACKOFF) {
+		peer->rnr_backoff_begin_ts = ofi_gettime_us();
+		return;
+	}
 
 	peer->flags |= RXR_PEER_IN_BACKOFF;
+	dlist_insert_tail(&peer->rnr_backoff_entry,
+			  &ep->peer_backoff_list);
 
-	if (!peer->timeout_interval) {
-		if (rxr_env.timeout_interval)
-			peer->timeout_interval = rxr_env.timeout_interval;
+	peer->rnr_backoff_begin_ts = ofi_gettime_us();
+	if (peer->rnr_backoff_wait_time == 0) {
+		if (rxr_env.rnr_backoff_initial_wait_time > 0)
+			peer->rnr_backoff_wait_time = rxr_env.rnr_backoff_initial_wait_time;
 		else
-			peer->timeout_interval = MAX(RXR_RAND_MIN_TIMEOUT,
-						     rand() %
-						     RXR_RAND_MAX_TIMEOUT);
+			peer->rnr_backoff_wait_time = MAX(RXR_RAND_MIN_TIMEOUT,
+							  rand() %
+							  RXR_RAND_MAX_TIMEOUT);
 
-		peer->rnr_timeout_exp = 1;
 		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 		       "initializing backoff timeout for peer: %" PRIu64
-		       " timeout: %d rnr_queued_pkts: %d\n",
-		       pkt_entry->addr, peer->timeout_interval,
+		       " timeout: %ld rnr_queued_pkts: %d\n",
+		       pkt_entry->addr, peer->rnr_backoff_wait_time,
 		       peer->rnr_queued_pkt_cnt);
 	} else {
-		/* Only backoff once per peer per progress thread loop. */
-		if (!(peer->flags & RXR_PEER_BACKED_OFF)) {
-			peer->flags |= RXR_PEER_BACKED_OFF;
-			peer->rnr_timeout_exp++;
-			FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-			       "increasing backoff for peer: %" PRIu64
-			       " rnr_timeout_exp: %d rnr_queued_pkts: %d\n",
-			       pkt_entry->addr, peer->rnr_timeout_exp,
-			       peer->rnr_queued_pkt_cnt);
-		}
-	}
-	dlist_insert_tail(&peer->rnr_entry,
-			  &ep->peer_backoff_list);
-
-queue_pkt:
-#if ENABLE_DEBUG
-	dlist_remove(&pkt_entry->dbg_entry);
-#endif
-	dlist_insert_tail(&pkt_entry->entry, list);
-}
-
-int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
-{
-	struct fi_cq_err_entry err_entry;
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_tx_entry *tx_entry;
-	struct rxr_read_entry *read_entry;
-	struct rxr_peer *peer;
-	ssize_t ret;
-
-	memset(&err_entry, 0, sizeof(err_entry));
-
-	/*
-	 * If the cq_read failed with another error besides -FI_EAVAIL or
-	 * the cq_readerr fails we don't know if this is an rx or tx error.
-	 * We'll write an error eq entry to the event queue instead.
-	 */
-
-	err_entry.err = FI_EIO;
-	err_entry.prov_errno = (int)err;
-
-	if (err != -FI_EAVAIL) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "fi_cq_read: %s\n",
-			fi_strerror(-err));
-		goto write_err;
-	}
-
-	ret = fi_cq_readerr(ep->rdm_cq, &err_entry, 0);
-	if (ret != 1) {
-		if (ret < 0) {
-			FI_WARN(&rxr_prov, FI_LOG_CQ, "fi_cq_readerr: %s\n",
-				fi_strerror(-ret));
-			err_entry.prov_errno = ret;
-		} else {
-			FI_WARN(&rxr_prov, FI_LOG_CQ,
-				"fi_cq_readerr unexpected size %zu expected %zu\n",
-				ret, sizeof(err_entry));
-			err_entry.prov_errno = -FI_EIO;
-		}
-		goto write_err;
-	}
-
-	if (err_entry.err != -FI_EAGAIN)
-		OFI_CQ_STRERROR(&rxr_prov, FI_LOG_WARN, FI_LOG_CQ, ep->rdm_cq,
-				&err_entry);
-
-	pkt_entry = (struct rxr_pkt_entry *)err_entry.op_context;
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-
-	/*
-	 * A handshake send could fail at the core provider if the peer endpoint
-	 * is shutdown soon after it receives a send completion for the REQ
-	 * packet that included src_address. The handshake itself is irrelevant if
-	 * that happens, so just squelch this error entry and move on without
-	 * writing an error completion or event to the application.
-	 */
-	if (rxr_get_base_hdr(pkt_entry->pkt)->type == RXR_HANDSHAKE_PKT) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"Squelching error CQE for RXR_HANDSHAKE_PKT\n");
-		/*
-		 * HANDSHAKE packets do not have an associated rx/tx entry. Use
-		 * the flags instead to determine if this is a send or recv.
-		 */
-		if (err_entry.flags & FI_SEND) {
-			rxr_ep_dec_tx_pending(ep, peer, 1);
-			rxr_pkt_entry_release_tx(ep, pkt_entry);
-		} else if (err_entry.flags & FI_RECV) {
-			rxr_pkt_entry_release_rx(ep, pkt_entry);
-		} else {
-			assert(0 && "unknown err_entry flags in HANDSHAKE packet");
-		}
-		return 0;
-	}
-
-	if (!pkt_entry->x_entry) {
-		/*
-		 * A NULL x_entry means this is a recv posted buf pkt_entry.
-		 * Since we don't have any context besides the error code,
-		 * we will write to the eq instead.
-		 */
-		rxr_pkt_entry_release_rx(ep, pkt_entry);
-		goto write_err;
-	}
-
-	/*
-	 * If x_entry is set this rx or tx entry error is for a sent
-	 * packet. Decrement the tx_pending counter and fall through to
-	 * the rx or tx entry handlers.
-	 */
-	if (!peer->is_local)
-		rxr_ep_dec_tx_pending(ep, peer, 1);
-	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
-		tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-		if (err_entry.prov_errno != IBV_WC_RNR_RETRY_EXC_ERR ||
-		    ep->handle_resource_management != FI_RM_ENABLED) {
-			ret = rxr_cq_handle_tx_error(ep, tx_entry,
-						     err_entry.prov_errno);
-			rxr_pkt_entry_release_tx(ep, pkt_entry);
-			return ret;
-		}
-
-		rxr_cq_queue_pkt(ep, &tx_entry->queued_pkts, pkt_entry);
-		if (tx_entry->state == RXR_TX_SEND) {
-			dlist_remove(&tx_entry->entry);
-			tx_entry->state = RXR_TX_QUEUED_DATA_RNR;
-			dlist_insert_tail(&tx_entry->queued_entry,
-					  &ep->tx_entry_queued_list);
-		} else if (tx_entry->state == RXR_TX_REQ) {
-			tx_entry->state = RXR_TX_QUEUED_REQ_RNR;
-			dlist_insert_tail(&tx_entry->queued_entry,
-					  &ep->tx_entry_queued_list);
-		}
-		return 0;
-	} else if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_RX_ENTRY) {
-		rx_entry = (struct rxr_rx_entry *)pkt_entry->x_entry;
-		if (err_entry.prov_errno != IBV_WC_RNR_RETRY_EXC_ERR ||
-		    ep->handle_resource_management != FI_RM_ENABLED) {
-			ret = rxr_cq_handle_rx_error(ep, rx_entry,
-						     err_entry.prov_errno);
-			rxr_pkt_entry_release_tx(ep, pkt_entry);
-			return ret;
-		}
-		rxr_cq_queue_pkt(ep, &rx_entry->queued_pkts, pkt_entry);
-		if (rx_entry->state == RXR_RX_RECV) {
-			rx_entry->state = RXR_RX_QUEUED_CTS_RNR;
-			dlist_insert_tail(&rx_entry->queued_entry,
-					  &ep->rx_entry_queued_list);
-		}
-		return 0;
-	} else if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_READ_ENTRY) {
-		read_entry = (struct rxr_read_entry *)pkt_entry->x_entry;
-		/* read requests is not expected to get RNR, so we call
-		 * rxr_read_handle_error() to handle general error here.
-		 */
-		ret = rxr_read_handle_error(ep, read_entry, err_entry.prov_errno);
-		rxr_pkt_entry_release_tx(ep, pkt_entry);
-		return ret;
+		peer->rnr_backoff_wait_time = MIN(peer->rnr_backoff_wait_time * 2,
+						  rxr_env.rnr_backoff_wait_time_cap);
+		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+		       "increasing backoff timeout for peer: %" PRIu64
+		       "to %ld rnr_queued_pkts: %d\n",
+		       pkt_entry->addr, peer->rnr_backoff_wait_time,
+		       peer->rnr_queued_pkt_cnt);
 	}
-
-	FI_WARN(&rxr_prov, FI_LOG_CQ,
-		"%s unknown x_entry state %d\n",
-		__func__, RXR_GET_X_ENTRY_TYPE(pkt_entry));
-	assert(0 && "unknown x_entry state");
-write_err:
-	efa_eq_write_error(&ep->util_ep, err_entry.err, err_entry.prov_errno);
-	return 0;
 }
 
 void rxr_cq_write_rx_completion(struct rxr_ep *ep,
@@ -470,11 +394,14 @@ void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 
 		rxr_rm_rx_cq_check(ep, rx_cq);
 
-		if (OFI_UNLIKELY(ret))
+		if (OFI_UNLIKELY(ret)) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Unable to write recv error cq: %s\n",
 				fi_strerror(-ret));
+			return;
+		}
 
+		rx_entry->fi_flags |= RXR_NO_COMPLETION;
 		efa_cntr_report_error(&ep->util_ep, rx_entry->cq_entry.flags);
 		return;
 	}
@@ -513,17 +440,17 @@ void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Unable to write recv completion: %s\n",
 				fi_strerror(-ret));
-			if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-				assert(0 && "failed to write err cq entry");
+			rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
 			return;
 		}
+
+		rx_entry->fi_flags |= RXR_NO_COMPLETION;
 	}
 
 	efa_cntr_report_rx_completion(&ep->util_ep, rx_entry->cq_entry.flags);
 }
 
 void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
-				 struct rxr_pkt_entry *pkt_entry,
 				 struct rxr_rx_entry *rx_entry)
 {
 	struct rxr_tx_entry *tx_entry = NULL;
@@ -535,7 +462,6 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 		if (rx_entry->cq_entry.flags & FI_REMOTE_CQ_DATA)
 			rxr_cq_write_rx_completion(ep, rx_entry);
 
-		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
 
@@ -567,20 +493,18 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 		 *     2. call rxr_cq_write_tx_completion()
 		 */
 		tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, rx_entry->rma_loc_tx_id);
-		assert(tx_entry->state == RXR_TX_WAIT_READ_FINISH);
+		assert(tx_entry->state == RXR_TX_REQ);
 		if (tx_entry->fi_flags & FI_COMPLETION) {
-			/* Note write_tx_completion() will release tx_entry */
 			rxr_cq_write_tx_completion(ep, tx_entry);
 		} else {
 			efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-			rxr_release_tx_entry(ep, tx_entry);
 		}
 
+		rxr_release_tx_entry(ep, tx_entry);
 		/*
 		 * do not call rxr_release_rx_entry here because
 		 * caller will release
 		 */
-		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
 
@@ -588,43 +512,53 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 		rxr_msg_multi_recv_handle_completion(ep, rx_entry);
 
 	rxr_cq_write_rx_completion(ep, rx_entry);
-	rxr_pkt_entry_release_rx(ep, pkt_entry);
 	return;
 }
 
 int rxr_cq_reorder_msg(struct rxr_ep *ep,
-		       struct rxr_peer *peer,
+		       struct rdm_peer *peer,
 		       struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_pkt_entry *ooo_entry;
 	struct rxr_pkt_entry *cur_ooo_entry;
+	struct rxr_robuf *robuf;
 	uint32_t msg_id;
 
 	assert(rxr_get_base_hdr(pkt_entry->pkt)->type >= RXR_REQ_PKT_BEGIN);
 
 	msg_id = rxr_pkt_msg_id(pkt_entry);
-	/*
-	 * TODO: Initialize peer state  at the time of AV insertion
-	 * where duplicate detection is available.
-	 */
-	if (!peer->rx_init)
-		rxr_ep_peer_init_rx(ep, peer);
 
+	robuf = &peer->robuf;
 #if ENABLE_DEBUG
-	if (msg_id != ofi_recvwin_next_exp_id(peer->robuf))
+	if (msg_id != ofi_recvwin_next_exp_id(robuf))
 		FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
 		       "msg OOO msg_id: %" PRIu32 " expected: %"
 		       PRIu32 "\n", msg_id,
-		       ofi_recvwin_next_exp_id(peer->robuf));
+		       ofi_recvwin_next_exp_id(robuf));
 #endif
-	if (ofi_recvwin_is_exp(peer->robuf, msg_id))
+	if (ofi_recvwin_is_exp(robuf, msg_id))
 		return 0;
-	else if (!ofi_recvwin_id_valid(peer->robuf, msg_id))
-		return -FI_EALREADY;
+	else if (!ofi_recvwin_id_valid(robuf, msg_id)) {
+		if (ofi_recvwin_id_processed(robuf, msg_id)) {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			       "Error: message id has already been processed. received: %" PRIu32 " expected: %"
+			       PRIu32 "\n", msg_id, ofi_recvwin_next_exp_id(robuf));
+			return -FI_EALREADY;
+		} else {
+			fprintf(stderr,
+				"Current receive window size (%d) is too small to hold incoming messages.\n"
+				"As a result, you application cannot proceed.\n"
+				"Receive window size can be increased by setting the environment variable:\n"
+				"              FI_EFA_RECVWIN_SIZE\n"
+				"\n"
+				"Your job will now abort.\n\n", rxr_env.recvwin_size);
+			abort();
+		}
+	}
 
 	if (OFI_LIKELY(rxr_env.rx_copy_ooo)) {
-		assert(pkt_entry->type == RXR_PKT_ENTRY_POSTED);
-		ooo_entry = rxr_pkt_entry_clone(ep, ep->rx_ooo_pkt_pool, pkt_entry, RXR_PKT_ENTRY_OOO);
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_EFA_RX_POOL);
+		ooo_entry = rxr_pkt_entry_clone(ep, ep->rx_ooo_pkt_pool, RXR_PKT_FROM_OOO_POOL, pkt_entry);
 		if (OFI_UNLIKELY(!ooo_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unable to allocate rx_pkt_entry for OOO msg\n");
@@ -635,7 +569,7 @@ int rxr_cq_reorder_msg(struct rxr_ep *ep,
 		ooo_entry = pkt_entry;
 	}
 
-	cur_ooo_entry = *ofi_recvwin_get_msg(peer->robuf, msg_id);
+	cur_ooo_entry = *ofi_recvwin_get_msg(robuf, msg_id);
 	if (cur_ooo_entry) {
 		assert(rxr_get_base_hdr(cur_ooo_entry->pkt)->type == RXR_MEDIUM_MSGRTM_PKT ||
 		       rxr_get_base_hdr(cur_ooo_entry->pkt)->type == RXR_MEDIUM_TAGRTM_PKT ||
@@ -645,22 +579,22 @@ int rxr_cq_reorder_msg(struct rxr_ep *ep,
 		assert(rxr_pkt_rtm_total_len(cur_ooo_entry) == rxr_pkt_rtm_total_len(ooo_entry));
 		rxr_pkt_entry_append(cur_ooo_entry, ooo_entry);
 	} else {
-		ofi_recvwin_queue_msg(peer->robuf, &ooo_entry, msg_id);
+		ofi_recvwin_queue_msg(robuf, &ooo_entry, msg_id);
 	}
 
 	return 1;
 }
 
 void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
-					  struct rxr_peer *peer)
+					  struct rdm_peer *peer)
 {
 	struct rxr_pkt_entry *pending_pkt;
 	int ret = 0;
 	uint32_t msg_id;
 
 	while (1) {
-		pending_pkt = *ofi_recvwin_peek(peer->robuf);
-		if (!pending_pkt || !pending_pkt->pkt)
+		pending_pkt = *ofi_recvwin_peek((&peer->robuf));
+		if (!pending_pkt)
 			return;
 
 		msg_id = rxr_pkt_msg_id(pending_pkt);
@@ -668,7 +602,7 @@ void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
 		       "Processing msg_id %d from robuf\n", msg_id);
 		/* rxr_pkt_proc_rtm_rta will write error cq entry if needed */
 		ret = rxr_pkt_proc_rtm_rta(ep, pending_pkt);
-		*ofi_recvwin_get_next_msg(peer->robuf) = NULL;
+		*ofi_recvwin_get_next_msg((&peer->robuf)) = NULL;
 		if (OFI_UNLIKELY(ret)) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Error processing msg_id %d from robuf: %s\n",
@@ -751,7 +685,14 @@ bool rxr_cq_need_tx_completion(struct rxr_ep *ep,
 	       tx_entry->fi_flags & FI_COMPLETION;
 }
 
-
+/**
+ * @brief write a cq entry for an tx operation (send/read/write) if application wants it.
+ *        Sometimes application does not want to receive a cq entry for an tx
+ *        operation.
+ *
+ * @param[in]	ep		end point
+ * @param[in]	tx_entry	tx entry that contains information of the TX operation
+ */
 void rxr_cq_write_tx_completion(struct rxr_ep *ep,
 				struct rxr_tx_entry *tx_entry)
 {
@@ -791,53 +732,54 @@ void rxr_cq_write_tx_completion(struct rxr_ep *ep,
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Unable to write send completion: %s\n",
 				fi_strerror(-ret));
-			if (rxr_cq_handle_tx_error(ep, tx_entry, ret))
-				assert(0 && "failed to write err cq entry");
+			rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
 			return;
 		}
 	}
 
 	efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-	rxr_release_tx_entry(ep, tx_entry);
+	tx_entry->fi_flags |= RXR_NO_COMPLETION;
 	return;
 }
 
 void rxr_cq_handle_tx_completion(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
 	if (tx_entry->state == RXR_TX_SEND)
 		dlist_remove(&tx_entry->entry);
 
 	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(peer);
 	peer->tx_credits += tx_entry->credit_allocated;
 
 	if (tx_entry->cq_entry.flags & FI_READ) {
 		/*
-		 * this must be on remote side
-		 * see explaination on rxr_cq_handle_rx_completion
+		 * This is on responder side of an emulated read operation.
+		 * In this case, we do not write any completion.
+		 * The TX entry is allocated for emulated read, so no need to write tx completion.
+		 * EFA does not support FI_RMA_EVENT, so no need to write rx completion.
 		 */
 		struct rxr_rx_entry *rx_entry = NULL;
 
 		rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, tx_entry->rma_loc_rx_id);
 		assert(rx_entry);
 		assert(rx_entry->state == RXR_RX_WAIT_READ_FINISH);
-
 		rxr_release_rx_entry(ep, rx_entry);
-		/* just release tx, do not write completion */
-		rxr_release_tx_entry(ep, tx_entry);
 	} else if (tx_entry->cq_entry.flags & FI_WRITE) {
 		if (tx_entry->fi_flags & FI_COMPLETION) {
 			rxr_cq_write_tx_completion(ep, tx_entry);
 		} else {
 			if (!(tx_entry->fi_flags & RXR_NO_COUNTER))
 				efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-			rxr_release_tx_entry(ep, tx_entry);
 		}
+
 	} else {
 		assert(tx_entry->cq_entry.flags & FI_SEND);
 		rxr_cq_write_tx_completion(ep, tx_entry);
 	}
+
+	rxr_release_tx_entry(ep, tx_entry);
 }
 
 static int rxr_cq_close(struct fid *fid)
diff --git a/prov/efa/src/rxr/rxr_domain.c b/prov/efa/src/rxr/rxr_domain.c
index 9624dc1..bdb745c 100644
--- a/prov/efa/src/rxr/rxr_domain.c
+++ b/prov/efa/src/rxr/rxr_domain.c
@@ -97,9 +97,6 @@ int rxr_mr_regattr(struct fid *domain_fid, const struct fi_mr_attr *attr,
 	rxr_domain = container_of(domain_fid, struct rxr_domain,
 				  util_domain.domain_fid.fid);
 
-	if (attr->iface == FI_HMEM_CUDA)
-		flags |= OFI_MR_NOCACHE;
-
 	ret = fi_mr_regattr(rxr_domain->rdm_domain, attr, flags, mr);
 	if (ret) {
 		FI_WARN(&rxr_prov, FI_LOG_MR,
@@ -153,14 +150,10 @@ int rxr_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 	struct fi_info *rdm_info;
 	struct rxr_domain *rxr_domain;
 	struct efa_domain *efa_domain;
-	struct rxr_fabric *rxr_fabric;
-
-	rxr_fabric = container_of(fabric, struct rxr_fabric,
-				  util_fabric.fabric_fid);
+	struct efa_fabric *efa_fabric;
 
 	if (info->ep_attr->type == FI_EP_DGRAM)
-		return fi_domain(rxr_fabric->lower_fabric, info, domain,
-				 context);
+		return efa_domain_open(fabric, info, domain, context);
 
 	rxr_info.addr_format = info->addr_format;
 
@@ -188,18 +181,20 @@ int rxr_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 	if (ret)
 		goto err_free_domain;
 
-	ret = fi_domain(rxr_fabric->lower_fabric, rdm_info,
+	ret = efa_domain_open(fabric, rdm_info,
 			&rxr_domain->rdm_domain, context);
 	if (ret)
 		goto err_free_core_info;
 
 	efa_domain = container_of(rxr_domain->rdm_domain, struct efa_domain,
-				  util_domain.domain_fid);
+				  	util_domain.domain_fid);
 
 	/* Open shm provider's access domain */
-	if (rxr_env.enable_shm_transfer) {
+	efa_fabric = container_of(fabric, struct efa_fabric,
+							  util_fabric.fabric_fid);
+	if (efa_fabric->shm_fabric) {
 		assert(!strcmp(shm_info->fabric_attr->name, "shm"));
-		ret = fi_domain(rxr_fabric->shm_fabric, shm_info,
+		ret = fi_domain(efa_fabric->shm_fabric, shm_info,
 				&efa_domain->shm_domain, context);
 		if (ret)
 			goto err_close_core_domain;
@@ -233,7 +228,7 @@ int rxr_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 	return 0;
 
 err_close_shm_domain:
-	if (rxr_env.enable_shm_transfer) {
+	if (efa_domain->shm_domain) {
 		retv = fi_close(&efa_domain->shm_domain->fid);
 		if (retv)
 			FI_WARN(&rxr_prov, FI_LOG_DOMAIN,
diff --git a/prov/efa/src/rxr/rxr_ep.c b/prov/efa/src/rxr/rxr_ep.c
index 4bab034..cf0419e 100644
--- a/prov/efa/src/rxr/rxr_ep.c
+++ b/prov/efa/src/rxr/rxr_ep.c
@@ -37,12 +37,12 @@
 #include "ofi.h"
 #include <ofi_util.h>
 #include <ofi_iov.h>
-
 #include "rxr.h"
 #include "efa.h"
 #include "rxr_msg.h"
 #include "rxr_rma.h"
 #include "rxr_pkt_cmd.h"
+#include "rxr_pkt_type_base.h"
 #include "rxr_read.h"
 #include "rxr_atomic.h"
 
@@ -64,9 +64,8 @@ struct efa_ep_addr *rxr_peer_raw_addr(struct rxr_ep *ep, fi_addr_t addr)
 
 	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
 	efa_av = efa_ep->av;
-	efa_conn = efa_av->conn_table[(int)addr];
-
-	return &efa_conn->ep_addr;
+	efa_conn = efa_av_addr_to_conn(efa_av, addr);
+	return efa_conn ? efa_conn->ep_addr : NULL;
 }
 
 const char *rxr_peer_raw_addr_str(struct rxr_ep *ep, fi_addr_t addr, char *buf, size_t *buflen)
@@ -74,55 +73,50 @@ const char *rxr_peer_raw_addr_str(struct rxr_ep *ep, fi_addr_t addr, char *buf,
 	return ofi_straddr(buf, buflen, FI_ADDR_EFA, rxr_peer_raw_addr(ep, addr));
 }
 
-struct rxr_rx_entry *rxr_ep_rx_entry_init(struct rxr_ep *ep,
-					  struct rxr_rx_entry *rx_entry,
-					  const struct fi_msg *msg,
-					  uint64_t tag,
-					  uint64_t ignore,
-					  uint32_t op,
-					  uint64_t flags)
+/**
+ * @brief allocate an rx entry for an operation
+ *
+ * @param ep[in]	end point
+ * @param addr[in]	fi address of the sender/requester.
+ * @param op[in]	operation type (ofi_op_msg/ofi_op_tagged/ofi_op_read/ofi_op_write/ofi_op_atomic_xxx)
+ * @return		if allocation succeeded, return pointer to rx_entry
+ * 			if allocation failed, return NULL
+ */
+struct rxr_rx_entry *rxr_ep_alloc_rx_entry(struct rxr_ep *ep, fi_addr_t addr, uint32_t op)
 {
-	rx_entry->type = RXR_RX_ENTRY;
-	rx_entry->rx_id = ofi_buf_index(rx_entry);
-	rx_entry->addr = msg->addr;
-	rx_entry->fi_flags = flags;
-	rx_entry->rxr_flags = 0;
-	rx_entry->bytes_received = 0;
-	rx_entry->bytes_copied = 0;
-	rx_entry->window = 0;
-	rx_entry->iov_count = msg->iov_count;
-	rx_entry->tag = tag;
-	rx_entry->op = op;
-	rx_entry->ignore = ignore;
-	rx_entry->unexp_pkt = NULL;
-	rx_entry->rma_iov_count = 0;
-	dlist_init(&rx_entry->queued_pkts);
-
-	memset(&rx_entry->cq_entry, 0, sizeof(rx_entry->cq_entry));
-
-	rx_entry->owner = ep->use_zcpy_rx ? RXR_RX_USER_BUF : RXR_RX_PROV_BUF;
+	struct rxr_rx_entry *rx_entry;
 
-	/* Handle case where we're allocating an unexpected rx_entry */
-	if (msg->msg_iov) {
-		memcpy(rx_entry->iov, msg->msg_iov, sizeof(*rx_entry->iov) * msg->iov_count);
-		rx_entry->cq_entry.len = ofi_total_iov_len(msg->msg_iov, msg->iov_count);
-		rx_entry->cq_entry.buf = msg->msg_iov[0].iov_base;
+	rx_entry = ofi_buf_alloc(ep->rx_entry_pool);
+	if (OFI_UNLIKELY(!rx_entry)) {
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "RX entries exhausted\n");
+		return NULL;
 	}
+	memset(rx_entry, 0, sizeof(struct rxr_rx_entry));
 
-	if (msg->desc)
-		memcpy(&rx_entry->desc[0], msg->desc, sizeof(*msg->desc) * msg->iov_count);
-	else
-		memset(&rx_entry->desc[0], 0, sizeof(rx_entry->desc));
+	dlist_insert_tail(&rx_entry->ep_entry, &ep->rx_entry_list);
+	rx_entry->type = RXR_RX_ENTRY;
+	rx_entry->rx_id = ofi_buf_index(rx_entry);
+	dlist_init(&rx_entry->queued_pkts);
 
-	rx_entry->cq_entry.op_context = msg->context;
-	rx_entry->cq_entry.tag = 0;
-	rx_entry->ignore = ~0;
+	rx_entry->state = RXR_RX_INIT;
+	rx_entry->addr = addr;
+	if (addr != FI_ADDR_UNSPEC) {
+		rx_entry->peer = rxr_ep_get_peer(ep, addr);
+		assert(rx_entry->peer);
+		dlist_insert_tail(&rx_entry->peer_entry, &rx_entry->peer->rx_entry_list);
+	} else {
+		/*
+		 * If msg->addr is not provided, rx_entry->peer will be set
+		 * after it is matched with a message.
+		 */
+		assert(op == ofi_op_msg || op == ofi_op_tagged);
+		rx_entry->peer = NULL;
+	} 
 
+	rx_entry->op = op;
 	switch (op) {
 	case ofi_op_tagged:
 		rx_entry->cq_entry.flags = (FI_RECV | FI_MSG | FI_TAGGED);
-		rx_entry->cq_entry.tag = tag;
-		rx_entry->ignore = ignore;
 		break;
 	case ofi_op_msg:
 		rx_entry->cq_entry.flags = (FI_RECV | FI_MSG);
@@ -149,147 +143,88 @@ struct rxr_rx_entry *rxr_ep_rx_entry_init(struct rxr_ep *ep,
 	return rx_entry;
 }
 
-struct rxr_rx_entry *rxr_ep_get_rx_entry(struct rxr_ep *ep,
-					 const struct fi_msg *msg,
-					 uint64_t tag,
-					 uint64_t ignore,
-					 uint32_t op,
-					 uint64_t flags)
-{
-	struct rxr_rx_entry *rx_entry;
-
-	rx_entry = ofi_buf_alloc(ep->rx_entry_pool);
-	if (OFI_UNLIKELY(!rx_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "RX entries exhausted\n");
-		return NULL;
-	}
-
-#if ENABLE_DEBUG
-	dlist_insert_tail(&rx_entry->rx_entry_entry, &ep->rx_entry_list);
-#endif
-	rx_entry = rxr_ep_rx_entry_init(ep, rx_entry, msg, tag, ignore, op, flags);
-	rx_entry->state = RXR_RX_INIT;
-	rx_entry->op = op;
-	return rx_entry;
-}
-
-struct rxr_rx_entry *rxr_ep_alloc_unexp_rx_entry_for_msgrtm(struct rxr_ep *ep,
-							    struct rxr_pkt_entry **pkt_entry_ptr)
+/**
+ * @brief post user provided receiving buffer to the device.
+ *
+ * The user receive buffer was converted to an RX packet, then posted to the device.
+ *
+ * @param[in]	ep		endpint
+ * @param[in]	rx_entry	rx_entry that contain user buffer information
+ * @param[in]	flags		user supplied flags passed to fi_recv
+ */
+int rxr_ep_post_user_recv_buf(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry, uint64_t flags)
 {
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_pkt_entry *unexp_pkt_entry;
+	struct rxr_pkt_entry *pkt_entry;
+	struct efa_mr *mr;
+	struct iovec msg_iov;
 	struct fi_msg msg = {0};
+	int err;
 
-	unexp_pkt_entry = rxr_pkt_get_unexp(ep, pkt_entry_ptr);
-	if (OFI_UNLIKELY(!unexp_pkt_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "packet entries exhausted.\n");
-		return NULL;
-	}
+	assert(rx_entry->iov_count == 1);
+	assert(rx_entry->iov[0].iov_len >= ep->msg_prefix_size);
+	pkt_entry = (struct rxr_pkt_entry *)rx_entry->iov[0].iov_base;
+	assert(pkt_entry);
 
-	msg.addr = unexp_pkt_entry->addr;
-	rx_entry = rxr_ep_get_rx_entry(ep, &msg, 0, ~0, ofi_op_msg, 0);
-	if (OFI_UNLIKELY(!rx_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "RX entries exhausted.\n");
-		return NULL;
-	}
+	/*
+	 * The ownership of the prefix buffer lies with the application, do not
+	 * put it on the dbg list for cleanup during shutdown or poison it. The
+	 * provider loses jurisdiction over it soon after writing the rx
+	 * completion.
+	 */
+	dlist_init(&pkt_entry->entry);
+	mr = (struct efa_mr *)rx_entry->desc[0];
+	pkt_entry->mr = &mr->mr_fid;
+	pkt_entry->alloc_type = RXR_PKT_FROM_USER_BUFFER;
+	pkt_entry->flags = RXR_PKT_ENTRY_IN_USE;
+	pkt_entry->next = NULL;
+	/*
+	 * The actual receiving buffer size (pkt_size) is
+	 *    rx_entry->total_len - sizeof(struct rxr_pkt_entry)
+	 * because the first part of user buffer was used to
+	 * construct pkt_entry. The actual receiving buffer
+	 * posted to device starts from pkt_entry->pkt.
+	 */
+	pkt_entry->pkt_size = rx_entry->iov[0].iov_len - sizeof(struct rxr_pkt_entry);
 
-	rx_entry->rxr_flags = 0;
-	rx_entry->state = RXR_RX_UNEXP;
-	rx_entry->unexp_pkt = unexp_pkt_entry;
-	rxr_pkt_rtm_init_rx_entry(unexp_pkt_entry, rx_entry);
-	dlist_insert_tail(&rx_entry->entry, &ep->rx_unexp_list);
-	return rx_entry;
-}
+	pkt_entry->x_entry = rx_entry;
+	rx_entry->state = RXR_RX_MATCHED;
 
-struct rxr_rx_entry *rxr_ep_alloc_unexp_rx_entry_for_tagrtm(struct rxr_ep *ep,
-							    struct rxr_pkt_entry **pkt_entry_ptr)
-{
-	uint64_t tag;
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_pkt_entry *unexp_pkt_entry;
-	struct fi_msg msg = {0};
+	msg_iov.iov_base = pkt_entry->pkt;
+	msg_iov.iov_len = pkt_entry->pkt_size;
+	assert(msg_iov.iov_len <= ep->mtu_size);
 
-	unexp_pkt_entry = rxr_pkt_get_unexp(ep, pkt_entry_ptr);
-	if (OFI_UNLIKELY(!unexp_pkt_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "packet entries exhausted.\n");
-		return NULL;
-	}
+	msg.iov_count = 1;
+	msg.msg_iov = &msg_iov;
+	msg.desc = rx_entry->desc;
+	msg.addr = FI_ADDR_UNSPEC;
+	msg.context = pkt_entry;
+	msg.data = 0;
 
-	tag = rxr_pkt_rtm_tag(unexp_pkt_entry);
-	msg.addr = unexp_pkt_entry->addr;
-	rx_entry = rxr_ep_get_rx_entry(ep, &msg, tag, ~0, ofi_op_tagged, 0);
-	if (OFI_UNLIKELY(!rx_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "RX entries exhausted.\n");
-		return NULL;
+	err = fi_recvmsg(ep->rdm_ep, &msg, flags);
+	if (OFI_UNLIKELY(err)) {
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"failed to post user supplied buffer %d (%s)\n", -err,
+			fi_strerror(-err));
+		return err;
 	}
 
-	rx_entry->rxr_flags = 0;
-	rx_entry->state = RXR_RX_UNEXP;
-	rx_entry->unexp_pkt = unexp_pkt_entry;
-	rxr_pkt_rtm_init_rx_entry(unexp_pkt_entry, rx_entry);
-	dlist_insert_tail(&rx_entry->entry, &ep->rx_unexp_tagged_list);
-	return rx_entry;
-}
-
-struct rxr_rx_entry *rxr_ep_split_rx_entry(struct rxr_ep *ep,
-					   struct rxr_rx_entry *posted_entry,
-					   struct rxr_rx_entry *consumer_entry,
-					   struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rx_entry *rx_entry;
-	size_t buf_len, consumed_len, data_len;
-	uint64_t tag;
-	struct fi_msg msg = {0};
-
-	assert(rxr_get_base_hdr(pkt_entry->pkt)->type >= RXR_REQ_PKT_BEGIN);
-	tag = 0;
-
-	if (!consumer_entry) {
-		msg.msg_iov = posted_entry->iov;
-		msg.iov_count = posted_entry->iov_count;
-		msg.addr = pkt_entry->addr;
-		rx_entry = rxr_ep_get_rx_entry(ep, &msg, tag, 0, ofi_op_msg,
-					       posted_entry->fi_flags);
-		if (OFI_UNLIKELY(!rx_entry))
-			return NULL;
-
-		FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
-		       "Splitting into new multi_recv consumer rx_entry %d from rx_entry %d\n",
-		       rx_entry->rx_id,
-		       posted_entry->rx_id);
-	} else {
-		rx_entry = consumer_entry;
-		memcpy(rx_entry->iov, posted_entry->iov,
-		       sizeof(*posted_entry->iov) * posted_entry->iov_count);
-		rx_entry->iov_count = posted_entry->iov_count;
-	}
-
-	rxr_pkt_rtm_init_rx_entry(pkt_entry, rx_entry);
-	data_len = rx_entry->total_len;
-	buf_len = ofi_total_iov_len(rx_entry->iov,
-				    rx_entry->iov_count);
-	consumed_len = MIN(buf_len, data_len);
-
-	rx_entry->rxr_flags |= RXR_MULTI_RECV_CONSUMER;
-	rx_entry->total_len = data_len;
-	rx_entry->fi_flags |= FI_MULTI_RECV;
-	rx_entry->master_entry = posted_entry;
-	rx_entry->cq_entry.len = consumed_len;
-	rx_entry->cq_entry.buf = rx_entry->iov[0].iov_base;
-	rx_entry->cq_entry.op_context = posted_entry->cq_entry.op_context;
-	rx_entry->cq_entry.flags = (FI_RECV | FI_MSG);
-
-	ofi_consume_iov(posted_entry->iov, &posted_entry->iov_count,
-			consumed_len);
-
-	dlist_init(&rx_entry->multi_recv_entry);
-	dlist_insert_tail(&rx_entry->multi_recv_entry,
-			  &posted_entry->multi_recv_consumers);
-	return rx_entry;
+	ep->efa_rx_pkts_posted++;
+	return 0;
 }
 
-/* Post buffers as undirected recv (FI_ADDR_UNSPEC) */
-int rxr_ep_post_buf(struct rxr_ep *ep, const struct fi_msg *posted_recv, uint64_t flags, enum rxr_lower_ep_type lower_ep_type)
+/**
+ * @brief post an internal receive buffer to lower endpoint
+ *
+ * The buffer was posted as undirected recv, (address was set to FI_ADDR_UNSPEC)
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	flags		flags passed to lower provider, can have FI_MORE
+ * @param[in]	lower_ep_type	lower endpoint type, can be either SHM_EP or EFA_EP
+ * @return	On success, return 0
+ * 		On failure, return a negative error code.
+ */
+int rxr_ep_post_internal_rx_pkt(struct rxr_ep *ep, uint64_t flags, enum rxr_lower_ep_type lower_ep_type)
 {
 	struct fi_msg msg = {0};
 	struct iovec msg_iov;
@@ -299,15 +234,15 @@ int rxr_ep_post_buf(struct rxr_ep *ep, const struct fi_msg *posted_recv, uint64_
 
 	switch (lower_ep_type) {
 	case SHM_EP:
-		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->rx_pkt_shm_pool);
+		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->shm_rx_pkt_pool, RXR_PKT_FROM_SHM_RX_POOL);
 		break;
 	case EFA_EP:
-		if (posted_recv)
-			rx_pkt_entry = rxr_pkt_entry_init_prefix(ep, posted_recv, ep->rx_pkt_efa_pool);
-		else
-			rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->rx_pkt_efa_pool);
+		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->efa_rx_pkt_pool, RXR_PKT_FROM_EFA_RX_POOL);
 		break;
 	default:
+		/* Coverity will complain about this being a dead code segment,
+		 * but it is useful for future proofing.
+		 */
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 			"invalid lower EP type %d\n", lower_ep_type);
 		assert(0 && "invalid lower EP type\n");
@@ -341,26 +276,15 @@ int rxr_ep_post_buf(struct rxr_ep *ep, const struct fi_msg *posted_recv, uint64_
 				fi_strerror(-ret));
 			return ret;
 		}
-		ep->posted_bufs_shm++;
+		ep->shm_rx_pkts_posted++;
 		break;
 	case EFA_EP:
 #if ENABLE_DEBUG
-		if (rx_pkt_entry->type != RXR_PKT_ENTRY_USER)
-			dlist_insert_tail(&rx_pkt_entry->dbg_entry,
-					  &ep->rx_posted_buf_list);
+		dlist_insert_tail(&rx_pkt_entry->dbg_entry,
+				  &ep->rx_posted_buf_list);
 #endif
 		desc = fi_mr_desc(rx_pkt_entry->mr);
 		msg.desc = &desc;
-		/*
-		 * Use the actual receive sizes from the application
-		 * rather than posting the full MTU size, like we do
-		 * when using the bufpool.
-		 */
-		if (posted_recv) {
-			msg_iov.iov_len = posted_recv->msg_iov->iov_len;
-			msg.data = posted_recv->data;
-			assert(msg_iov.iov_len <= ep->mtu_size);
-		}
 		ret = fi_recvmsg(ep->rdm_ep, &msg, flags);
 		if (OFI_UNLIKELY(ret)) {
 			rxr_pkt_entry_release_rx(ep, rx_pkt_entry);
@@ -369,7 +293,7 @@ int rxr_ep_post_buf(struct rxr_ep *ep, const struct fi_msg *posted_recv, uint64_
 				fi_strerror(-ret));
 			return ret;
 		}
-		ep->posted_bufs_efa++;
+		ep->efa_rx_pkts_posted++;
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
@@ -380,6 +304,39 @@ int rxr_ep_post_buf(struct rxr_ep *ep, const struct fi_msg *posted_recv, uint64_
 	return 0;
 }
 
+/**
+ * @brief bulk post internal receive buffer(s) to device
+ *
+ * When posting multiple buffers, this function will use
+ * FI_MORE flag to achieve better performance.
+ *
+ * @param[in]	ep		endpint
+ * @param[in]	nrecv		number of receive buffers to post
+ * @param[in]	lower_ep_type	device type, can be SHM_EP or EFA_EP
+ * @return	On success, return 0
+ * 		On failure, return negative libfabric error code
+ */
+static inline
+ssize_t rxr_ep_bulk_post_internal_rx_pkts(struct rxr_ep *ep, int nrecv,
+					  enum rxr_lower_ep_type lower_ep_type)
+{
+	int i;
+	ssize_t err;
+	uint64_t flags;
+
+	flags = FI_MORE;
+	for (i = 0; i < nrecv; ++i) {
+		if (i == nrecv - 1)
+			flags = 0;
+
+		err = rxr_ep_post_internal_rx_pkt(ep, flags, lower_ep_type);
+		if (OFI_UNLIKELY(err))
+			return err;
+	}
+
+	return 0;
+}
+
 void rxr_tx_entry_init(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 		       const struct fi_msg *msg, uint32_t op, uint64_t flags)
 {
@@ -390,13 +347,14 @@ void rxr_tx_entry_init(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	tx_entry->tx_id = ofi_buf_index(tx_entry);
 	tx_entry->state = RXR_TX_REQ;
 	tx_entry->addr = msg->addr;
+	tx_entry->peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(tx_entry->peer);
+	dlist_insert_tail(&tx_entry->peer_entry, &tx_entry->peer->tx_entry_list);
 
-	tx_entry->send_flags = 0;
 	tx_entry->rxr_flags = 0;
 	tx_entry->bytes_acked = 0;
 	tx_entry->bytes_sent = 0;
 	tx_entry->window = 0;
-	tx_entry->total_len = ofi_total_iov_len(msg->msg_iov, msg->iov_count);
 	tx_entry->iov_count = msg->iov_count;
 	tx_entry->iov_index = 0;
 	tx_entry->iov_mr_start = 0;
@@ -411,17 +369,14 @@ void rxr_tx_entry_init(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	else
 		memset(tx_entry->desc, 0, sizeof(tx_entry->desc));
 
-	/*
-	 * The prefix is currently not used by the sender, but needs to be
-	 * accounted for when copying the payload into the bounce-buffer.
-	 */
-	if (ep->use_zcpy_rx) {
-		assert(tx_entry->iov[0].iov_len >= sizeof(struct rxr_pkt_entry) + sizeof(struct rxr_eager_msgrtm_hdr));
-		tx_entry->iov[0].iov_base = (char *)tx_entry->iov[0].iov_base
-					     + sizeof(struct rxr_pkt_entry)
-					     + sizeof(struct rxr_eager_msgrtm_hdr);
+	if (ep->msg_prefix_size > 0) {
+		assert(tx_entry->iov[0].iov_len >= ep->msg_prefix_size);
+		tx_entry->iov[0].iov_base = (char *)tx_entry->iov[0].iov_base + ep->msg_prefix_size;
+		tx_entry->iov[0].iov_len -= ep->msg_prefix_size;
 	}
 
+	tx_entry->total_len = ofi_total_iov_len(tx_entry->iov, tx_entry->iov_count);
+
 	/* set flags */
 	assert(ep->util_ep.tx_msg_flags == 0 ||
 	       ep->util_ep.tx_msg_flags == FI_COMPLETION);
@@ -486,15 +441,18 @@ struct rxr_tx_entry *rxr_ep_alloc_tx_entry(struct rxr_ep *rxr_ep,
 		tx_entry->tag = tag;
 	}
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
 	return tx_entry;
 }
 
 void rxr_release_tx_entry(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 {
 	int i, err = 0;
+	struct dlist_entry *tmp;
+	struct rxr_pkt_entry *pkt_entry;
+
+	assert(tx_entry->peer);
+	dlist_remove(&tx_entry->peer_entry);
 
 	for (i = 0; i < tx_entry->iov_count; i++) {
 		if (tx_entry->mr[i]) {
@@ -508,10 +466,20 @@ void rxr_release_tx_entry(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 		}
 	}
 
-#if ENABLE_DEBUG
-	dlist_remove(&tx_entry->tx_entry_entry);
-#endif
-	assert(dlist_empty(&tx_entry->queued_pkts));
+	dlist_remove(&tx_entry->ep_entry);
+
+	dlist_foreach_container_safe(&tx_entry->queued_pkts,
+				     struct rxr_pkt_entry,
+				     pkt_entry, entry, tmp) {
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+	}
+
+	if (tx_entry->rxr_flags & RXR_TX_ENTRY_QUEUED_RNR)
+		dlist_remove(&tx_entry->queued_rnr_entry);
+
+	if (tx_entry->state == RXR_TX_QUEUED_CTRL)
+		dlist_remove(&tx_entry->queued_ctrl_entry);
+
 #ifdef ENABLE_EFA_POISONING
 	rxr_poison_mem_region((uint32_t *)tx_entry,
 			      sizeof(struct rxr_tx_entry));
@@ -559,6 +527,30 @@ int rxr_ep_tx_init_mr_desc(struct rxr_domain *rxr_domain,
 	return ret;
 }
 
+/**
+ * @brief convert EFA descriptors to shm descriptors.
+ *
+ * Each provider defines its descriptors format. The descriptor for
+ * EFA provider is of struct efa_mr *, which shm provider cannot
+ * understand. This function convert EFA descriptors to descriptors
+ * shm can use.
+ *
+ * @param numdesc[in]       number of descriptors in the array
+ * @param desc[in,out]      descriptors input is EFA descriptor, output
+ *                          is shm descriptor.
+ */
+void rxr_convert_desc_for_shm(int numdesc, void **desc)
+{
+	int i;
+	struct efa_mr *efa_mr;
+
+	for (i = 0; i < numdesc; ++i) {
+		efa_mr = desc[i];
+		if (efa_mr)
+			desc[i] = fi_mr_desc(efa_mr->shm_mr);
+	}
+}
+
 void rxr_prepare_desc_send(struct rxr_domain *rxr_domain,
 			   struct rxr_tx_entry *tx_entry)
 {
@@ -588,26 +580,19 @@ void rxr_prepare_desc_send(struct rxr_domain *rxr_domain,
 /* Generic send */
 int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 {
-	struct rxr_peer *peer;
-	int pending;
+	struct rdm_peer *peer;
+	int outstanding;
 
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
-
-	/*
-	 * Init tx state for this peer. The rx state and reorder buffers will be
-	 * initialized on the first recv so as to not allocate resources unless
-	 * necessary.
-	 */
-	if (!peer->tx_init)
-		rxr_ep_peer_init_tx(peer);
+	assert(peer);
 
 	/*
 	 * Divy up available credits to outstanding transfers and request the
 	 * minimum of that and the amount required to finish the current long
 	 * message.
 	 */
-	pending = peer->tx_pending + 1;
-	tx_entry->credit_request = MIN(ofi_div_ceil(peer->tx_credits, pending),
+	outstanding = peer->efa_outstanding_tx_ops + 1;
+	tx_entry->credit_request = MIN(ofi_div_ceil(peer->tx_credits, outstanding),
 				       ofi_div_ceil(tx_entry->total_len,
 						    rxr_ep->max_data_payload_size));
 	tx_entry->credit_request = MAX(tx_entry->credit_request,
@@ -624,107 +609,114 @@ int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_
 
 static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 {
-	size_t i = 0;
-	struct rxr_peer *peer;
-	struct efa_av *av;
-#if ENABLE_DEBUG
-	struct dlist_entry *tmp;
-	struct dlist_entry *entry;
+	struct dlist_entry *entry, *tmp;
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_tx_entry *tx_entry;
+#if ENABLE_DEBUG
 	struct rxr_pkt_entry *pkt;
 #endif
 
-	if (rxr_need_sas_ordering(rxr_ep)) {
-		av = container_of(rxr_ep->util_ep.av, struct efa_av, util_av);
-		for (i = 0; i < av->count; ++i) {
-			peer = rxr_ep_get_peer(rxr_ep, i);
-			if (peer->rx_init)
-				efa_free_robuf(peer);
-		}
-		if (rxr_ep->robuf_pool)
-			ofi_bufpool_destroy(rxr_ep->robuf_pool);
-	}
-
-#if ENABLE_DEBUG
-	av = container_of(rxr_ep->util_ep.av, struct efa_av, util_av);
-	for (i = 0; i < av->count; ++i) {
-		peer = rxr_ep_get_peer(rxr_ep, i);
-		/*
-		 * TODO: Add support for wait/signal until all pending messages
-		 * have been sent/received so the core does not attempt to
-		 * complete a data operation or an internal RxR transfer after
-		 * the EP is shutdown.
-		 */
-		if ((peer->flags & RXR_PEER_REQ_SENT) && !(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED))
-			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "Closing EP with unacked CONNREQs in flight\n");
-	}
-
-	dlist_foreach(&rxr_ep->rx_unexp_list, entry) {
+	dlist_foreach_safe(&rxr_ep->rx_unexp_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unmatched unexpected rx_entry: %p pkt_entry %p\n",
+			rx_entry, rx_entry->unexp_pkt);
 		rxr_pkt_entry_release_rx(rxr_ep, rx_entry->unexp_pkt);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
 
-	dlist_foreach(&rxr_ep->rx_unexp_tagged_list, entry) {
+	dlist_foreach_safe(&rxr_ep->rx_unexp_tagged_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unmatched unexpected tagged rx_entry: %p pkt_entry %p\n",
+			rx_entry, rx_entry->unexp_pkt);
 		rxr_pkt_entry_release_rx(rxr_ep, rx_entry->unexp_pkt);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
 
-	dlist_foreach(&rxr_ep->rx_entry_queued_list, entry) {
+	dlist_foreach_safe(&rxr_ep->rx_entry_queued_rnr_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry,
-					queued_entry);
-		dlist_foreach_container_safe(&rx_entry->queued_pkts,
-					     struct rxr_pkt_entry,
-					     pkt, entry, tmp)
-			rxr_pkt_entry_release_tx(rxr_ep, pkt);
+					queued_rnr_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with queued rnr rx_entry: %p\n",
+			rx_entry);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
+	}
+
+	dlist_foreach_safe(&rxr_ep->rx_entry_queued_ctrl_list, entry, tmp) {
+		rx_entry = container_of(entry, struct rxr_rx_entry,
+					queued_ctrl_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with queued ctrl rx_entry: %p\n",
+			rx_entry);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
 
-	dlist_foreach(&rxr_ep->tx_entry_queued_list, entry) {
+	dlist_foreach_safe(&rxr_ep->tx_entry_queued_rnr_list, entry, tmp) {
 		tx_entry = container_of(entry, struct rxr_tx_entry,
-					queued_entry);
-		dlist_foreach_container_safe(&tx_entry->queued_pkts,
-					     struct rxr_pkt_entry,
-					     pkt, entry, tmp)
-			rxr_pkt_entry_release_tx(rxr_ep, pkt);
+					queued_rnr_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with queued rnr tx_entry: %p\n",
+			tx_entry);
+		rxr_release_tx_entry(rxr_ep, tx_entry);
 	}
 
-	if (!rxr_ep->use_zcpy_rx) {
-		/*
-		 * The provider does not own these entries, and there's no need
-		 * to deep-free them even in a debug build.
-		 */
-		dlist_foreach_safe(&rxr_ep->rx_pkt_list, entry, tmp) {
-			pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-			rxr_pkt_entry_release_rx(rxr_ep, pkt);
-		}
-		dlist_foreach_safe(&rxr_ep->rx_posted_buf_list, entry, tmp) {
+	dlist_foreach_safe(&rxr_ep->tx_entry_queued_ctrl_list, entry, tmp) {
+		tx_entry = container_of(entry, struct rxr_tx_entry,
+					queued_ctrl_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with queued ctrl tx_entry: %p\n",
+			tx_entry);
+		rxr_release_tx_entry(rxr_ep, tx_entry);
+	}
+
+#if ENABLE_DEBUG
+	dlist_foreach_safe(&rxr_ep->rx_posted_buf_list, entry, tmp) {
+		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
+		ofi_buf_free(pkt);
+	}
+
+	if (rxr_ep->use_shm) {
+		dlist_foreach_safe(&rxr_ep->rx_posted_buf_shm_list, entry, tmp) {
 			pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
 			ofi_buf_free(pkt);
 		}
 	}
 
+	dlist_foreach_safe(&rxr_ep->rx_pkt_list, entry, tmp) {
+		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased RX pkt_entry: %p\n",
+			pkt);
+		rxr_pkt_entry_release_rx(rxr_ep, pkt);
+	}
+
 	dlist_foreach_safe(&rxr_ep->tx_pkt_list, entry, tmp) {
 		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased TX pkt_entry: %p\n",
+			pkt);
 		rxr_pkt_entry_release_tx(rxr_ep, pkt);
 	}
+#endif
 
 	dlist_foreach_safe(&rxr_ep->rx_entry_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry,
-					rx_entry_entry);
+					ep_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased rx_entry: %p\n",
+			rx_entry);
 		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
+
 	dlist_foreach_safe(&rxr_ep->tx_entry_list, entry, tmp) {
 		tx_entry = container_of(entry, struct rxr_tx_entry,
-					tx_entry_entry);
+					ep_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased tx_entry: %p\n",
+			tx_entry);
 		rxr_release_tx_entry(rxr_ep, tx_entry);
 	}
-	if (rxr_ep->use_shm) {
-		dlist_foreach_safe(&rxr_ep->rx_posted_buf_shm_list, entry, tmp) {
-			pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-			ofi_buf_free(pkt);
-		}
-	}
-#endif
 
 	if (rxr_ep->rx_entry_pool)
 		ofi_bufpool_destroy(rxr_ep->rx_entry_pool);
@@ -756,28 +748,90 @@ static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 	if (rxr_ep->rx_unexp_pkt_pool)
 		ofi_bufpool_destroy(rxr_ep->rx_unexp_pkt_pool);
 
-	if (rxr_ep->rx_pkt_efa_pool)
-		ofi_bufpool_destroy(rxr_ep->rx_pkt_efa_pool);
+	if (rxr_ep->efa_rx_pkt_pool)
+		ofi_bufpool_destroy(rxr_ep->efa_rx_pkt_pool);
 
-	if (rxr_ep->tx_pkt_efa_pool)
-		ofi_bufpool_destroy(rxr_ep->tx_pkt_efa_pool);
+	if (rxr_ep->efa_tx_pkt_pool)
+		ofi_bufpool_destroy(rxr_ep->efa_tx_pkt_pool);
+
+	if (rxr_ep->pkt_sendv_pool)
+		ofi_bufpool_destroy(rxr_ep->pkt_sendv_pool);
 
 	if (rxr_ep->use_shm) {
-		if (rxr_ep->rx_pkt_shm_pool)
-			ofi_bufpool_destroy(rxr_ep->rx_pkt_shm_pool);
+		if (rxr_ep->shm_rx_pkt_pool)
+			ofi_bufpool_destroy(rxr_ep->shm_rx_pkt_pool);
+
+		if (rxr_ep->shm_tx_pkt_pool)
+			ofi_bufpool_destroy(rxr_ep->shm_tx_pkt_pool);
+	}
+}
+
+/*
+ * @brief determine whether an endpoint has unfinished send
+ *
+ * Unfinished send includes queued ctrl packets, queued
+ * RNR packets and inflight TX packets.
+ *
+ * @param[in]	rxr_ep	endpoint
+ * @return	a boolean
+ */
+static
+bool rxr_ep_has_unfinished_send(struct rxr_ep *rxr_ep)
+{
+	return !dlist_empty(&rxr_ep->rx_entry_queued_rnr_list) ||
+	       !dlist_empty(&rxr_ep->rx_entry_queued_ctrl_list) ||
+	       !dlist_empty(&rxr_ep->tx_entry_queued_rnr_list) ||
+	       !dlist_empty(&rxr_ep->tx_entry_queued_ctrl_list) ||
+	       (rxr_ep->efa_outstanding_tx_ops > 0) ||
+	       (rxr_ep->shm_outstanding_tx_ops > 0);
+}
+
+/*
+ * @brief wait for send to finish
+ *
+ * Wait for queued packet to be sent, and inflight send to
+ * complete.
+ *
+ * @param[in]	rxr_ep		endpoint
+ * @param[in]	max_wait_time	maximum wait time in second
+ * @return 	Return 0 on success.
+ * 		Return -FI_ETIMEDOUT, if maximum wait time has been reached.
+ */
+static
+int rxr_ep_wait_send(struct rxr_ep *rxr_ep, int max_wait_time)
+{
+	uint64_t begin_time;
+	uint64_t max_wait_time_us = max_wait_time * 1000000;
+	bool finished;
+
+	fastlock_acquire(&rxr_ep->util_ep.lock);
 
-		if (rxr_ep->tx_pkt_shm_pool)
-			ofi_bufpool_destroy(rxr_ep->tx_pkt_shm_pool);
+	begin_time = ofi_gettime_us();
+	while (rxr_ep_has_unfinished_send(rxr_ep) &&
+	       (ofi_gettime_us() - begin_time < max_wait_time_us)) {
+		rxr_ep_progress_internal(rxr_ep);
 	}
+
+	finished = !rxr_ep_has_unfinished_send(rxr_ep);
+	fastlock_release(&rxr_ep->util_ep.lock);
+
+	return finished ? 0 : -FI_ETIMEDOUT;
 }
 
 static int rxr_ep_close(struct fid *fid)
 {
 	int ret, retv = 0;
+	int finish_send_timeout = 10; // seconds
 	struct rxr_ep *rxr_ep;
 
 	rxr_ep = container_of(fid, struct rxr_ep, util_ep.ep_fid.fid);
 
+	ret = rxr_ep_wait_send(rxr_ep, finish_send_timeout);
+	if (ret == -FI_ETIMEDOUT) {
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "Unable to finish queued/inflight send in %d seconds\n",
+			finish_send_timeout);
+	}
+
 	ret = fi_close(&rxr_ep->rdm_ep->fid);
 	if (ret) {
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "Unable to close EP\n");
@@ -812,7 +866,6 @@ static int rxr_ep_close(struct fid *fid)
 		retv = ret;
 	}
 	rxr_ep_free_res(rxr_ep);
-	free(rxr_ep->peer);
 	free(rxr_ep);
 	return retv;
 }
@@ -825,16 +878,21 @@ static int rxr_ep_bind(struct fid *ep_fid, struct fid *bfid, uint64_t flags)
 	struct efa_av *av;
 	struct util_cntr *cntr;
 	struct util_eq *eq;
-	struct dlist_entry *ep_list_first_entry;
-	struct util_ep *util_ep;
-	struct rxr_ep *rxr_first_ep;
-	struct rxr_peer *first_ep_peer, *peer;
 	int ret = 0;
-	size_t i;
 
 	switch (bfid->fclass) {
 	case FI_CLASS_AV:
 		av = container_of(bfid, struct efa_av, util_av.av_fid.fid);
+		/*
+		 * Binding multiple endpoints to a single AV is currently not
+		 * supported.
+		 */
+		if (av->ep) {
+			EFA_WARN(FI_LOG_EP_CTRL,
+				 "Address vector already has endpoint bound to it.\n");
+			return -FI_ENOSYS;
+		}
+
 		/* Bind util provider endpoint and av */
 		ret = ofi_ep_bind_av(&rxr_ep->util_ep, &av->util_av);
 		if (ret)
@@ -844,48 +902,11 @@ static int rxr_ep_bind(struct fid *ep_fid, struct fid *bfid, uint64_t flags)
 		if (ret)
 			return ret;
 
-		rxr_ep->peer = calloc(av->count,
-				      sizeof(struct rxr_peer));
-		if (!rxr_ep->peer)
-			return -FI_ENOMEM;
-
-		if (rxr_need_sas_ordering(rxr_ep)) {
-			ret = ofi_bufpool_create(&rxr_ep->robuf_pool,
-						 sizeof(struct rxr_robuf), 16,
-						 0, 0, 0);
-			if (ret)
-				return ret;
-		}
-
 		/* Bind shm provider endpoint & shm av */
 		if (rxr_ep->use_shm) {
 			ret = fi_ep_bind(rxr_ep->shm_ep, &av->shm_rdm_av->fid, flags);
 			if (ret)
 				return ret;
-
-			/*
-			 * We always update the new added EP's local information with the first
-			 * bound EP. The if (ep_list_first_entry->next) check here is to skip the
-			 * update for the first bound EP.
-			 */
-			ep_list_first_entry = av->util_av.ep_list.next;
-			if (ep_list_first_entry->next) {
-				util_ep = container_of(ep_list_first_entry, struct util_ep, av_entry);
-				rxr_first_ep = container_of(util_ep, struct rxr_ep, util_ep);
-
-				/*
-				 * Copy the entire peer array, because we may not be able to make the
-				 * assumption that insertions are always indexed in order in the future.
-				 */
-				for (i = 0; i < av->count; i++) {
-					first_ep_peer = rxr_ep_get_peer(rxr_first_ep, i);
-					if (first_ep_peer->is_local) {
-						peer = rxr_ep_get_peer(rxr_ep, i);
-						peer->shm_fiaddr = first_ep_peer->shm_fiaddr;
-						peer->is_local = 1;
-					}
-				}
-			}
 		}
 		break;
 	case FI_CLASS_CQ:
@@ -918,58 +939,46 @@ static int rxr_ep_bind(struct fid *ep_fid, struct fid *bfid, uint64_t flags)
 }
 
 static
-void rxr_ep_set_features(struct rxr_ep *ep)
+void rxr_ep_set_extra_info(struct rxr_ep *ep)
 {
-	memset(ep->features, 0, sizeof(ep->features));
+	memset(ep->extra_info, 0, sizeof(ep->extra_info));
 
 	/* RDMA read is an extra feature defined in protocol version 4 (the base version) */
 	if (efa_ep_support_rdma_read(ep->rdm_ep))
-		ep->features[0] |= RXR_REQ_FEATURE_RDMA_READ;
+		ep->extra_info[0] |= RXR_EXTRA_FEATURE_RDMA_READ;
+
+	ep->extra_info[0] |= RXR_EXTRA_FEATURE_DELIVERY_COMPLETE;
+
+	if (ep->use_zcpy_rx) {
+		/*
+		 * zero copy receive requires the packet header length remains
+		 * constant, so the application receive buffer is match with
+		 * incoming application data.
+		 */
+		ep->extra_info[0] |= RXR_EXTRA_REQUEST_CONSTANT_HEADER_LENGTH;
+	}
 
-	ep->features[0] |= RXR_REQ_FEATURE_DELIVERY_COMPLETE;
+	ep->extra_info[0] |= RXR_EXTRA_REQUEST_CONNID_HEADER;
 }
 
 static int rxr_ep_ctrl(struct fid *fid, int command, void *arg)
 {
 	ssize_t ret;
-	size_t i;
 	struct rxr_ep *ep;
-	uint64_t flags = FI_MORE;
-	size_t rx_size, shm_rx_size;
-	char shm_ep_name[NAME_MAX];
+	char shm_ep_name[EFA_SHM_NAME_MAX];
 
 	switch (command) {
 	case FI_ENABLE:
 		/* Enable core endpoints & post recv buff */
 		ep = container_of(fid, struct rxr_ep, util_ep.ep_fid.fid);
 
-		/*
-		 * If the endpoint is configured for zero-copy receives, the
-		 * provider will use the application's undirected receives for
-		 * its internal control packets as well. The onus will be on the
-		 * application to ensure the receive queue is hydrated to avoid
-		 * RNRs.
-		 */
-		rx_size = ep->use_zcpy_rx ? rxr_env.zcpy_rx_seed : rxr_get_rx_pool_chunk_cnt(ep);
 		ret = fi_enable(ep->rdm_ep);
 		if (ret)
 			return ret;
 
 		fastlock_acquire(&ep->util_ep.lock);
 
-		rxr_ep_set_features(ep);
-
-		for (i = 0; i < rx_size; i++) {
-			if (i == rx_size - 1)
-				flags = 0;
-
-			ret = rxr_ep_post_buf(ep, NULL, flags, EFA_EP);
-
-			if (ret)
-				goto out;
-		}
-
-		ep->available_data_bufs = rx_size;
+		rxr_ep_set_extra_info(ep);
 
 		ep->core_addrlen = RXR_MAX_NAME_LENGTH;
 		ret = fi_getname(&ep->rdm_ep->fid,
@@ -987,25 +996,13 @@ static int rxr_ep_ctrl(struct fid *fid, int command, void *arg)
 		 * shared memory region.
 		 */
 		if (ep->use_shm) {
-			ret = rxr_ep_efa_addr_to_str(ep->core_addr, shm_ep_name);
+			ret = rxr_raw_addr_to_smr_name(ep->core_addr, shm_ep_name);
 			if (ret < 0)
 				goto out;
-
 			fi_setname(&ep->shm_ep->fid, shm_ep_name, sizeof(shm_ep_name));
-			shm_rx_size = shm_info->rx_attr->size;
 			ret = fi_enable(ep->shm_ep);
 			if (ret)
-				return ret;
-			/* Pre-post buffer to receive from shm provider */
-			for (i = 0; i < shm_rx_size; i++) {
-				if (i == shm_rx_size - 1)
-					flags = 0;
-
-				ret = rxr_ep_post_buf(ep, NULL, flags, SHM_EP);
-
-				if (ret)
-					goto out;
-			}
+				goto out;
 		}
 
 out:
@@ -1115,14 +1112,29 @@ static ssize_t rxr_ep_cancel(fid_t fid_ep, void *context)
 static int rxr_ep_getopt(fid_t fid, int level, int optname, void *optval,
 			 size_t *optlen)
 {
-	struct rxr_ep *rxr_ep = container_of(fid, struct rxr_ep,
-					     util_ep.ep_fid.fid);
+	struct rxr_ep *rxr_ep;
+	struct efa_ep *efa_ep;
+
+	rxr_ep = container_of(fid, struct rxr_ep, util_ep.ep_fid.fid);
+	efa_ep = container_of(rxr_ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
 
-	if (level != FI_OPT_ENDPOINT || optname != FI_OPT_MIN_MULTI_RECV)
+	if (level != FI_OPT_ENDPOINT)
 		return -FI_ENOPROTOOPT;
 
-	*(size_t *)optval = rxr_ep->min_multi_recv_size;
-	*optlen = sizeof(size_t);
+	switch (optname) {
+	case FI_OPT_MIN_MULTI_RECV:
+		*(size_t *)optval = rxr_ep->min_multi_recv_size;
+		*optlen = sizeof(size_t);
+		break;
+	case FI_OPT_EFA_RNR_RETRY:
+		*(size_t *)optval = efa_ep->rnr_retry;
+		*optlen = sizeof(size_t);
+		break;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Unknown endpoint option %s\n", __func__);
+		return -FI_ENOPROTOOPT;
+	}
 
 	return FI_SUCCESS;
 }
@@ -1130,16 +1142,46 @@ static int rxr_ep_getopt(fid_t fid, int level, int optname, void *optval,
 static int rxr_ep_setopt(fid_t fid, int level, int optname,
 			 const void *optval, size_t optlen)
 {
-	struct rxr_ep *rxr_ep = container_of(fid, struct rxr_ep,
-					     util_ep.ep_fid.fid);
-
-	if (level != FI_OPT_ENDPOINT || optname != FI_OPT_MIN_MULTI_RECV)
+	struct rxr_ep *rxr_ep;
+	struct efa_ep *efa_ep;
+
+	rxr_ep = container_of(fid, struct rxr_ep, util_ep.ep_fid.fid);
+	efa_ep = container_of(rxr_ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
+
+	if (level != FI_OPT_ENDPOINT)
 		return -FI_ENOPROTOOPT;
 
 	if (optlen < sizeof(size_t))
 		return -FI_EINVAL;
 
-	rxr_ep->min_multi_recv_size = *(size_t *)optval;
+	switch (optname) {
+	case FI_OPT_MIN_MULTI_RECV:
+		rxr_ep->min_multi_recv_size = *(size_t *)optval;
+		break;
+	case FI_OPT_EFA_RNR_RETRY:
+		/*
+		 * Application is required to call to fi_setopt before EP
+		 * enabled. If it's calling to fi_setopt after EP enabled,
+		 * fail the call.
+		 *
+		 * efa_ep->qp will be NULL before EP enabled, use it to check
+		 * if the call to fi_setopt is before or after EP enabled for
+		 * convience, instead of calling to ibv_query_qp
+		 */
+		if (!efa_ep->qp) {
+			efa_ep->rnr_retry = *(size_t *)optval;
+		} else {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"The option FI_OPT_EFA_RNR_RETRY is required \
+				to be set before EP enabled %s\n", __func__);
+			return -FI_EINVAL;
+		}
+		break;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Unknown endpoint option %s\n", __func__);
+		return -FI_ENOPROTOOPT;
+	}
 
 	return FI_SUCCESS;
 }
@@ -1209,9 +1251,18 @@ static int rxr_create_pkt_pool(struct rxr_ep *ep, size_t size,
 	return ofi_bufpool_create_attr(&attr, buf_pool);
 }
 
+/** @brief Initializes the endpoint.
+ *
+ * This function allocates the various buffer pools for the EFA and SHM
+ * provider and does other endpoint initialization.
+ *
+ * @param ep rxr_ep struct to initialize.
+ * @return 0 on success, fi_errno on error.
+ */
 int rxr_ep_init(struct rxr_ep *ep)
 {
-	size_t entry_sz;
+	size_t entry_sz, sendv_pool_size;
+	int hp_pool_flag;
 	int ret;
 
 	entry_sz = ep->mtu_size + sizeof(struct rxr_pkt_entry);
@@ -1220,22 +1271,27 @@ int rxr_ep_init(struct rxr_ep *ep)
 	ep->rx_pkt_pool_entry_sz = entry_sz;
 #endif
 
+	if (efa_fork_status == EFA_FORK_SUPPORT_ON)
+		hp_pool_flag = 0;
+	else
+		hp_pool_flag = OFI_BUFPOOL_HUGEPAGES;
+
 	ret = rxr_create_pkt_pool(ep, entry_sz, rxr_get_tx_pool_chunk_cnt(ep),
-				  OFI_BUFPOOL_HUGEPAGES,
-				  &ep->tx_pkt_efa_pool);
+				  hp_pool_flag,
+				  &ep->efa_tx_pkt_pool);
 	if (ret)
 		goto err_free;
 
 	ret = rxr_create_pkt_pool(ep, entry_sz, rxr_get_rx_pool_chunk_cnt(ep),
-				  OFI_BUFPOOL_HUGEPAGES,
-				  &ep->rx_pkt_efa_pool);
+				  hp_pool_flag,
+				  &ep->efa_rx_pkt_pool);
 	if (ret)
 		goto err_free;
 
 	if (rxr_env.rx_copy_unexp) {
 		ret = ofi_bufpool_create(&ep->rx_unexp_pkt_pool, entry_sz,
 					 RXR_BUF_POOL_ALIGNMENT, 0,
-					 rxr_get_rx_pool_chunk_cnt(ep), 0);
+					 rxr_env.unexp_pool_chunk_size, 0);
 
 		if (ret)
 			goto err_free;
@@ -1244,10 +1300,11 @@ int rxr_ep_init(struct rxr_ep *ep)
 	if (rxr_env.rx_copy_ooo) {
 		ret = ofi_bufpool_create(&ep->rx_ooo_pkt_pool, entry_sz,
 					 RXR_BUF_POOL_ALIGNMENT, 0,
-					 rxr_env.recvwin_size, 0);
+					 rxr_env.ooo_pool_chunk_size, 0);
 
 		if (ret)
 			goto err_free;
+
 	}
 
 	if ((rxr_env.rx_copy_unexp || rxr_env.rx_copy_ooo) &&
@@ -1261,15 +1318,6 @@ int rxr_ep_init(struct rxr_ep *ep)
 
 		if (ret)
 			goto err_free;
-
-		ret = ofi_bufpool_grow(ep->rx_readcopy_pkt_pool);
-		if (ret) {
-			FI_WARN(&rxr_prov, FI_LOG_CQ,
-				"cannot allocate and register memory for readcopy packet pool. error: %s\n",
-				strerror(-ret));
-			goto err_free;
-		}
-
 		ep->rx_readcopy_pkt_pool_used = 0;
 		ep->rx_readcopy_pkt_pool_max_used = 0;
 	}
@@ -1322,9 +1370,20 @@ int rxr_ep_init(struct rxr_ep *ep)
 	if (ret)
 		goto err_free;
 
+	sendv_pool_size = rxr_get_tx_pool_chunk_cnt(ep);
+	if (ep->use_shm)
+		sendv_pool_size += shm_info->tx_attr->size;
+	ret = ofi_bufpool_create(&ep->pkt_sendv_pool,
+				 sizeof(struct rxr_pkt_sendv),
+				 RXR_BUF_POOL_ALIGNMENT,
+				 sendv_pool_size,
+				 sendv_pool_size, 0);
+	if (ret)
+		goto err_free;
+
 	/* create pkt pool for shm */
 	if (ep->use_shm) {
-		ret = ofi_bufpool_create(&ep->tx_pkt_shm_pool,
+		ret = ofi_bufpool_create(&ep->shm_tx_pkt_pool,
 					 entry_sz,
 					 RXR_BUF_POOL_ALIGNMENT,
 					 shm_info->tx_attr->size,
@@ -1332,7 +1391,7 @@ int rxr_ep_init(struct rxr_ep *ep)
 		if (ret)
 			goto err_free;
 
-		ret = ofi_bufpool_create(&ep->rx_pkt_shm_pool,
+		ret = ofi_bufpool_create(&ep->shm_rx_pkt_pool,
 					 entry_sz,
 					 RXR_BUF_POOL_ALIGNMENT,
 					 shm_info->rx_attr->size,
@@ -1349,25 +1408,32 @@ int rxr_ep_init(struct rxr_ep *ep)
 	dlist_init(&ep->rx_tagged_list);
 	dlist_init(&ep->rx_unexp_tagged_list);
 	dlist_init(&ep->rx_posted_buf_list);
-	dlist_init(&ep->rx_entry_queued_list);
-	dlist_init(&ep->tx_entry_queued_list);
+	dlist_init(&ep->rx_entry_queued_rnr_list);
+	dlist_init(&ep->rx_entry_queued_ctrl_list);
+	dlist_init(&ep->tx_entry_queued_rnr_list);
+	dlist_init(&ep->tx_entry_queued_ctrl_list);
 	dlist_init(&ep->tx_pending_list);
 	dlist_init(&ep->read_pending_list);
 	dlist_init(&ep->peer_backoff_list);
+	dlist_init(&ep->handshake_queued_peer_list);
 #if ENABLE_DEBUG
 	dlist_init(&ep->rx_pending_list);
 	dlist_init(&ep->rx_pkt_list);
 	dlist_init(&ep->tx_pkt_list);
+#endif
 	dlist_init(&ep->rx_entry_list);
 	dlist_init(&ep->tx_entry_list);
-#endif
+
 	/* Initialize pkt to rx map */
 	ep->pkt_rx_map = NULL;
 	return 0;
 
 err_free:
-	if (ep->tx_pkt_shm_pool)
-		ofi_bufpool_destroy(ep->tx_pkt_shm_pool);
+	if (ep->shm_tx_pkt_pool)
+		ofi_bufpool_destroy(ep->shm_tx_pkt_pool);
+
+	if (ep->pkt_sendv_pool)
+		ofi_bufpool_destroy(ep->pkt_sendv_pool);
 
 	if (ep->rx_atomrsp_pool)
 		ofi_bufpool_destroy(ep->rx_atomrsp_pool);
@@ -1396,11 +1462,11 @@ err_free:
 	if (rxr_env.rx_copy_unexp && ep->rx_unexp_pkt_pool)
 		ofi_bufpool_destroy(ep->rx_unexp_pkt_pool);
 
-	if (ep->rx_pkt_efa_pool)
-		ofi_bufpool_destroy(ep->rx_pkt_efa_pool);
+	if (ep->efa_rx_pkt_pool)
+		ofi_bufpool_destroy(ep->efa_rx_pkt_pool);
 
-	if (ep->tx_pkt_efa_pool)
-		ofi_bufpool_destroy(ep->tx_pkt_efa_pool);
+	if (ep->efa_tx_pkt_pool)
+		ofi_bufpool_destroy(ep->efa_tx_pkt_pool);
 
 	return ret;
 }
@@ -1434,35 +1500,189 @@ struct fi_ops_cm rxr_ep_cm = {
 	.join = fi_no_join,
 };
 
-static inline int rxr_ep_bulk_post_recv(struct rxr_ep *ep)
+/*
+ * @brief explicitly allocate a chunk of memory for 5 packet pools on RX side:
+ *     efa's receive packet pool (efa_rx_pkt_pool)
+ *     shm's receive packet pool (shm_rx_pkt_pool)
+ *     unexpected packet pool (rx_unexp_pkt_pool),
+ *     out-of-order packet pool (rx_ooo_pkt_pool), and
+ *     local read-copy packet pool (rx_readcopy_pkt_pool).
+ *
+ * @param ep[in]	endpoint
+ * @return		On success, return 0
+ * 			On failure, return a negative error code.
+ */
+int rxr_ep_grow_rx_pkt_pools(struct rxr_ep *ep)
 {
-	uint64_t flags = FI_MORE;
-	int ret;
+	int err;
+
+	assert(ep->efa_rx_pkt_pool);
+	err = ofi_bufpool_grow(ep->efa_rx_pkt_pool);
+	if (err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"cannot allocate memory for EFA's RX packet pool. error: %s\n",
+			strerror(-err));
+		return err;
+	}
 
-	while (ep->rx_bufs_efa_to_post) {
-		if (ep->rx_bufs_efa_to_post == 1)
-			flags = 0;
-		ret = rxr_ep_post_buf(ep, NULL, flags, EFA_EP);
-		if (OFI_LIKELY(!ret))
-			ep->rx_bufs_efa_to_post--;
-		else
-			return ret;
+	if (ep->use_shm) {
+		assert(ep->shm_rx_pkt_pool);
+		err = ofi_bufpool_grow(ep->shm_rx_pkt_pool);
+		if (err) {
+			FI_WARN(&rxr_prov, FI_LOG_CQ,
+				"cannot allocate memory for SHM's RX packet pool. error: %s\n",
+				strerror(-err));
+			return err;
+		}
 	}
-	/* bulk post recv buf for shm provider */
-	flags = FI_MORE;
-	while (ep->use_shm && ep->rx_bufs_shm_to_post) {
-		if (ep->rx_bufs_shm_to_post == 1)
-			flags = 0;
-		ret = rxr_ep_post_buf(ep, NULL, flags, SHM_EP);
-		if (OFI_LIKELY(!ret))
-			ep->rx_bufs_shm_to_post--;
-		else
-			return ret;
+
+	assert(ep->rx_unexp_pkt_pool);
+	err = ofi_bufpool_grow(ep->rx_unexp_pkt_pool);
+	if (err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"cannot allocate memory for unexpected packet pool. error: %s\n",
+			strerror(-err));
+		return err;
+	}
+
+	assert(ep->rx_ooo_pkt_pool);
+	err = ofi_bufpool_grow(ep->rx_ooo_pkt_pool);
+	if (err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"cannot allocate memory for out-of-order packet pool. error: %s\n",
+			strerror(-err));
+		return err;
+	}
+
+	if (ep->rx_readcopy_pkt_pool) {
+		err = ofi_bufpool_grow(ep->rx_readcopy_pkt_pool);
+		if (err) {
+			FI_WARN(&rxr_prov, FI_LOG_CQ,
+				"cannot allocate and register memory for readcopy packet pool. error: %s\n",
+				strerror(-err));
+			return err;
+		}
 	}
 
 	return 0;
 }
 
+/**
+ * @brief post internal receive buffers for progress engine.
+ *
+ * It is more efficient to post multiple receive buffers
+ * to the device at once than to post each receive buffer
+ * individually.
+ *
+ * Therefore, after an internal receive buffer (a packet
+ * entry) was processed, it is not posted to the device
+ * right away.
+ *
+ * Instead, we increase counter
+ *      ep->efa/shm_rx_pkts_to_post
+ * by one.
+ *
+ * Later, progress engine calls this function to
+ * bulk post internal receive buffers (according to
+ * the counter).
+ *
+ * This function also control number of internal
+ * buffers posted to the device in zero copy receive
+ * mode.
+ *
+ * param[in]	ep	endpoint
+ */
+static inline
+void rxr_ep_progress_post_internal_rx_pkts(struct rxr_ep *ep)
+{
+	int err;
+
+	if (ep->use_zcpy_rx) {
+		/*
+		 * In zero copy receive mode,
+		 *
+		 * If application did not post any receive buffer,
+		 * we post one internal buffer so endpoint can
+		 * receive RxR control packets such as handshake.
+		 *
+		 * If buffers have posted to the device, we do NOT
+		 * repost internal buffers to maximize the chance
+		 * user buffer is used to receive data.
+		 */
+		if (ep->efa_rx_pkts_posted == 0 && ep->efa_rx_pkts_to_post == 0) {
+			ep->efa_rx_pkts_to_post = 1;
+		} else if (ep->efa_rx_pkts_posted > 0 && ep->efa_rx_pkts_to_post > 0){
+			ep->efa_rx_pkts_to_post = 0;
+		}
+	} else {
+		if (ep->efa_rx_pkts_posted == 0 && ep->efa_rx_pkts_to_post == 0) {
+			/* Both efa_rx_pkts_posted and efa_rx_pkts_to_post equal to 0 means
+			 * this is the first call of the progress engine on this endpoint.
+			 *
+			 * In this case, we explictly allocate the 1st chunk of memory
+			 * for unexp/ooo/readcopy RX packet pool.
+			 *
+			 * The reason to explicitly allocate the memory for RX packet
+			 * pool is to improve efficiency.
+			 *
+			 * Without explicit memory allocation, a pkt pools's memory
+			 * is allocated when 1st packet is allocated from it.
+			 * During the computation, different processes got their 1st
+			 * unexp/ooo/read-copy packet at different time. Therefore,
+			 * if we do not explicitly allocate memory at the beginning,
+			 * memory will be allocated at different time.
+			 *
+			 * When one process is allocating memory, other processes
+			 * have to wait. When each process allocate memory at different
+			 * time, the accumulated waiting time became significant.
+			 *
+			 * By explicitly allocating memory at 1st call to progress
+			 * engine, the memory allocation is parallelized.
+			 * (This assumes the 1st call to the progress engine on
+			 * all processes happen at roughly the same time, which
+			 * is a valid assumption according to our knowledge of
+			 * the workflow of most application)
+			 *
+			 * The memory was not allocated during endpoint initialization
+			 * because some applications will initialize some endpoints
+			 * but never uses it, thus allocating memory initialization
+			 * causes waste.
+			 */
+			err = rxr_ep_grow_rx_pkt_pools(ep);
+			if (err)
+				goto err_exit;
+
+			ep->efa_rx_pkts_to_post = rxr_get_rx_pool_chunk_cnt(ep);
+			ep->available_data_bufs = rxr_get_rx_pool_chunk_cnt(ep);
+
+			if (ep->use_shm) {
+				assert(ep->shm_rx_pkts_posted == 0 && ep->shm_rx_pkts_to_post == 0);
+				ep->shm_rx_pkts_to_post = shm_info->rx_attr->size;
+			}
+		}
+	}
+
+	err = rxr_ep_bulk_post_internal_rx_pkts(ep, ep->efa_rx_pkts_to_post, EFA_EP);
+	if (err)
+		goto err_exit;
+
+	ep->efa_rx_pkts_to_post = 0;
+
+	if (ep->use_shm) {
+		err = rxr_ep_bulk_post_internal_rx_pkts(ep, ep->shm_rx_pkts_to_post, SHM_EP);
+		if (err)
+			goto err_exit;
+
+		ep->shm_rx_pkts_to_post = 0;
+	}
+
+	return;
+
+err_exit:
+
+	efa_eq_write_error(&ep->util_ep, err, err);
+}
+
 static inline int rxr_ep_send_queued_pkts(struct rxr_ep *ep,
 					  struct dlist_entry *pkts)
 {
@@ -1476,10 +1696,22 @@ static inline int rxr_ep_send_queued_pkts(struct rxr_ep *ep,
 			dlist_remove(&pkt_entry->entry);
 			continue;
 		}
-		ret = rxr_pkt_entry_send(ep, pkt_entry, pkt_entry->addr);
-		if (ret)
-			return ret;
+
+		/* If send succeeded, pkt_entry->entry will be added
+		 * to peer->outstanding_tx_pkts. Therefore, it must
+		 * be removed from the list before send.
+		 */
 		dlist_remove(&pkt_entry->entry);
+
+		ret = rxr_pkt_entry_send(ep, pkt_entry, 0);
+		if (ret) {
+			if (ret == -FI_EAGAIN) {
+				/* add the pkt back to pkts, so it can be resent again */
+				dlist_insert_tail(&pkt_entry->entry, pkts);
+			}
+
+			return ret;
+		}
 	}
 	return 0;
 }
@@ -1500,28 +1732,142 @@ static inline void rxr_ep_check_available_data_bufs_timer(struct rxr_ep *ep)
 
 static inline void rxr_ep_check_peer_backoff_timer(struct rxr_ep *ep)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct dlist_entry *tmp;
 
 	if (OFI_LIKELY(dlist_empty(&ep->peer_backoff_list)))
 		return;
 
-	dlist_foreach_container_safe(&ep->peer_backoff_list, struct rxr_peer,
-				     peer, rnr_entry, tmp) {
-		peer->flags &= ~RXR_PEER_BACKED_OFF;
-		if (!rxr_peer_timeout_expired(ep, peer, ofi_gettime_us()))
-			continue;
-		peer->flags &= ~RXR_PEER_IN_BACKOFF;
-		dlist_remove(&peer->rnr_entry);
+	dlist_foreach_container_safe(&ep->peer_backoff_list, struct rdm_peer,
+				     peer, rnr_backoff_entry, tmp) {
+		if (ofi_gettime_us() >= peer->rnr_backoff_begin_ts +
+					peer->rnr_backoff_wait_time) {
+			peer->flags &= ~RXR_PEER_IN_BACKOFF;
+			dlist_remove(&peer->rnr_backoff_entry);
+		}
+	}
+}
+
+/**
+ * @brief poll rdma-core cq and process the cq entry
+ *
+ * @param[in]	ep		end point
+ * @param[in]	cqe_to_process	max number of cq entry to poll and process
+ */
+static inline void rdm_ep_poll_ibv_cq(struct rxr_ep *ep,
+				      size_t cqe_to_process)
+{
+	struct ibv_wc ibv_wc;
+	uint32_t *connid;
+	struct efa_cq *efa_cq;
+	struct efa_av *efa_av;
+	struct efa_ep *efa_ep;
+	struct rxr_pkt_entry *pkt_entry;
+	ssize_t ret;
+	int i, err, prov_errno;
+
+	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
+	efa_av = efa_ep->av;
+	efa_cq = container_of(ep->rdm_cq, struct efa_cq, util_cq.cq_fid);
+	for (i = 0; i < cqe_to_process; i++) {
+		ret = ibv_poll_cq(efa_cq->ibv_cq, 1, &ibv_wc);
+
+		if (ret == 0)
+			return;
+
+		if (OFI_UNLIKELY(ret < 0 || ibv_wc.status)) {
+			if (ret < 0) {
+				efa_eq_write_error(&ep->util_ep, -ret, -ret);
+				return;
+			}
+
+			pkt_entry = (void *)(uintptr_t)ibv_wc.wr_id;
+			err = ibv_wc.status;
+			prov_errno = ibv_wc.status;
+			if (ibv_wc.opcode == IBV_WC_SEND) {
+#if ENABLE_DEBUG
+				ep->failed_send_comps++;
+#endif
+				rxr_pkt_handle_send_error(ep, pkt_entry, err, prov_errno);
+			} else {
+				assert(ibv_wc.opcode == IBV_WC_RECV);
+				rxr_pkt_handle_recv_error(ep, pkt_entry, err, prov_errno);
+			}
+
+			return;
+		}
+
+		pkt_entry = (void *)(uintptr_t)ibv_wc.wr_id;
+
+		switch (ibv_wc.opcode) {
+		case IBV_WC_SEND:
+#if ENABLE_DEBUG
+			ep->send_comps++;
+#endif
+			rxr_pkt_handle_send_completion(ep, pkt_entry);
+			break;
+		case IBV_WC_RECV:
+			connid = rxr_pkt_connid_ptr(pkt_entry);
+			if (!connid) {
+				FI_WARN_ONCE(&rxr_prov, FI_LOG_EP_CTRL,
+					     "An incoming packet does have connection ID in its header.\n"
+					     "This means the peer is using an older version of libfabric.\n"
+					     "The communication can continue but it is encouraged to use\n"
+					     "a newer version of libfabric\n");
+
+				connid = efa_av_lookup_latest_connid(efa_av, ibv_wc.slid, ibv_wc.src_qp);
+				/* We should always be able to find latest connid for a given ahn+qpn,
+				 * therefore an assertion is used here.
+				 */
+				if (!connid)
+					FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "Cannot find latest connid in connid map!");
+				assert(connid);
+			}
+
+			pkt_entry->addr = efa_av_reverse_lookup(efa_av, ibv_wc.slid, ibv_wc.src_qp, *connid);
+			pkt_entry->pkt_size = ibv_wc.byte_len;
+			assert(pkt_entry->pkt_size > 0);
+			rxr_pkt_handle_recv_completion(ep, pkt_entry);
+#if ENABLE_DEBUG
+			ep->recv_comps++;
+#endif
+			break;
+		default:
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"Unhandled cq type\n");
+			assert(0 && "Unhandled cq type");
+		}
 	}
 }
 
-static inline void rxr_ep_poll_cq(struct rxr_ep *ep,
-				  struct fid_cq *cq,
-				  size_t cqe_to_process,
-				  bool is_shm_cq)
+static inline
+void rdm_ep_poll_shm_err_cq(struct fid_cq *shm_cq, struct fi_cq_err_entry *cq_err_entry)
+{
+	int ret;
+
+	ret = fi_cq_readerr(shm_cq, cq_err_entry, 0);
+	if (ret == 1)
+		return;
+
+	if (ret < 0) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "encountered error when fi_cq_readerr: %s\n",
+			fi_strerror(-ret));
+		cq_err_entry->err = -ret;
+		cq_err_entry->prov_errno = -ret;
+		return;
+	}
+
+	FI_WARN(&rxr_prov, FI_LOG_CQ, "fi_cq_readerr got expected return: %d\n", ret);
+	cq_err_entry->err = FI_EIO;
+	cq_err_entry->prov_errno = FI_EIO;
+}
+
+static inline void rdm_ep_poll_shm_cq(struct rxr_ep *ep,
+				      size_t cqe_to_process)
 {
 	struct fi_cq_data_entry cq_entry;
+	struct fi_cq_err_entry cq_err_entry = { 0 };
+	struct rxr_pkt_entry *pkt_entry;
 	fi_addr_t src_addr;
 	ssize_t ret;
 	struct efa_ep *efa_ep;
@@ -1533,43 +1879,50 @@ static inline void rxr_ep_poll_cq(struct rxr_ep *ep,
 	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
 	efa_av = efa_ep->av;
 	for (i = 0; i < cqe_to_process; i++) {
-		ret = fi_cq_readfrom(cq, &cq_entry, 1, &src_addr);
+		ret = fi_cq_readfrom(ep->shm_cq, &cq_entry, 1, &src_addr);
 
 		if (ret == -FI_EAGAIN)
 			return;
 
 		if (OFI_UNLIKELY(ret < 0)) {
-			if (rxr_cq_handle_cq_error(ep, ret))
-				assert(0 &&
-				       "error writing error cq entry after reading from cq");
-			if (!ep->use_zcpy_rx)
-				rxr_ep_bulk_post_recv(ep);
+			if (ret != -FI_EAVAIL) {
+				efa_eq_write_error(&ep->util_ep, -ret, -ret);
+				return;
+			}
+
+			rdm_ep_poll_shm_err_cq(ep->shm_cq, &cq_err_entry);
+			if (cq_err_entry.flags & (FI_SEND | FI_READ | FI_WRITE)) {
+				assert(cq_entry.op_context);
+				rxr_pkt_handle_send_error(ep, cq_entry.op_context, cq_err_entry.err, cq_err_entry.prov_errno);
+			} else if (cq_err_entry.flags & FI_RECV) {
+				assert(cq_entry.op_context);
+				rxr_pkt_handle_recv_error(ep, cq_entry.op_context, cq_err_entry.err, cq_err_entry.prov_errno);
+			} else {
+				efa_eq_write_error(&ep->util_ep, cq_err_entry.err, cq_err_entry.prov_errno);
+			}
+
 			return;
 		}
 
 		if (OFI_UNLIKELY(ret == 0))
 			return;
 
-		if (is_shm_cq && src_addr != FI_ADDR_UNSPEC) {
+		pkt_entry = cq_entry.op_context;
+		if (src_addr != FI_ADDR_UNSPEC) {
 			/* convert SHM address to EFA address */
 			assert(src_addr < EFA_SHM_MAX_AV_COUNT);
 			src_addr = efa_av->shm_rdm_addr_map[src_addr];
 		}
 
-		if (is_shm_cq && (cq_entry.flags & (FI_ATOMIC | FI_REMOTE_CQ_DATA))) {
+		if (cq_entry.flags & (FI_ATOMIC | FI_REMOTE_CQ_DATA)) {
 			rxr_cq_handle_shm_completion(ep, &cq_entry, src_addr);
 		} else if (cq_entry.flags & (FI_SEND | FI_READ | FI_WRITE)) {
-#if ENABLE_DEBUG
-			if (!is_shm_cq)
-				ep->send_comps++;
-#endif
-			rxr_pkt_handle_send_completion(ep, &cq_entry);
+			rxr_pkt_handle_send_completion(ep, pkt_entry);
 		} else if (cq_entry.flags & (FI_RECV | FI_REMOTE_CQ_DATA)) {
-			rxr_pkt_handle_recv_completion(ep, &cq_entry, src_addr);
-#if ENABLE_DEBUG
-			if (!is_shm_cq)
-				ep->recv_comps++;
-#endif
+			pkt_entry->addr = src_addr;
+			pkt_entry->pkt_size = cq_entry.len;
+			assert(pkt_entry->pkt_size > 0);
+			rxr_pkt_handle_recv_completion(ep, pkt_entry);
 		} else {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unhandled cq type\n");
@@ -1585,107 +1938,172 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_tx_entry *tx_entry;
 	struct rxr_read_entry *read_entry;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct dlist_entry *tmp;
 	ssize_t ret;
-
-	if (!ep->use_zcpy_rx)
-		rxr_ep_check_available_data_bufs_timer(ep);
+	uint64_t flags;
 
 	// Poll the EFA completion queue
-	rxr_ep_poll_cq(ep, ep->rdm_cq, rxr_env.efa_cq_read_size, 0);
+	rdm_ep_poll_ibv_cq(ep, rxr_env.efa_cq_read_size);
 
 	// Poll the SHM completion queue if enabled
 	if (ep->use_shm)
-		rxr_ep_poll_cq(ep, ep->shm_cq, rxr_env.shm_cq_read_size, 1);
+		rdm_ep_poll_shm_cq(ep, rxr_env.shm_cq_read_size);
 
-	if (!ep->use_zcpy_rx) {
-		ret = rxr_ep_bulk_post_recv(ep);
+	rxr_ep_progress_post_internal_rx_pkts(ep);
+
+	rxr_ep_check_peer_backoff_timer(ep);
+
+	if (!ep->use_zcpy_rx)
+		rxr_ep_check_available_data_bufs_timer(ep);
+	/*
+	 * Resend handshake packet for any peers where the first
+	 * handshake send failed.
+	 */
+	dlist_foreach_container_safe(&ep->handshake_queued_peer_list,
+				     struct rdm_peer, peer,
+				     handshake_queued_entry, tmp) {
+		if (peer->flags & RXR_PEER_IN_BACKOFF)
+			continue;
+
+		ret = rxr_pkt_post_handshake(ep, peer);
+		if (ret == -FI_EAGAIN)
+			break;
 
 		if (OFI_UNLIKELY(ret)) {
-			if (rxr_cq_handle_cq_error(ep, ret))
-				assert(0 &&
-				       "error writing error cq entry after failed post recv");
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"Failed to post HANDSHAKE to peer %ld: %s\n",
+				peer->efa_fiaddr, fi_strerror(-ret));
+			efa_eq_write_error(&ep->util_ep, FI_EIO, -ret);
 			return;
 		}
-	}
 
-	rxr_ep_check_peer_backoff_timer(ep);
+		dlist_remove(&peer->handshake_queued_entry);
+		peer->flags &= ~RXR_PEER_HANDSHAKE_QUEUED;
+		peer->flags |= RXR_PEER_HANDSHAKE_SENT;
+	}
 
 	/*
 	 * Send any queued ctrl packets.
 	 */
-	dlist_foreach_container_safe(&ep->rx_entry_queued_list,
+	dlist_foreach_container_safe(&ep->rx_entry_queued_rnr_list,
 				     struct rxr_rx_entry,
-				     rx_entry, queued_entry, tmp) {
+				     rx_entry, queued_rnr_entry, tmp) {
 		peer = rxr_ep_get_peer(ep, rx_entry->addr);
+		assert(peer);
 
 		if (peer->flags & RXR_PEER_IN_BACKOFF)
 			continue;
 
-		if (rx_entry->state == RXR_RX_QUEUED_CTRL) {
-			/*
-			 * We should only have one packet pending at a time for
-			 * rx_entry. Either the send failed due to RNR or the
-			 * rx_entry is queued but not both.
-			 */
-			assert(dlist_empty(&rx_entry->queued_pkts));
-			ret = rxr_pkt_post_ctrl(ep, RXR_RX_ENTRY, rx_entry,
-						rx_entry->queued_ctrl.type,
-						rx_entry->queued_ctrl.inject);
-		} else {
-			ret = rxr_ep_send_queued_pkts(ep, &rx_entry->queued_pkts);
+		assert(rx_entry->rxr_flags & RXR_RX_ENTRY_QUEUED_RNR);
+		assert(!dlist_empty(&rx_entry->queued_pkts));
+		ret = rxr_ep_send_queued_pkts(ep, &rx_entry->queued_pkts);
+
+		if (ret == -FI_EAGAIN)
+			break;
+
+		if (OFI_UNLIKELY(ret)) {
+			rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
+			return;
 		}
 
+		dlist_remove(&rx_entry->queued_rnr_entry);
+		rx_entry->rxr_flags &= ~RXR_RX_ENTRY_QUEUED_RNR;
+	}
+
+	dlist_foreach_container_safe(&ep->rx_entry_queued_ctrl_list,
+				     struct rxr_rx_entry,
+				     rx_entry, queued_ctrl_entry, tmp) {
+		peer = rxr_ep_get_peer(ep, rx_entry->addr);
+		assert(peer);
+
+		if (peer->flags & RXR_PEER_IN_BACKOFF)
+			continue;
+		/*
+		 * rx_entry only send one ctrl packet at a time. The
+		 * ctrl packet can be CTS, EOR, RECEIPT.
+		 */
+		assert(rx_entry->state == RXR_RX_QUEUED_CTRL);
+		ret = rxr_pkt_post_ctrl(ep, RXR_RX_ENTRY, rx_entry,
+					rx_entry->queued_ctrl.type,
+					rx_entry->queued_ctrl.inject,
+					0);
 		if (ret == -FI_EAGAIN)
 			break;
-		if (OFI_UNLIKELY(ret))
-			goto rx_err;
 
-		dlist_remove(&rx_entry->queued_entry);
+		if (OFI_UNLIKELY(ret)) {
+			rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
+			return;
+		}
+
+		/* it can happen that rxr_pkt_post_ctrl() released rx_entry
+		 * (if the packet type is EOR and inject is used). In
+		 * that case rx_entry's state has been set to RXR_RX_FREE and
+		 * it has been removed from ep->rx_queued_entry_list, so nothing
+		 * is left to do.
+		 */
+		if (rx_entry->state == RXR_RX_FREE)
+			continue;
+
+		dlist_remove(&rx_entry->queued_ctrl_entry);
+		/*
+		 * For CTS packet, the state need to be RXR_RX_RECV.
+		 * For EOR/RECEIPT, all data has been received, so any state
+		 * other than RXR_RX_QUEUED_CTRL should work.
+		 * In all, we set the state to RXR_RX_RECV
+		 */
 		rx_entry->state = RXR_RX_RECV;
 	}
 
-	dlist_foreach_container_safe(&ep->tx_entry_queued_list,
+	dlist_foreach_container_safe(&ep->tx_entry_queued_rnr_list,
 				     struct rxr_tx_entry,
-				     tx_entry, queued_entry, tmp) {
+				     tx_entry, queued_rnr_entry, tmp) {
 		peer = rxr_ep_get_peer(ep, tx_entry->addr);
+		assert(peer);
 
 		if (peer->flags & RXR_PEER_IN_BACKOFF)
 			continue;
 
-		/*
-		 * It is possible to receive an RNR after we queue this
-		 * tx_entry if we run out of resources in the medium message
-		 * protocol. Ensure all queued packets are posted before
-		 * continuing to post additional control messages.
-		 */
+		assert(tx_entry->rxr_flags & RXR_TX_ENTRY_QUEUED_RNR);
 		ret = rxr_ep_send_queued_pkts(ep, &tx_entry->queued_pkts);
 		if (ret == -FI_EAGAIN)
 			break;
-		if (OFI_UNLIKELY(ret))
-			goto tx_err;
 
-		if (tx_entry->state == RXR_TX_QUEUED_CTRL) {
-			ret = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry,
-						tx_entry->queued_ctrl.type,
-						tx_entry->queued_ctrl.inject);
-			if (ret == -FI_EAGAIN)
-				break;
-			if (OFI_UNLIKELY(ret))
-				goto tx_err;
+		if (OFI_UNLIKELY(ret)) {
+			rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
+			return;
 		}
 
-		dlist_remove(&tx_entry->queued_entry);
+		dlist_remove(&tx_entry->queued_rnr_entry);
+		tx_entry->rxr_flags &= ~RXR_TX_ENTRY_QUEUED_RNR;
+	}
 
-		if (tx_entry->state == RXR_TX_QUEUED_REQ_RNR ||
-		    tx_entry->state == RXR_TX_QUEUED_CTRL) {
-			tx_entry->state = RXR_TX_REQ;
-		} else if (tx_entry->state == RXR_TX_QUEUED_DATA_RNR) {
-			tx_entry->state = RXR_TX_SEND;
-			dlist_insert_tail(&tx_entry->entry,
-					  &ep->tx_pending_list);
+	dlist_foreach_container_safe(&ep->tx_entry_queued_ctrl_list,
+				     struct rxr_tx_entry,
+				     tx_entry, queued_ctrl_entry, tmp) {
+		peer = rxr_ep_get_peer(ep, tx_entry->addr);
+		assert(peer);
+
+		if (peer->flags & RXR_PEER_IN_BACKOFF)
+			continue;
+
+		assert(tx_entry->state == RXR_TX_QUEUED_CTRL);
+
+		ret = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry,
+					tx_entry->queued_ctrl.type,
+					tx_entry->queued_ctrl.inject,
+					0);
+		if (ret == -FI_EAGAIN)
+			break;
+
+		if (OFI_UNLIKELY(ret)) {
+			rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
+			return;
 		}
+
+		dlist_remove(&tx_entry->queued_ctrl_entry);
+		if (tx_entry->state == RXR_TX_QUEUED_CTRL)
+			tx_entry->state = RXR_TX_REQ;
 	}
 
 	/*
@@ -1694,35 +2112,58 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 	dlist_foreach_container(&ep->tx_pending_list, struct rxr_tx_entry,
 				tx_entry, entry) {
 		peer = rxr_ep_get_peer(ep, tx_entry->addr);
+		assert(peer);
 
 		if (peer->flags & RXR_PEER_IN_BACKOFF)
 			continue;
 
-		if (tx_entry->window > 0)
-			tx_entry->send_flags |= FI_MORE;
-		else
+		/*
+		 * Do not send DATA packet until we received HANDSHAKE packet from the peer,
+		 * this is because endpoint does not know whether peer need connid in header
+		 * until it get the HANDSHAKE packet.
+		 *
+		 * We only do this for DATA packet because other types of packets always
+		 * has connid in there packet header. If peer does not make use of the connid,
+		 * the connid can be safely ignored.
+		 *
+		 * DATA packet is different because for DATA packet connid is an optional
+		 * header inserted between the mandatory header and the application data.
+		 * Therefore if peer does not use/understand connid, it will take connid
+		 * as application data thus cause data corruption.
+		 *
+		 * This will not cause deadlock because peer will send a HANDSHAKE packet
+		 * back upon receiving 1st packet from the endpoint, and in all 3 sub0protocols
+		 * (long-CTS message, emulated long-CTS write and emulated long-CTS read)
+		 * where DATA packet is used, endpoint will send other types of packet to
+		 * peer before sending DATA packets. The workflow of the 3 sub-protocol
+		 * can be found in protocol v4 document chapter 3.
+		 */
+		if (!(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED))
 			continue;
 
 		while (tx_entry->window > 0) {
-			if (ep->max_outstanding_tx - ep->tx_pending <= 1 ||
+			flags = FI_MORE;
+			if (ep->efa_max_outstanding_tx_ops - ep->efa_outstanding_tx_ops <= 1 ||
 			    tx_entry->window <= ep->max_data_payload_size)
-				tx_entry->send_flags &= ~FI_MORE;
+				flags = 0;
 			/*
 			 * The core's TX queue is full so we can't do any
 			 * additional work.
 			 */
-			if (ep->tx_pending == ep->max_outstanding_tx)
+			if (ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 				goto out;
 
 			if (peer->flags & RXR_PEER_IN_BACKOFF)
 				break;
 
-			ret = rxr_pkt_post_data(ep, tx_entry);
+			ret = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry,
+						RXR_DATA_PKT, false, flags);
 			if (OFI_UNLIKELY(ret)) {
-				tx_entry->send_flags &= ~FI_MORE;
 				if (ret == -FI_EAGAIN)
 					goto out;
-				goto tx_err;
+
+				rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
+				return;
 			}
 		}
 	}
@@ -1733,6 +2174,7 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 	dlist_foreach_container_safe(&ep->read_pending_list, struct rxr_read_entry,
 				     read_entry, pending_entry, tmp) {
 		peer = rxr_ep_get_peer(ep, read_entry->addr);
+		assert(peer);
 
 		if (peer->flags & RXR_PEER_IN_BACKOFF)
 			continue;
@@ -1741,15 +2183,17 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 		 * The core's TX queue is full so we can't do any
 		 * additional work.
 		 */
-		if (ep->tx_pending == ep->max_outstanding_tx)
+		if (ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 			goto out;
 
 		ret = rxr_read_post(ep, read_entry);
 		if (ret == -FI_EAGAIN)
 			break;
 
-		if (OFI_UNLIKELY(ret))
-			goto read_err;
+		if (OFI_UNLIKELY(ret)) {
+			rxr_read_write_error(ep, read_entry, -ret, -ret);
+			return;
+		}
 
 		read_entry->state = RXR_RDMA_ENTRY_SUBMITTED;
 		dlist_remove(&read_entry->pending_entry);
@@ -1760,26 +2204,10 @@ out:
 	if (efa_ep->xmit_more_wr_tail != &efa_ep->xmit_more_wr_head) {
 		ret = efa_post_flush(efa_ep, &bad_wr);
 		if (OFI_UNLIKELY(ret))
-			goto tx_err;
+			efa_eq_write_error(&ep->util_ep, -ret, -ret);
 	}
 
 	return;
-rx_err:
-	if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-		assert(0 &&
-		       "error writing error cq entry when handling RX error");
-	return;
-tx_err:
-	if (rxr_cq_handle_tx_error(ep, tx_entry, ret))
-		assert(0 &&
-		       "error writing error cq entry when handling TX error");
-	return;
-
-read_err:
-	if (rxr_read_handle_error(ep, read_entry, ret))
-		assert(0 &&
-		       "error writing err cq entry while handling RDMA error");
-	return;
 }
 
 void rxr_ep_progress(struct util_ep *util_ep)
@@ -1812,6 +2240,23 @@ bool rxr_ep_use_shm(struct fi_info *info)
 	    && !(info->caps & FI_LOCAL_COMM))
 		return 0;
 
+	/*
+	 * Currently, shm provider uses the SAR protocol for cuda
+	 * memory buffer, whose performance is worse than using EFA device.
+	 *
+	 * To address this issue, shm usage is disabled if application
+	 * requested the FI_HMEM capablity.
+	 *
+	 * This is not ideal, because host memory commuications are
+	 * also going through device.
+	 *
+	 * The long term fix is make shm provider to support cuda
+	 * buffers through cuda IPC. Once that is implemented, the
+	 * following two lines need to be removed.
+	 */
+	if (info && (info->caps & FI_HMEM))
+		return 0;
+
 	return rxr_env.enable_shm_transfer;
 }
 
@@ -1870,7 +2315,8 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	rxr_ep->tx_size = info->tx_attr->size;
 	rxr_ep->rx_iov_limit = info->rx_attr->iov_limit;
 	rxr_ep->tx_iov_limit = info->tx_attr->iov_limit;
-	rxr_ep->max_outstanding_tx = rdm_info->tx_attr->size;
+	rxr_ep->inject_size = info->tx_attr->inject_size;
+	rxr_ep->efa_max_outstanding_tx_ops = rdm_info->tx_attr->size;
 	rxr_ep->core_rx_size = rdm_info->rx_attr->size;
 	rxr_ep->core_iov_limit = rdm_info->tx_attr->iov_limit;
 	rxr_ep->core_caps = rdm_info->caps;
@@ -1886,6 +2332,7 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	rxr_ep->core_msg_order = rdm_info->rx_attr->msg_order;
 	rxr_ep->core_inject_size = rdm_info->tx_attr->inject_size;
 	rxr_ep->max_msg_size = info->ep_attr->max_msg_size;
+	rxr_ep->msg_prefix_size = info->ep_attr->msg_prefix_size;
 	rxr_ep->max_proto_hdr_size = rxr_pkt_max_header_size();
 	rxr_ep->mtu_size = rdm_info->ep_attr->max_msg_size;
 	fi_freeinfo(rdm_info);
@@ -1896,12 +2343,12 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	if (rxr_ep->mtu_size > RXR_MTU_MAX_LIMIT)
 		rxr_ep->mtu_size = RXR_MTU_MAX_LIMIT;
 
-	rxr_ep->max_data_payload_size = rxr_ep->mtu_size - sizeof(struct rxr_data_hdr);
+	rxr_ep->max_data_payload_size = rxr_ep->mtu_size - sizeof(struct rxr_data_hdr) - sizeof(struct rxr_data_opt_connid_hdr);
 	rxr_ep->min_multi_recv_size = rxr_ep->mtu_size - rxr_ep->max_proto_hdr_size;
 
 	if (rxr_env.tx_queue_size > 0 &&
-	    rxr_env.tx_queue_size < rxr_ep->max_outstanding_tx)
-		rxr_ep->max_outstanding_tx = rxr_env.tx_queue_size;
+	    rxr_env.tx_queue_size < rxr_ep->efa_max_outstanding_tx_ops)
+		rxr_ep->efa_max_outstanding_tx_ops = rxr_env.tx_queue_size;
 
 
 	rxr_ep->use_zcpy_rx = rxr_ep_use_zcpy_rx(rxr_ep, info);
@@ -1913,17 +2360,19 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 		rxr_ep->handle_resource_management);
 
 #if ENABLE_DEBUG
-	rxr_ep->sends = 0;
+	rxr_ep->efa_total_posted_tx_ops = 0;
+	rxr_ep->shm_total_posted_tx_ops = 0;
 	rxr_ep->send_comps = 0;
 	rxr_ep->failed_send_comps = 0;
 	rxr_ep->recv_comps = 0;
 #endif
 
-	rxr_ep->posted_bufs_shm = 0;
-	rxr_ep->rx_bufs_shm_to_post = 0;
-	rxr_ep->posted_bufs_efa = 0;
-	rxr_ep->rx_bufs_efa_to_post = 0;
-	rxr_ep->tx_pending = 0;
+	rxr_ep->shm_rx_pkts_posted = 0;
+	rxr_ep->shm_rx_pkts_to_post = 0;
+	rxr_ep->efa_rx_pkts_posted = 0;
+	rxr_ep->efa_rx_pkts_to_post = 0;
+	rxr_ep->efa_outstanding_tx_ops = 0;
+	rxr_ep->shm_outstanding_tx_ops = 0;
 	rxr_ep->available_data_bufs_ts = 0;
 
 	ret = fi_cq_open(rxr_domain->rdm_domain, &cq_attr,
@@ -1999,3 +2448,120 @@ err_free_ep:
 	free(rxr_ep);
 	return ret;
 }
+
+/**
+ * @brief record the event that a TX op has been submitted
+ *
+ * This function is called after a TX operation has been posted
+ * successfully. It will:
+ *
+ *  1. increase the outstanding tx_op counter in endpoint and
+ *     in the peer structure.
+ *
+ *  2. add the TX packet to peer's outstanding TX packet list.
+ *
+ * Both send and read are considered TX operation.
+ *
+ * The tx_op counters used to prevent over posting the device
+ * and used in flow control. They are also usefull for debugging.
+ *
+ * Peer's outstanding TX packet list is used when removing a peer
+ * to invalidate address of these packets, so that the completion
+ * of these packet is ignored.
+ *
+ * @param[in,out]	ep		endpoint
+ * @param[in]		pkt_entry	TX pkt_entry, which contains
+ * 					the info of the TX op.
+ */
+void rxr_ep_record_tx_op_submitted(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rdm_peer *peer;
+
+	/*
+	 * peer can be NULL when the pkt_entry is a RMA_CONTEXT_PKT,
+	 * and the RMA is a local read toward the endpoint itself
+	 */
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (peer)
+		dlist_insert_tail(&pkt_entry->entry, &peer->outstanding_tx_pkts);
+
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL) {
+		ep->efa_outstanding_tx_ops++;
+		if (peer)
+			peer->efa_outstanding_tx_ops++;
+#if ENABLE_DEBUG
+		ep->efa_total_posted_tx_ops++;
+#endif
+	} else {
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_SHM_TX_POOL);
+		ep->shm_outstanding_tx_ops++;
+		if (peer)
+			peer->shm_outstanding_tx_ops++;
+#if ENABLE_DEBUG
+		ep->shm_total_posted_tx_ops++;
+#endif
+	}
+}
+
+/**
+ * @brief record the event that an TX op is completed
+ *
+ * This function is called when the completion of
+ * a TX operation is received. It will
+ *
+ * 1. decrease the outstanding tx_op counter in the endpoint
+ *    and in the peer.
+ *
+ * 2. remove the TX packet from peer's outstanding
+ *    TX packet list.
+ *
+ * Both send and read are considered TX operation.
+ *
+ * One may ask why this function is not integrated
+ * into rxr_pkt_entry_relase_tx()?
+ *
+ * The reason is the action of decrease tx_op counter
+ * is not tied to releasing a TX pkt_entry.
+ *
+ * Sometimes we need to decreate the tx_op counter
+ * without releasing a TX pkt_entry. For example,
+ * we handle a TX pkt_entry encountered RNR. We need
+ * to decrease the tx_op counter and queue the packet.
+ *
+ * Sometimes we need release TX pkt_entry without
+ * decreasing the tx_op counter. For example, when
+ * rxr_pkt_post_ctrl() failed to post a pkt entry.
+ *
+ * @param[in,out]	ep		endpoint
+ * @param[in]		pkt_entry	TX pkt_entry, which contains
+ * 					the info of the TX op
+ */
+void rxr_ep_record_tx_op_completed(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rdm_peer *peer;
+
+	/*
+	 * peer can be NULL when:
+	 *
+	 * 1. the pkt_entry is a RMA_CONTEXT_PKT, and the RMA op is a local read
+	 *    toward the endpoint itself.
+	 * 2. peer's address has been removed from address vector. Either because
+	 *    a new peer has the same GID+QPN was inserted to address, or because
+	 *    application removed the peer from address vector.
+	 */
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (peer)
+		dlist_remove(&pkt_entry->entry);
+
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL) {
+		ep->efa_outstanding_tx_ops--;
+		if (peer)
+			peer->efa_outstanding_tx_ops--;
+	} else {
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_SHM_TX_POOL);
+		ep->shm_outstanding_tx_ops--;
+		if (peer)
+			peer->shm_outstanding_tx_ops--;
+	}
+}
+
diff --git a/prov/efa/src/rxr/rxr_fabric.c b/prov/efa/src/rxr/rxr_fabric.c
deleted file mode 100644
index 163c525..0000000
--- a/prov/efa/src/rxr/rxr_fabric.c
+++ /dev/null
@@ -1,181 +0,0 @@
-/*
- * Copyright (c) 2019 Amazon.com, Inc. or its affiliates.
- * All rights reserved.
- *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * BSD license below:
- *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
- */
-
-#include <stdlib.h>
-#include <string.h>
-
-#include <ofi_perf.h>
-
-#include "rxr.h"
-
-#ifdef RXR_PERF_ENABLED
-const char *rxr_perf_counters_str[] = {
-	RXR_PERF_FOREACH(OFI_STR)
-};
-#endif
-
-static struct fi_ops_fabric rxr_fabric_ops = {
-	.size = sizeof(struct fi_ops_fabric),
-	.domain = rxr_domain_open,
-	.passive_ep = fi_no_passive_ep,
-	.eq_open = ofi_eq_create,
-	.wait_open = ofi_wait_fd_open,
-	.trywait = ofi_trywait
-};
-
-static int rxr_fabric_close(fid_t fid)
-{
-	int ret;
-	struct rxr_fabric *rxr_fabric;
-
-	rxr_fabric = container_of(fid, struct rxr_fabric,
-				  util_fabric.fabric_fid.fid);
-	ret = fi_close(&rxr_fabric->lower_fabric->fid);
-	if (ret)
-		return ret;
-
-	if (rxr_env.enable_shm_transfer) {
-		ret = fi_close(&rxr_fabric->shm_fabric->fid);
-		if (ret)
-			return ret;
-	}
-
-	ret = ofi_fabric_close(&rxr_fabric->util_fabric);
-	if (ret)
-		return ret;
-
-#ifdef RXR_PERF_ENABLED
-	ofi_perfset_log(&rxr_fabric->perf_set, rxr_perf_counters_str);
-	ofi_perfset_close(&rxr_fabric->perf_set);
-#endif
-	free(rxr_fabric);
-	return 0;
-}
-
-static struct fi_ops rxr_fabric_fi_ops = {
-	.size = sizeof(struct fi_ops),
-	.close = rxr_fabric_close,
-	.bind = fi_no_bind,
-	.control = fi_no_control,
-	.ops_open = fi_no_ops_open,
-};
-
-int rxr_fabric(struct fi_fabric_attr *attr, struct fid_fabric **fabric,
-	       void *context)
-{
-	struct rxr_fabric *rxr_fabric;
-	struct fi_info hints, *rdm_info;
-	int ret, retv;
-
-	rxr_fabric = calloc(1, sizeof(*rxr_fabric));
-	if (!rxr_fabric)
-		return -FI_ENOMEM;
-
-	ret = ofi_fabric_init(&rxr_prov, &rxr_fabric_attr, attr,
-			      &rxr_fabric->util_fabric, context);
-	if (ret)
-		goto err_free_fabric;
-
-	memset(&hints, 0, sizeof(hints));
-	hints.fabric_attr = calloc(1, sizeof(*hints.fabric_attr));
-	if (!hints.fabric_attr) {
-		ret = -FI_ENOMEM;
-		goto err_close_util_fabric;
-	}
-	hints.fabric_attr->name = attr->name;
-	hints.fabric_attr->api_version = attr->api_version;
-	hints.mode = ~0;
-
-	ret = lower_efa_prov->getinfo(attr->api_version, NULL, NULL, 0, &hints,
-				      &rdm_info);
-	if (ret) {
-		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
-			"Unable to get core info!\n");
-		ret = -FI_EINVAL;
-		goto err_free_hints;
-	}
-
-	ret = lower_efa_prov->fabric(rdm_info->fabric_attr,
-				     &rxr_fabric->lower_fabric, context);
-	if (ret)
-		goto err_free_rdm_info;
-
-	/* Open shm provider's fabric domain */
-	if (rxr_env.enable_shm_transfer) {
-		assert(!strcmp(shm_info->fabric_attr->name, "shm"));
-		ret = fi_fabric(shm_info->fabric_attr,
-				       &rxr_fabric->shm_fabric, context);
-		if (ret)
-			goto err_close_rdm_fabric;
-	}
-
-
-#ifdef RXR_PERF_ENABLED
-	ret = ofi_perfset_create(&rxr_prov, &rxr_fabric->perf_set,
-				 rxr_perf_size, perf_domain, perf_cntr,
-				 perf_flags);
-
-	if (ret)
-		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
-			"Error initializing RxR perfset: %s\n",
-			fi_strerror(-ret));
-#endif
-
-	*fabric = &rxr_fabric->util_fabric.fabric_fid;
-	(*fabric)->fid.ops = &rxr_fabric_fi_ops;
-	(*fabric)->ops = &rxr_fabric_ops;
-
-	free(hints.fabric_attr);
-	fi_freeinfo(rdm_info);
-	return 0;
-
-err_close_rdm_fabric:
-	retv = fi_close(&rxr_fabric->lower_fabric->fid);
-	if (retv)
-		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
-			"Unable to close lower rdm fabric: %s\n",
-			fi_strerror(-retv));
-err_free_rdm_info:
-	fi_freeinfo(rdm_info);
-err_free_hints:
-	free(hints.fabric_attr);
-err_close_util_fabric:
-	retv = ofi_fabric_close(&rxr_fabric->util_fabric);
-	if (retv)
-		FI_WARN(&rxr_prov, FI_LOG_FABRIC,
-			"Unable to close fabric: %s\n",
-			fi_strerror(-retv));
-err_free_fabric:
-	free(rxr_fabric);
-	return ret;
-}
diff --git a/prov/efa/src/rxr/rxr_init.c b/prov/efa/src/rxr/rxr_init.c
index dfc5a5e..776ab36 100644
--- a/prov/efa/src/rxr/rxr_init.c
+++ b/prov/efa/src/rxr/rxr_init.c
@@ -40,7 +40,6 @@
 
 struct fi_info *shm_info;
 
-struct fi_provider *lower_efa_prov;
 struct efa_ep_addr *local_efa_addr;
 
 
@@ -56,6 +55,8 @@ struct rxr_env rxr_env = {
 	.shm_av_size = 128,
 	.shm_max_medium_size = 4096,
 	.recvwin_size = RXR_RECVWIN_SIZE,
+	.ooo_pool_chunk_size = 64,
+	.unexp_pool_chunk_size = 1024,
 	.readcopy_pool_size = 256,
 	.atomrsp_pool_size = 1024,
 	.cq_size = RXR_DEF_CQ_SIZE,
@@ -67,18 +68,29 @@ struct rxr_env rxr_env = {
 	.rx_iov_limit = 0,
 	.rx_copy_unexp = 1,
 	.rx_copy_ooo = 1,
-	.max_timeout = RXR_DEF_RNR_MAX_TIMEOUT,
-	.timeout_interval = 0, /* 0 is random timeout */
+	.rnr_backoff_wait_time_cap = RXR_DEFAULT_RNR_BACKOFF_WAIT_TIME_CAP,
+	.rnr_backoff_initial_wait_time = 0, /* 0 is random wait time  */
 	.efa_cq_read_size = 50,
 	.shm_cq_read_size = 50,
 	.efa_max_medium_msg_size = 65536,
 	.efa_min_read_msg_size = 1048576,
 	.efa_min_read_write_size = 65536,
 	.efa_read_segment_size = 1073741824,
+	.rnr_retry = 3, /* Setting this value to EFA_RNR_INFINITE_RETRY makes the firmware retry indefinitey */
 };
 
+/* @brief Read and store the FI_EFA_* environment variables.
+ */
 static void rxr_init_env(void)
 {
+	int fork_safe = 0;
+
+	if (getenv("FI_EFA_SHM_MAX_MEDIUM_SIZE")) {
+		fprintf(stderr,
+			"FI_EFA_SHM_MAX_MEDIUM_SIZE env variable detected! The use of this variable has been deprecated and as such execution cannot proceed.\n");
+		abort();
+	};
+
 	fi_param_get_int(&rxr_prov, "rx_window_size", &rxr_env.rx_window_size);
 	fi_param_get_int(&rxr_prov, "tx_max_credits", &rxr_env.tx_max_credits);
 	fi_param_get_int(&rxr_prov, "tx_min_credits", &rxr_env.tx_min_credits);
@@ -88,7 +100,6 @@ static void rxr_init_env(void)
 	fi_param_get_int(&rxr_prov, "use_zcpy_rx", &rxr_env.use_zcpy_rx);
 	fi_param_get_int(&rxr_prov, "zcpy_rx_seed", &rxr_env.zcpy_rx_seed);
 	fi_param_get_int(&rxr_prov, "shm_av_size", &rxr_env.shm_av_size);
-	fi_param_get_int(&rxr_prov, "shm_max_medium_size", &rxr_env.shm_max_medium_size);
 	fi_param_get_int(&rxr_prov, "recvwin_size", &rxr_env.recvwin_size);
 	fi_param_get_int(&rxr_prov, "readcopy_pool_size", &rxr_env.readcopy_pool_size);
 	fi_param_get_int(&rxr_prov, "cq_size", &rxr_env.cq_size);
@@ -110,9 +121,13 @@ static void rxr_init_env(void)
 			  &rxr_env.rx_copy_unexp);
 	fi_param_get_bool(&rxr_prov, "rx_copy_ooo",
 			  &rxr_env.rx_copy_ooo);
-	fi_param_get_int(&rxr_prov, "max_timeout", &rxr_env.max_timeout);
+
+	fi_param_get_int(&rxr_prov, "max_timeout", &rxr_env.rnr_backoff_wait_time_cap);
+	if (rxr_env.rnr_backoff_wait_time_cap > RXR_MAX_RNR_BACKOFF_WAIT_TIME_CAP)
+		rxr_env.rnr_backoff_wait_time_cap = RXR_MAX_RNR_BACKOFF_WAIT_TIME_CAP;
+
 	fi_param_get_int(&rxr_prov, "timeout_interval",
-			 &rxr_env.timeout_interval);
+			 &rxr_env.rnr_backoff_initial_wait_time);
 	fi_param_get_size_t(&rxr_prov, "efa_cq_read_size",
 			 &rxr_env.efa_cq_read_size);
 	fi_param_get_size_t(&rxr_prov, "shm_cq_read_size",
@@ -125,37 +140,74 @@ static void rxr_init_env(void)
 			    &rxr_env.efa_min_read_write_size);
 	fi_param_get_size_t(&rxr_prov, "inter_read_segment_size",
 			    &rxr_env.efa_read_segment_size);
+
+	/* Initialize EFA's fork support flag based on the environment and
+	 * system support. */
+	efa_fork_status = EFA_FORK_SUPPORT_OFF;
+
+#if HAVE_IBV_IS_FORK_INITIALIZED == 1
+	if (ibv_is_fork_initialized() == IBV_FORK_UNNEEDED)
+		efa_fork_status = EFA_FORK_SUPPORT_UNNEEDED;
+#endif
+
+	if (efa_fork_status != EFA_FORK_SUPPORT_UNNEEDED) {
+		fi_param_get_bool(&rxr_prov, "fork_safe", &fork_safe);
+
+		/*
+		 * Check if any environment variables which would trigger
+		 * libibverbs' fork support are set. These variables are
+		 * defined by ibv_fork_init(3).
+		 */
+		if (fork_safe || getenv("RDMAV_FORK_SAFE") || getenv("IBV_FORK_SAFE"))
+			efa_fork_status = EFA_FORK_SUPPORT_ON;
+	}
 }
 
-/*
- * Stringify the void *addr to a string smr_name formatted as `gid_qpn`, which
- * will be used to insert into shm provider's AV. Then shm uses smr_name as
- * ep_name to create the shared memory region.
+/* @brief convert raw address to an unique shm endpoint name (smr_name)
+ *
+ * Note even though all shm endpoints are on same instance. But because
+ * one instance can have multiple EFA device, it is still necessary
+ * to include GID on the name.
+ *
+ * a smr name consist of the following 4 parts:
+ *
+ *    GID:   ipv6 address from inet_ntop
+ *    QPN:   %04x format
+ *    QKEY:  %08x format
+ *    UID:   %04x format
+ *
+ * each part is connected via an underscore.
  *
- * The IPv6 address length is 46, but the max supported name length for shm is 32.
- * The string `gid_qpn` could be truncated during snprintf.
- * The current way works because the IPv6 addresses starting with FE in hexadecimals represent
- * link local IPv6 addresses, which has reserved first 64 bits (FE80::/64).
- * e.g., fe80:0000:0000:0000:0436:29ff:fe8e:ceaa -> fe80::436:29ff:fe8e:ceaa
- * And the length of string `gid_qpn` (fe80::436:29ff:fe8e:ceaa_***) will not exceed 32.
- * If the address is NOT link local, we need to think another reasonable way to
- * generate the string.
+ * The following is an example:
+ *
+ *    fe80::4a5:28ff:fe98:e500_0001_12918366_03e8
+ *
+ * @param[in]	ptr		pointer to raw address (struct efa_ep_addr)
+ * @param[out]	smr_name	an unique name for shm ep
+ * @return	0 on success.
+ * 		negative error code on failure.
  */
-int rxr_ep_efa_addr_to_str(const void *addr, char *smr_name)
+int rxr_raw_addr_to_smr_name(void *ptr, char *smr_name)
 {
-	char gid[INET6_ADDRSTRLEN] = { 0 };
-	uint16_t qpn;
+	struct efa_ep_addr *raw_addr;
+	char gidstr[INET6_ADDRSTRLEN] = { 0 };
 	int ret;
 
-	if (!inet_ntop(AF_INET6, ((struct efa_ep_addr *)addr)->raw, gid, INET6_ADDRSTRLEN)) {
-		printf("Failed to get current EFA's GID, errno: %d\n", errno);
-		return 0;
+	raw_addr = (struct efa_ep_addr *)ptr;
+	if (!inet_ntop(AF_INET6, raw_addr->raw, gidstr, INET6_ADDRSTRLEN)) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "Failed to convert GID to string errno: %d\n", errno);
+		return -errno;
 	}
-	qpn = ((struct efa_ep_addr *)addr)->qpn;
 
-	ret = snprintf(smr_name, NAME_MAX, "%ld_%s_%d", (size_t) getuid(), gid, qpn);
+	ret = snprintf(smr_name, EFA_SHM_NAME_MAX, "%s_%04x_%08x_%04x",
+		       gidstr, raw_addr->qpn, raw_addr->qkey, getuid());
+	if (ret <= 0)
+		return ret;
+
+	if (ret >= EFA_SHM_NAME_MAX)
+		return -FI_EINVAL;
 
-	return (ret <= 0) ? ret : FI_SUCCESS;
+	return FI_SUCCESS;
 }
 
 void rxr_info_to_core_mr_modes(uint32_t version,
@@ -255,7 +307,7 @@ static int rxr_info_to_core(uint32_t version, const struct fi_info *rxr_info,
 }
 
 /* Explicitly set all necessary bits before calling shm provider's getinfo function */
-void rxr_set_shm_hints(struct fi_info *shm_hints)
+static void rxr_set_shm_hints(const struct fi_info *app_hints, struct fi_info *shm_hints)
 {
 	shm_hints->caps = FI_MSG | FI_TAGGED | FI_RECV | FI_SEND | FI_READ
 			   | FI_WRITE | FI_REMOTE_READ | FI_REMOTE_WRITE
@@ -268,6 +320,18 @@ void rxr_set_shm_hints(struct fi_info *shm_hints)
 	shm_hints->fabric_attr->name = strdup("shm");
 	shm_hints->fabric_attr->prov_name = strdup("shm");
 	shm_hints->ep_attr->type = FI_EP_RDM;
+
+	/*
+	 * We validate whether FI_HMEM is supported before this function is
+	 * called, so it's safe to check for this via the app hints directly.
+	 * We should combine this and the earlier FI_HMEM validation when we
+	 * clean up the getinfo path. That's not possible at the moment as we
+	 * only have one SHM info for the entire provider which isn't right.
+	 */
+	if (app_hints && (app_hints->caps & FI_HMEM)) {
+		shm_hints->caps |= FI_HMEM;
+		shm_hints->domain_attr->mr_mode |= FI_MR_HMEM;
+	}
 }
 
 /* Pass tx/rx attr that user specifies down to core provider */
@@ -326,6 +390,11 @@ static int rxr_info_to_rxr(uint32_t version, const struct fi_info *core_info,
 {
 	uint64_t atomic_ordering;
 	uint64_t max_atomic_size;
+	uint64_t min_pkt_size;
+
+	if (!core_info) {
+		return -FI_EINVAL;
+	}
 
 	info->caps = rxr_info.caps;
 	info->mode = rxr_info.mode;
@@ -335,8 +404,24 @@ static int rxr_info_to_rxr(uint32_t version, const struct fi_info *core_info,
 	*info->ep_attr = *rxr_info.ep_attr;
 	*info->domain_attr = *rxr_info.domain_attr;
 
-	/* TODO: update inject_size when we implement inject */
-	info->tx_attr->inject_size = 0;
+	/*
+	 * The requirement for inject is: upon return, the user buffer can be reused immediately.
+	 *
+	 * For EFA, inject is implement as: construct a packet entry, copy user data to packet entry
+	 * then send the packet entry. Therefore the maximum inject size is
+	 *    pkt_entry_size - maximum_header_size.
+	 */
+	if (rxr_env.enable_shm_transfer)
+		min_pkt_size = MIN(core_info->ep_attr->max_msg_size, rxr_env.shm_max_medium_size);
+	else
+		min_pkt_size = core_info->ep_attr->max_msg_size;
+
+	if (min_pkt_size < rxr_pkt_max_header_size()) {
+		info->tx_attr->inject_size = 0;
+	} else {
+		info->tx_attr->inject_size = min_pkt_size - rxr_pkt_max_header_size();
+	}
+
 	rxr_info.tx_attr->inject_size = info->tx_attr->inject_size;
 
 	info->addr_format = core_info->addr_format;
@@ -350,7 +435,7 @@ static int rxr_info_to_rxr(uint32_t version, const struct fi_info *core_info,
 	 * cap). The logic for device-specific checks pertaining to HMEM comes
 	 * further along this path.
 	 */
-	if ((core_info && !(core_info->caps & FI_HMEM)) || !hints) {
+	if (!(core_info->caps & FI_HMEM) || !hints) {
 		info->caps &= ~FI_HMEM;
 	}
 
@@ -405,17 +490,6 @@ static int rxr_info_to_rxr(uint32_t version, const struct fi_info *core_info,
 		 * which means FI_MR_HMEM implies FI_MR_LOCAL for cuda buffer
 		 */
 		if (hints->caps & FI_HMEM) {
-			/*
-			 * XXX: remove this once CUDA IPC is supported by SHM
-			 * and we have a fallback path to use the device when
-			 * SHM doesn't support CUDA IPC.
-			 */
-			if (hints->caps & FI_LOCAL_COMM) {
-				FI_WARN(&rxr_prov, FI_LOG_CORE,
-				        "FI_HMEM is currently not supported by the EFA provider when FI_LOCAL_COMM is requested.\n");
-				return -FI_ENODATA;
-			}
-			info->caps &= ~FI_LOCAL_COMM;
 
 			if (!efa_device_support_rdma_read()) {
 				FI_WARN(&rxr_prov, FI_LOG_CORE,
@@ -471,14 +545,7 @@ static int rxr_info_to_rxr(uint32_t version, const struct fi_info *core_info,
 			info->mode |= FI_MSG_PREFIX;
 			info->tx_attr->mode |= FI_MSG_PREFIX;
 			info->rx_attr->mode |= FI_MSG_PREFIX;
-
-			/*
-			 * The prefix needs to be a multiple of 8. The pkt_entry
-			 * is already at 64 bytes (128 with debug).
-			 */
-			info->ep_attr->msg_prefix_size =  sizeof(struct rxr_pkt_entry)
-							  + sizeof(struct rxr_eager_msgrtm_hdr);
-			assert(!(info->ep_attr->msg_prefix_size % 8));
+			info->ep_attr->msg_prefix_size = RXR_MSG_PREFIX_SIZE;
 			FI_INFO(&rxr_prov, FI_LOG_CORE,
 				"FI_MSG_PREFIX size = %ld\n", info->ep_attr->msg_prefix_size);
 		}
@@ -520,17 +587,16 @@ int rxr_get_lower_rdm_info(uint32_t version, const char *node,
 	if (ret)
 		return ret;
 
-	ret = lower_efa_prov->getinfo(version, node, service, flags,
-				      core_hints, core_info);
+	ret = efa_getinfo(version, node, service, flags, core_hints, core_info);
 	fi_freeinfo(core_hints);
 	return ret;
 }
 
 /*
- * Call getinfo on lower efa provider to get all locally qualified fi_info
+ * Call efa_getinfo() to get all locally qualified fi_info
  * structure, then store the corresponding efa nic GIDs
  */
-int rxr_get_local_gids(struct fi_provider *lower_efa_prov)
+int rxr_get_local_gids(void)
 {
 	struct fi_info *core_info, *cur;
 	struct efa_ep_addr *cur_efa_addr;
@@ -539,7 +605,7 @@ int rxr_get_local_gids(struct fi_provider *lower_efa_prov)
 	cur_efa_addr = local_efa_addr = NULL;
 	core_info = cur = NULL;
 
-	ret = lower_efa_prov->getinfo(rxr_prov.fi_version, NULL, NULL, 0, NULL, &core_info);
+	ret = efa_getinfo(rxr_prov.fi_version, NULL, NULL, 0, NULL, &core_info);
 	if (ret)
 		return ret;
 
@@ -579,8 +645,7 @@ static int rxr_dgram_getinfo(uint32_t version, const char *node,
 
 	core_info = NULL;
 
-	ret = lower_efa_prov->getinfo(version, node, service,
-				      flags, hints, &core_info);
+	ret = efa_getinfo(version, node, service, flags, hints, &core_info);
 
 	if (ret)
 		return ret;
@@ -682,7 +747,7 @@ dgram_info:
 	if (!ret && rxr_env.enable_shm_transfer && !shm_info) {
 		shm_info = NULL;
 		shm_hints = fi_allocinfo();
-		rxr_set_shm_hints(shm_hints);
+		rxr_set_shm_hints(hints, shm_hints);
 		ret = fi_getinfo(FI_VERSION(1, 8), NULL, NULL,
 		                 OFI_GETINFO_HIDDEN, shm_hints, &shm_info);
 		fi_freeinfo(shm_hints);
@@ -710,8 +775,7 @@ static void rxr_fini(void)
 {
 	struct efa_ep_addr *cur;
 
-	if (lower_efa_prov)
-		lower_efa_prov->cleanup();
+	efa_finalize_prov();
 
 	if (rxr_env.enable_shm_transfer) {
 		/* Cleanup all local efa nic GIDs */
@@ -736,7 +800,7 @@ struct fi_provider rxr_prov = {
 	.version = OFI_VERSION_DEF_PROV,
 	.fi_version = OFI_VERSION_LATEST,
 	.getinfo = rxr_getinfo,
-	.fabric = rxr_fabric,
+	.fabric = efa_fabric,
 	.cleanup = rxr_fini
 };
 
@@ -760,8 +824,6 @@ EFA_INI
 			"Defines the number of bounce-buffers the provider will prepost during EP initialization.  (Default: 0)");
 	fi_param_define(&rxr_prov, "shm_av_size", FI_PARAM_INT,
 			"Defines the maximum number of entries in SHM provider's address vector (Default 128).");
-	fi_param_define(&rxr_prov, "shm_max_medium_size", FI_PARAM_INT,
-			"Defines the switch point between small/medium message and large message. The message larger than this switch point will be transferred with large message protocol (Default 4096).");
 	fi_param_define(&rxr_prov, "recvwin_size", FI_PARAM_INT,
 			"Defines the size of sliding receive window. (Default: 16384)");
 	fi_param_define(&rxr_prov, "readcopy_pool_size", FI_PARAM_INT,
@@ -807,6 +869,9 @@ EFA_INI
 			"The mimimum message size for inter EFA write to use read write protocol. If firmware support RDMA read, and FI_EFA_USE_DEVICE_RDMA is 1, write requests whose size is larger than this value will use the read write protocol (Default 65536).");
 	fi_param_define(&rxr_prov, "inter_read_segment_size", FI_PARAM_INT,
 			"Calls to RDMA read is segmented using this value.");
+	fi_param_define(&rxr_prov, "fork_safe", FI_PARAM_BOOL,
+			"Enables fork support and disables internal usage of huge pages. Has no effect on kernels which set copy-on-fork for registered pages, generally 5.13 and later. (Default: false)");
+
 	rxr_init_env();
 
 #if HAVE_EFA_DL
@@ -815,11 +880,10 @@ EFA_INI
 	ofi_monitors_init();
 #endif
 
-	lower_efa_prov = init_lower_efa_prov();
-	if (!lower_efa_prov)
+	if (efa_init_prov())
 		return NULL;
 
-	if (rxr_env.enable_shm_transfer && rxr_get_local_gids(lower_efa_prov))
+	if (rxr_env.enable_shm_transfer && rxr_get_local_gids())
 		return NULL;
 
 	return &rxr_prov;
diff --git a/prov/efa/src/rxr/rxr_msg.c b/prov/efa/src/rxr/rxr_msg.c
index 4777f75..31a72b6 100644
--- a/prov/efa/src/rxr/rxr_msg.c
+++ b/prov/efa/src/rxr/rxr_msg.c
@@ -62,12 +62,12 @@ static inline
 ssize_t rxr_msg_post_cuda_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 {
 	int err, tagged;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	int pkt_type;
 	bool delivery_complete_requested;
 
 	assert(RXR_EAGER_MSGRTM_PKT + 1 == RXR_EAGER_TAGRTM_PKT);
-	assert(RXR_READ_MSGRTM_PKT + 1 == RXR_READ_TAGRTM_PKT);
+	assert(RXR_LONGREAD_MSGRTM_PKT + 1 == RXR_LONGREAD_TAGRTM_PKT);
 	assert(RXR_DC_EAGER_MSGRTM_PKT + 1 == RXR_DC_EAGER_TAGRTM_PKT);
 
 	tagged = (tx_entry->op == ofi_op_tagged);
@@ -77,7 +77,7 @@ ssize_t rxr_msg_post_cuda_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 	if (tx_entry->total_len == 0) {
 		pkt_type = delivery_complete_requested ? RXR_DC_EAGER_MSGRTM_PKT : RXR_EAGER_MSGRTM_PKT;
 		return rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry,
-					 pkt_type + tagged, 0);
+					 pkt_type + tagged, 0, 0);
 	}
 
 	/* Currently cuda data must be sent using read message protocol.
@@ -87,6 +87,7 @@ ssize_t rxr_msg_post_cuda_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 	 * from the receiver, so here we call rxr_pkt_wait_handshake().
 	 */
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
+	assert(peer);
 
 	err = rxr_pkt_wait_handshake(rxr_ep, tx_entry->addr, peer);
 	if (OFI_UNLIKELY(err)) {
@@ -101,7 +102,7 @@ ssize_t rxr_msg_post_cuda_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 	}
 
 	return rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry,
-				 RXR_READ_MSGRTM_PKT + tagged, 0);
+				 RXR_LONGREAD_MSGRTM_PKT + tagged, 0, 0);
 }
 
 ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
@@ -111,18 +112,18 @@ ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 	 * always the correspondent message rtm packet type id + 1, thus the assertion here.
 	 */
 	assert(RXR_EAGER_MSGRTM_PKT + 1 == RXR_EAGER_TAGRTM_PKT);
-	assert(RXR_READ_MSGRTM_PKT + 1 == RXR_READ_TAGRTM_PKT);
-	assert(RXR_LONG_MSGRTM_PKT + 1 == RXR_LONG_TAGRTM_PKT);
+	assert(RXR_LONGREAD_MSGRTM_PKT + 1 == RXR_LONGREAD_TAGRTM_PKT);
+	assert(RXR_LONGCTS_MSGRTM_PKT + 1 == RXR_LONGCTS_TAGRTM_PKT);
 	assert(RXR_MEDIUM_MSGRTM_PKT + 1 == RXR_MEDIUM_TAGRTM_PKT);
 
 	assert(RXR_DC_EAGER_MSGRTM_PKT + 1 == RXR_DC_EAGER_TAGRTM_PKT);
 	assert(RXR_DC_MEDIUM_MSGRTM_PKT + 1 == RXR_DC_MEDIUM_TAGRTM_PKT);
-	assert(RXR_DC_LONG_MSGRTM_PKT + 1 == RXR_DC_LONG_TAGRTM_PKT);
+	assert(RXR_DC_LONGCTS_MSGRTM_PKT + 1 == RXR_DC_LONGCTS_TAGRTM_PKT);
 
 	int tagged;
 	size_t max_rtm_data_size;
 	ssize_t err;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	bool delivery_complete_requested;
 	int ctrl_type;
 	struct efa_domain *efa_domain;
@@ -135,8 +136,12 @@ ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 	tagged = (tx_entry->op == ofi_op_tagged);
 	assert(tagged == 0 || tagged == 1);
 
-	delivery_complete_requested = tx_entry->fi_flags & FI_DELIVERY_COMPLETE;
+	if (tx_entry->fi_flags & FI_INJECT)
+		delivery_complete_requested = false;
+	else
+		delivery_complete_requested = tx_entry->fi_flags & FI_DELIVERY_COMPLETE;
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
+	assert(peer);
 
 	if (delivery_complete_requested && !(peer->is_local)) {
 		tx_entry->rxr_flags |= RXR_DELIVERY_COMPLETE_REQUESTED;
@@ -176,31 +181,26 @@ ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
-		/* intra instance message */
-		if (tx_entry->total_len > max_rtm_data_size)
+		/* intra instance message
+		 *
+		 * Currently shm proivder does not support mixed memory type iov
+		 * (it will crash), which will happen is eager message protocol
+		 * is used for cuda buffer. An GitHub issue has been opened
+		 * regarding this
+		 *     https://github.com/ofiwg/libfabric/issues/6639
+		 * Before it is addressed, we use read message protocol for
+		 * all cuda messages
+		 */
+		if (tx_entry->total_len > max_rtm_data_size || efa_ep_is_cuda_mr(tx_entry->desc[0]))
 			/*
 			 * Read message support
 			 * FI_DELIVERY_COMPLETE implicitly.
 			 */
-			ctrl_type = RXR_READ_MSGRTM_PKT;
+			ctrl_type = RXR_LONGREAD_MSGRTM_PKT;
 		else
 			ctrl_type = delivery_complete_requested ? RXR_DC_EAGER_MSGRTM_PKT : RXR_EAGER_MSGRTM_PKT;
 
-		return rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry, ctrl_type + tagged, 0);
-	}
-
-	if (rxr_ep->use_zcpy_rx) {
-		/*
-		 * The application can not deal with varying packet header sizes
-		 * before and after receiving a handshake. Forcing a handshake
-		 * here so we can always use the smallest eager msg packet
-		 * header size to determine the msg_prefix_size.
-		 */
-		err = rxr_pkt_wait_handshake(rxr_ep, tx_entry->addr, peer);
-		if (OFI_UNLIKELY(err))
-			return err;
-
-		assert(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED);
+		return rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry, ctrl_type + tagged, 0, 0);
 	}
 
 	if (efa_ep_is_cuda_mr(tx_entry->desc[0])) {
@@ -212,7 +212,7 @@ ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 		ctrl_type = (delivery_complete_requested) ?
 			RXR_DC_EAGER_MSGRTM_PKT : RXR_EAGER_MSGRTM_PKT;
 		return rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry,
-					 ctrl_type + tagged, 0);
+					 ctrl_type + tagged, 0, 0);
 	}
 
 	if (tx_entry->total_len <= rxr_env.efa_max_medium_msg_size) {
@@ -238,7 +238,7 @@ ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 	    (tx_entry->desc[0] || efa_is_cache_available(efa_domain))) {
 		/* Read message support FI_DELIVERY_COMPLETE implicitly. */
 		err = rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry,
-					RXR_READ_MSGRTM_PKT + tagged, 0);
+					RXR_LONGREAD_MSGRTM_PKT + tagged, 0, 0);
 
 		if (err != -FI_ENOMEM)
 			return err;
@@ -253,10 +253,10 @@ ssize_t rxr_msg_post_rtm(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 	if (OFI_UNLIKELY(err))
 		return err;
 
-	ctrl_type = delivery_complete_requested ? RXR_DC_LONG_MSGRTM_PKT : RXR_LONG_MSGRTM_PKT;
+	ctrl_type = delivery_complete_requested ? RXR_DC_LONGCTS_MSGRTM_PKT : RXR_LONGCTS_MSGRTM_PKT;
 	tx_entry->rxr_flags |= RXR_LONGCTS_PROTOCOL;
 	return rxr_pkt_post_ctrl(rxr_ep, RXR_TX_ENTRY, tx_entry,
-				 ctrl_type + tagged, 0);
+				 ctrl_type + tagged, 0, 0);
 }
 
 ssize_t rxr_msg_generic_send(struct fid_ep *ep, const struct fi_msg *msg,
@@ -265,7 +265,7 @@ ssize_t rxr_msg_generic_send(struct fid_ep *ep, const struct fi_msg *msg,
 	struct rxr_ep *rxr_ep;
 	ssize_t err;
 	struct rxr_tx_entry *tx_entry;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 	       "iov_len: %lu tag: %lx op: %x flags: %lx\n",
@@ -275,7 +275,7 @@ ssize_t rxr_msg_generic_send(struct fid_ep *ep, const struct fi_msg *msg,
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	assert(msg->iov_count <= rxr_ep->tx_iov_limit);
 
-	rxr_perfset_start(rxr_ep, perf_rxr_tx);
+	efa_perfset_start(rxr_ep, perf_efa_tx);
 	fastlock_acquire(&rxr_ep->util_ep.lock);
 
 	if (OFI_UNLIKELY(is_tx_res_full(rxr_ep))) {
@@ -284,6 +284,7 @@ ssize_t rxr_msg_generic_send(struct fid_ep *ep, const struct fi_msg *msg,
 	}
 
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 
 	if (peer->flags & RXR_PEER_IN_BACKOFF) {
 		err = -FI_EAGAIN;
@@ -309,7 +310,7 @@ ssize_t rxr_msg_generic_send(struct fid_ep *ep, const struct fi_msg *msg,
 
 out:
 	fastlock_release(&rxr_ep->util_ep.lock);
-	rxr_perfset_end(rxr_ep, perf_rxr_tx);
+	efa_perfset_end(rxr_ep, perf_efa_tx);
 	return err;
 }
 
@@ -378,7 +379,10 @@ ssize_t rxr_msg_inject(struct fid_ep *ep, const void *buf, size_t len,
 
 	rxr_setup_msg(&msg, &iov, NULL, 1, dest_addr, NULL, 0);
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
-	assert(len <= rxr_ep->core_inject_size - sizeof(struct rxr_eager_msgrtm_hdr));
+	if (len > rxr_ep->inject_size) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid message size %ld for inject.\n", len);
+		return -FI_EINVAL;
+	}
 
 	return rxr_msg_generic_send(ep, &msg, 0, ofi_op_msg,
 				    rxr_tx_flags(rxr_ep) | RXR_NO_COMPLETION | FI_INJECT);
@@ -398,12 +402,11 @@ ssize_t rxr_msg_injectdata(struct fid_ep *ep, const void *buf,
 
 	rxr_setup_msg(&msg, &iov, NULL, 1, dest_addr, NULL, data);
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
-	/*
-	 * We advertise the largest possible inject size with no cq data or
-	 * source address. This means that we may end up not using the core
-	 * providers inject for this send.
-	 */
-	assert(len <= rxr_ep->core_inject_size - sizeof(struct rxr_eager_msgrtm_hdr));
+	if (len > rxr_ep->inject_size) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid message size %ld for inject.\n", len);
+		return -FI_EINVAL;
+	}
+
 	return rxr_msg_generic_send(ep, &msg, 0, ofi_op_msg,
 				    rxr_tx_flags(rxr_ep) | RXR_NO_COMPLETION |
 				    FI_REMOTE_CQ_DATA | FI_INJECT);
@@ -485,7 +488,10 @@ ssize_t rxr_msg_tinject(struct fid_ep *ep_fid, const void *buf, size_t len,
 
 	rxr_setup_msg(&msg, &iov, NULL, 1, dest_addr, NULL, 0);
 	rxr_ep = container_of(ep_fid, struct rxr_ep, util_ep.ep_fid.fid);
-	assert(len <= rxr_ep->core_inject_size - sizeof(struct rxr_eager_tagrtm_hdr));
+	if (len > rxr_ep->inject_size) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid message size %ld for inject.\n", len);
+		return -FI_EINVAL;
+	}
 
 	return rxr_msg_generic_send(ep_fid, &msg, tag, ofi_op_tagged,
 				    rxr_tx_flags(rxr_ep) | RXR_NO_COMPLETION | FI_INJECT);
@@ -504,12 +510,10 @@ ssize_t rxr_msg_tinjectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
 
 	rxr_setup_msg(&msg, &iov, NULL, 1, dest_addr, NULL, data);
 	rxr_ep = container_of(ep_fid, struct rxr_ep, util_ep.ep_fid.fid);
-	/*
-	 * We advertise the largest possible inject size with no cq data or
-	 * source address. This means that we may end up not using the core
-	 * providers inject for this send.
-	 */
-	assert(len <= rxr_ep->core_inject_size - sizeof(struct rxr_eager_tagrtm_hdr));
+	if (len > rxr_ep->inject_size) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid message size %ld for inject.\n", len);
+		return -FI_EINVAL;
+	}
 
 	return rxr_msg_generic_send(ep_fid, &msg, tag, ofi_op_tagged,
 				    rxr_tx_flags(rxr_ep) | RXR_NO_COMPLETION |
@@ -524,30 +528,19 @@ ssize_t rxr_msg_tinjectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
  *   Utility functions and data structures
  */
 struct rxr_match_info {
-	fi_addr_t addr;
 	uint64_t tag;
 	uint64_t ignore;
 };
 
+/**
+ * @brief match function for rx_entry in ep->unexp_tagged_list
+ *
+ * @param[in]	item	pointer to rx_entry->entry.
+ * @param[in]	arg	pointer to rxr_match_info
+ * @return   0 or 1 indicating wether this entry is a match
+ */
 static
-int rxr_msg_match_unexp_anyaddr(struct dlist_entry *item, const void *arg)
-{
-	return 1;
-}
-
-static
-int rxr_msg_match_unexp(struct dlist_entry *item, const void *arg)
-{
-	const struct rxr_match_info *match_info = arg;
-	struct rxr_rx_entry *rx_entry;
-
-	rx_entry = container_of(item, struct rxr_rx_entry, entry);
-
-	return rxr_match_addr(match_info->addr, rx_entry->addr);
-}
-
-static
-int rxr_msg_match_unexp_tagged_anyaddr(struct dlist_entry *item, const void *arg)
+int rxr_msg_match_ep_unexp_by_tag(struct dlist_entry *item, const void *arg)
 {
 	const struct rxr_match_info *match_info = arg;
 	struct rxr_rx_entry *rx_entry;
@@ -558,16 +551,22 @@ int rxr_msg_match_unexp_tagged_anyaddr(struct dlist_entry *item, const void *arg
 			     match_info->tag);
 }
 
+/**
+ * @brief match function for rx_entry in peer->unexp_tagged_list
+ *
+ * @param[in]	item	pointer to rx_entry->peer_unexp_entry.
+ * @param[in]	arg	pointer to rxr_match_info
+ * @return   0 or 1 indicating wether this entry is a match
+ */
 static
-int rxr_msg_match_unexp_tagged(struct dlist_entry *item, const void *arg)
+int rxr_msg_match_peer_unexp_by_tag(struct dlist_entry *item, const void *arg)
 {
 	const struct rxr_match_info *match_info = arg;
 	struct rxr_rx_entry *rx_entry;
 
-	rx_entry = container_of(item, struct rxr_rx_entry, entry);
+	rx_entry = container_of(item, struct rxr_rx_entry, peer_unexp_entry);
 
-	return rxr_match_addr(match_info->addr, rx_entry->addr) &&
-	       rxr_match_tag(rx_entry->tag, match_info->ignore,
+	return rxr_match_tag(rx_entry->tag, match_info->ignore,
 			     match_info->tag);
 }
 
@@ -618,6 +617,239 @@ int rxr_msg_handle_unexp_match(struct rxr_ep *ep,
 	return rxr_pkt_proc_matched_rtm(ep, rx_entry, pkt_entry);
 }
 
+/**
+ * @brief allocate an rx entry for a fi_msg.
+ *        This function is used by two sided operation only.
+ *
+ * @param ep[in]	end point
+ * @param msg[in]	fi_msg contains iov,iov_count,context for ths operation
+ * @param op[in]	operation type (ofi_op_msg or ofi_op_tagged)
+ * @param flags[in]	flags application used to call fi_recv/fi_trecv functions
+ * @param tag[in]	tag (used only if op is ofi_op_tagged)
+ * @param ignore[in]	ignore mask (used only if op is ofi_op_tagged)
+ * @return		if allocation succeeded, return pointer to rx_entry
+ * 			if allocation failed, return NULL
+ */
+struct rxr_rx_entry *rxr_msg_alloc_rx_entry(struct rxr_ep *ep,
+					    const struct fi_msg *msg,
+					    uint32_t op, uint64_t flags,
+					    uint64_t tag, uint64_t ignore)
+{
+	struct rxr_rx_entry *rx_entry;
+	fi_addr_t addr;
+
+	if (ep->util_ep.caps & FI_DIRECTED_RECV)
+		addr = msg->addr;
+	else
+		addr = FI_ADDR_UNSPEC;
+
+	rx_entry = rxr_ep_alloc_rx_entry(ep, addr, op);
+	if (!rx_entry)
+		return NULL;
+
+	rx_entry->fi_flags = flags;
+	if (op == ofi_op_tagged) {
+		rx_entry->tag = tag;
+		rx_entry->cq_entry.tag = tag;
+		rx_entry->ignore = ignore;
+	}
+
+	/* Handle case where we're allocating an unexpected rx_entry */
+	rx_entry->iov_count = msg->iov_count;
+	if (rx_entry->iov_count) {
+		assert(msg->msg_iov);
+		memcpy(rx_entry->iov, msg->msg_iov, sizeof(*rx_entry->iov) * msg->iov_count);
+		rx_entry->cq_entry.len = ofi_total_iov_len(msg->msg_iov, msg->iov_count);
+		rx_entry->cq_entry.buf = msg->msg_iov[0].iov_base;
+	}
+
+	if (msg->desc)
+		memcpy(&rx_entry->desc[0], msg->desc, sizeof(*msg->desc) * msg->iov_count);
+	else
+		memset(&rx_entry->desc[0], 0, sizeof(rx_entry->desc));
+
+	rx_entry->cq_entry.op_context = msg->context;
+	return rx_entry;
+}
+
+struct rxr_rx_entry *rxr_msg_alloc_unexp_rx_entry_for_msgrtm(struct rxr_ep *ep,
+							     struct rxr_pkt_entry **pkt_entry_ptr)
+{
+	struct rdm_peer *peer;
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_pkt_entry *unexp_pkt_entry;
+
+	unexp_pkt_entry = rxr_pkt_get_unexp(ep, pkt_entry_ptr);
+	if (OFI_UNLIKELY(!unexp_pkt_entry)) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "packet entries exhausted.\n");
+		return NULL;
+	}
+
+	rx_entry = rxr_ep_alloc_rx_entry(ep, unexp_pkt_entry->addr, ofi_op_msg);
+	if (OFI_UNLIKELY(!rx_entry))
+		return NULL;
+
+	rx_entry->rxr_flags = 0;
+	rx_entry->state = RXR_RX_UNEXP;
+	rx_entry->unexp_pkt = unexp_pkt_entry;
+	rxr_pkt_rtm_update_rx_entry(unexp_pkt_entry, rx_entry);
+	dlist_insert_tail(&rx_entry->entry, &ep->rx_unexp_list);
+	peer = rxr_ep_get_peer(ep, unexp_pkt_entry->addr);
+	dlist_insert_tail(&rx_entry->peer_unexp_entry, &peer->rx_unexp_list);
+	return rx_entry;
+}
+
+struct rxr_rx_entry *rxr_msg_alloc_unexp_rx_entry_for_tagrtm(struct rxr_ep *ep,
+							     struct rxr_pkt_entry **pkt_entry_ptr)
+{
+	struct rdm_peer *peer;
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_pkt_entry *unexp_pkt_entry;
+
+	unexp_pkt_entry = rxr_pkt_get_unexp(ep, pkt_entry_ptr);
+	if (OFI_UNLIKELY(!unexp_pkt_entry)) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "packet entries exhausted.\n");
+		return NULL;
+	}
+
+	rx_entry = rxr_ep_alloc_rx_entry(ep, unexp_pkt_entry->addr, ofi_op_tagged);
+	if (OFI_UNLIKELY(!rx_entry))
+		return NULL;
+
+	rx_entry->tag = rxr_pkt_rtm_tag(unexp_pkt_entry);
+	rx_entry->rxr_flags = 0;
+	rx_entry->state = RXR_RX_UNEXP;
+	rx_entry->unexp_pkt = unexp_pkt_entry;
+	rxr_pkt_rtm_update_rx_entry(unexp_pkt_entry, rx_entry);
+	dlist_insert_tail(&rx_entry->entry, &ep->rx_unexp_tagged_list);
+	peer = rxr_ep_get_peer(ep, unexp_pkt_entry->addr);
+	dlist_insert_tail(&rx_entry->peer_unexp_entry, &peer->rx_unexp_tagged_list);
+	return rx_entry;
+}
+
+struct rxr_rx_entry *rxr_msg_split_rx_entry(struct rxr_ep *ep,
+					    struct rxr_rx_entry *posted_entry,
+					    struct rxr_rx_entry *consumer_entry,
+					    struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rx_entry *rx_entry;
+	size_t buf_len, consumed_len, data_len;
+	uint64_t tag, ignore;
+	struct fi_msg msg = {0};
+
+	assert(rxr_get_base_hdr(pkt_entry->pkt)->type >= RXR_REQ_PKT_BEGIN);
+
+	if (!consumer_entry) {
+		tag = 0;
+		ignore = ~0;
+		msg.msg_iov = posted_entry->iov;
+		msg.iov_count = posted_entry->iov_count;
+		msg.addr = pkt_entry->addr;
+		rx_entry = rxr_msg_alloc_rx_entry(ep, &msg,
+						  ofi_op_msg,
+						  posted_entry->fi_flags,
+						  tag, ignore);
+		if (OFI_UNLIKELY(!rx_entry))
+			return NULL;
+
+		FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
+		       "Splitting into new multi_recv consumer rx_entry %d from rx_entry %d\n",
+		       rx_entry->rx_id,
+		       posted_entry->rx_id);
+	} else {
+		rx_entry = consumer_entry;
+		memcpy(rx_entry->iov, posted_entry->iov,
+		       sizeof(*posted_entry->iov) * posted_entry->iov_count);
+		rx_entry->iov_count = posted_entry->iov_count;
+	}
+
+	rxr_pkt_rtm_update_rx_entry(pkt_entry, rx_entry);
+	data_len = rx_entry->total_len;
+	buf_len = ofi_total_iov_len(rx_entry->iov,
+				    rx_entry->iov_count);
+	consumed_len = MIN(buf_len, data_len);
+
+	rx_entry->rxr_flags |= RXR_MULTI_RECV_CONSUMER;
+	rx_entry->total_len = data_len;
+	rx_entry->fi_flags |= FI_MULTI_RECV;
+	rx_entry->master_entry = posted_entry;
+	rx_entry->cq_entry.len = consumed_len;
+	rx_entry->cq_entry.buf = rx_entry->iov[0].iov_base;
+	rx_entry->cq_entry.op_context = posted_entry->cq_entry.op_context;
+	rx_entry->cq_entry.flags = (FI_RECV | FI_MSG);
+
+	ofi_consume_iov(posted_entry->iov, &posted_entry->iov_count,
+			consumed_len);
+
+	dlist_init(&rx_entry->multi_recv_entry);
+	dlist_insert_tail(&rx_entry->multi_recv_entry,
+			  &posted_entry->multi_recv_consumers);
+	return rx_entry;
+}
+
+/**
+ * @brief find an unexpected rx entry for a receive operation.
+ *
+ * @param[in]	ep	endpoint
+ * @param[in]	addr	fi_addr of the peer want to receive from, can be FI_ADDR_UNSPEC
+ * @param[in]	tag	tag of the unexpected message, used only if op is ofi_op_tagged.
+ * @param[in]	ignore	mask of the tag, used only if op is ofi_op_tagged.
+ * @param[in]	op	either ofi_op_tagged or ofi_op_msg.
+ * @param[in]	claim   whether to claim the rx_entry, e.g. remove it from unexpected queue.
+ * @return	If an unexpected rx_entry was found, return the pointer.
+ * 		Otherwise, return NULL.
+ */
+static inline
+struct rxr_rx_entry *rxr_msg_find_unexp_rx_entry(struct rxr_ep *ep, fi_addr_t addr,
+						 int64_t tag, uint64_t ignore, uint32_t op,
+						 bool claim)
+{
+	struct rxr_match_info match_info;
+	struct rxr_rx_entry *rx_entry;
+	struct dlist_entry *match;
+	struct rdm_peer *peer;
+
+	peer = (ep->util_ep.caps & FI_DIRECTED_RECV) ? rxr_ep_get_peer(ep, addr) : NULL;
+
+	switch(op) {
+	case ofi_op_msg:
+		if (peer) {
+			match = dlist_empty(&peer->rx_unexp_list) ? NULL : peer->rx_unexp_list.next;
+			rx_entry = match ? container_of(match, struct rxr_rx_entry, peer_unexp_entry) : NULL;
+		} else {
+			match = dlist_empty(&ep->rx_unexp_list) ? NULL : ep->rx_unexp_list.next;
+			rx_entry = match ? container_of(match, struct rxr_rx_entry, entry) : NULL;
+		}
+		break;
+	case ofi_op_tagged:
+		match_info.tag = tag;
+		match_info.ignore = ignore;
+
+		if (peer) {
+			match = dlist_find_first_match(&peer->rx_unexp_tagged_list,
+			                               rxr_msg_match_peer_unexp_by_tag,
+						       (void *)&match_info);
+			rx_entry = match ? container_of(match, struct rxr_rx_entry, peer_unexp_entry) : NULL;
+		} else {
+			match = dlist_find_first_match(&ep->rx_unexp_tagged_list,
+						       rxr_msg_match_ep_unexp_by_tag,
+						       (void *)&match_info);
+			rx_entry = match ? container_of(match, struct rxr_rx_entry, entry) : NULL;
+		}
+		break;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "Error: wrong op in rxr_msg_find_unexp_rx_entry()");
+		abort();
+	}
+
+	if (rx_entry && claim) {
+		dlist_remove(&rx_entry->entry);
+		dlist_remove(&rx_entry->peer_unexp_entry);
+	}
+
+	return rx_entry;
+}
+
 /*
  *    Search unexpected list for matching message and process it if found.
  *    Returns 0 if the message is processed, -FI_ENOMSG if no match is found.
@@ -627,50 +859,24 @@ int rxr_msg_proc_unexp_msg_list(struct rxr_ep *ep, const struct fi_msg *msg,
 				uint64_t tag, uint64_t ignore, uint32_t op, uint64_t flags,
 				struct rxr_rx_entry *posted_entry)
 {
-	struct rxr_match_info match_info;
-	struct dlist_entry *match;
 	struct rxr_rx_entry *rx_entry;
-	dlist_func_t *match_func;
 	int ret;
+	bool claim;
 
-	if (op == ofi_op_tagged) {
-		if (ep->util_ep.caps & FI_DIRECTED_RECV)
-			match_func = &rxr_msg_match_unexp_tagged;
-		else
-			match_func = &rxr_msg_match_unexp_tagged_anyaddr;
-
-		match_info.addr = msg->addr;
-		match_info.tag = tag;
-		match_info.ignore = ignore;
-		match = dlist_remove_first_match(&ep->rx_unexp_tagged_list,
-		                                 match_func,
-						 (void *)&match_info);
-	} else {
-		if (ep->util_ep.caps & FI_DIRECTED_RECV)
-			match_func = &rxr_msg_match_unexp;
-		else
-			match_func = &rxr_msg_match_unexp_anyaddr;
-
-		match_info.addr = msg->addr;
-		match = dlist_remove_first_match(&ep->rx_unexp_list,
-		                                 match_func,
-						 (void *)&match_info);
-	}
-
-	if (!match)
+	claim = true;
+	rx_entry = rxr_msg_find_unexp_rx_entry(ep, msg->addr, tag, ignore, op, claim);
+	if (!rx_entry)
 		return -FI_ENOMSG;
 
-	rx_entry = container_of(match, struct rxr_rx_entry, entry);
-
 	/*
 	 * Initialize the matched entry as a multi-recv consumer if the posted
 	 * buffer is a multi-recv buffer.
 	 */
 	if (posted_entry) {
 		/*
-		 * rxr_ep_split_rx_entry will setup rx_entry iov and count
+		 * rxr_msg_split_rx_entry will setup rx_entry iov and count
 		 */
-		rx_entry = rxr_ep_split_rx_entry(ep, posted_entry, rx_entry,
+		rx_entry = rxr_msg_split_rx_entry(ep, posted_entry, rx_entry,
 						 rx_entry->unexp_pkt);
 		if (OFI_UNLIKELY(!rx_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
@@ -743,7 +949,7 @@ ssize_t rxr_msg_multi_recv(struct rxr_ep *rxr_ep, const struct fi_msg *msg,
 	 * messages but will be used for tracking the application's buffer and
 	 * when to write the completion to release the buffer.
 	 */
-	rx_entry = rxr_ep_get_rx_entry(rxr_ep, msg, tag, ignore, op, flags);
+	rx_entry = rxr_msg_alloc_rx_entry(rxr_ep, msg, op, flags, tag, ignore);
 	if (OFI_UNLIKELY(!rx_entry)) {
 		rxr_ep_progress_internal(rxr_ep);
 		return -FI_EAGAIN;
@@ -835,7 +1041,7 @@ ssize_t rxr_msg_generic_recv(struct fid_ep *ep, const struct fi_msg *msg,
 
 	assert(msg->iov_count <= rxr_ep->rx_iov_limit);
 
-	rxr_perfset_start(rxr_ep, perf_rxr_recv);
+	efa_perfset_start(rxr_ep, perf_efa_recv);
 
 	assert(rxr_ep->util_ep.rx_msg_flags == 0 || rxr_ep->util_ep.rx_msg_flags == FI_COMPLETION);
 	rx_op_flags = rxr_ep->util_ep.rx_op_flags;
@@ -857,12 +1063,7 @@ ssize_t rxr_msg_generic_recv(struct fid_ep *ep, const struct fi_msg *msg,
 	unexp_list = (op == ofi_op_tagged) ? &rxr_ep->rx_unexp_tagged_list :
 		     &rxr_ep->rx_unexp_list;
 
-	/*
-	 * Attempt to match against stashed unexpected messages. This is not
-	 * applicable to the zero-copy path where unexpected messages are not
-	 * applicable, since there's no tag or address to match against.
-	 */
-	if (!dlist_empty(unexp_list) && !rxr_ep->use_zcpy_rx) {
+	if (!dlist_empty(unexp_list)) {
 		ret = rxr_msg_proc_unexp_msg_list(rxr_ep, msg, tag,
 						  ignore, op, flags, NULL);
 
@@ -871,8 +1072,7 @@ ssize_t rxr_msg_generic_recv(struct fid_ep *ep, const struct fi_msg *msg,
 		ret = 0;
 	}
 
-	rx_entry = rxr_ep_get_rx_entry(rxr_ep, msg, tag,
-				       ignore, op, flags);
+	rx_entry = rxr_msg_alloc_rx_entry(rxr_ep, msg, op, flags, tag, ignore);
 
 	if (OFI_UNLIKELY(!rx_entry)) {
 		ret = -FI_EAGAIN;
@@ -880,18 +1080,20 @@ ssize_t rxr_msg_generic_recv(struct fid_ep *ep, const struct fi_msg *msg,
 		goto out;
 	}
 
-	if (op == ofi_op_tagged)
+	if (rxr_ep->use_zcpy_rx) {
+		ret = rxr_ep_post_user_recv_buf(rxr_ep, rx_entry, flags);
+		if (ret == -FI_EAGAIN)
+			rxr_ep_progress_internal(rxr_ep);
+	} else if (op == ofi_op_tagged) {
 		dlist_insert_tail(&rx_entry->entry, &rxr_ep->rx_tagged_list);
-	else
+	} else {
 		dlist_insert_tail(&rx_entry->entry, &rxr_ep->rx_list);
-
-	if (rxr_ep->use_zcpy_rx)
-		rxr_ep_post_buf(rxr_ep, msg, flags, EFA_EP);
+	}
 
 out:
 	fastlock_release(&rxr_ep->util_ep.lock);
 
-	rxr_perfset_end(rxr_ep, perf_rxr_recv);
+	efa_perfset_end(rxr_ep, perf_efa_recv);
 	return ret;
 }
 
@@ -962,33 +1164,23 @@ ssize_t rxr_msg_peek_trecv(struct fid_ep *ep_fid,
 {
 	ssize_t ret = 0;
 	struct rxr_ep *ep;
-	struct dlist_entry *match;
-	dlist_func_t *match_func;
-	struct rxr_match_info match_info;
 	struct rxr_rx_entry *rx_entry;
 	struct fi_context *context;
 	struct rxr_pkt_entry *pkt_entry;
 	size_t data_len;
 	int64_t tag;
+	bool claim;
 
 	ep = container_of(ep_fid, struct rxr_ep, util_ep.ep_fid.fid);
 
 	fastlock_acquire(&ep->util_ep.lock);
 
 	rxr_ep_progress_internal(ep);
-	match_info.addr = msg->addr;
-	match_info.tag = msg->tag;
-	match_info.ignore = msg->ignore;
 
-	if (ep->util_ep.caps & FI_DIRECTED_RECV)
-		match_func = &rxr_msg_match_unexp_tagged;
-	else
-		match_func = &rxr_msg_match_unexp_tagged_anyaddr;
-
-	match = dlist_find_first_match(&ep->rx_unexp_tagged_list,
-	                               match_func,
-				       (void *)&match_info);
-	if (!match) {
+	claim = (flags & (FI_CLAIM | FI_DISCARD));
+	rx_entry = rxr_msg_find_unexp_rx_entry(ep, msg->addr, msg->tag, msg->ignore, ofi_op_tagged,
+					       claim);
+	if (!rx_entry) {
 		FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
 		       "Message not found addr: %" PRIu64
 		       " tag: %lx ignore %lx\n", msg->addr, msg->tag,
@@ -998,14 +1190,10 @@ ssize_t rxr_msg_peek_trecv(struct fid_ep *ep_fid,
 		goto out;
 	}
 
-	rx_entry = container_of(match, struct rxr_rx_entry, entry);
 	context = (struct fi_context *)msg->context;
 	if (flags & FI_CLAIM) {
 		context->internal[0] = rx_entry;
-		dlist_remove(match);
 	} else if (flags & FI_DISCARD) {
-		dlist_remove(match);
-
 		ret = rxr_msg_discard_trecv(ep, rx_entry, msg, flags);
 		if (ret)
 			goto out;
diff --git a/prov/efa/src/rxr/rxr_msg.h b/prov/efa/src/rxr/rxr_msg.h
index 58349d1..130b5c0 100644
--- a/prov/efa/src/rxr/rxr_msg.h
+++ b/prov/efa/src/rxr/rxr_msg.h
@@ -43,6 +43,24 @@ void rxr_msg_multi_recv_handle_completion(struct rxr_ep *ep,
 void rxr_msg_multi_recv_free_posted_entry(struct rxr_ep *ep,
 					  struct rxr_rx_entry *rx_entry);
 
+/**
+ * functions to allocate rx_entry for two sided operations
+ */
+struct rxr_rx_entry *rxr_msg_alloc_rx_entry(struct rxr_ep *ep,
+					    const struct fi_msg *msg,
+					    uint32_t op, uint64_t flags,
+					    uint64_t tag, uint64_t ignore);
+
+struct rxr_rx_entry *rxr_msg_alloc_unexp_rx_entry_for_msgrtm(struct rxr_ep *ep,
+							     struct rxr_pkt_entry **pkt_entry);
+
+struct rxr_rx_entry *rxr_msg_alloc_unexp_rx_entry_for_tagrtm(struct rxr_ep *ep,
+							     struct rxr_pkt_entry **pkt_entry);
+
+struct rxr_rx_entry *rxr_msg_split_rx_entry(struct rxr_ep *ep,
+					    struct rxr_rx_entry *posted_entry,
+					    struct rxr_rx_entry *consumer_entry,
+					    struct rxr_pkt_entry *pkt_entry);
 /*
  * The following 2 OP structures are defined in rxr_msg.c and is
  * used by rxr_endpoint()
diff --git a/prov/efa/src/rxr/rxr_pkt_cmd.c b/prov/efa/src/rxr/rxr_pkt_cmd.c
index 2382069..712fe42 100644
--- a/prov/efa/src/rxr/rxr_pkt_cmd.c
+++ b/prov/efa/src/rxr/rxr_pkt_cmd.c
@@ -37,6 +37,7 @@
 #include "rxr_cntr.h"
 #include "rxr_read.h"
 #include "rxr_pkt_cmd.h"
+#include "rxr_pkt_type_base.h"
 
 /* Handshake wait timeout in microseconds */
 #define RXR_HANDSHAKE_WAIT_TIMEOUT 1000000
@@ -49,58 +50,6 @@
  */
 
 /*
- *  Functions used to post a packet
- */
-ssize_t rxr_pkt_post_data(struct rxr_ep *rxr_ep,
-			  struct rxr_tx_entry *tx_entry)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_data_pkt *data_pkt;
-	ssize_t ret;
-
-	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_efa_pool);
-	if (OFI_UNLIKELY(!pkt_entry)) {
-		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-		       "TX packets exhausted, current packets in flight %lu",
-		       rxr_ep->tx_pending);
-		return -FI_EAGAIN;
-	}
-
-	pkt_entry->x_entry = (void *)tx_entry;
-	pkt_entry->addr = tx_entry->addr;
-
-	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
-
-	data_pkt->hdr.type = RXR_DATA_PKT;
-	data_pkt->hdr.version = RXR_BASE_PROTOCOL_VERSION;
-	data_pkt->hdr.flags = 0;
-
-	data_pkt->hdr.rx_id = tx_entry->rx_id;
-
-	/*
-	 * Data packets are sent in order so using bytes_sent is okay here.
-	 */
-	data_pkt->hdr.seg_offset = tx_entry->bytes_sent;
-
-	if (tx_entry->desc[0])
-		ret = rxr_pkt_send_data_desc(rxr_ep, tx_entry, pkt_entry);
-	else
-		ret = rxr_pkt_send_data(rxr_ep, tx_entry, pkt_entry);
-
-	if (OFI_UNLIKELY(ret)) {
-		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
-		return ret;
-	}
-
-	data_pkt = rxr_get_data_pkt(pkt_entry->pkt);
-	tx_entry->bytes_sent += data_pkt->hdr.seg_size;
-	tx_entry->window -= data_pkt->hdr.seg_size;
-	assert(data_pkt->hdr.seg_size > 0);
-	assert(tx_entry->window >= 0);
-	return ret;
-}
-
-/*
  *   rxr_pkt_init_ctrl() uses init functions declared in rxr_pkt_type.h
  */
 static
@@ -137,32 +86,32 @@ int rxr_pkt_init_ctrl(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
 	case RXR_MEDIUM_TAGRTM_PKT:
 		ret = rxr_pkt_init_medium_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_LONG_MSGRTM_PKT:
-		ret = rxr_pkt_init_long_msgrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGCTS_MSGRTM_PKT:
+		ret = rxr_pkt_init_longcts_msgrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_LONG_TAGRTM_PKT:
-		ret = rxr_pkt_init_long_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGCTS_TAGRTM_PKT:
+		ret = rxr_pkt_init_longcts_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_READ_MSGRTM_PKT:
-		ret = rxr_pkt_init_read_msgrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGREAD_MSGRTM_PKT:
+		ret = rxr_pkt_init_longread_msgrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_READ_TAGRTM_PKT:
-		ret = rxr_pkt_init_read_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGREAD_TAGRTM_PKT:
+		ret = rxr_pkt_init_longread_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
 	case RXR_EAGER_RTW_PKT:
 		ret = rxr_pkt_init_eager_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_LONG_RTW_PKT:
-		ret = rxr_pkt_init_long_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGCTS_RTW_PKT:
+		ret = rxr_pkt_init_longcts_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_READ_RTW_PKT:
-		ret = rxr_pkt_init_read_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGREAD_RTW_PKT:
+		ret = rxr_pkt_init_longread_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
 	case RXR_SHORT_RTR_PKT:
 		ret = rxr_pkt_init_short_rtr(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_LONG_RTR_PKT:
-		ret = rxr_pkt_init_long_rtr(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_LONGCTS_RTR_PKT:
+		ret = rxr_pkt_init_longcts_rtr(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
 	case RXR_WRITE_RTA_PKT:
 		ret = rxr_pkt_init_write_rta(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
@@ -185,24 +134,27 @@ int rxr_pkt_init_ctrl(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
 	case RXR_DC_MEDIUM_TAGRTM_PKT:
 		ret = rxr_pkt_init_dc_medium_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_DC_LONG_MSGRTM_PKT:
-		ret = rxr_pkt_init_dc_long_msgrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_DC_LONGCTS_MSGRTM_PKT:
+		ret = rxr_pkt_init_dc_longcts_msgrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_DC_LONG_TAGRTM_PKT:
-		ret = rxr_pkt_init_dc_long_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_DC_LONGCTS_TAGRTM_PKT:
+		ret = rxr_pkt_init_dc_longcts_tagrtm(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
 	case RXR_DC_EAGER_RTW_PKT:
 		ret = rxr_pkt_init_dc_eager_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
-	case RXR_DC_LONG_RTW_PKT:
-		ret = rxr_pkt_init_dc_long_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+	case RXR_DC_LONGCTS_RTW_PKT:
+		ret = rxr_pkt_init_dc_longcts_rtw(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
 	case RXR_DC_WRITE_RTA_PKT:
 		ret = rxr_pkt_init_dc_write_rta(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
 		break;
+	case RXR_DATA_PKT:
+		ret = rxr_pkt_init_data(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+		break;
 	default:
-		ret = -FI_EINVAL;
 		assert(0 && "unknown pkt type to init");
+		ret = -FI_EINVAL;
 		break;
 	}
 
@@ -243,29 +195,29 @@ void rxr_pkt_handle_ctrl_sent(struct rxr_ep *rxr_ep, struct rxr_pkt_entry *pkt_e
 	case RXR_DC_MEDIUM_TAGRTM_PKT:
 		rxr_pkt_handle_medium_rtm_sent(rxr_ep, pkt_entry);
 		break;
-	case RXR_LONG_MSGRTM_PKT:
-	case RXR_DC_LONG_MSGRTM_PKT:
-	case RXR_LONG_TAGRTM_PKT:
-	case RXR_DC_LONG_TAGRTM_PKT:
-		rxr_pkt_handle_long_rtm_sent(rxr_ep, pkt_entry);
+	case RXR_LONGCTS_MSGRTM_PKT:
+	case RXR_DC_LONGCTS_MSGRTM_PKT:
+	case RXR_LONGCTS_TAGRTM_PKT:
+	case RXR_DC_LONGCTS_TAGRTM_PKT:
+		rxr_pkt_handle_longcts_rtm_sent(rxr_ep, pkt_entry);
 		break;
-	case RXR_READ_MSGRTM_PKT:
-	case RXR_READ_TAGRTM_PKT:
-		rxr_pkt_handle_read_rtm_sent(rxr_ep, pkt_entry);
+	case RXR_LONGREAD_MSGRTM_PKT:
+	case RXR_LONGREAD_TAGRTM_PKT:
+		rxr_pkt_handle_longread_rtm_sent(rxr_ep, pkt_entry);
 		break;
 	case RXR_EAGER_RTW_PKT:
 		rxr_pkt_handle_eager_rtw_sent(rxr_ep, pkt_entry);
 		break;
-	case RXR_LONG_RTW_PKT:
-	case RXR_DC_LONG_RTW_PKT:
-		rxr_pkt_handle_long_rtw_sent(rxr_ep, pkt_entry);
+	case RXR_LONGCTS_RTW_PKT:
+	case RXR_DC_LONGCTS_RTW_PKT:
+		rxr_pkt_handle_longcts_rtw_sent(rxr_ep, pkt_entry);
 		break;
-	case RXR_READ_RTW_PKT:
-		rxr_pkt_handle_read_rtw_sent(rxr_ep, pkt_entry);
+	case RXR_LONGREAD_RTW_PKT:
+		rxr_pkt_handle_longread_rtw_sent(rxr_ep, pkt_entry);
 		break;
 	case RXR_SHORT_RTR_PKT:
-	case RXR_LONG_RTR_PKT:
-		rxr_pkt_handle_rtr_sent(rxr_ep, pkt_entry);
+	case RXR_LONGCTS_RTR_PKT:
+		/* nothing can be done when RTR packets are sent */
 		break;
 	case RXR_WRITE_RTA_PKT:
 	case RXR_DC_WRITE_RTA_PKT:
@@ -277,20 +229,35 @@ void rxr_pkt_handle_ctrl_sent(struct rxr_ep *rxr_ep, struct rxr_pkt_entry *pkt_e
 	case RXR_DC_EAGER_TAGRTM_PKT:
 	case RXR_DC_EAGER_RTW_PKT:
 		break;
+	case RXR_DATA_PKT:
+		rxr_pkt_handle_data_sent(rxr_ep, pkt_entry);
+		break;
 	default:
 		assert(0 && "Unknown packet type to handle sent");
 		break;
 	}
 }
 
+/**
+ * @brief post a single control packet.
+ *
+ *
+ * @param[in]   rxr_ep          endpoint
+ * @param[in]   entry_type      type of x_entry, allowed values: RXR_TX_ENTRY, RXR_RX_ENTRY
+ * @param[in]   x_entry         x_entry pointer
+ * @param[in]   ctrl_type       type of control packet
+ * @param[in]   inject          send control packet via inject or not.
+ * @param[in]   flags           additional flags to apply for fi_sendmsg.
+ *                              currently only accepted flags is FI_MORE.
+ * @return      On success return 0, otherwise return a negative error code
+ */
 ssize_t rxr_pkt_post_ctrl_once(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
-			       int ctrl_type, bool inject)
+			       int ctrl_type, bool inject, uint64_t flags)
 {
-	struct rxr_pkt_sendv send;
 	struct rxr_pkt_entry *pkt_entry;
 	struct rxr_tx_entry *tx_entry;
 	struct rxr_rx_entry *rx_entry;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	ssize_t err;
 	fi_addr_t addr;
 
@@ -303,19 +270,17 @@ ssize_t rxr_pkt_post_ctrl_once(struct rxr_ep *rxr_ep, int entry_type, void *x_en
 	}
 
 	peer = rxr_ep_get_peer(rxr_ep, addr);
+	assert(peer);
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
-		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_shm_pool);
+		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 	} else {
-		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_efa_pool);
+		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 	}
 
 	if (!pkt_entry)
 		return -FI_EAGAIN;
 
-	send.iov_count = 0;
-	pkt_entry->send = &send;
-
 	/*
 	 * rxr_pkt_init_ctrl will set pkt_entry->send if it want to use multi iov
 	 */
@@ -325,20 +290,22 @@ ssize_t rxr_pkt_post_ctrl_once(struct rxr_ep *rxr_ep, int entry_type, void *x_en
 		return err;
 	}
 
-	/* if send, tx_pkt_entry will be released while handle completion
-	 * if inject, there will not be completion, therefore tx_pkt_entry has to be
-	 * released here
+	/* If the send (or inject) succeeded, the function rxr_pkt_entry_send
+	 * (or rxr_pkt_entry_inject) will increase the counter in rxr_ep that
+	 * tracks number of outstanding TX ops.
 	 */
-	if (inject)
+	if (inject) {
+		/*
+		 * Currently, the only accepted flags is FI_MORE, which is not
+		 * compatible with inject. Add an additional check here to make
+		 * sure flags is set by the caller correctly.
+		 */
+		assert(!flags);
 		err = rxr_pkt_entry_inject(rxr_ep, pkt_entry, addr);
-	else if (pkt_entry->send->iov_count > 0)
-		err = rxr_pkt_entry_sendv(rxr_ep, pkt_entry, addr,
-					  pkt_entry->send->iov, pkt_entry->send->desc,
-					  pkt_entry->send->iov_count, 0);
+	}
 	else
-		err = rxr_pkt_entry_send(rxr_ep, pkt_entry, addr);
+		err = rxr_pkt_entry_send(rxr_ep, pkt_entry, flags);
 
-	pkt_entry->send = NULL;
 	if (OFI_UNLIKELY(err)) {
 		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
 		return err;
@@ -346,14 +313,33 @@ ssize_t rxr_pkt_post_ctrl_once(struct rxr_ep *rxr_ep, int entry_type, void *x_en
 
 	peer->flags |= RXR_PEER_REQ_SENT;
 	rxr_pkt_handle_ctrl_sent(rxr_ep, pkt_entry);
+
+	/* If injection succeeded, packet should be considered as sent completed.
+	 * therefore call rxr_pkt_handle_send_completion().
+	 * rxr_pkt_handle_send_completion() will release pkt_entry and decrease
+	 * the counter in rxr_ep that tracks number of outstanding TX ops.
+	 */
 	if (inject)
-		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
+		rxr_pkt_handle_send_completion(rxr_ep, pkt_entry);
 
 	return 0;
 }
 
+/**
+ * @brief post control packets.
+ *
+ *
+ * @param[in]   rxr_ep          endpoint
+ * @param[in]   entry_type      type of x_entry, allowed values: RXR_TX_ENTRY, RXR_RX_ENTRY
+ * @param[in]   x_entry         x_entry pointer
+ * @param[in]   ctrl_type       type of control packet
+ * @param[in]   inject          send control packet via inject or not.
+ * @param[in]   flags           additional flags to apply for fi_sendmsg.
+ *                              currently only accepted flags is FI_MORE.
+ * @return      On success return 0, otherwise return a negative error code
+ */
 ssize_t rxr_pkt_post_ctrl(struct rxr_ep *ep, int entry_type, void *x_entry,
-			  int ctrl_type, bool inject)
+			  int ctrl_type, bool inject, uint64_t flags)
 {
 	ssize_t err;
 	struct rxr_tx_entry *tx_entry;
@@ -367,7 +353,7 @@ ssize_t rxr_pkt_post_ctrl(struct rxr_ep *ep, int entry_type, void *x_entry,
 
 		tx_entry = (struct rxr_tx_entry *)x_entry;
 		while (tx_entry->bytes_sent < tx_entry->total_len) {
-			err = rxr_pkt_post_ctrl_once(ep, RXR_TX_ENTRY, x_entry, ctrl_type, 0);
+			err = rxr_pkt_post_ctrl_once(ep, RXR_TX_ENTRY, x_entry, ctrl_type, 0, flags);
 			if (OFI_UNLIKELY(err))
 				return err;
 		}
@@ -375,7 +361,7 @@ ssize_t rxr_pkt_post_ctrl(struct rxr_ep *ep, int entry_type, void *x_entry,
 		return 0;
 	}
 
-	return rxr_pkt_post_ctrl_once(ep, entry_type, x_entry, ctrl_type, inject);
+	return rxr_pkt_post_ctrl_once(ep, entry_type, x_entry, ctrl_type, inject, flags);
 }
 
 ssize_t rxr_pkt_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry, int ctrl_type, bool inject)
@@ -384,27 +370,25 @@ ssize_t rxr_pkt_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_en
 	struct rxr_tx_entry *tx_entry;
 	struct rxr_rx_entry *rx_entry;
 
-	err = rxr_pkt_post_ctrl(ep, entry_type, x_entry, ctrl_type, inject);
+	err = rxr_pkt_post_ctrl(ep, entry_type, x_entry, ctrl_type, inject, 0);
 	if (err == -FI_EAGAIN) {
 		if (entry_type == RXR_TX_ENTRY) {
 			tx_entry = (struct rxr_tx_entry *)x_entry;
-			assert(tx_entry->state != RXR_TX_QUEUED_CTRL ||
-			       tx_entry->state != RXR_TX_QUEUED_REQ_RNR);
+			assert(!(tx_entry->rxr_flags & RXR_TX_ENTRY_QUEUED_RNR));
 			tx_entry->state = RXR_TX_QUEUED_CTRL;
 			tx_entry->queued_ctrl.type = ctrl_type;
 			tx_entry->queued_ctrl.inject = inject;
-			dlist_insert_tail(&tx_entry->queued_entry,
-					  &ep->tx_entry_queued_list);
+			dlist_insert_tail(&tx_entry->queued_ctrl_entry,
+					  &ep->tx_entry_queued_ctrl_list);
 		} else {
 			assert(entry_type == RXR_RX_ENTRY);
 			rx_entry = (struct rxr_rx_entry *)x_entry;
-			assert(rx_entry->state != RXR_RX_QUEUED_CTRL ||
-			       rx_entry->state != RXR_RX_QUEUED_CTS_RNR);
+			assert(rx_entry->state != RXR_RX_QUEUED_CTRL);
 			rx_entry->state = RXR_RX_QUEUED_CTRL;
 			rx_entry->queued_ctrl.type = ctrl_type;
 			rx_entry->queued_ctrl.inject = inject;
-			dlist_insert_tail(&rx_entry->queued_entry,
-					  &ep->rx_entry_queued_list);
+			dlist_insert_tail(&rx_entry->queued_ctrl_entry,
+					  &ep->rx_entry_queued_ctrl_list);
 		}
 
 		err = 0;
@@ -434,7 +418,7 @@ ssize_t rxr_pkt_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_en
  * handshake packet within a certain period of time.
  */
 
-ssize_t rxr_pkt_wait_handshake(struct rxr_ep *ep, fi_addr_t addr, struct rxr_peer *peer)
+ssize_t rxr_pkt_wait_handshake(struct rxr_ep *ep, fi_addr_t addr, struct rdm_peer *peer)
 {
 	ssize_t ret;
 
@@ -480,7 +464,7 @@ ssize_t rxr_pkt_wait_handshake(struct rxr_ep *ep, fi_addr_t addr, struct rxr_pee
  * This function will return 0 if the eager rtw packet is successfully sent.
  */
 ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
-				  fi_addr_t addr, struct rxr_peer *peer)
+				  fi_addr_t addr, struct rdm_peer *peer)
 {
 	struct rxr_tx_entry *tx_entry;
 	ssize_t err;
@@ -497,6 +481,9 @@ ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
 
 	tx_entry->total_len = 0;
 	tx_entry->addr = addr;
+	tx_entry->peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(tx_entry->peer);
+	dlist_insert_tail(&tx_entry->peer_entry, &tx_entry->peer->tx_entry_list);
 	tx_entry->msg_id = -1;
 	tx_entry->cq_entry.flags = FI_RMA | FI_WRITE;
 	tx_entry->cq_entry.buf = NULL;
@@ -506,7 +493,6 @@ ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
 	tx_entry->op = ofi_op_write;
 	tx_entry->state = RXR_TX_REQ;
 
-	tx_entry->send_flags = 0;
 	tx_entry->bytes_acked = 0;
 	tx_entry->bytes_sent = 0;
 	tx_entry->window = 0;
@@ -516,12 +502,11 @@ ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
 	tx_entry->iov_mr_start = 0;
 	tx_entry->iov_offset = 0;
 	tx_entry->fi_flags = RXR_NO_COMPLETION | RXR_NO_COUNTER;
+	tx_entry->rxr_flags = 0;
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &ep->tx_entry_list);
 
-	err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_EAGER_RTW_PKT, 0);
+	err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_EAGER_RTW_PKT, 0, 0);
 
 	if (OFI_UNLIKELY(err))
 		return err;
@@ -529,95 +514,6 @@ ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
 	return 0;
 }
 
-/* return the data size in a packet entry */
-size_t rxr_pkt_data_size(struct rxr_pkt_entry *pkt_entry)
-{
-	int pkt_type;
-
-	assert(pkt_entry);
-	pkt_type = rxr_get_base_hdr(pkt_entry->pkt)->type;
-
-	if (pkt_type == RXR_DATA_PKT)
-		return pkt_entry->pkt_size - sizeof(struct rxr_data_hdr);
-
-	if (pkt_type == RXR_READRSP_PKT)
-		return pkt_entry->pkt_size - sizeof(struct rxr_readrsp_hdr);
-
-	if (pkt_type >= RXR_REQ_PKT_BEGIN) {
-		assert(pkt_type == RXR_EAGER_MSGRTM_PKT || pkt_type == RXR_EAGER_TAGRTM_PKT ||
-		       pkt_type == RXR_MEDIUM_MSGRTM_PKT || pkt_type == RXR_MEDIUM_TAGRTM_PKT ||
-		       pkt_type == RXR_LONG_MSGRTM_PKT || pkt_type == RXR_LONG_TAGRTM_PKT ||
-		       pkt_type == RXR_EAGER_RTW_PKT ||
-		       pkt_type == RXR_LONG_RTW_PKT ||
-		       pkt_type == RXR_DC_EAGER_MSGRTM_PKT ||
-		       pkt_type == RXR_DC_EAGER_TAGRTM_PKT ||
-		       pkt_type == RXR_DC_MEDIUM_MSGRTM_PKT ||
-		       pkt_type == RXR_DC_MEDIUM_TAGRTM_PKT ||
-		       pkt_type == RXR_DC_LONG_MSGRTM_PKT ||
-		       pkt_type == RXR_DC_LONG_TAGRTM_PKT ||
-		       pkt_type == RXR_DC_EAGER_RTW_PKT ||
-		       pkt_type == RXR_DC_LONG_RTW_PKT);
-
-		return pkt_entry->pkt_size - rxr_pkt_req_hdr_size(pkt_entry);
-	}
-
-	/* other packet type does not contain data, thus return 0
-	 */
-	return 0;
-}
-
-/*
- * rxr_pkt_copy_to_rx() copy data to receiving buffer then
- * update counter in rx_entry.
- *
- * If receiving buffer is on GPU memory, it will post a
- * read request, otherwise it will copy data.
- *
- * If all data has been copied to receiving buffer,
- * it will write rx completion and release rx_entry.
- *
- * Return value and states:
- *
- *    On success, return 0 and release pkt_entry
- *    On failure, return error code
- */
-ssize_t rxr_pkt_copy_to_rx(struct rxr_ep *ep,
-			   struct rxr_rx_entry *rx_entry,
-			   size_t data_offset,
-			   struct rxr_pkt_entry *pkt_entry,
-			   char *data, size_t data_size)
-{
-	ssize_t err, bytes_copied;
-
-	pkt_entry->x_entry = rx_entry;
-
-	if (data_size > 0 && efa_ep_is_cuda_mr(rx_entry->desc[0])) {
-		err = rxr_read_post_local_read_or_queue(ep, rx_entry, data_offset,
-							pkt_entry, data, data_size);
-		if (err)
-			FI_WARN(&rxr_prov, FI_LOG_CQ, "cannot post read to copy data\n");
-
-		return err;
-	}
-
-	if (OFI_LIKELY(!(rx_entry->rxr_flags & RXR_RECV_CANCEL)) &&
-	    rx_entry->cq_entry.len > data_offset && data_size > 0) {
-		bytes_copied = ofi_copy_to_iov(rx_entry->iov,
-					       rx_entry->iov_count,
-					       data_offset,
-					       data,
-					       data_size);
-		if (bytes_copied != MIN(data_size, rx_entry->cq_entry.len - data_offset)) {
-			FI_WARN(&rxr_prov, FI_LOG_CQ, "wrong size! bytes_copied: %ld\n",
-				bytes_copied);
-			return -FI_EINVAL;
-		}
-	}
-
-	rxr_pkt_handle_data_copied(ep, pkt_entry, data_size);
-	return 0;
-}
-
 void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 				struct rxr_pkt_entry *pkt_entry,
 				size_t data_size)
@@ -629,6 +525,8 @@ void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 	assert(rx_entry);
 	rx_entry->bytes_copied += data_size;
 
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+
 	if (rx_entry->total_len == rx_entry->bytes_copied) {
 		if (rx_entry->rxr_flags & RXR_DELIVERY_COMPLETE_REQUESTED) {
 			ret = rxr_pkt_post_ctrl_or_queue(ep,
@@ -647,7 +545,7 @@ void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 						     rx_entry);
 				return;
 			}
-			rxr_cq_handle_rx_completion(ep, pkt_entry, rx_entry);
+			rxr_cq_handle_rx_completion(ep, rx_entry);
 			rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
 			/* rx_entry will be released
 			 * when sender receives the
@@ -655,23 +553,190 @@ void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 			 */
 			return;
 		}
-		rxr_cq_handle_rx_completion(ep, pkt_entry, rx_entry);
+		rxr_cq_handle_rx_completion(ep, rx_entry);
 		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
 		rxr_release_rx_entry(ep, rx_entry);
-	} else {
-		rxr_pkt_entry_release_rx(ep, pkt_entry);
 	}
 }
 
-/*
- *   Functions used to handle packet send completion
+/**
+ * @brief handle the a packet that encountered error completion while sending
+ *
+ * Depend on the packet type and error type, the error are handled differently.
+ *
+ * If the packet is associated with an user initialized TX operation:
+ * (TX means send,read or write; such packets include all REQ packets and DATA):
+ *
+ *    If the error is Receiver Not Ready (RNR). there are two cases:
+ *
+ *         If user wants to manager RNR by itself (FI_RM_DISABLED),
+ *         an error CQ entry will be written.
+ *
+ *         Otherwise, the packet will be queued and resnt by progress engine.
+ *
+ *    For other type of error, an error CQ entry is written.
+ *
+ * If the packet is associated with an user initialized recv operiaton,
+ * (such packets include EOR, CTS):
+ *
+ *      If the error is RNR, the packet is queued and resent by progress
+ *      engine. No CQ entry is written.
+ *
+ *      For other types of error, an error CQ entry is written.
+ *
+ * If the packet is not associated with an user operation (such packet include
+ * HANDSHAKE):
+ *
+ *      If the error is RNR, the packet is queued and resent by progress engine.
+ *
+ *      For othre types of error, an error EQ entry is written.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	pkt_entry	pkt entry
+ * @param[in]	err		libfabric error code
+ * @param[in]	prov_errno	provider specific error code
  */
-void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct fi_cq_data_entry *comp)
+void rxr_pkt_handle_send_error(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry, int err, int prov_errno)
 {
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_rx_entry *rx_entry;
+
+	assert(pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL ||
+	       pkt_entry->alloc_type == RXR_PKT_FROM_SHM_TX_POOL);
+
+	rxr_ep_record_tx_op_completed(ep, pkt_entry);
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (!peer) {
+		/*
+		 * If peer is NULL, it means the peer has been removed from AV.
+		 * In this case, ignore this error completion.
+		 */
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "ignoring send error completion of a packet to a removed peer.\n");
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
+	if (!pkt_entry->x_entry) {
+		/* only handshake packet is not associated with any TX/RX operation */
+		assert(rxr_get_base_hdr(pkt_entry->pkt)->type == RXR_HANDSHAKE_PKT);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR) {
+			/*
+			 * handshake should always be queued for RNR
+			 */
+			assert(!(peer->flags & RXR_PEER_HANDSHAKE_QUEUED));
+			peer->flags |= RXR_PEER_HANDSHAKE_QUEUED;
+			dlist_insert_tail(&peer->handshake_queued_entry,
+					  &ep->handshake_queued_peer_list);
+		} else if (prov_errno != IBV_WC_REM_INV_RD_REQ_ERR) {
+			/* If prov_errno is IBV_WC_REM_INV_RD_REQ_ERR, the peer has been destroyed.
+			 * Which is normal, as peer does not always need a handshake packet perform
+			 * its duty. (For example, if a peer just want to sent 1 message to the ep, it
+			 * does not need handshake.)
+			 * In this case, it is safe to ignore this error completion.
+			 * In all other cases, we write an eq entry because there is no application
+			 * operation associated with handshake.
+			 */
+			efa_eq_write_error(&ep->util_ep, err, prov_errno);
+		}
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
+		tx_entry = pkt_entry->x_entry;
+		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR &&
+		    ep->handle_resource_management == FI_RM_DISABLED) {
+			/*
+			 * Write an error to the application for RNR when
+			 * resource management is disabled.
+			 */
+			rxr_cq_write_tx_error(ep, pkt_entry->x_entry, FI_ENORX, 0);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		} if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR) {
+			/*
+			 * This packet is assoiciated with a send operation,
+			 * (such packets include all REQ, DATA)
+			 * thus shoud be queued for RNR only if
+			 * application want EFA to manager resource.
+			 */
+			rxr_cq_queue_rnr_pkt(ep, &tx_entry->queued_pkts, pkt_entry);
+			if (!(tx_entry->rxr_flags & RXR_TX_ENTRY_QUEUED_RNR)) {
+				tx_entry->rxr_flags |= RXR_TX_ENTRY_QUEUED_RNR;
+				dlist_insert_tail(&tx_entry->queued_rnr_entry,
+						  &ep->tx_entry_queued_rnr_list);
+			}
+		} else {
+			rxr_cq_write_tx_error(ep, pkt_entry->x_entry, err, prov_errno);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		}
+
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_RX_ENTRY) {
+		rx_entry = pkt_entry->x_entry;
+		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR) {
+			/*
+			 * This packet is associated with a recv operation,
+			 * (such packets include CTS and EOR)
+			 * thus should always be queued for RNR.
+			 * This is regardless value of ep->handle_resource_management,
+			 * because resource management is only applied to send operation.
+			 */
+			rxr_cq_queue_rnr_pkt(ep, &rx_entry->queued_pkts, pkt_entry);
+			/*
+			 * rx_entry send one ctrl packet at a time, so if we
+			 * received RNR for the packet, the rx_entry must not
+			 * be in ep's rx_queued_entry_rnr_list, thus cannot
+			 * have the QUEUED_RNR flag
+			 */
+			assert(!(rx_entry->rxr_flags & RXR_RX_ENTRY_QUEUED_RNR));
+			rx_entry->rxr_flags |= RXR_RX_ENTRY_QUEUED_RNR;
+			dlist_insert_tail(&rx_entry->queued_rnr_entry,
+					  &ep->rx_entry_queued_rnr_list);
 
-	pkt_entry = (struct rxr_pkt_entry *)comp->op_context;
+		} else {
+			rxr_cq_write_rx_error(ep, pkt_entry->x_entry, err, prov_errno);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		}
+
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_READ_ENTRY) {
+		/* read will not encounter RNR */
+		assert(prov_errno != IBV_WC_RNR_RETRY_EXC_ERR);
+		rxr_read_write_error(ep, pkt_entry->x_entry, err, prov_errno);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
+	FI_WARN(&rxr_prov, FI_LOG_CQ,
+		"%s unknown x_entry type %d\n",
+		__func__, RXR_GET_X_ENTRY_TYPE(pkt_entry));
+	assert(0 && "unknown x_entry state");
+	efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	rxr_pkt_entry_release_tx(ep, pkt_entry);
+}
+
+void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	/*
+	 * For a send completion, pkt_entry->addr can be FI_ADDR_NOTAVAIL in 3 situations:
+	 * 1. the pkt_entry is used for a local read operation
+	 * 2. a new peer with same gid+qpn was inserted to av, thus the peer was removed from AV.
+	 * 3. application removed the peer's address from av.
+	 * In 1, we should proceed. For 2 and 3, the send completion should be ignored.
+	 */
+	if (pkt_entry->addr == FI_ADDR_NOTAVAIL &&
+	    !(pkt_entry->flags & RXR_PKT_ENTRY_LOCAL_READ)) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "ignoring send completion of a packet to a removed peer.\n");
+		rxr_ep_record_tx_op_completed(ep, pkt_entry);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
 
 	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
 	case RXR_HANDSHAKE_PKT:
@@ -704,25 +769,25 @@ void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct fi_cq_data_entry *
 	case RXR_MEDIUM_TAGRTM_PKT:
 		rxr_pkt_handle_medium_rtm_send_completion(ep, pkt_entry);
 		break;
-	case RXR_LONG_MSGRTM_PKT:
-	case RXR_LONG_TAGRTM_PKT:
-		rxr_pkt_handle_long_rtm_send_completion(ep, pkt_entry);
+	case RXR_LONGCTS_MSGRTM_PKT:
+	case RXR_LONGCTS_TAGRTM_PKT:
+		rxr_pkt_handle_longcts_rtm_send_completion(ep, pkt_entry);
 		break;
-	case RXR_READ_MSGRTM_PKT:
-	case RXR_READ_TAGRTM_PKT:
-		rxr_pkt_handle_read_rtm_send_completion(ep, pkt_entry);
+	case RXR_LONGREAD_MSGRTM_PKT:
+	case RXR_LONGREAD_TAGRTM_PKT:
+		rxr_pkt_handle_longread_rtm_send_completion(ep, pkt_entry);
 		break;
 	case RXR_EAGER_RTW_PKT:
 		rxr_pkt_handle_eager_rtw_send_completion(ep, pkt_entry);
 		break;
-	case RXR_LONG_RTW_PKT:
-		rxr_pkt_handle_long_rtw_send_completion(ep, pkt_entry);
+	case RXR_LONGCTS_RTW_PKT:
+		rxr_pkt_handle_longcts_rtw_send_completion(ep, pkt_entry);
 		break;
-	case RXR_READ_RTW_PKT:
-		rxr_pkt_handle_read_rtw_send_completion(ep, pkt_entry);
+	case RXR_LONGREAD_RTW_PKT:
+		rxr_pkt_handle_longread_rtw_send_completion(ep, pkt_entry);
 		break;
 	case RXR_SHORT_RTR_PKT:
-	case RXR_LONG_RTR_PKT:
+	case RXR_LONGCTS_RTR_PKT:
 		rxr_pkt_handle_rtr_send_completion(ep, pkt_entry);
 		break;
 	case RXR_WRITE_RTA_PKT:
@@ -751,31 +816,59 @@ void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct fi_cq_data_entry *
 		 * any action on tx_entry here.
 		 */
 		break;
-	case RXR_DC_LONG_MSGRTM_PKT:
-	case RXR_DC_LONG_TAGRTM_PKT:
-		rxr_pkt_handle_dc_long_rtm_send_completion(ep, pkt_entry);
+	case RXR_DC_LONGCTS_MSGRTM_PKT:
+	case RXR_DC_LONGCTS_TAGRTM_PKT:
+		rxr_pkt_handle_dc_longcts_rtm_send_completion(ep, pkt_entry);
 		break;
-	case RXR_DC_LONG_RTW_PKT:
-		rxr_pkt_handle_dc_long_rtw_send_completion(ep, pkt_entry);
+	case RXR_DC_LONGCTS_RTW_PKT:
+		rxr_pkt_handle_dc_longcts_rtw_send_completion(ep, pkt_entry);
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"invalid control pkt type %d\n",
 			rxr_get_base_hdr(pkt_entry->pkt)->type);
 		assert(0 && "invalid control pkt type");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
 		return;
 	}
 
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	if (!peer->is_local)
-		rxr_ep_dec_tx_pending(ep, peer, 0);
+	rxr_ep_record_tx_op_completed(ep, pkt_entry);
 	rxr_pkt_entry_release_tx(ep, pkt_entry);
 }
 
-/*
- *  Functions used to handle packet receive completion
+/**
+ * @brief handle the a packet that encountered error completion while receiving
+ *
+ * This function will write error cq or eq entry, then release the packet entry.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	pkt_entry	pkt entry
+ * @param[in]	err		libfabric error code
+ * @param[in]	prov_errno	provider specific error code
  */
+void rxr_pkt_handle_recv_error(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry, int err, int prov_errno)
+{
+	if (!pkt_entry->x_entry) {
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
+		rxr_cq_write_tx_error(ep, pkt_entry->x_entry, err, prov_errno);
+	} else if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_RX_ENTRY) {
+		rxr_cq_write_rx_error(ep, pkt_entry->x_entry, err, prov_errno);
+	} else {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+		"%s unknown x_entry type %d\n",
+			__func__, RXR_GET_X_ENTRY_TYPE(pkt_entry));
+		assert(0 && "unknown x_entry state");
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	}
+
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+}
+
 static
 fi_addr_t rxr_pkt_insert_addr(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry, void *raw_addr)
 {
@@ -785,7 +878,7 @@ fi_addr_t rxr_pkt_insert_addr(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry
 	struct rxr_base_hdr *base_hdr;
 
 	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
-	if (base_hdr->version < RXR_BASE_PROTOCOL_VERSION) {
+	if (base_hdr->version < RXR_PROTOCOL_VERSION) {
 		char host_gid[ep->core_addrlen * 3];
 		int length = 0;
 
@@ -795,19 +888,19 @@ fi_addr_t rxr_pkt_insert_addr(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Host %s received a packet with invalid protocol version %d.\n"
 			"This host can only support protocol version %d and above.\n",
-			host_gid, base_hdr->version, RXR_BASE_PROTOCOL_VERSION);
+			host_gid, base_hdr->version, RXR_PROTOCOL_VERSION);
 		efa_eq_write_error(&ep->util_ep, FI_EIO, -FI_EINVAL);
 		fprintf(stderr, "Host %s received a packet with invalid protocol version %d.\n"
 			"This host can only support protocol version %d and above. %s:%d\n",
-			host_gid, base_hdr->version, RXR_BASE_PROTOCOL_VERSION, __FILE__, __LINE__);
+			host_gid, base_hdr->version, RXR_PROTOCOL_VERSION, __FILE__, __LINE__);
 		abort();
 	}
 
 	assert(base_hdr->type >= RXR_REQ_PKT_BEGIN);
 
 	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
-	ret = efa_av_insert_addr(efa_ep->av, (struct efa_ep_addr *)raw_addr,
-				 &rdm_addr, 0, NULL);
+	ret = efa_av_insert_one(efa_ep->av, (struct efa_ep_addr *)raw_addr,
+	                        &rdm_addr, 0, NULL);
 	if (OFI_UNLIKELY(ret != 0)) {
 		efa_eq_write_error(&ep->util_ep, FI_EINVAL, ret);
 		return -1;
@@ -816,79 +909,31 @@ fi_addr_t rxr_pkt_insert_addr(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry
 	return rdm_addr;
 }
 
-void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
-				    struct fi_cq_data_entry *cq_entry,
-				    fi_addr_t src_addr)
+/**
+ * @brief process a received packet
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	pkt_entry	received packet entry
+ */
+void rxr_pkt_proc_received(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_peer *peer;
 	struct rxr_base_hdr *base_hdr;
-	struct rxr_pkt_entry *pkt_entry;
-
-	pkt_entry = (struct rxr_pkt_entry *)cq_entry->op_context;
-	pkt_entry->pkt_size = cq_entry->len;
-	assert(pkt_entry->pkt_size > 0);
 
 	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
-	if (base_hdr->type >= RXR_EXTRA_REQ_PKT_END) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"Peer %d is requesting feature %d, which this EP does not support.\n",
-			(int)src_addr, base_hdr->type);
-
-		assert(0 && "invalid REQ packe type");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
-		return;
-	}
-
-	if (base_hdr->type >= RXR_REQ_PKT_BEGIN) {
-		/*
-		 * as long as the REQ packet contain raw address
-		 * we will need to call insert because it might be a new
-		 * EP with new Q-Key.
-		 */
-		void *raw_addr;
-
-		raw_addr = rxr_pkt_req_raw_addr(pkt_entry);
-		if (OFI_UNLIKELY(raw_addr != NULL))
-			pkt_entry->addr = rxr_pkt_insert_addr(ep, pkt_entry, raw_addr);
-		else
-			pkt_entry->addr = src_addr;
-	} else {
-		assert(src_addr != FI_ADDR_NOTAVAIL);
-		pkt_entry->addr = src_addr;
-	}
-
-#if ENABLE_DEBUG
-	if (!ep->use_zcpy_rx) {
-		dlist_remove(&pkt_entry->dbg_entry);
-		dlist_insert_tail(&pkt_entry->dbg_entry, &ep->rx_pkt_list);
-	}
-#ifdef ENABLE_RXR_PKT_DUMP
-	rxr_pkt_print("Received", ep, (struct rxr_base_hdr *)pkt_entry->pkt);
-#endif
-#endif
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	if (!(peer->flags & RXR_PEER_HANDSHAKE_SENT))
-		rxr_pkt_post_handshake(ep, peer, pkt_entry->addr);
-
-	if (peer->is_local) {
-		assert(ep->use_shm);
-		ep->posted_bufs_shm--;
-	} else {
-		ep->posted_bufs_efa--;
-	}
-
 	switch (base_hdr->type) {
 	case RXR_RETIRED_RTS_PKT:
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Received a RTS packet, which has been retired since protocol version 4\n");
 		assert(0 && "deprecated RTS pakcet received");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	case RXR_RETIRED_CONNACK_PKT:
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Received a CONNACK packet, which has been retired since protocol version 4\n");
 		assert(0 && "deprecated CONNACK pakcet received");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	case RXR_EOR_PKT:
 		rxr_pkt_handle_eor_recv(ep, pkt_entry);
@@ -912,11 +957,6 @@ void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 		rxr_pkt_handle_receipt_recv(ep, pkt_entry);
 		return;
 	case RXR_EAGER_MSGRTM_PKT:
-		if (ep->use_zcpy_rx && pkt_entry->type == RXR_PKT_ENTRY_USER)
-			rxr_pkt_handle_zcpy_recv(ep, pkt_entry);
-		else
-			rxr_pkt_handle_rtm_rta_recv(ep, pkt_entry);
-		return;
 	case RXR_EAGER_TAGRTM_PKT:
 	case RXR_DC_EAGER_MSGRTM_PKT:
 	case RXR_DC_EAGER_TAGRTM_PKT:
@@ -924,12 +964,12 @@ void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 	case RXR_MEDIUM_TAGRTM_PKT:
 	case RXR_DC_MEDIUM_MSGRTM_PKT:
 	case RXR_DC_MEDIUM_TAGRTM_PKT:
-	case RXR_LONG_MSGRTM_PKT:
-	case RXR_LONG_TAGRTM_PKT:
-	case RXR_DC_LONG_MSGRTM_PKT:
-	case RXR_DC_LONG_TAGRTM_PKT:
-	case RXR_READ_MSGRTM_PKT:
-	case RXR_READ_TAGRTM_PKT:
+	case RXR_LONGCTS_MSGRTM_PKT:
+	case RXR_LONGCTS_TAGRTM_PKT:
+	case RXR_DC_LONGCTS_MSGRTM_PKT:
+	case RXR_DC_LONGCTS_TAGRTM_PKT:
+	case RXR_LONGREAD_MSGRTM_PKT:
+	case RXR_LONGREAD_TAGRTM_PKT:
 	case RXR_WRITE_RTA_PKT:
 	case RXR_DC_WRITE_RTA_PKT:
 	case RXR_FETCH_RTA_PKT:
@@ -939,15 +979,15 @@ void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 	case RXR_EAGER_RTW_PKT:
 		rxr_pkt_handle_eager_rtw_recv(ep, pkt_entry);
 		return;
-	case RXR_LONG_RTW_PKT:
-	case RXR_DC_LONG_RTW_PKT:
-		rxr_pkt_handle_long_rtw_recv(ep, pkt_entry);
+	case RXR_LONGCTS_RTW_PKT:
+	case RXR_DC_LONGCTS_RTW_PKT:
+		rxr_pkt_handle_longcts_rtw_recv(ep, pkt_entry);
 		return;
-	case RXR_READ_RTW_PKT:
-		rxr_pkt_handle_read_rtw_recv(ep, pkt_entry);
+	case RXR_LONGREAD_RTW_PKT:
+		rxr_pkt_handle_longread_rtw_recv(ep, pkt_entry);
 		return;
 	case RXR_SHORT_RTR_PKT:
-	case RXR_LONG_RTR_PKT:
+	case RXR_LONGCTS_RTR_PKT:
 		rxr_pkt_handle_rtr_recv(ep, pkt_entry);
 		return;
 	case RXR_DC_EAGER_RTW_PKT:
@@ -958,11 +998,93 @@ void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 			"invalid control pkt type %d\n",
 			rxr_get_base_hdr(pkt_entry->pkt)->type);
 		assert(0 && "invalid control pkt type");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
 }
 
+void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
+				    struct rxr_pkt_entry *pkt_entry)
+{
+	int pkt_type;
+	struct rdm_peer *peer;
+	struct rxr_base_hdr *base_hdr;
+	struct rxr_rx_entry *zcpy_rx_entry = NULL;
+
+	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
+	pkt_type = base_hdr->type;
+	if (pkt_type >= RXR_EXTRA_REQ_PKT_END) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Peer %d is requesting feature %d, which this EP does not support.\n",
+			(int)pkt_entry->addr, base_hdr->type);
+
+		assert(0 && "invalid REQ packe type");
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
+		return;
+	}
+	
+	if (pkt_entry->addr == FI_ADDR_NOTAVAIL) {
+		if (pkt_type >= RXR_REQ_PKT_BEGIN && rxr_pkt_req_raw_addr(pkt_entry)) {
+			/*
+			 * We have not communicated with this peer before.
+			 * rxr_pkt_insert_addr() will insert the address to address vector,
+			 * and pkt_entry->addr should be updated accordingly.
+			 */
+			void *raw_addr;
+
+			raw_addr = rxr_pkt_req_raw_addr(pkt_entry);
+			assert(raw_addr);
+			pkt_entry->addr = rxr_pkt_insert_addr(ep, pkt_entry, raw_addr);
+		} else {
+			/*
+			 * We had prior communication with the peer.
+			 * Application called fi_av_remove() to remove the address
+			 * from address vector. In this case, this packet should be ignored.
+			 */
+			FI_WARN(&rxr_prov, FI_LOG_CQ, "Warning: ignoring a received packet from a removed address\n");
+			rxr_pkt_entry_release_rx(ep, pkt_entry);
+			return;
+		}
+	}
+
+	assert(pkt_entry->addr != FI_ADDR_NOTAVAIL);
+
+#if ENABLE_DEBUG
+	if (!ep->use_zcpy_rx) {
+		dlist_remove(&pkt_entry->dbg_entry);
+		dlist_insert_tail(&pkt_entry->dbg_entry, &ep->rx_pkt_list);
+	}
+#ifdef ENABLE_RXR_PKT_DUMP
+	rxr_pkt_print("Received", ep, (struct rxr_base_hdr *)pkt_entry->pkt);
+#endif
+#endif
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
+	rxr_pkt_post_handshake_or_queue(ep, peer);
+
+	if (peer->is_local) {
+		assert(ep->use_shm);
+		ep->shm_rx_pkts_posted--;
+	} else {
+		ep->efa_rx_pkts_posted--;
+	}
+
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_USER_BUFFER) {
+		assert(pkt_entry->x_entry);
+		zcpy_rx_entry = pkt_entry->x_entry;
+	}
+
+	rxr_pkt_proc_received(ep, pkt_entry);
+
+	if (zcpy_rx_entry && pkt_type != RXR_EAGER_MSGRTM_PKT) {
+		/* user buffer was not matched with a message,
+		 * therefore reposting the buffer */
+		rxr_ep_post_user_recv_buf(ep, zcpy_rx_entry, 0);
+	}
+}
+
 #if ENABLE_DEBUG
 
 /*
@@ -981,8 +1103,8 @@ void rxr_pkt_print_handshake(char *prefix,
 	       handshake_hdr->flags);
 
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "%s RxR HANDSHAKE packet, maxproto: %d\n",
-	       prefix, handshake_hdr->maxproto);
+	       "%s RxR HANDSHAKE packet, nextra_p3: %d\n",
+	       prefix, handshake_hdr->nextra_p3);
 }
 
 static
@@ -994,46 +1116,64 @@ void rxr_pkt_print_cts(char *prefix, struct rxr_cts_hdr *cts_hdr)
 	       " rx_id: %"	   PRIu32
 	       " window: %"	   PRIu64
 	       "\n", prefix, cts_hdr->version, cts_hdr->flags,
-	       cts_hdr->tx_id, cts_hdr->rx_id, cts_hdr->window);
+	       cts_hdr->send_id, cts_hdr->recv_id, cts_hdr->recv_length);
 }
 
 static
-void rxr_pkt_print_data(char *prefix, struct rxr_data_pkt *data_pkt)
+void rxr_pkt_print_data(char *prefix, struct rxr_pkt_entry *pkt_entry)
 {
+	struct rxr_data_hdr *data_hdr;
 	char str[RXR_PKT_DUMP_DATA_LEN * 4];
-	size_t str_len = RXR_PKT_DUMP_DATA_LEN * 4, l;
+	size_t str_len = RXR_PKT_DUMP_DATA_LEN * 4, l, hdr_size;
+	uint8_t *data;
 	int i;
 
 	str[str_len - 1] = '\0';
 
+	data_hdr = rxr_get_data_hdr(pkt_entry->pkt);
+
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 	       "%s RxR DATA packet -  version: %" PRIu8
 	       " flags: %x rx_id: %" PRIu32
 	       " seg_size: %"	     PRIu64
 	       " seg_offset: %"	     PRIu64
-	       "\n", prefix, data_pkt->hdr.version, data_pkt->hdr.flags,
-	       data_pkt->hdr.rx_id, data_pkt->hdr.seg_size,
-	       data_pkt->hdr.seg_offset);
+	       "\n", prefix, data_hdr->version, data_hdr->flags,
+	       data_hdr->recv_id, data_hdr->seg_length,
+	       data_hdr->seg_offset);
+
+	hdr_size = sizeof(struct rxr_data_hdr);
+	if (data_hdr->flags & RXR_PKT_CONNID_HDR) {
+		hdr_size += sizeof(struct rxr_data_opt_connid_hdr);
+		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+		       "sender_connid: %d\n",
+		       data_hdr->connid_hdr->connid);
+	}
+
+	data = (uint8_t *)pkt_entry->pkt + hdr_size;
 
 	l = snprintf(str, str_len, ("\tdata:    "));
-	for (i = 0; i < MIN(data_pkt->hdr.seg_size, RXR_PKT_DUMP_DATA_LEN);
+	for (i = 0; i < MIN(data_hdr->seg_length, RXR_PKT_DUMP_DATA_LEN);
 	     i++)
 		l += snprintf(str + l, str_len - l, "%02x ",
-			      ((uint8_t *)data_pkt->data)[i]);
+			      data[i]);
 	FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
 }
 
-void rxr_pkt_print(char *prefix, struct rxr_ep *ep, struct rxr_base_hdr *hdr)
+void rxr_pkt_print(char *prefix, struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 {
+	struct rxr_base_hdr *hdr;
+
+	hdr = rxr_get_base_hdr(pkt_entry->pkt);
+
 	switch (hdr->type) {
 	case RXR_HANDSHAKE_PKT:
-		rxr_pkt_print_handshake(prefix, (struct rxr_handshake_hdr *)hdr);
+		rxr_pkt_print_handshake(prefix, rxr_get_handshake_hdr(pkt_entry->pkt));
 		break;
 	case RXR_CTS_PKT:
-		rxr_pkt_print_cts(prefix, (struct rxr_cts_hdr *)hdr);
+		rxr_pkt_print_cts(prefix, rxr_get_cts_hdr(pkt_entry->pkt));
 		break;
 	case RXR_DATA_PKT:
-		rxr_pkt_print_data(prefix, (struct rxr_data_pkt *)hdr);
+		rxr_pkt_print_data(prefix, pkt_entry);
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid ctl pkt type %d\n",
diff --git a/prov/efa/src/rxr/rxr_pkt_cmd.h b/prov/efa/src/rxr/rxr_pkt_cmd.h
index 34a04dd..bc43ba5 100644
--- a/prov/efa/src/rxr/rxr_pkt_cmd.h
+++ b/prov/efa/src/rxr/rxr_pkt_cmd.h
@@ -36,42 +36,39 @@
 
 #include "rxr.h"
 
-ssize_t rxr_pkt_post_data(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry);
-
 ssize_t rxr_pkt_post_ctrl(struct rxr_ep *ep, int entry_type, void *x_entry,
-			  int ctrl_type, bool inject);
+			  int ctrl_type, bool inject, uint64_t flags);
 
 ssize_t rxr_pkt_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry,
 				   int ctrl_type, bool inject);
 
-size_t rxr_pkt_data_size(struct rxr_pkt_entry *pkt_entry);
-
-ssize_t rxr_pkt_copy_to_rx(struct rxr_ep *ep,
-			   struct rxr_rx_entry *rx_entry,
-			   size_t data_offset,
-			   struct rxr_pkt_entry *pkt_entry,
-			   char *data, size_t data_size);
-
 void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 				struct rxr_pkt_entry *pkt_entry,
 				size_t data_size);
 
+void rxr_pkt_handle_send_error(struct rxr_ep *ep,
+			       struct rxr_pkt_entry *pkt_entry,
+			       int err, int prov_errno);
+
 void rxr_pkt_handle_send_completion(struct rxr_ep *ep,
-				    struct fi_cq_data_entry *cq_entry);
+				    struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_recv_error(struct rxr_ep *ep,
+			       struct rxr_pkt_entry *pkt_entry,
+			       int err, int prov_errno);
 
 void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
-				    struct fi_cq_data_entry *cq_entry,
-				    fi_addr_t src_addr);
+				    struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_wait_handshake(struct rxr_ep *ep, fi_addr_t addr, struct rxr_peer *peer);
+ssize_t rxr_pkt_wait_handshake(struct rxr_ep *ep, fi_addr_t addr, struct rdm_peer *peer);
 
 ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
-				  fi_addr_t addr, struct rxr_peer *peer);
+				  fi_addr_t addr, struct rdm_peer *peer);
 
 #if ENABLE_DEBUG
 void rxr_pkt_print(char *prefix,
 		   struct rxr_ep *ep,
-		   struct rxr_base_hdr *hdr);
+		   struct rxr_pkt_entry *pkt_entry);
 #endif
 
 #endif
diff --git a/prov/efa/src/rxr/rxr_pkt_entry.c b/prov/efa/src/rxr/rxr_pkt_entry.c
index 0780620..39b4ae9 100644
--- a/prov/efa/src/rxr/rxr_pkt_entry.c
+++ b/prov/efa/src/rxr/rxr_pkt_entry.c
@@ -47,43 +47,9 @@
 /*
  *   General purpose utility functions
  */
-
-struct rxr_pkt_entry *rxr_pkt_entry_init_prefix(struct rxr_ep *ep,
-						const struct fi_msg *posted_buf,
-						struct ofi_bufpool *pkt_pool)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	struct efa_mr *mr;
-
-	/*
-	 * Given the pkt_entry->pkt immediately follows the pkt_entry
-	 * fields, we can directly map the user-provided fi_msg address
-	 * as the pkt_entry, which will hold the metadata in the prefix.
-	 */
-	assert(posted_buf->msg_iov->iov_len >= sizeof(struct rxr_pkt_entry) + sizeof(struct rxr_eager_msgrtm_hdr));
-	pkt_entry = (struct rxr_pkt_entry *) posted_buf->msg_iov->iov_base;
-	if (!pkt_entry)
-		return NULL;
-
-	/*
-	 * The ownership of the prefix buffer lies with the application, do not
-	 * put it on the dbg list for cleanup during shutdown or poison it. The
-	 * provider loses jurisdiction over it soon after writing the rx
-	 * completion.
-	 */
-	dlist_init(&pkt_entry->entry);
-	mr = (struct efa_mr *) posted_buf->desc[0];
-	pkt_entry->mr = &mr->mr_fid;
-
-	pkt_entry->type = RXR_PKT_ENTRY_USER;
-	pkt_entry->state = RXR_PKT_ENTRY_IN_USE;
-	pkt_entry->next = NULL;
-
-	return pkt_entry;
-}
-
 struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
-					  struct ofi_bufpool *pkt_pool)
+					  struct ofi_bufpool *pkt_pool,
+					  enum rxr_pkt_entry_alloc_type alloc_type)
 {
 	struct rxr_pkt_entry *pkt_entry;
 	void *mr = NULL;
@@ -103,18 +69,23 @@ struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
 #ifdef ENABLE_EFA_POISONING
 	memset(pkt_entry->pkt, 0, ep->mtu_size);
 #endif
-	pkt_entry->type = RXR_PKT_ENTRY_POSTED;
-	pkt_entry->state = RXR_PKT_ENTRY_IN_USE;
+	pkt_entry->alloc_type = alloc_type;
+	pkt_entry->flags = RXR_PKT_ENTRY_IN_USE;
 	pkt_entry->next = NULL;
-
+	pkt_entry->x_entry = NULL;
 	return pkt_entry;
 }
 
-static
-void rxr_pkt_entry_release_single_tx(struct rxr_ep *ep,
-				     struct rxr_pkt_entry *pkt)
+/**
+ * @brief release a TX packet entry
+ *
+ * @param[in]     ep  the end point
+ * @param[in,out] pkt the pkt_entry to be released
+ */
+void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
 #if ENABLE_DEBUG
 	dlist_remove(&pkt->dbg_entry);
@@ -123,37 +94,30 @@ void rxr_pkt_entry_release_single_tx(struct rxr_ep *ep,
 	 * Decrement rnr_queued_pkts counter and reset backoff for this peer if
 	 * we get a send completion for a retransmitted packet.
 	 */
-	if (OFI_UNLIKELY(pkt->state == RXR_PKT_ENTRY_RNR_RETRANSMIT)) {
+	if (OFI_UNLIKELY(pkt->flags & RXR_PKT_ENTRY_RNR_RETRANSMIT)) {
 		peer = rxr_ep_get_peer(ep, pkt->addr);
+		assert(peer);
 		peer->rnr_queued_pkt_cnt--;
-		peer->timeout_interval = 0;
-		peer->rnr_timeout_exp = 0;
-		if (peer->flags & RXR_PEER_IN_BACKOFF)
-			dlist_remove(&peer->rnr_entry);
-		peer->flags &= ~RXR_PEER_IN_BACKOFF;
+		peer->rnr_backoff_wait_time = 0;
+		if (peer->flags & RXR_PEER_IN_BACKOFF) {
+			dlist_remove(&peer->rnr_backoff_entry);
+			peer->flags &= ~RXR_PEER_IN_BACKOFF;
+		}
 		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 		       "reset backoff timer for peer: %" PRIu64 "\n",
 		       pkt->addr);
 	}
+	if (pkt->send) {
+		ofi_buf_free(pkt->send);
+		pkt->send = NULL;
+	}
 #ifdef ENABLE_EFA_POISONING
 	rxr_poison_mem_region((uint32_t *)pkt, ep->tx_pkt_pool_entry_sz);
 #endif
-	pkt->state = RXR_PKT_ENTRY_FREE;
+	pkt->flags = 0;
 	ofi_buf_free(pkt);
 }
 
-void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
-			      struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_pkt_entry *next;
-
-	while (pkt_entry) {
-		next = pkt_entry->next;
-		rxr_pkt_entry_release_single_tx(ep, pkt_entry);
-		pkt_entry = next;
-	}
-}
-
 /*
  * rxr_pkt_entry_release_rx() release a rx packet entry.
  * It requires input pkt_entry to be unlinked.
@@ -169,21 +133,14 @@ void rxr_pkt_entry_release_rx(struct rxr_ep *ep,
 {
 	assert(pkt_entry->next == NULL);
 
-	if (ep->use_zcpy_rx && pkt_entry->type == RXR_PKT_ENTRY_USER)
+	if (ep->use_zcpy_rx && pkt_entry->alloc_type == RXR_PKT_FROM_USER_BUFFER)
 		return;
 
-	if (pkt_entry->type == RXR_PKT_ENTRY_POSTED) {
-		struct rxr_peer *peer;
-
-		peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-
-		if (peer->is_local)
-			ep->rx_bufs_shm_to_post++;
-		else
-			ep->rx_bufs_efa_to_post++;
-	}
-
-	if (pkt_entry->type == RXR_PKT_ENTRY_READ_COPY) {
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_RX_POOL) {
+		ep->efa_rx_pkts_to_post++;
+	} else if (pkt_entry->alloc_type == RXR_PKT_FROM_SHM_RX_POOL) {
+		ep->shm_rx_pkts_to_post++;
+	} else if (pkt_entry->alloc_type == RXR_PKT_FROM_READ_COPY_POOL) {
 		assert(ep->rx_readcopy_pkt_pool_used > 0);
 		ep->rx_readcopy_pkt_pool_used--;
 	}
@@ -195,18 +152,17 @@ void rxr_pkt_entry_release_rx(struct rxr_ep *ep,
 	/* the same pool size is used for all types of rx pkt_entries */
 	rxr_poison_mem_region((uint32_t *)pkt_entry, ep->rx_pkt_pool_entry_sz);
 #endif
-	pkt_entry->state = RXR_PKT_ENTRY_FREE;
+	pkt_entry->flags = 0;
 	ofi_buf_free(pkt_entry);
 }
 
 void rxr_pkt_entry_copy(struct rxr_ep *ep,
 			struct rxr_pkt_entry *dest,
-			struct rxr_pkt_entry *src,
-			int new_entry_type)
+			struct rxr_pkt_entry *src)
 {
 	FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
-	       "Copying packet out of posted buffer! src_entry_type: %d new_entry_type: %d\n",
-		src->type, new_entry_type);
+	       "Copying packet out of posted buffer! src_entry_alloc_type: %d desc_entry_alloc_type: %d\n",
+		src->alloc_type, dest->alloc_type);
 	dlist_init(&dest->entry);
 #if ENABLE_DEBUG
 	dlist_init(&dest->dbg_entry);
@@ -218,8 +174,7 @@ void rxr_pkt_entry_copy(struct rxr_ep *ep,
 	dest->x_entry = src->x_entry;
 	dest->pkt_size = src->pkt_size;
 	dest->addr = src->addr;
-	dest->type = new_entry_type;
-	dest->state = RXR_PKT_ENTRY_IN_USE;
+	dest->flags = RXR_PKT_ENTRY_IN_USE;
 	dest->next = NULL;
 	memcpy(dest->pkt, src->pkt, ep->mtu_size);
 }
@@ -233,8 +188,8 @@ struct rxr_pkt_entry *rxr_pkt_get_unexp(struct rxr_ep *ep,
 {
 	struct rxr_pkt_entry *unexp_pkt_entry;
 
-	if (rxr_env.rx_copy_unexp && (*pkt_entry_ptr)->type == RXR_PKT_ENTRY_POSTED) {
-		unexp_pkt_entry = rxr_pkt_entry_clone(ep, ep->rx_unexp_pkt_pool, *pkt_entry_ptr, RXR_PKT_ENTRY_UNEXP);
+	if (rxr_env.rx_copy_unexp && (*pkt_entry_ptr)->alloc_type == RXR_PKT_FROM_EFA_RX_POOL) {
+		unexp_pkt_entry = rxr_pkt_entry_clone(ep, ep->rx_unexp_pkt_pool, RXR_PKT_FROM_UNEXP_POOL, *pkt_entry_ptr);
 		if (OFI_UNLIKELY(!unexp_pkt_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unable to allocate rx_pkt_entry for unexp msg\n");
@@ -254,12 +209,12 @@ void rxr_pkt_entry_release_cloned(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_e
 	struct rxr_pkt_entry *next;
 
 	while (pkt_entry) {
-		assert(pkt_entry->type == RXR_PKT_ENTRY_OOO  ||
-		       pkt_entry->type == RXR_PKT_ENTRY_UNEXP);
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_OOO_POOL ||
+		       pkt_entry->alloc_type == RXR_PKT_FROM_UNEXP_POOL);
 #ifdef ENABLE_EFA_POISONING
 		rxr_poison_mem_region((uint32_t *)pkt_entry, ep->tx_pkt_pool_entry_sz);
 #endif
-		pkt_entry->state = RXR_PKT_ENTRY_FREE;
+		pkt_entry->flags = 0;
 		ofi_buf_free(pkt_entry);
 		next = pkt_entry->next;
 		pkt_entry = next;
@@ -268,38 +223,38 @@ void rxr_pkt_entry_release_cloned(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_e
 
 struct rxr_pkt_entry *rxr_pkt_entry_clone(struct rxr_ep *ep,
 					  struct ofi_bufpool *pkt_pool,
-					  struct rxr_pkt_entry *src,
-					  int new_entry_type)
+					  enum rxr_pkt_entry_alloc_type alloc_type,
+					  struct rxr_pkt_entry *src)
 {
 	struct rxr_pkt_entry *root = NULL;
 	struct rxr_pkt_entry *dst;
 
 	assert(src);
-	assert(new_entry_type == RXR_PKT_ENTRY_OOO ||
-	       new_entry_type == RXR_PKT_ENTRY_UNEXP ||
-	       new_entry_type == RXR_PKT_ENTRY_READ_COPY);
+	assert(alloc_type == RXR_PKT_FROM_OOO_POOL ||
+	       alloc_type == RXR_PKT_FROM_UNEXP_POOL ||
+	       alloc_type == RXR_PKT_FROM_READ_COPY_POOL);
 
-	dst = rxr_pkt_entry_alloc(ep, pkt_pool);
+	dst = rxr_pkt_entry_alloc(ep, pkt_pool, alloc_type);
 	if (!dst)
 		return NULL;
 
-	if (new_entry_type == RXR_PKT_ENTRY_READ_COPY) {
+	if (alloc_type == RXR_PKT_FROM_READ_COPY_POOL) {
 		assert(pkt_pool == ep->rx_readcopy_pkt_pool);
 		ep->rx_readcopy_pkt_pool_used++;
 		ep->rx_readcopy_pkt_pool_max_used = MAX(ep->rx_readcopy_pkt_pool_used,
 							ep->rx_readcopy_pkt_pool_max_used);
 	}
 
-	rxr_pkt_entry_copy(ep, dst, src, new_entry_type);
+	rxr_pkt_entry_copy(ep, dst, src);
 	root = dst;
 	while (src->next) {
-		dst->next = rxr_pkt_entry_alloc(ep, pkt_pool);
+		dst->next = rxr_pkt_entry_alloc(ep, pkt_pool, alloc_type);
 		if (!dst->next) {
 			rxr_pkt_entry_release_cloned(ep, root);
 			return NULL;
 		}
 
-		rxr_pkt_entry_copy(ep, dst->next, src->next, new_entry_type);
+		rxr_pkt_entry_copy(ep, dst->next, src->next);
 		src = src->next;
 		dst = dst->next;
 	}
@@ -319,19 +274,31 @@ void rxr_pkt_entry_append(struct rxr_pkt_entry *dst,
 	dst->next = src;
 }
 
+/**
+ * @brief send a packet using lower provider
+ *
+ * @param ep[in]        rxr end point
+ * @param pkt_entry[in] packet entry to be sent
+ * @param msg[in]       information regarding that the send operation, such as
+ *                      memory buffer, remote EP address and local descriptor.
+ *                      If the shm provider is to be used. Remote EP address
+ *                      and local descriptor must be prepared for shm usage.
+ * @param flags[in]     flags to be passed on to lower provider's send.
+ */
 static inline
 ssize_t rxr_pkt_entry_sendmsg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
 			      const struct fi_msg *msg, uint64_t flags)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	size_t ret;
 
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(ep->tx_pending <= ep->max_outstanding_tx);
-
-	if (ep->tx_pending == ep->max_outstanding_tx)
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL &&
+	    ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 		return -FI_EAGAIN;
 
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
+
 	if (peer->flags & RXR_PEER_IN_BACKOFF)
 		return -FI_EAGAIN;
 
@@ -346,72 +313,82 @@ ssize_t rxr_pkt_entry_sendmsg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry
 		ret = fi_sendmsg(ep->shm_ep, msg, flags);
 	} else {
 		ret = fi_sendmsg(ep->rdm_ep, msg, flags);
-		if (OFI_LIKELY(!ret))
-			rxr_ep_inc_tx_pending(ep, peer);
 	}
 
-	return ret;
-}
-
-ssize_t rxr_pkt_entry_sendv(struct rxr_ep *ep,
-			    struct rxr_pkt_entry *pkt_entry,
-			    fi_addr_t addr, const struct iovec *iov,
-			    void **desc, size_t count, uint64_t flags)
-{
-	struct fi_msg msg;
-	struct rxr_peer *peer;
-
-	msg.msg_iov = iov;
-	msg.desc = desc;
-	msg.iov_count = count;
-	peer = rxr_ep_get_peer(ep, addr);
-	msg.addr = (peer->is_local) ? peer->shm_fiaddr : addr;
-	msg.context = pkt_entry;
-	msg.data = 0;
+	if (OFI_UNLIKELY(ret))
+		return ret;
 
-	return rxr_pkt_entry_sendmsg(ep, pkt_entry, &msg, flags);
+	rxr_ep_record_tx_op_submitted(ep, pkt_entry);
+	return 0;
 }
 
-/* rxr_pkt_start currently expects data pkt right after pkt hdr */
-ssize_t rxr_pkt_entry_send_with_flags(struct rxr_ep *ep,
-				      struct rxr_pkt_entry *pkt_entry,
-				      fi_addr_t addr, uint64_t flags)
+/**
+ * @brief Construct a fi_msg object with the information stored in pkt_entry,
+ * and send it out
+ *
+ * @param[in] ep	rxr endpoint
+ * @param[in] pkt_entry	packet entry used to construct the fi_msg object
+ * @param[in] flags	flags to be applied to lower provider's send operation
+ * @return		0 on success
+ * 			On error, a negative value corresponding to fabric errno
+ *
+ */
+ssize_t rxr_pkt_entry_send(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
+			   uint64_t flags)
 {
 	struct iovec iov;
 	void *desc;
+	struct fi_msg msg;
+	struct rdm_peer *peer;
 
-	iov.iov_base = rxr_pkt_start(pkt_entry);
-	iov.iov_len = pkt_entry->pkt_size;
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
 
-	if (rxr_ep_get_peer(ep, addr)->is_local) {
-		assert(ep->use_shm);
-		desc = NULL;
+	if (pkt_entry->send && pkt_entry->send->iov_count > 0) {
+		msg.msg_iov = pkt_entry->send->iov;
+		msg.iov_count = pkt_entry->send->iov_count;
+		msg.desc = pkt_entry->send->desc;
 	} else {
-		desc = fi_mr_desc(pkt_entry->mr);
+		iov.iov_base = rxr_pkt_start(pkt_entry);
+		iov.iov_len = pkt_entry->pkt_size;
+		desc = peer->is_local ? NULL : fi_mr_desc(pkt_entry->mr);
+		msg.msg_iov = &iov;
+		msg.iov_count = 1;
+		msg.desc = &desc;
 	}
 
-	return rxr_pkt_entry_sendv(ep, pkt_entry, addr, &iov, &desc, 1, flags);
-}
+	msg.addr = pkt_entry->addr;
+	msg.context = pkt_entry;
+	msg.data = 0;
 
-ssize_t rxr_pkt_entry_send(struct rxr_ep *ep,
-			   struct rxr_pkt_entry *pkt_entry,
-			   fi_addr_t addr)
-{
-	return rxr_pkt_entry_send_with_flags(ep, pkt_entry, addr, 0);
+	if (peer->is_local) {
+		msg.addr = peer->shm_fiaddr;
+		rxr_convert_desc_for_shm(msg.iov_count, msg.desc);
+	}
+
+	return rxr_pkt_entry_sendmsg(ep, pkt_entry, &msg, flags);
 }
 
 ssize_t rxr_pkt_entry_inject(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry,
 			     fi_addr_t addr)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
+	ssize_t ret;
 
 	/* currently only EOR packet is injected using shm ep */
 	peer = rxr_ep_get_peer(ep, addr);
+	assert(peer);
 
 	assert(ep->use_shm && peer->is_local);
-	return fi_inject(ep->shm_ep, rxr_pkt_start(pkt_entry), pkt_entry->pkt_size,
+	ret = fi_inject(ep->shm_ep, rxr_pkt_start(pkt_entry), pkt_entry->pkt_size,
 			 peer->shm_fiaddr);
+
+	if (OFI_UNLIKELY(ret))
+		return ret;
+
+	rxr_ep_record_tx_op_submitted(ep, pkt_entry);
+	return 0;
 }
 
 /*
@@ -423,6 +400,7 @@ struct rxr_rx_entry *rxr_pkt_rx_map_lookup(struct rxr_ep *ep,
 	struct rxr_pkt_rx_map *entry = NULL;
 	struct rxr_pkt_rx_key key;
 
+	memset(&key, 0, sizeof(key));
 	key.msg_id = rxr_pkt_msg_id(pkt_entry);
 	key.addr = pkt_entry->addr;
 	HASH_FIND(hh, ep->pkt_rx_map, &key, sizeof(struct rxr_pkt_rx_key), entry);
@@ -443,6 +421,7 @@ void rxr_pkt_rx_map_insert(struct rxr_ep *ep,
 		return;
 	}
 
+	memset(&entry->key, 0, sizeof(entry->key));
 	entry->key.msg_id = rxr_pkt_msg_id(pkt_entry);
 	entry->key.addr = pkt_entry->addr;
 
@@ -466,6 +445,7 @@ void rxr_pkt_rx_map_remove(struct rxr_ep *ep,
 	struct rxr_pkt_rx_map *entry;
 	struct rxr_pkt_rx_key key;
 
+	memset(&key, 0, sizeof(key));
 	key.msg_id = rxr_pkt_msg_id(pkt_entry);
 	key.addr = pkt_entry->addr;
 
diff --git a/prov/efa/src/rxr/rxr_pkt_entry.h b/prov/efa/src/rxr/rxr_pkt_entry.h
index ee58e07..25a9535 100644
--- a/prov/efa/src/rxr/rxr_pkt_entry.h
+++ b/prov/efa/src/rxr/rxr_pkt_entry.h
@@ -36,20 +36,20 @@
 
 #include <ofi_list.h>
 
-/* pkt_entry state for retransmit tracking */
-enum rxr_pkt_entry_state {
-	RXR_PKT_ENTRY_FREE = 0,
-	RXR_PKT_ENTRY_IN_USE,
-	RXR_PKT_ENTRY_RNR_RETRANSMIT,
-};
-
-/* pkt_entry types for rx pkts */
-enum rxr_pkt_entry_type {
-	RXR_PKT_ENTRY_POSTED = 1,   /* entries that are posted to the device from the RX bufpool */
-	RXR_PKT_ENTRY_UNEXP,        /* entries used to stage unexpected msgs */
-	RXR_PKT_ENTRY_OOO,	    /* entries used to stage out-of-order RTM or RTA */
-	RXR_PKT_ENTRY_USER,	    /* entries backed by user-provided msg prefix (FI_MSG_PREFIX)*/
-	RXR_PKT_ENTRY_READ_COPY,    /* entries used to stage copy by read */
+#define RXR_PKT_ENTRY_IN_USE		BIT_ULL(0)
+#define RXR_PKT_ENTRY_RNR_RETRANSMIT	BIT_ULL(1)
+#define RXR_PKT_ENTRY_LOCAL_READ	BIT_ULL(2) /* this packet entry is used as context of a local read operation */
+
+/* pkt_entry_alloc_type indicate where the packet entry is allocated from */
+enum rxr_pkt_entry_alloc_type {
+	RXR_PKT_FROM_EFA_TX_POOL = 1, /* packet is allcoated from ep->efa_tx_pkt_pool */
+	RXR_PKT_FROM_EFA_RX_POOL,     /* packet is allocated from ep->efa_rx_pkt_pool */
+	RXR_PKT_FROM_SHM_TX_POOL,     /* packet is allocated from ep->shm_tx_pkt_pool */
+	RXR_PKT_FROM_SHM_RX_POOL,     /* packet is allocated from ep->shm_rx_pkt_pool */
+	RXR_PKT_FROM_UNEXP_POOL,      /* packet is allocated from ep->rx_unexp_pkt_pool */
+	RXR_PKT_FROM_OOO_POOL,	      /* packet is allocated from ep->rx_ooo_pkt_pool */
+	RXR_PKT_FROM_USER_BUFFER,     /* packet is from user proivded buffer */
+	RXR_PKT_FROM_READ_COPY_POOL,  /* packet is allocated from ep->rx_readcopy_pkt_pool */
 };
 
 struct rxr_pkt_sendv {
@@ -64,8 +64,13 @@ struct rxr_pkt_sendv {
 	void *desc[2];
 };
 
+/* rxr_pkt_entry is used both for sending data to a peer and for receiving dat from a peer.
+ */
 struct rxr_pkt_entry {
-	/* for rx/tx_entry queued_pkts list */
+	/* entry is used for sending only.
+	 * It is either linked peer->outstanding_tx_pkts (after a packet has been successfully sent, but it get a completion),
+	 * or linked to tx_rx_entry->queued_pkts (after it encountered RNR error completion).
+	 */
 	struct dlist_entry entry;
 #if ENABLE_DEBUG
 	/* for tx/rx debug list or posted buf list */
@@ -75,9 +80,27 @@ struct rxr_pkt_entry {
 	size_t pkt_size;
 
 	struct fid_mr *mr;
+	/* `addr` is used for both sending data and receiving data.
+	 *
+	 * When sending a packet, `addr` will be provided by application and it cannot be FI_ADDR_NOTAVAIL.
+	 * However, after a packet is sent, application can remove a peer by calling fi_av_remove().
+	 * When removing the peering, `addr` will be set to FI_ADDR_NOTAVAIL. Later, when device report
+	 * completion for such a TX packet, the TX completion will be ignored.
+	 *
+	 * When receiving a packet, lower device will set `addr`. If the sender's address is not in
+	 * address vector (AV), `lower device will set `addr` to FI_ADDR_NOTAVAIL. This can happen in
+	 * two scenarios:
+	 *
+	 * 1. there has been no prior communication with the peer. In this case, the packet should have
+	 *    peer's raw address in the header, and progress engien will insert the raw address into
+	 *    addres vector, and update `addr`.
+	 *
+	 * 2. this packet is from a peer whose address has been removed from AV. In this case, the
+	 *    recived packet will be ignored because all resources associated with peer has been released.
+	 */
 	fi_addr_t addr;
-	enum rxr_pkt_entry_type type;
-	enum rxr_pkt_entry_state state;
+	enum rxr_pkt_entry_alloc_type alloc_type; /* where the memory of this packet entry reside */
+	uint32_t flags;
 
 	/*
 	 * next is used on receiving end.
@@ -120,7 +143,8 @@ struct rxr_pkt_entry *rxr_pkt_entry_init_prefix(struct rxr_ep *ep,
 						struct ofi_bufpool *pkt_pool);
 
 struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
-					  struct ofi_bufpool *pkt_pool);
+					  struct ofi_bufpool *pkt_pool,
+					  enum rxr_pkt_entry_alloc_type alloc_type);
 
 void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
 			      struct rxr_pkt_entry *pkt_entry);
@@ -133,24 +157,14 @@ void rxr_pkt_entry_append(struct rxr_pkt_entry *dst,
 
 struct rxr_pkt_entry *rxr_pkt_entry_clone(struct rxr_ep *ep,
 					  struct ofi_bufpool *pkt_pool,
-					  struct rxr_pkt_entry *src,
-					  int new_entry_type);
+					  enum rxr_pkt_entry_alloc_type alloc_type,
+					  struct rxr_pkt_entry *src);
 
 struct rxr_pkt_entry *rxr_pkt_get_unexp(struct rxr_ep *ep,
 					struct rxr_pkt_entry **pkt_entry_ptr);
 
-ssize_t rxr_pkt_entry_send_with_flags(struct rxr_ep *ep,
-				      struct rxr_pkt_entry *pkt_entry,
-				      fi_addr_t addr, uint64_t flags);
-
-ssize_t rxr_pkt_entry_sendv(struct rxr_ep *ep,
-			    struct rxr_pkt_entry *pkt_entry,
-			    fi_addr_t addr, const struct iovec *iov,
-			    void **desc, size_t count, uint64_t flags);
-
 ssize_t rxr_pkt_entry_send(struct rxr_ep *ep,
-			   struct rxr_pkt_entry *pkt_entry,
-			   fi_addr_t addr);
+			   struct rxr_pkt_entry *pkt_entry, uint64_t flags);
 
 ssize_t rxr_pkt_entry_inject(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry,
diff --git a/prov/efa/src/rxr/rxr_pkt_type.h b/prov/efa/src/rxr/rxr_pkt_type.h
index 2555f10..5ad4736 100644
--- a/prov/efa/src/rxr/rxr_pkt_type.h
+++ b/prov/efa/src/rxr/rxr_pkt_type.h
@@ -34,90 +34,7 @@
 #ifndef _RXR_PKT_TYPE_H
 #define _RXR_PKT_TYPE_H
 
-/* This header file contain the ID of all RxR packet types, and
- * the necessary data structures and functions for each packet type
- *
- * RxR packet types can be classified into 3 categories:
- *     data packet, control packet and context packet
- *
- * For each packet type, the following items are needed:
- *
- *   First, each packet type need to define a struct for its header,
- *       and the header must be start with ```struct rxr_base_hdr```.
- *
- *   Second, each control packet type need to define an init()
- *       function and a handle_sent() function. These functions
- *       are called by rxr_pkt_post_ctrl_or_queue().
- *
- *   Finally, each packet type (except context packet) need to
- *     define a handle_recv() functions which is called by
- *     rxr_pkt_handle_recv_completion().
- */
-
-/* ID of each packet type. Changing ID would break inter
- * operability thus is strictly prohibited.
- */
-
-#define RXR_RETIRED_RTS_PKT	1
-#define RXR_RETIRED_CONNACK_PKT	2
-#define RXR_CTS_PKT		3
-#define RXR_DATA_PKT		4
-#define RXR_READRSP_PKT		5
-#define RXR_RMA_CONTEXT_PKT	6
-#define RXR_EOR_PKT		7
-#define RXR_ATOMRSP_PKT         8
-#define RXR_HANDSHAKE_PKT	9
-#define RXR_RECEIPT_PKT 10
-
-#define RXR_REQ_PKT_BEGIN		64
-#define RXR_BASELINE_REQ_PKT_BEGIN	64
-#define RXR_EAGER_MSGRTM_PKT		64
-#define RXR_EAGER_TAGRTM_PKT		65
-#define RXR_MEDIUM_MSGRTM_PKT		66
-#define RXR_MEDIUM_TAGRTM_PKT		67
-#define RXR_LONG_MSGRTM_PKT		68
-#define RXR_LONG_TAGRTM_PKT		69
-#define RXR_EAGER_RTW_PKT		70
-#define RXR_LONG_RTW_PKT		71
-#define RXR_SHORT_RTR_PKT		72
-#define RXR_LONG_RTR_PKT		73
-#define RXR_WRITE_RTA_PKT		74
-#define RXR_FETCH_RTA_PKT		75
-#define RXR_COMPARE_RTA_PKT		76
-#define RXR_BASELINE_REQ_PKT_END	77
-
-#define RXR_EXTRA_REQ_PKT_BEGIN		128
-#define RXR_READ_MSGRTM_PKT		128
-#define RXR_READ_TAGRTM_PKT		129
-#define RXR_READ_RTW_PKT		130
-#define RXR_READ_RTR_PKT		131
-
-#define RXR_DC_REQ_PKT_BEGIN		132
-#define RXR_DC_EAGER_MSGRTM_PKT 	133
-#define RXR_DC_EAGER_TAGRTM_PKT 	134
-#define RXR_DC_MEDIUM_MSGRTM_PKT 	135
-#define RXR_DC_MEDIUM_TAGRTM_PKT 	136
-#define RXR_DC_LONG_MSGRTM_PKT  	137
-#define RXR_DC_LONG_TAGRTM_PKT  	138
-#define RXR_DC_EAGER_RTW_PKT    	139
-#define RXR_DC_LONG_RTW_PKT     	140
-#define RXR_DC_WRITE_RTA_PKT    	141
-#define RXR_DC_REQ_PKT_END		142
-#define RXR_EXTRA_REQ_PKT_END   	142
-
-/*
- *  Packet fields common to all rxr packets. The other packet headers below must
- *  be changed if this is updated.
- */
-struct rxr_base_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_base_hdr check");
-#endif
+#include "rdm_proto_v4.h"
 
 static inline struct rxr_base_hdr *rxr_get_base_hdr(void *pkt)
 {
@@ -125,75 +42,52 @@ static inline struct rxr_base_hdr *rxr_get_base_hdr(void *pkt)
 }
 
 struct rxr_ep;
-struct rxr_peer;
+struct rdm_peer;
 struct rxr_tx_entry;
 struct rxr_rx_entry;
 struct rxr_read_entry;
 
-/*
- *  HANDSHAKE packet header and functions
- *  implementation of the functions are in rxr_pkt_type_misc.c
- */
-struct rxr_handshake_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t maxproto;
-	uint64_t features[0];
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_handshake_hdr) == 8, "rxr_handshake_hdr check");
-#endif
-
+/* HANDSHAKE packet related functions */
 static inline
 struct rxr_handshake_hdr *rxr_get_handshake_hdr(void *pkt)
 {
 	return (struct rxr_handshake_hdr *)pkt;
 }
 
+static inline
+struct rxr_handshake_opt_connid_hdr *rxr_get_handshake_opt_connid_hdr(void *pkt)
+{
+	struct rxr_handshake_hdr *handshake_hdr;
+	size_t base_hdr_size;
+
+	handshake_hdr = (struct rxr_handshake_hdr *)pkt;
+	assert(handshake_hdr->type == RXR_HANDSHAKE_PKT);
+	assert(handshake_hdr->flags & RXR_PKT_CONNID_HDR);
+	base_hdr_size = sizeof(struct rxr_handshake_hdr) +
+			(handshake_hdr->nextra_p3 - 3) * sizeof(uint64_t);
+	return (struct rxr_handshake_opt_connid_hdr *)((char *)pkt + base_hdr_size);
+}
+
 ssize_t rxr_pkt_init_handshake(struct rxr_ep *ep,
 			       struct rxr_pkt_entry *pkt_entry,
 			       fi_addr_t addr);
 
-void rxr_pkt_post_handshake(struct rxr_ep *ep,
-			    struct rxr_peer *peer,
-			    fi_addr_t addr);
+ssize_t rxr_pkt_post_handshake(struct rxr_ep *ep, struct rdm_peer *peer);
+
+void rxr_pkt_post_handshake_or_queue(struct rxr_ep *ep,
+				     struct rdm_peer *peer);
 
 void rxr_pkt_handle_handshake_recv(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry);
-/*
- *  CTS packet data structures and functions.
- *  Definition of the functions is in rxr_pkt_type_misc.c
- */
-struct rxr_cts_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint8_t pad[4];
-	/* TODO: need to add msg_id -> tx_id/rx_id mapping */
-	uint32_t tx_id;
-	uint32_t rx_id;
-	uint64_t window;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_cts_hdr) == 24, "rxr_cts_hdr check");
-#endif
-
-/* this flag is to indicated the CTS is the response of a RTR packet */
-#define RXR_CTS_READ_REQ		BIT_ULL(7)
-#define RXR_CTS_HDR_SIZE		(sizeof(struct rxr_cts_hdr))
 
+/* CTS packet related functions */
 static inline
 struct rxr_cts_hdr *rxr_get_cts_hdr(void *pkt)
 {
 	return (struct rxr_cts_hdr *)pkt;
 }
 
-void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
+void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rdm_peer *peer,
 				     uint64_t size, int request,
 				     int *window, int *credits);
 
@@ -207,45 +101,18 @@ void rxr_pkt_handle_cts_sent(struct rxr_ep *ep,
 void rxr_pkt_handle_cts_recv(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry);
 
-/*
- *  DATA packet data structures and functions
- *  Definition of the functions is in rxr_pkt_data.c
- */
-struct rxr_data_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	/* TODO: need to add msg_id -> tx_id/rx_id mapping */
-	uint32_t rx_id;
-	uint64_t seg_size;
-	uint64_t seg_offset;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_data_hdr) == 24, "rxr_data_hdr check");
-#endif
-
-#define RXR_DATA_HDR_SIZE		(sizeof(struct rxr_data_hdr))
-
-struct rxr_data_pkt {
-	struct rxr_data_hdr hdr;
-	char data[];
-};
-
 static inline
-struct rxr_data_pkt *rxr_get_data_pkt(void *pkt)
+struct rxr_data_hdr *rxr_get_data_hdr(void *pkt)
 {
-	return (struct rxr_data_pkt *)pkt;
+	return (struct rxr_data_hdr *)pkt;
 }
 
-ssize_t rxr_pkt_send_data(struct rxr_ep *ep,
-			  struct rxr_tx_entry *tx_entry,
-			  struct rxr_pkt_entry *pkt_entry);
+int rxr_pkt_init_data(struct rxr_ep *ep,
+		      struct rxr_tx_entry *tx_entry,
+		      struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_send_data_desc(struct rxr_ep *ep,
-			       struct rxr_tx_entry *tx_entry,
-			       struct rxr_pkt_entry *pkt_entry);
+void rxr_pkt_handle_data_sent(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt_entry);
 
 void rxr_pkt_proc_data(struct rxr_ep *ep,
 		       struct rxr_rx_entry *rx_entry,
@@ -256,41 +123,15 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 void rxr_pkt_handle_data_send_completion(struct rxr_ep *ep,
 					 struct rxr_pkt_entry *pkt_entry);
 
-
 void rxr_pkt_handle_data_recv(struct rxr_ep *ep,
 			      struct rxr_pkt_entry *pkt_entry);
 
-/*
- *  READRSP packet data structures and functions
- *  The definition of functions are in rxr_pkt_type_misc.c
- */
-struct rxr_readrsp_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint8_t pad[4];
-	uint32_t rx_id;
-	uint32_t tx_id;
-	uint64_t seg_size;
-};
-
+/* READRSP packet related functions */
 static inline struct rxr_readrsp_hdr *rxr_get_readrsp_hdr(void *pkt)
 {
 	return (struct rxr_readrsp_hdr *)pkt;
 }
 
-#define RXR_READRSP_HDR_SIZE	(sizeof(struct rxr_readrsp_hdr))
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_readrsp_hdr) == sizeof(struct rxr_data_hdr), "rxr_readrsp_hdr check");
-#endif
-
-struct rxr_readrsp_pkt {
-	struct rxr_readrsp_hdr hdr;
-	char data[];
-};
-
 int rxr_pkt_init_readrsp(struct rxr_ep *ep,
 			 struct rxr_tx_entry *tx_entry,
 			 struct rxr_pkt_entry *pkt_entry);
@@ -336,24 +177,7 @@ void rxr_pkt_init_read_context(struct rxr_ep *rxr_ep,
 void rxr_pkt_handle_rma_completion(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry);
 
-/*
- *  EOR packet, used to acknowledge the sender that large message
- *  copy has been finished.
- *  Implementaion of the functions are in rxr_pkt_misc.c
- */
-struct rxr_eor_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t tx_id;
-	uint32_t rx_id;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_eor_hdr) == 12, "rxr_eor_hdr check");
-#endif
-
+/* EOR packet related functions */
 static inline
 struct rxr_eor_hdr *rxr_get_eor_hdr(void *pkt)
 {
@@ -373,52 +197,28 @@ void rxr_pkt_handle_eor_send_completion(struct rxr_ep *ep,
 void rxr_pkt_handle_eor_recv(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry);
 
-/* atomrsp types */
-struct rxr_atomrsp_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint8_t pad[4];
-	uint32_t rx_id;
-	uint32_t tx_id;
-	uint64_t seg_size;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_atomrsp_hdr) == 24, "rxr_atomrsp_hdr check");
-#endif
-
-#define RXR_ATOMRSP_HDR_SIZE	(sizeof(struct rxr_atomrsp_hdr))
-
-struct rxr_atomrsp_pkt {
-	struct rxr_atomrsp_hdr hdr;
-	char data[];
-};
-
+/* ATOMRSP packet related functions */
 static inline struct rxr_atomrsp_hdr *rxr_get_atomrsp_hdr(void *pkt)
 {
 	return (struct rxr_atomrsp_hdr *)pkt;
 }
 
-/* receipt packet headers */
-struct rxr_receipt_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t tx_id;
-	uint32_t msg_id;
-	int32_t padding;
-};
+int rxr_pkt_init_atomrsp(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+			 struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_atomrsp_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_atomrsp_send_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
+void rxr_pkt_handle_atomrsp_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
+
+/* RECEIPT packet related functions */
 static inline
 struct rxr_receipt_hdr *rxr_get_receipt_hdr(void *pkt)
 {
 	return (struct rxr_receipt_hdr *)pkt;
 }
 
-/* receipt packet functions: init, handle_sent, handle_send_completion, recv*/
 int rxr_pkt_init_receipt(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 			 struct rxr_pkt_entry *pkt_entry);
 
@@ -431,16 +231,6 @@ void rxr_pkt_handle_receipt_send_completion(struct rxr_ep *ep,
 void rxr_pkt_handle_receipt_recv(struct rxr_ep *ep,
 				 struct rxr_pkt_entry *pkt_entry);
 
-/* atomrsp functions: init, handle_sent, handle_send_completion, recv */
-int rxr_pkt_init_atomrsp(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-			 struct rxr_pkt_entry *pkt_entry);
-
-void rxr_pkt_handle_atomrsp_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
-
-void rxr_pkt_handle_atomrsp_send_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
-
-void rxr_pkt_handle_atomrsp_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
-
 #endif
 
 #include "rxr_pkt_type_req.h"
diff --git a/prov/efa/src/rxr/rxr_pkt_type_base.c b/prov/efa/src/rxr/rxr_pkt_type_base.c
new file mode 100644
index 0000000..340039e
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_type_base.c
@@ -0,0 +1,269 @@
+/*
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "efa.h"
+#include "rxr_read.h"
+#include "rxr_pkt_cmd.h"
+
+/**
+ * @brief return the optional connid header pointer in a packet
+ *
+ * @param[in]	pkt_entry	an packet entry
+ * @return	If the input has the optional connid header, return the pointer to connid header
+ * 		Otherwise, return NULL
+ */
+uint32_t *rxr_pkt_connid_ptr(struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_base_hdr *base_hdr;
+
+	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
+
+	if (base_hdr->type >= RXR_REQ_PKT_BEGIN)
+		return rxr_pkt_req_connid_ptr(pkt_entry);
+
+	if (!(base_hdr->flags & RXR_PKT_CONNID_HDR))
+		return NULL;
+
+	switch (base_hdr->type) {
+	case RXR_CTS_PKT:
+		return &(rxr_get_cts_hdr(pkt_entry->pkt)->connid);
+
+	case RXR_RECEIPT_PKT:
+		return &(rxr_get_receipt_hdr(pkt_entry->pkt)->connid);
+
+	case RXR_DATA_PKT:
+		return &(rxr_get_data_hdr(pkt_entry->pkt)->connid_hdr->connid);
+
+	case RXR_READRSP_PKT:
+		return &(rxr_get_readrsp_hdr(pkt_entry->pkt)->connid);
+
+	case RXR_ATOMRSP_PKT:
+		return &(rxr_get_atomrsp_hdr(pkt_entry->pkt)->connid);
+
+	case RXR_EOR_PKT:
+		return &rxr_get_eor_hdr(pkt_entry->pkt)->connid;
+
+	case RXR_HANDSHAKE_PKT:
+		return &(rxr_get_handshake_opt_connid_hdr(pkt_entry->pkt)->connid);
+
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "unknown packet type: %d\n", base_hdr->type);
+		assert(0 && "Unknown packet type");
+	}
+
+	return NULL;
+}
+
+/**
+ * @brief set up data in a packet entry using tx_entry information, such that the packet is ready to be sent.
+ *        Depend on the tx_entry, this function can either copy data to packet entry, or point
+ *        pkt_entry->iov to tx_entry->iov.
+ *        It requires the packet header to be set.
+ *
+ * @param[in]		ep		end point.
+ * @param[in,out]	pkt_entry	packet entry. Header must have been set when the function is called
+ * @param[in]		hdr_size	packet header size.
+ * @param[in]		tx_entry	This function will use iov, iov_count and desc of tx_entry
+ * @param[in]		data_offset	offset of the data to be set up. In reference to tx_entry->total_len.
+ * @param[in]		data_size	length of the data to be set up. In reference to tx_entry->total_len.
+ * @return		no return
+ */
+void rxr_pkt_init_data_from_tx_entry(struct rxr_ep *ep,
+				     struct rxr_pkt_entry *pkt_entry,
+				     size_t hdr_size,
+				     struct rxr_tx_entry *tx_entry,
+				     size_t data_offset,
+				     size_t data_size)
+{
+	int tx_iov_index;
+	char *data;
+	size_t tx_iov_offset, copied;
+	struct efa_mr *desc;
+
+	assert(hdr_size > 0);
+
+	pkt_entry->x_entry = tx_entry;
+	/* pkt_sendv_pool's size equal efa_tx_pkt_pool size +
+	 * shm_tx_pkt_pool size. As long as we have a pkt_entry,
+	 * pkt_entry->send should be allocated successfully
+	 */
+	pkt_entry->send = ofi_buf_alloc(ep->pkt_sendv_pool);
+	if (!pkt_entry->send)
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL, "allocate pkt_entry->send failed\n");
+	assert(pkt_entry->send);
+
+	if (data_size == 0) {
+		pkt_entry->send->iov_count = 0;
+		pkt_entry->pkt_size = hdr_size;
+		return;
+	}
+
+	rxr_locate_iov_pos(tx_entry->iov, tx_entry->iov_count, data_offset,
+			   &tx_iov_index, &tx_iov_offset);
+	desc = tx_entry->desc[0];
+	assert(tx_iov_index < tx_entry->iov_count);
+	assert(tx_iov_offset < tx_entry->iov[tx_iov_index].iov_len);
+
+	/*
+	 * Copy can be avoid if the following 2 conditions are true:
+	 * 1. user provided memory descriptor, or message is sent via shm provider
+	 *    (which does not require a memory descriptor)
+	 * 2. data to be send is in 1 iov, because device only support 2 iov, and we use
+	 *    1st iov for header.
+	 */
+	if ((!pkt_entry->mr || tx_entry->desc[tx_iov_index]) &&
+	    (tx_iov_offset + data_size <= tx_entry->iov[tx_iov_index].iov_len)) {
+
+		assert(ep->core_iov_limit >= 2);
+		pkt_entry->send->iov[0].iov_base = pkt_entry->pkt;
+		pkt_entry->send->iov[0].iov_len = hdr_size;
+		pkt_entry->send->desc[0] = pkt_entry->mr ? fi_mr_desc(pkt_entry->mr) : NULL;
+
+		pkt_entry->send->iov[1].iov_base = (char *)tx_entry->iov[tx_iov_index].iov_base + tx_iov_offset;
+		pkt_entry->send->iov[1].iov_len = data_size;
+		pkt_entry->send->desc[1] = tx_entry->desc[tx_iov_index];
+		pkt_entry->send->iov_count = 2;
+		pkt_entry->pkt_size = hdr_size + data_size;
+		return;
+	}
+
+	data = pkt_entry->pkt + hdr_size;
+	copied = ofi_copy_from_hmem_iov(data,
+					data_size,
+					desc ? desc->peer.iface : FI_HMEM_SYSTEM,
+					desc ? desc->peer.device.reserved : 0,
+					tx_entry->iov,
+					tx_entry->iov_count,
+					data_offset);
+	assert(copied == data_size);
+	pkt_entry->send->iov_count = 0;
+	pkt_entry->pkt_size = hdr_size + copied;
+}
+
+/* @brief return the data size in a packet entry
+ *
+ * @param[in]	pkt_entry		packet entry
+ * @return	the data size in the packet entry.
+ * 		if the packet entry does not contain data,
+ * 		return 0.
+ */
+size_t rxr_pkt_data_size(struct rxr_pkt_entry *pkt_entry)
+{
+	int pkt_type;
+
+	assert(pkt_entry);
+	pkt_type = rxr_get_base_hdr(pkt_entry->pkt)->type;
+
+	if (pkt_type == RXR_DATA_PKT)
+		return rxr_get_data_hdr(pkt_entry->pkt)->seg_length;
+
+	if (pkt_type == RXR_READRSP_PKT)
+		return rxr_get_readrsp_hdr(pkt_entry->pkt)->seg_length;
+
+	if (pkt_type >= RXR_REQ_PKT_BEGIN) {
+		assert(pkt_type == RXR_EAGER_MSGRTM_PKT || pkt_type == RXR_EAGER_TAGRTM_PKT ||
+		       pkt_type == RXR_MEDIUM_MSGRTM_PKT || pkt_type == RXR_MEDIUM_TAGRTM_PKT ||
+		       pkt_type == RXR_LONGCTS_MSGRTM_PKT || pkt_type == RXR_LONGCTS_TAGRTM_PKT ||
+		       pkt_type == RXR_EAGER_RTW_PKT ||
+		       pkt_type == RXR_LONGCTS_RTW_PKT ||
+		       pkt_type == RXR_DC_EAGER_MSGRTM_PKT ||
+		       pkt_type == RXR_DC_EAGER_TAGRTM_PKT ||
+		       pkt_type == RXR_DC_MEDIUM_MSGRTM_PKT ||
+		       pkt_type == RXR_DC_MEDIUM_TAGRTM_PKT ||
+		       pkt_type == RXR_DC_LONGCTS_MSGRTM_PKT ||
+		       pkt_type == RXR_DC_LONGCTS_TAGRTM_PKT ||
+		       pkt_type == RXR_DC_EAGER_RTW_PKT ||
+		       pkt_type == RXR_DC_LONGCTS_RTW_PKT);
+
+		return pkt_entry->pkt_size - rxr_pkt_req_hdr_size(pkt_entry);
+	}
+
+	/* other packet type does not contain data, thus return 0
+	 */
+	return 0;
+}
+
+/**
+ * @brief copy data to receive buffer and update counter in rx_entry.
+ *
+ * If receiving buffer is on GPU memory, it will post a local
+ * read request. Otherwise it will copy data directly, and call
+ * rxr_pkt_handle_data_copied().
+ *
+ * @param[in]		ep		endpoint
+ * @param[in,out]	rx_entry	rx_entry contains information of the receive
+ *                      	        op. This function uses receive buffer in it.
+ * @param[in]		data_offset	the offset of the data in the packet in respect
+ *					of the receiving buffer.
+ * @param[in]		pkt_entry	the packet entry that contains data
+ * @param[in]		data		the pointer pointing to the beginning of data
+ * @param[in]		data_size	the length of data
+ * @return		On success, return 0
+ * 			On failure, return libfabric error code
+ */
+ssize_t rxr_pkt_copy_data_to_rx_entry(struct rxr_ep *ep,
+				      struct rxr_rx_entry *rx_entry,
+				      size_t data_offset,
+				      struct rxr_pkt_entry *pkt_entry,
+				      char *data, size_t data_size)
+{
+	ssize_t err, bytes_copied;
+
+	pkt_entry->x_entry = rx_entry;
+
+	if (data_size > 0 && efa_ep_is_cuda_mr(rx_entry->desc[0])) {
+		err = rxr_read_post_local_read_or_queue(ep, rx_entry, data_offset,
+							pkt_entry, data, data_size);
+		if (err)
+			FI_WARN(&rxr_prov, FI_LOG_CQ, "cannot post read to copy data\n");
+
+		return err;
+	}
+
+	if (OFI_LIKELY(!(rx_entry->rxr_flags & RXR_RECV_CANCEL)) &&
+	    rx_entry->cq_entry.len > data_offset && data_size > 0) {
+		bytes_copied = ofi_copy_to_iov(rx_entry->iov,
+					       rx_entry->iov_count,
+					       data_offset + ep->msg_prefix_size,
+					       data,
+					       data_size);
+		if (bytes_copied != MIN(data_size, rx_entry->cq_entry.len - data_offset)) {
+			FI_WARN(&rxr_prov, FI_LOG_CQ, "wrong size! bytes_copied: %ld\n",
+				bytes_copied);
+			return -FI_EIO;
+		}
+	}
+
+	rxr_pkt_handle_data_copied(ep, pkt_entry, data_size);
+	return 0;
+}
diff --git a/prov/efa/src/rxr/rxr_pkt_type_base.h b/prov/efa/src/rxr/rxr_pkt_type_base.h
new file mode 100644
index 0000000..d7388d7
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_type_base.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _RXR_PKT_TYPE_BASE_H
+#define _RXR_PKT_TYPE_BASE_H
+
+#include "rxr.h"
+
+uint32_t *rxr_pkt_connid_ptr(struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_init_data_from_tx_entry(struct rxr_ep *ep,
+				     struct rxr_pkt_entry *pkt_entry,
+				     size_t hdr_size,
+				     struct rxr_tx_entry *tx_entry,
+				     size_t data_offset, size_t data_size);
+
+ssize_t rxr_pkt_copy_data_to_rx_entry(struct rxr_ep *ep,
+				      struct rxr_rx_entry *rx_entry,
+				      size_t data_offset,
+				      struct rxr_pkt_entry *pkt_entry,
+				      char *data, size_t data_size);
+
+size_t rxr_pkt_data_size(struct rxr_pkt_entry *pkt_entry);
+
+#endif
diff --git a/prov/efa/src/rxr/rxr_pkt_type_data.c b/prov/efa/src/rxr/rxr_pkt_type_data.c
index 732836c..b4c1c12 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_data.c
+++ b/prov/efa/src/rxr/rxr_pkt_type_data.c
@@ -35,194 +35,59 @@
 #include "rxr.h"
 #include "rxr_msg.h"
 #include "rxr_pkt_cmd.h"
+#include "rxr_pkt_type_base.h"
 
-/*
- * This function contains data packet related functions
- * Data packet is used by long message protocol.
- */
-
-/*
- * Functions to send data packet, including
- */
-
-ssize_t rxr_pkt_send_data(struct rxr_ep *ep,
-			  struct rxr_tx_entry *tx_entry,
-			  struct rxr_pkt_entry *pkt_entry)
+int rxr_pkt_init_data(struct rxr_ep *ep,
+		      struct rxr_tx_entry *tx_entry,
+		      struct rxr_pkt_entry *pkt_entry)
 {
-	uint64_t payload_size, copied_size;
-	struct rxr_data_pkt *data_pkt;
-	struct efa_mr *desc;
-
-	pkt_entry->x_entry = (void *)tx_entry;
-	pkt_entry->addr = tx_entry->addr;
-	desc = tx_entry->desc[0];
-
-	payload_size = MIN(tx_entry->total_len - tx_entry->bytes_sent,
-			   ep->max_data_payload_size);
-	payload_size = MIN(payload_size, tx_entry->window);
-
-	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
-	data_pkt->hdr.seg_size = payload_size;
+	struct rxr_data_hdr *data_hdr;
+	struct rdm_peer *peer;
+	size_t hdr_size;
+
+	data_hdr = rxr_get_data_hdr(pkt_entry->pkt);
+	data_hdr->type = RXR_DATA_PKT;
+	data_hdr->version = RXR_PROTOCOL_VERSION;
+	data_hdr->flags = 0;
+	data_hdr->recv_id = tx_entry->rx_id;
+
+	hdr_size = sizeof(struct rxr_data_hdr);
+	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(peer);
+	if (rxr_peer_need_connid(peer)) {
+		data_hdr->flags |= RXR_PKT_CONNID_HDR;
+		data_hdr->connid_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
+		hdr_size += sizeof(struct rxr_data_opt_connid_hdr);
+	}
 
-	copied_size = ofi_copy_from_hmem_iov(data_pkt->data,
-					     payload_size,
-					     desc ? desc->peer.iface : FI_HMEM_SYSTEM,
-					     desc ? desc->peer.device.reserved : 0,
-					     tx_entry->iov,
-					     tx_entry->iov_count,
-					     tx_entry->bytes_sent);
-	assert(copied_size == payload_size);
+	/*
+	 * Data packets are sent in order so using bytes_sent is okay here.
+	 */
+	data_hdr->seg_offset = tx_entry->bytes_sent;
+	data_hdr->seg_length = MIN(tx_entry->total_len - tx_entry->bytes_sent,
+				   ep->max_data_payload_size);
+	data_hdr->seg_length = MIN(data_hdr->seg_length, tx_entry->window);
+	rxr_pkt_init_data_from_tx_entry(ep, pkt_entry, hdr_size,
+					tx_entry, tx_entry->bytes_sent, data_hdr->seg_length);
 
-	pkt_entry->pkt_size = copied_size + sizeof(struct rxr_data_hdr);
+	pkt_entry->x_entry = (void *)tx_entry;
 	pkt_entry->addr = tx_entry->addr;
 
-	return rxr_pkt_entry_send_with_flags(ep, pkt_entry, pkt_entry->addr,
-					     tx_entry->send_flags);
-}
-
-/*
- * Copies all consecutive small iov's into one buffer. If the function reaches
- * an iov greater than the max memcpy size, it will end, only copying up to
- * that iov.
- */
-static size_t rxr_copy_from_iov(void *buf, uint64_t remaining_len,
-				struct rxr_tx_entry *tx_entry)
-{
-	struct iovec *tx_iov = tx_entry->iov;
-	uint64_t done = 0, len;
-
-	while (tx_entry->iov_index < tx_entry->iov_count &&
-	       done < remaining_len) {
-		len = tx_iov[tx_entry->iov_index].iov_len;
-		if (tx_entry->mr[tx_entry->iov_index])
-			break;
-
-		len -= tx_entry->iov_offset;
-
-		/*
-		 * If the amount to be written surpasses the remaining length,
-		 * copy up to the remaining length and return, else copy the
-		 * entire iov and continue.
-		 */
-		if (done + len > remaining_len) {
-			len = remaining_len - done;
-			memcpy((char *)buf + done,
-			       (char *)tx_iov[tx_entry->iov_index].iov_base +
-			       tx_entry->iov_offset, len);
-			tx_entry->iov_offset += len;
-			done += len;
-			break;
-		}
-		memcpy((char *)buf + done,
-		       (char *)tx_iov[tx_entry->iov_index].iov_base +
-		       tx_entry->iov_offset, len);
-		tx_entry->iov_index++;
-		tx_entry->iov_offset = 0;
-		done += len;
-	}
-	return done;
+	return 0;
 }
 
-ssize_t rxr_pkt_send_data_desc(struct rxr_ep *ep,
-			       struct rxr_tx_entry *tx_entry,
-			       struct rxr_pkt_entry *pkt_entry)
+void rxr_pkt_handle_data_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_data_pkt *data_pkt;
-	/* The user's iov */
-	struct iovec *tx_iov = tx_entry->iov;
-	/* The constructed iov to be passed to sendv
-	 * and corresponding fid_mrs
-	 */
-	struct iovec iov[ep->core_iov_limit];
-	void *desc[ep->core_iov_limit];
-	/* Constructed iov's total size */
-	uint64_t payload_size = 0;
-	/* pkt_entry offset to write data into */
-	uint64_t pkt_used = 0;
-	uint64_t orig_iov_index;
-	uint64_t orig_iov_offset;
-	/* Remaining size that can fit in the constructed iov */
-	uint64_t remaining_len = MIN(tx_entry->window,
-				     ep->max_data_payload_size);
-	/* The constructed iov's index */
-	size_t i = 0;
-	size_t len = 0;
-
-	ssize_t ret;
-
-	orig_iov_index = tx_entry->iov_index;
-	orig_iov_offset = tx_entry->iov_offset;
-
-	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
-	/* Assign packet header in constructed iov */
-	iov[i].iov_base = rxr_pkt_start(pkt_entry);
-	iov[i].iov_len = sizeof(struct rxr_data_hdr);
-	desc[i] = fi_mr_desc(pkt_entry->mr);
-	i++;
-
-	/*
-	 * Loops until payload size is at max, all user iovs are sent, the
-	 * constructed iov count is greater than the core iov limit, or the tx
-	 * entry window is exhausted.  Each iteration fills one entry of the
-	 * iov to be sent.
-	 */
-	while (tx_entry->iov_index < tx_entry->iov_count &&
-	       remaining_len > 0 && i < ep->core_iov_limit) {
-		if (tx_entry->desc[tx_entry->iov_index]) {
-			iov[i].iov_base =
-				(char *)tx_iov[tx_entry->iov_index].iov_base +
-				tx_entry->iov_offset;
-			desc[i] = tx_entry->desc[tx_entry->iov_index];
-
-			len = tx_iov[tx_entry->iov_index].iov_len
-			      - tx_entry->iov_offset;
-			if (len > remaining_len) {
-				len = remaining_len;
-				tx_entry->iov_offset += len;
-			} else {
-				tx_entry->iov_index++;
-				tx_entry->iov_offset = 0;
-			}
-			iov[i].iov_len = len;
-		} else {
-			/* It should be noted for cuda buffer, caller will always
-			 * provide desc, and will not enter this branch.
-			 *
-			 * Copies any consecutive small iov's, returning size
-			 * written while updating iov index and offset
-			 */
-
-			len = rxr_copy_from_iov((char *)data_pkt->data +
-						 pkt_used,
-						 remaining_len,
-						 tx_entry);
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_data_hdr *data_hdr;
 
-			iov[i].iov_base = (char *)data_pkt->data + pkt_used;
-			iov[i].iov_len = len;
-			desc[i] = fi_mr_desc(pkt_entry->mr);
-			pkt_used += len;
-		}
-		payload_size += len;
-		remaining_len -= len;
-		i++;
-	}
-	data_pkt->hdr.seg_size = (uint16_t)payload_size;
-	pkt_entry->pkt_size = payload_size + RXR_DATA_HDR_SIZE;
-	pkt_entry->x_entry = tx_entry;
-	pkt_entry->addr = tx_entry->addr;
+	data_hdr = rxr_get_data_hdr(pkt_entry->pkt);
+	assert(data_hdr->seg_length > 0);
 
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "Sending an iov count, %zu with payload size: %lu.\n",
-	       i, payload_size);
-	ret = rxr_pkt_entry_sendv(ep, pkt_entry, tx_entry->addr,
-				  (const struct iovec *)iov,
-				  desc, i, tx_entry->send_flags);
-	if (OFI_UNLIKELY(ret)) {
-		/* Reset tx_entry iov pointer on send failure. */
-		tx_entry->iov_index = orig_iov_index;
-		tx_entry->iov_offset = orig_iov_offset;
-	}
-	return ret;
+	tx_entry = pkt_entry->x_entry;
+	tx_entry->bytes_sent += data_hdr->seg_length;
+	tx_entry->window -= data_hdr->seg_length;
+	assert(tx_entry->window >= 0);
 }
 
 void rxr_pkt_handle_data_send_completion(struct rxr_ep *ep,
@@ -232,7 +97,7 @@ void rxr_pkt_handle_data_send_completion(struct rxr_ep *ep,
 
 	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
 	tx_entry->bytes_acked +=
-		rxr_get_data_pkt(pkt_entry->pkt)->hdr.seg_size;
+		rxr_get_data_hdr(pkt_entry->pkt)->seg_length;
 
 	if (tx_entry->total_len == tx_entry->bytes_acked) {
 		if (!(tx_entry->rxr_flags & RXR_DELIVERY_COMPLETE_REQUESTED))
@@ -270,7 +135,7 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 		       char *data, size_t seg_offset,
 		       size_t seg_size)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	bool all_received = 0;
 	ssize_t err;
 
@@ -284,6 +149,7 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 	all_received = (rx_entry->bytes_received == rx_entry->total_len);
 
 	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	assert(peer);
 	peer->rx_credits += ofi_div_ceil(seg_size, ep->max_data_payload_size);
 
 	rx_entry->window -= seg_size;
@@ -291,20 +157,20 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 		ep->available_data_bufs++;
 
 #if ENABLE_DEBUG
-	/* rx_entry can be released by rxr_pkt_copy_to_rx
+	/* rx_entry can be released by rxr_pkt_copy_data_to_rx_entry
 	 * so the call to dlist_remove must happen before
-	 * call to rxr_copy_to_rx
+	 * call to rxr_copy_data_to_rx_entry
 	 */
 	if (all_received) {
 		dlist_remove(&rx_entry->rx_pending_entry);
 		ep->rx_pending--;
 	}
 #endif
-	err = rxr_pkt_copy_to_rx(ep, rx_entry, seg_offset,
-				 pkt_entry, data, seg_size);
+	err = rxr_pkt_copy_data_to_rx_entry(ep, rx_entry, seg_offset,
+					    pkt_entry, data, seg_size);
 	if (err) {
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
-		rxr_cq_handle_rx_error(ep, rx_entry, err);
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 	}
 
 	if (all_received)
@@ -315,7 +181,7 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 		err = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
 		if (err) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ, "post CTS packet failed!\n");
-			rxr_cq_handle_rx_error(ep, rx_entry, err);
+			rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 		}
 	}
 }
@@ -323,18 +189,23 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 void rxr_pkt_handle_data_recv(struct rxr_ep *ep,
 			      struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_data_pkt *data_pkt;
+	struct rxr_data_hdr *data_hdr;
 	struct rxr_rx_entry *rx_entry;
+	size_t hdr_size;
 
-	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
+	data_hdr = rxr_get_data_hdr(pkt_entry->pkt);
 
 	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool,
-					data_pkt->hdr.rx_id);
+					data_hdr->recv_id);
+
+	hdr_size = sizeof(struct rxr_data_hdr);
+	if (data_hdr->flags & RXR_PKT_CONNID_HDR)
+		hdr_size += sizeof(struct rxr_data_opt_connid_hdr);
 
 	rxr_pkt_proc_data(ep, rx_entry,
 			  pkt_entry,
-			  data_pkt->data,
-			  data_pkt->hdr.seg_offset,
-			  data_pkt->hdr.seg_size);
+			  pkt_entry->pkt + hdr_size,
+			  data_hdr->seg_offset,
+			  data_hdr->seg_length);
 }
 
diff --git a/prov/efa/src/rxr/rxr_pkt_type_misc.c b/prov/efa/src/rxr/rxr_pkt_type_misc.c
index 65a8dc1..782385c 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_misc.c
+++ b/prov/efa/src/rxr/rxr_pkt_type_misc.c
@@ -36,6 +36,7 @@
 #include "rxr_msg.h"
 #include "rxr_cntr.h"
 #include "rxr_pkt_cmd.h"
+#include "rxr_pkt_type_base.h"
 #include "rxr_read.h"
 
 /* This file define functons for the following packet type:
@@ -51,53 +52,111 @@ ssize_t rxr_pkt_init_handshake(struct rxr_ep *ep,
 			       struct rxr_pkt_entry *pkt_entry,
 			       fi_addr_t addr)
 {
+	int nex;
 	struct rxr_handshake_hdr *handshake_hdr;
+	struct rxr_handshake_opt_connid_hdr *connid_hdr;
 
 	handshake_hdr = (struct rxr_handshake_hdr *)pkt_entry->pkt;
 	handshake_hdr->type = RXR_HANDSHAKE_PKT;
-	handshake_hdr->version = RXR_BASE_PROTOCOL_VERSION;
+	handshake_hdr->version = RXR_PROTOCOL_VERSION;
 	handshake_hdr->flags = 0;
-	handshake_hdr->maxproto = RXR_CUR_PROTOCOL_VERSION;
-	memcpy(handshake_hdr->features, ep->features,
-	       RXR_NUM_PROTOCOL_VERSION * sizeof(uint64_t));
 
-	pkt_entry->pkt_size = sizeof(struct rxr_handshake_hdr)
-			      + RXR_NUM_PROTOCOL_VERSION * sizeof(uint64_t);
+	nex = (RXR_NUM_EXTRA_FEATURE_OR_REQUEST-1)/64 + 1;
+	/*
+	 * The action of plus 3 is for backward compatibility.
+	 * See section 2.1 of protocol v4 document for detail.
+	 */
+	handshake_hdr->nextra_p3 = nex + 3;
+	memcpy(handshake_hdr->extra_info, ep->extra_info, nex * sizeof(uint64_t));
+	pkt_entry->pkt_size = sizeof(struct rxr_handshake_hdr) + nex * sizeof(uint64_t);
+
+	/*
+	 * Always include connid at the end of a handshake packet.
+	 * If peer cannot make use of connid, the connid will be ignored.
+	 */
+	connid_hdr = (struct rxr_handshake_opt_connid_hdr *)(pkt_entry->pkt + pkt_entry->pkt_size);
+	connid_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
+	handshake_hdr->flags |= RXR_PKT_CONNID_HDR;
+	pkt_entry->pkt_size += sizeof(struct rxr_handshake_opt_connid_hdr);
+
 	pkt_entry->addr = addr;
 	return 0;
 }
 
-void rxr_pkt_post_handshake(struct rxr_ep *ep,
-			    struct rxr_peer *peer,
-			    fi_addr_t addr)
+/** @brief Post a handshake packet to a peer.
+ *
+ * @param ep The endpoint on which the handshake packet is sent out.
+ * @param peer The peer to which the handshake packet is posted.
+ * @return 0 on success, fi_errno on error.
+ */
+ssize_t rxr_pkt_post_handshake(struct rxr_ep *ep, struct rdm_peer *peer)
 {
 	struct rxr_pkt_entry *pkt_entry;
+	fi_addr_t addr;
 	ssize_t ret;
 
-	assert(!(peer->flags & RXR_PEER_HANDSHAKE_SENT));
-
-	pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_efa_pool);
+	addr = peer->efa_fiaddr;
+	if (peer->is_local)
+		pkt_entry = rxr_pkt_entry_alloc(ep, ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
+	else
+		pkt_entry = rxr_pkt_entry_alloc(ep, ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 	if (OFI_UNLIKELY(!pkt_entry))
-		return;
+		return -FI_EAGAIN;
 
 	rxr_pkt_init_handshake(ep, pkt_entry, addr);
 
-	/*
-	 * TODO: Once we start using a core's selective completion capability,
-	 * post the HANDSHAKE packets without FI_COMPLETION.
-	 */
-	ret = rxr_pkt_entry_send(ep, pkt_entry, addr);
-
-	/*
-	 * Skip sending this handshake on error and try again when processing the
-	 * next REQ from this peer containing the source information
-	 */
+	ret = rxr_pkt_entry_send(ep, pkt_entry, 0);
 	if (OFI_UNLIKELY(ret)) {
 		rxr_pkt_entry_release_tx(ep, pkt_entry);
-		if (ret == -FI_EAGAIN)
-			return;
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"Failed to send a HANDSHAKE packet: ret %zd\n", ret);
+	}
+	return ret;
+}
+
+/** @brief Post a handshake packet to a peer.
+ *
+ * This function ensures an endpoint post one and only one handshake
+ * to a peer.
+ *
+ * For a peer that the endpoint has not attempted to send handshake,
+ * it will send a handshake packet.
+ *
+ * If the send succeeded, RXR_PEER_HANDSHAKE_SENT flag will be set to peer->flags.
+ *
+ * If the send encountered FI_EAGAIN failure, the peer will be added to
+ * rxr_ep->handshake_queued_peer_list. The handshake will be resend later
+ * by the progress engine.
+ *
+ * If the send encountered other failure, an EQ entry will be written.
+ *
+ * To ensure only one handshake is send to a peer, the function will not send
+ * packet to a peer whose peer->flags has either RXR_PEER_HANDSHAKE_SENT or
+ * RXR_PEER_HANDSHAKE_QUEUED.
+ *
+ * @param[in]	ep	The endpoint on which the handshake packet is sent out.
+ * @param[in]	peer	The peer to which the handshake packet is posted.
+ * @return 	void.
+ */
+void rxr_pkt_post_handshake_or_queue(struct rxr_ep *ep, struct rdm_peer *peer)
+{
+	ssize_t err;
+
+	if (peer->flags & (RXR_PEER_HANDSHAKE_SENT | RXR_PEER_HANDSHAKE_QUEUED))
+		return;
+
+	err = rxr_pkt_post_handshake(ep, peer);
+	if (OFI_UNLIKELY(err == -FI_EAGAIN)) {
+		/* add peer to handshake_queued_peer_list for retry later */
+		peer->flags |= RXR_PEER_HANDSHAKE_QUEUED;
+		dlist_insert_tail(&peer->handshake_queued_entry,
+				  &ep->handshake_queued_peer_list);
+		return;
+	}
+
+	if (OFI_UNLIKELY(err)) {
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Failed to post HANDSHAKE to peer %ld: %s\n",
+			peer->efa_fiaddr, fi_strerror(-err));
+		efa_eq_write_error(&ep->util_ep, FI_EIO, -err);
 		return;
 	}
 
@@ -107,19 +166,23 @@ void rxr_pkt_post_handshake(struct rxr_ep *ep,
 void rxr_pkt_handle_handshake_recv(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_handshake_hdr *handshake_pkt;
 
 	assert(pkt_entry->addr != FI_ADDR_NOTAVAIL);
 
 	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
 	assert(!(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED));
 
 	handshake_pkt = (struct rxr_handshake_hdr *)pkt_entry->pkt;
 
-	peer->maxproto = handshake_pkt->maxproto;
-	memcpy(peer->features, handshake_pkt->features,
-	       (handshake_pkt->maxproto - RXR_BASE_PROTOCOL_VERSION + 1) * sizeof(uint64_t));
+	/* nextra_p3 is number of members in extra_info plus 3.
+	 * See section 2.1 of protocol v4 document for detail
+	 */
+	peer->nextra_p3 = handshake_pkt->nextra_p3;
+	memcpy(peer->extra_info, handshake_pkt->extra_info,
+	       (handshake_pkt->nextra_p3 - 3) * sizeof(uint64_t));
 	peer->flags |= RXR_PEER_HANDSHAKE_RECEIVED;
 	FI_DBG(&rxr_prov, FI_LOG_CQ,
 	       "HANDSHAKE received from %" PRIu64 "\n", pkt_entry->addr);
@@ -128,7 +191,7 @@ void rxr_pkt_handle_handshake_recv(struct rxr_ep *ep,
 }
 
 /*  CTS packet related functions */
-void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
+void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rdm_peer *peer,
 				     uint64_t size, int request,
 				     int *window, int *credits)
 {
@@ -151,7 +214,7 @@ void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
 	 * number of credits are allocated to the transfer so the sender can
 	 * make progress.
 	 */
-	*credits = MIN(MIN(ep->available_data_bufs, ep->posted_bufs_efa),
+	*credits = MIN(MIN(ep->available_data_bufs, ep->efa_rx_pkts_posted),
 		       peer->rx_credits);
 	*credits = MIN(request, *credits);
 	*credits = MAX(*credits, rxr_env.tx_min_credits);
@@ -166,27 +229,36 @@ ssize_t rxr_pkt_init_cts(struct rxr_ep *ep,
 {
 	int window = 0;
 	struct rxr_cts_hdr *cts_hdr;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	size_t bytes_left;
 
 	cts_hdr = (struct rxr_cts_hdr *)pkt_entry->pkt;
 	cts_hdr->type = RXR_CTS_PKT;
-	cts_hdr->version = RXR_BASE_PROTOCOL_VERSION;
+	cts_hdr->version = RXR_PROTOCOL_VERSION;
 	cts_hdr->flags = 0;
 
 	if (rx_entry->cq_entry.flags & FI_READ)
 		cts_hdr->flags |= RXR_CTS_READ_REQ;
 
-	cts_hdr->tx_id = rx_entry->tx_id;
-	cts_hdr->rx_id = rx_entry->rx_id;
+	cts_hdr->send_id = rx_entry->tx_id;
+	cts_hdr->recv_id = rx_entry->rx_id;
 
 	bytes_left = rx_entry->total_len - rx_entry->bytes_received;
 	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	assert(peer);
 	rxr_pkt_calc_cts_window_credits(ep, peer, bytes_left,
 					rx_entry->credit_request,
 					&window, &rx_entry->credit_cts);
-	cts_hdr->window = window;
+	cts_hdr->recv_length = window;
 	pkt_entry->pkt_size = sizeof(struct rxr_cts_hdr);
+
+	/*
+	 * always set connid header. If the peer does not need it,
+	 * it will be ignored.
+	 */
+	cts_hdr->flags |= RXR_PKT_CONNID_HDR;
+	cts_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
+
 	pkt_entry->addr = rx_entry->addr;
 	pkt_entry->x_entry = (void *)rx_entry;
 	return 0;
@@ -198,7 +270,7 @@ void rxr_pkt_handle_cts_sent(struct rxr_ep *ep,
 	struct rxr_rx_entry *rx_entry;
 
 	rx_entry = (struct rxr_rx_entry *)pkt_entry->x_entry;
-	rx_entry->window = rxr_get_cts_hdr(pkt_entry->pkt)->window;
+	rx_entry->window = rxr_get_cts_hdr(pkt_entry->pkt)->recv_length;
 	ep->available_data_bufs -= rx_entry->credit_cts;
 
 	/*
@@ -213,24 +285,26 @@ void rxr_pkt_handle_cts_sent(struct rxr_ep *ep,
 void rxr_pkt_handle_cts_recv(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_cts_hdr *cts_pkt;
 	struct rxr_tx_entry *tx_entry;
 
 	cts_pkt = (struct rxr_cts_hdr *)pkt_entry->pkt;
 	if (cts_pkt->flags & RXR_CTS_READ_REQ)
-		tx_entry = ofi_bufpool_get_ibuf(ep->readrsp_tx_entry_pool, cts_pkt->tx_id);
+		tx_entry = ofi_bufpool_get_ibuf(ep->readrsp_tx_entry_pool, cts_pkt->send_id);
 	else
-		tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, cts_pkt->tx_id);
+		tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, cts_pkt->send_id);
 
-	tx_entry->rx_id = cts_pkt->rx_id;
-	tx_entry->window = cts_pkt->window;
+	tx_entry->rx_id = cts_pkt->recv_id;
+	tx_entry->window = cts_pkt->recv_length;
 
 	/* Return any excess tx_credits that were borrowed for the request */
 	peer = rxr_ep_get_peer(ep, tx_entry->addr);
-	tx_entry->credit_allocated = ofi_div_ceil(cts_pkt->window, ep->max_data_payload_size);
-	if (tx_entry->credit_allocated < tx_entry->credit_request)
+	tx_entry->credit_allocated = ofi_div_ceil(cts_pkt->recv_length, ep->max_data_payload_size);
+	if (tx_entry->credit_allocated < tx_entry->credit_request) {
+		assert(peer);
 		peer->tx_credits += tx_entry->credit_request - tx_entry->credit_allocated;
+	}
 
 	rxr_pkt_entry_release_rx(ep, pkt_entry);
 
@@ -245,24 +319,22 @@ int rxr_pkt_init_readrsp(struct rxr_ep *ep,
 			 struct rxr_tx_entry *tx_entry,
 			 struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_readrsp_pkt *readrsp_pkt;
 	struct rxr_readrsp_hdr *readrsp_hdr;
-	size_t mtu = ep->mtu_size;
 
-	readrsp_pkt = (struct rxr_readrsp_pkt *)pkt_entry->pkt;
-	readrsp_hdr = &readrsp_pkt->hdr;
+	readrsp_hdr = rxr_get_readrsp_hdr(pkt_entry->pkt);
 	readrsp_hdr->type = RXR_READRSP_PKT;
-	readrsp_hdr->version = RXR_BASE_PROTOCOL_VERSION;
+	readrsp_hdr->version = RXR_PROTOCOL_VERSION;
 	readrsp_hdr->flags = 0;
-	readrsp_hdr->tx_id = tx_entry->tx_id;
-	readrsp_hdr->rx_id = tx_entry->rx_id;
-	readrsp_hdr->seg_size = ofi_copy_from_iov(readrsp_pkt->data,
-						  mtu - RXR_READRSP_HDR_SIZE,
-						  tx_entry->iov,
-						  tx_entry->iov_count, 0);
-	pkt_entry->pkt_size = RXR_READRSP_HDR_SIZE + readrsp_hdr->seg_size;
+	readrsp_hdr->send_id = tx_entry->tx_id;
+	readrsp_hdr->recv_id = tx_entry->rx_id;
+	readrsp_hdr->flags |= RXR_PKT_CONNID_HDR;
+	readrsp_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
+	readrsp_hdr->seg_length = MIN(ep->mtu_size - sizeof(struct rxr_readrsp_hdr),
+				      tx_entry->total_len);
+
 	pkt_entry->addr = tx_entry->addr;
-	pkt_entry->x_entry = tx_entry;
+	rxr_pkt_init_data_from_tx_entry(ep, pkt_entry, sizeof(struct rxr_readrsp_hdr), 
+					tx_entry, 0, readrsp_hdr->seg_length);
 	return 0;
 }
 
@@ -277,7 +349,7 @@ void rxr_pkt_handle_readrsp_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_en
 				  util_domain.domain_fid);
 
 	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-	data_len = rxr_get_readrsp_hdr(pkt_entry->pkt)->seg_size;
+	data_len = rxr_get_readrsp_hdr(pkt_entry->pkt)->seg_length;
 	tx_entry->bytes_sent += data_len;
 	tx_entry->window -= data_len;
 	assert(tx_entry->window >= 0);
@@ -303,7 +375,7 @@ void rxr_pkt_handle_readrsp_send_completion(struct rxr_ep *ep,
 	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
 	assert(tx_entry->cq_entry.flags & FI_READ);
 
-	tx_entry->bytes_acked += readrsp_hdr->seg_size;
+	tx_entry->bytes_acked += readrsp_hdr->seg_length;
 	if (tx_entry->total_len == tx_entry->bytes_acked)
 		rxr_cq_handle_tx_completion(ep, tx_entry);
 }
@@ -317,12 +389,12 @@ void rxr_pkt_handle_readrsp_recv(struct rxr_ep *ep,
 
 	readrsp_pkt = (struct rxr_readrsp_pkt *)pkt_entry->pkt;
 	readrsp_hdr = &readrsp_pkt->hdr;
-	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, readrsp_hdr->rx_id);
+	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, readrsp_hdr->recv_id);
 	assert(rx_entry->cq_entry.flags & FI_READ);
-	rx_entry->tx_id = readrsp_hdr->tx_id;
+	rx_entry->tx_id = readrsp_hdr->send_id;
 	rxr_pkt_proc_data(ep, rx_entry, pkt_entry,
 			  readrsp_pkt->data,
-			  0, readrsp_hdr->seg_size);
+			  0, readrsp_hdr->seg_length);
 }
 
 /*  RMA_CONTEXT packet functions
@@ -339,7 +411,7 @@ void rxr_pkt_init_write_context(struct rxr_tx_entry *tx_entry,
 	pkt_entry->x_entry = (void *)tx_entry;
 	rma_context_pkt = (struct rxr_rma_context_pkt *)pkt_entry->pkt;
 	rma_context_pkt->type = RXR_RMA_CONTEXT_PKT;
-	rma_context_pkt->version = RXR_BASE_PROTOCOL_VERSION;
+	rma_context_pkt->version = RXR_PROTOCOL_VERSION;
 	rma_context_pkt->context_type = RXR_WRITE_CONTEXT;
 	rma_context_pkt->tx_id = tx_entry->tx_id;
 }
@@ -358,7 +430,7 @@ void rxr_pkt_init_read_context(struct rxr_ep *rxr_ep,
 	ctx_pkt = (struct rxr_rma_context_pkt *)pkt_entry->pkt;
 	ctx_pkt->type = RXR_RMA_CONTEXT_PKT;
 	ctx_pkt->flags = 0;
-	ctx_pkt->version = RXR_BASE_PROTOCOL_VERSION;
+	ctx_pkt->version = RXR_PROTOCOL_VERSION;
 	ctx_pkt->context_type = RXR_READ_CONTEXT;
 	ctx_pkt->read_id = read_entry->read_id;
 	ctx_pkt->seg_size = seg_size;
@@ -373,7 +445,6 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 	struct rxr_pkt_entry *pkt_entry;
 	struct rxr_read_entry *read_entry;
 	struct rxr_rma_context_pkt *rma_context_pkt;
-	struct rxr_peer *peer;
 	int inject;
 	size_t data_size;
 	ssize_t ret;
@@ -391,6 +462,7 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 			tx_entry = read_entry->context;
 			assert(tx_entry && tx_entry->cq_entry.flags & FI_READ);
 			rxr_cq_write_tx_completion(ep, tx_entry);
+			rxr_release_tx_entry(ep, tx_entry);
 		} else if (read_entry->context_type == RXR_READ_CONTEXT_RX_ENTRY) {
 			rx_entry = read_entry->context;
 			if (rx_entry->op == ofi_op_msg || rx_entry->op == ofi_op_tagged) {
@@ -404,8 +476,7 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 			inject = (read_entry->lower_ep_type == SHM_EP);
 			ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_EOR_PKT, inject);
 			if (OFI_UNLIKELY(ret)) {
-				if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-					assert(0 && "failed to write err cq entry");
+				rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
 				rxr_release_rx_entry(ep, rx_entry);
 			}
 		} else {
@@ -419,14 +490,7 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 		rxr_read_release_entry(ep, read_entry);
 	}
 
-	if (read_entry->context_type == RXR_READ_CONTEXT_PKT_ENTRY) {
-		assert(context_pkt_entry->addr == FI_ADDR_NOTAVAIL);
-		ep->tx_pending--;
-	} else {
-		peer = rxr_ep_get_peer(ep, context_pkt_entry->addr);
-		if (!peer->is_local)
-			rxr_ep_dec_tx_pending(ep, peer, 0);
-	}
+	rxr_ep_record_tx_op_completed(ep, context_pkt_entry);
 }
 
 void rxr_pkt_handle_rma_completion(struct rxr_ep *ep,
@@ -435,19 +499,19 @@ void rxr_pkt_handle_rma_completion(struct rxr_ep *ep,
 	struct rxr_tx_entry *tx_entry = NULL;
 	struct rxr_rma_context_pkt *rma_context_pkt;
 
-	assert(rxr_get_base_hdr(context_pkt_entry->pkt)->version == RXR_BASE_PROTOCOL_VERSION);
+	assert(rxr_get_base_hdr(context_pkt_entry->pkt)->version == RXR_PROTOCOL_VERSION);
 
 	rma_context_pkt = (struct rxr_rma_context_pkt *)context_pkt_entry->pkt;
 
 	switch (rma_context_pkt->context_type) {
 	case RXR_WRITE_CONTEXT:
 		tx_entry = (struct rxr_tx_entry *)context_pkt_entry->x_entry;
-		if (tx_entry->fi_flags & FI_COMPLETION) {
+		if (tx_entry->fi_flags & FI_COMPLETION)
 			rxr_cq_write_tx_completion(ep, tx_entry);
-		} else {
+		else
 			efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-			rxr_release_tx_entry(ep, tx_entry);
-		}
+
+		rxr_release_tx_entry(ep, tx_entry);
 		break;
 	case RXR_READ_CONTEXT:
 		rxr_pkt_handle_rma_read_completion(ep, context_pkt_entry);
@@ -468,10 +532,12 @@ int rxr_pkt_init_eor(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry, struct rx
 
 	eor_hdr = (struct rxr_eor_hdr *)pkt_entry->pkt;
 	eor_hdr->type = RXR_EOR_PKT;
-	eor_hdr->version = RXR_BASE_PROTOCOL_VERSION;
+	eor_hdr->version = RXR_PROTOCOL_VERSION;
 	eor_hdr->flags = 0;
-	eor_hdr->tx_id = rx_entry->tx_id;
-	eor_hdr->rx_id = rx_entry->rx_id;
+	eor_hdr->send_id = rx_entry->tx_id;
+	eor_hdr->recv_id = rx_entry->rx_id;
+	eor_hdr->flags |= RXR_PKT_CONNID_HDR;
+	eor_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
 	pkt_entry->pkt_size = sizeof(struct rxr_eor_hdr);
 	pkt_entry->addr = rx_entry->addr;
 	pkt_entry->x_entry = rx_entry;
@@ -488,7 +554,7 @@ void rxr_pkt_handle_eor_send_completion(struct rxr_ep *ep,
 	struct rxr_rx_entry *rx_entry;
 
 	rx_entry = pkt_entry->x_entry;
-	assert(rx_entry && rx_entry->rx_id == rxr_get_eor_hdr(pkt_entry->pkt)->rx_id);
+	assert(rx_entry && rx_entry->rx_id == rxr_get_eor_hdr(pkt_entry->pkt)->recv_id);
 	rxr_release_rx_entry(ep, rx_entry);
 }
 
@@ -505,8 +571,9 @@ void rxr_pkt_handle_eor_recv(struct rxr_ep *ep,
 	eor_hdr = (struct rxr_eor_hdr *)pkt_entry->pkt;
 
 	/* pre-post buf used here, so can NOT track back to tx_entry with x_entry */
-	tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, eor_hdr->tx_id);
+	tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, eor_hdr->send_id);
 	rxr_cq_write_tx_completion(ep, tx_entry);
+	rxr_release_tx_entry(ep, tx_entry);
 	rxr_pkt_entry_release_rx(ep, pkt_entry);
 }
 
@@ -518,10 +585,12 @@ int rxr_pkt_init_receipt(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 
 	receipt_hdr = rxr_get_receipt_hdr(pkt_entry->pkt);
 	receipt_hdr->type = RXR_RECEIPT_PKT;
-	receipt_hdr->version = RXR_BASE_PROTOCOL_VERSION;
+	receipt_hdr->version = RXR_PROTOCOL_VERSION;
 	receipt_hdr->flags = 0;
 	receipt_hdr->tx_id = rx_entry->tx_id;
 	receipt_hdr->msg_id = rx_entry->msg_id;
+	receipt_hdr->flags |= RXR_PKT_CONNID_HDR;
+	receipt_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
 
 	pkt_entry->pkt_size = sizeof(struct rxr_receipt_hdr);
 	pkt_entry->addr = rx_entry->addr;
@@ -553,25 +622,26 @@ void rxr_pkt_handle_receipt_send_completion(struct rxr_ep *ep,
 int rxr_pkt_init_atomrsp(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 			 struct rxr_pkt_entry *pkt_entry)
 {
+	struct rxr_atomrsp_pkt *atomrsp_pkt;
 	struct rxr_atomrsp_hdr *atomrsp_hdr;
 
 	assert(rx_entry->atomrsp_data);
 	pkt_entry->addr = rx_entry->addr;
 	pkt_entry->x_entry = rx_entry;
 
-	atomrsp_hdr = (struct rxr_atomrsp_hdr *)pkt_entry->pkt;
+	atomrsp_pkt = (struct rxr_atomrsp_pkt *)pkt_entry->pkt;
+	atomrsp_hdr = &atomrsp_pkt->hdr;
 	atomrsp_hdr->type = RXR_ATOMRSP_PKT;
-	atomrsp_hdr->version = RXR_BASE_PROTOCOL_VERSION;
+	atomrsp_hdr->version = RXR_PROTOCOL_VERSION;
 	atomrsp_hdr->flags = 0;
-	atomrsp_hdr->tx_id = rx_entry->tx_id;
-	atomrsp_hdr->rx_id = rx_entry->rx_id;
-	atomrsp_hdr->seg_size = ofi_total_iov_len(rx_entry->iov, rx_entry->iov_count);
-
-	assert(RXR_ATOMRSP_HDR_SIZE + atomrsp_hdr->seg_size < ep->mtu_size);
-
+	atomrsp_hdr->recv_id = rx_entry->tx_id;
+	atomrsp_hdr->seg_length = ofi_total_iov_len(rx_entry->iov, rx_entry->iov_count);
+	atomrsp_hdr->flags |= RXR_PKT_CONNID_HDR;
+	atomrsp_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
+	assert(sizeof(struct rxr_atomrsp_hdr) + atomrsp_hdr->seg_length < ep->mtu_size);
 	/* rx_entry->atomrsp_data was filled in rxr_pkt_handle_req_recv() */
-	memcpy((char*)pkt_entry->pkt + RXR_ATOMRSP_HDR_SIZE, rx_entry->atomrsp_data, atomrsp_hdr->seg_size);
-	pkt_entry->pkt_size = RXR_ATOMRSP_HDR_SIZE + atomrsp_hdr->seg_size;
+	memcpy(atomrsp_pkt->data, rx_entry->atomrsp_data, atomrsp_hdr->seg_length);
+	pkt_entry->pkt_size = sizeof(struct rxr_atomrsp_hdr) + atomrsp_hdr->seg_length;
 	return 0;
 }
 
@@ -598,21 +668,19 @@ void rxr_pkt_handle_atomrsp_recv(struct rxr_ep *ep,
 
 	atomrsp_pkt = (struct rxr_atomrsp_pkt *)pkt_entry->pkt;
 	atomrsp_hdr = &atomrsp_pkt->hdr;
-	tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, atomrsp_hdr->tx_id);
+	tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, atomrsp_hdr->recv_id);
 
 	ofi_copy_to_iov(tx_entry->atomic_ex.resp_iov,
 			tx_entry->atomic_ex.resp_iov_count,
 			0, atomrsp_pkt->data,
-			atomrsp_hdr->seg_size);
+			atomrsp_hdr->seg_length);
 
-	if (tx_entry->fi_flags & FI_COMPLETION) {
-		/* Note write_tx_completion() will release tx_entry */
+	if (tx_entry->fi_flags & FI_COMPLETION)
 		rxr_cq_write_tx_completion(ep, tx_entry);
-	} else {
+	else
 		efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-		rxr_release_tx_entry(ep, tx_entry);
-	}
 
+	rxr_release_tx_entry(ep, tx_entry);
 	rxr_pkt_entry_release_rx(ep, pkt_entry);
 }
 
diff --git a/prov/efa/src/rxr/rxr_pkt_type_req.c b/prov/efa/src/rxr/rxr_pkt_type_req.c
index 1325b18..8ae6ec5 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_req.c
+++ b/prov/efa/src/rxr/rxr_pkt_type_req.c
@@ -37,6 +37,7 @@
 #include "rxr_rma.h"
 #include "rxr_msg.h"
 #include "rxr_pkt_cmd.h"
+#include "rxr_pkt_type_base.h"
 #include "rxr_read.h"
 
 /*
@@ -66,29 +67,29 @@ struct rxr_req_inf REQ_INF_LIST[] = {
 	[RXR_EAGER_TAGRTM_PKT] = {4, sizeof(struct rxr_eager_tagrtm_hdr), 0},
 	[RXR_MEDIUM_MSGRTM_PKT] = {4, sizeof(struct rxr_medium_msgrtm_hdr), 0},
 	[RXR_MEDIUM_TAGRTM_PKT] = {4, sizeof(struct rxr_medium_tagrtm_hdr), 0},
-	[RXR_LONG_MSGRTM_PKT] = {4, sizeof(struct rxr_long_msgrtm_hdr), 0},
-	[RXR_LONG_TAGRTM_PKT] = {4, sizeof(struct rxr_long_tagrtm_hdr), 0},
-	[RXR_READ_MSGRTM_PKT] = {4, sizeof(struct rxr_read_msgrtm_hdr), RXR_REQ_FEATURE_RDMA_READ},
-	[RXR_READ_TAGRTM_PKT] = {4, sizeof(struct rxr_read_tagrtm_hdr), RXR_REQ_FEATURE_RDMA_READ},
-	[RXR_DC_EAGER_MSGRTM_PKT] = {4, sizeof(struct rxr_dc_eager_msgrtm_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_DC_EAGER_TAGRTM_PKT] = {4, sizeof(struct rxr_dc_eager_tagrtm_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_DC_MEDIUM_MSGRTM_PKT] = {4, sizeof(struct rxr_dc_medium_msgrtm_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_DC_MEDIUM_TAGRTM_PKT] = {4, sizeof(struct rxr_dc_medium_tagrtm_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_DC_LONG_MSGRTM_PKT] = {4, sizeof(struct rxr_long_msgrtm_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_DC_LONG_TAGRTM_PKT] = {4, sizeof(struct rxr_long_tagrtm_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
+	[RXR_LONGCTS_MSGRTM_PKT] = {4, sizeof(struct rxr_longcts_msgrtm_hdr), 0},
+	[RXR_LONGCTS_TAGRTM_PKT] = {4, sizeof(struct rxr_longcts_tagrtm_hdr), 0},
+	[RXR_LONGREAD_MSGRTM_PKT] = {4, sizeof(struct rxr_longread_msgrtm_hdr), RXR_EXTRA_FEATURE_RDMA_READ},
+	[RXR_LONGREAD_TAGRTM_PKT] = {4, sizeof(struct rxr_longread_tagrtm_hdr), RXR_EXTRA_FEATURE_RDMA_READ},
+	[RXR_DC_EAGER_MSGRTM_PKT] = {4, sizeof(struct rxr_dc_eager_msgrtm_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_DC_EAGER_TAGRTM_PKT] = {4, sizeof(struct rxr_dc_eager_tagrtm_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_DC_MEDIUM_MSGRTM_PKT] = {4, sizeof(struct rxr_dc_medium_msgrtm_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_DC_MEDIUM_TAGRTM_PKT] = {4, sizeof(struct rxr_dc_medium_tagrtm_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_DC_LONGCTS_MSGRTM_PKT] = {4, sizeof(struct rxr_longcts_msgrtm_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_DC_LONGCTS_TAGRTM_PKT] = {4, sizeof(struct rxr_longcts_tagrtm_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
 	/* rtw header */
 	[RXR_EAGER_RTW_PKT] = {4, sizeof(struct rxr_eager_rtw_hdr), 0},
-	[RXR_DC_EAGER_RTW_PKT] = {4, sizeof(struct rxr_dc_eager_rtw_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_LONG_RTW_PKT] = {4, sizeof(struct rxr_long_rtw_hdr), 0},
-	[RXR_DC_LONG_RTW_PKT] = {4, sizeof(struct rxr_long_rtw_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
-	[RXR_READ_RTW_PKT] = {4, sizeof(struct rxr_read_rtw_hdr), RXR_REQ_FEATURE_RDMA_READ},
+	[RXR_DC_EAGER_RTW_PKT] = {4, sizeof(struct rxr_dc_eager_rtw_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_LONGCTS_RTW_PKT] = {4, sizeof(struct rxr_longcts_rtw_hdr), 0},
+	[RXR_DC_LONGCTS_RTW_PKT] = {4, sizeof(struct rxr_longcts_rtw_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
+	[RXR_LONGREAD_RTW_PKT] = {4, sizeof(struct rxr_longread_rtw_hdr), RXR_EXTRA_FEATURE_RDMA_READ},
 	/* rtr header */
 	[RXR_SHORT_RTR_PKT] = {4, sizeof(struct rxr_rtr_hdr), 0},
-	[RXR_LONG_RTR_PKT] = {4, sizeof(struct rxr_rtr_hdr), 0},
-	[RXR_READ_RTR_PKT] = {4, sizeof(struct rxr_base_hdr), RXR_REQ_FEATURE_RDMA_READ},
+	[RXR_LONGCTS_RTR_PKT] = {4, sizeof(struct rxr_rtr_hdr), 0},
+	[RXR_READ_RTR_PKT] = {4, sizeof(struct rxr_base_hdr), RXR_EXTRA_FEATURE_RDMA_READ},
 	/* rta header */
 	[RXR_WRITE_RTA_PKT] = {4, sizeof(struct rxr_rta_hdr), 0},
-	[RXR_DC_WRITE_RTA_PKT] = {4, sizeof(struct rxr_rta_hdr), RXR_REQ_FEATURE_DELIVERY_COMPLETE},
+	[RXR_DC_WRITE_RTA_PKT] = {4, sizeof(struct rxr_rta_hdr), RXR_EXTRA_FEATURE_DELIVERY_COMPLETE},
 	[RXR_FETCH_RTA_PKT] = {4, sizeof(struct rxr_rta_hdr), 0},
 	[RXR_COMPARE_RTA_PKT] = {4, sizeof(struct rxr_rta_hdr), 0},
 };
@@ -108,7 +109,7 @@ void rxr_pkt_init_req_hdr(struct rxr_ep *ep,
 			  struct rxr_pkt_entry *pkt_entry)
 {
 	char *opt_hdr;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_base_hdr *base_hdr;
 
 	/* init the base header */
@@ -118,14 +119,27 @@ void rxr_pkt_init_req_hdr(struct rxr_ep *ep,
 	base_hdr->flags = 0;
 
 	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(peer);
 
-	if (OFI_UNLIKELY(!(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED))) {
+	if (rxr_peer_need_raw_addr_hdr(peer)) {
 		/*
 		 * This is the first communication with this peer on this
 		 * endpoint, so send the core's address for this EP in the REQ
 		 * so the remote side can insert it into its address vector.
 		 */
 		base_hdr->flags |= RXR_REQ_OPT_RAW_ADDR_HDR;
+	} else if (rxr_peer_need_connid(peer)) {
+		/*
+		 * After receiving handshake packet, we will know the peer's capability.
+		 *
+		 * If the peer need connid, we will include the optional connid
+		 * header in the req packet header.The peer will use it
+		 * to verify my identity.
+		 *
+		 * This logic means that a req packet cannot have both
+		 * the optional raw address header and the optional connid header.
+		 */
+		base_hdr->flags |= RXR_PKT_CONNID_HDR;
 	}
 
 	if (tx_entry->fi_flags & FI_REMOTE_CQ_DATA) {
@@ -138,9 +152,10 @@ void rxr_pkt_init_req_hdr(struct rxr_ep *ep,
 		struct rxr_req_opt_raw_addr_hdr *raw_addr_hdr;
 
 		raw_addr_hdr = (struct rxr_req_opt_raw_addr_hdr *)opt_hdr;
-		raw_addr_hdr->addr_len = ep->core_addrlen;
-		memcpy(raw_addr_hdr->raw_addr, ep->core_addr, raw_addr_hdr->addr_len);
-		opt_hdr += sizeof(*raw_addr_hdr) + raw_addr_hdr->addr_len;
+		raw_addr_hdr->addr_len = RXR_REQ_OPT_RAW_ADDR_HDR_SIZE - sizeof(struct rxr_req_opt_raw_addr_hdr);
+		assert(raw_addr_hdr->addr_len >= ep->core_addrlen);
+		memcpy(raw_addr_hdr->raw_addr, ep->core_addr, ep->core_addrlen);
+		opt_hdr += RXR_REQ_OPT_RAW_ADDR_HDR_SIZE;
 	}
 
 	if (base_hdr->flags & RXR_REQ_OPT_CQ_DATA_HDR) {
@@ -151,7 +166,16 @@ void rxr_pkt_init_req_hdr(struct rxr_ep *ep,
 		opt_hdr += sizeof(*cq_data_hdr);
 	}
 
+	if (base_hdr->flags & RXR_PKT_CONNID_HDR) {
+		struct rxr_req_opt_connid_hdr *connid_hdr;
+
+		connid_hdr = (struct rxr_req_opt_connid_hdr *)opt_hdr;
+		connid_hdr->connid = rxr_ep_raw_addr(ep)->qkey;
+		opt_hdr += sizeof(*connid_hdr);
+	}
+
 	pkt_entry->addr = tx_entry->addr;
+	assert(opt_hdr - pkt_entry->pkt == rxr_pkt_req_hdr_size(pkt_entry));
 }
 
 size_t rxr_pkt_req_base_hdr_size(struct rxr_pkt_entry *pkt_entry)
@@ -165,12 +189,12 @@ size_t rxr_pkt_req_base_hdr_size(struct rxr_pkt_entry *pkt_entry)
 	hdr_size = REQ_INF_LIST[base_hdr->type].base_hdr_size;
 	if (base_hdr->type == RXR_EAGER_RTW_PKT ||
 	    base_hdr->type == RXR_DC_EAGER_RTW_PKT ||
-	    base_hdr->type == RXR_LONG_RTW_PKT ||
-	    base_hdr->type == RXR_DC_LONG_RTW_PKT ||
-	    base_hdr->type == RXR_READ_RTW_PKT)
+	    base_hdr->type == RXR_LONGCTS_RTW_PKT ||
+	    base_hdr->type == RXR_DC_LONGCTS_RTW_PKT ||
+	    base_hdr->type == RXR_LONGREAD_RTW_PKT)
 		hdr_size += rxr_get_rtw_base_hdr(pkt_entry->pkt)->rma_iov_count * sizeof(struct fi_rma_iov);
 	else if (base_hdr->type == RXR_SHORT_RTR_PKT ||
-		 base_hdr->type == RXR_LONG_RTR_PKT)
+		 base_hdr->type == RXR_LONGCTS_RTR_PKT)
 		hdr_size += rxr_get_rtr_hdr(pkt_entry->pkt)->rma_iov_count * sizeof(struct fi_rma_iov);
 	else if (base_hdr->type == RXR_WRITE_RTA_PKT ||
 		 base_hdr->type == RXR_DC_WRITE_RTA_PKT ||
@@ -181,6 +205,13 @@ size_t rxr_pkt_req_base_hdr_size(struct rxr_pkt_entry *pkt_entry)
 	return hdr_size;
 }
 
+/**
+ * @brief return the optional raw addr header pointer in a req packet
+ *
+ * @param[in]	pkt_entry	an REQ packet entry
+ * @return	If the input has the optional raw addres header, return the pointer to it.
+ *		Otherwise, return NULL
+ */
 void *rxr_pkt_req_raw_addr(struct rxr_pkt_entry *pkt_entry)
 {
 	char *opt_hdr;
@@ -190,6 +221,10 @@ void *rxr_pkt_req_raw_addr(struct rxr_pkt_entry *pkt_entry)
 	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
 	opt_hdr = (char *)pkt_entry->pkt + rxr_pkt_req_base_hdr_size(pkt_entry);
 	if (base_hdr->flags & RXR_REQ_OPT_RAW_ADDR_HDR) {
+		/* For req packet, the optional connid header and the optional
+		 * raw address header are mutually exclusive.
+		 */
+		assert(!(base_hdr->flags & RXR_PKT_CONNID_HDR));
 		raw_addr_hdr = (struct rxr_req_opt_raw_addr_hdr *)opt_hdr;
 		return raw_addr_hdr->raw_addr;
 	}
@@ -197,6 +232,42 @@ void *rxr_pkt_req_raw_addr(struct rxr_pkt_entry *pkt_entry)
 	return NULL;
 }
 
+/**
+ * @brief return the pointer to connid in a req packet
+ *
+ * @param[in]	pkt_entry	an REQ packet entry
+ * @return	If the input has the optional connid header, return the pointer to connid
+ * 		Otherwise, return NULL
+ */
+uint32_t *rxr_pkt_req_connid_ptr(struct rxr_pkt_entry *pkt_entry)
+{
+	char *opt_hdr;
+	struct rxr_base_hdr *base_hdr;
+	struct rxr_req_opt_connid_hdr *connid_hdr;
+
+	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
+	opt_hdr = (char *)pkt_entry->pkt + rxr_pkt_req_base_hdr_size(pkt_entry);
+
+	if (base_hdr->flags & RXR_REQ_OPT_RAW_ADDR_HDR) {
+		struct rxr_req_opt_raw_addr_hdr *raw_addr_hdr;
+		struct efa_ep_addr *raw_addr;
+
+		raw_addr_hdr = (struct rxr_req_opt_raw_addr_hdr *)opt_hdr;
+		raw_addr = (struct efa_ep_addr *)raw_addr_hdr->raw_addr;
+		return &raw_addr->qkey;
+	}
+
+	if (base_hdr->flags & RXR_REQ_OPT_CQ_DATA_HDR)
+		opt_hdr += sizeof(struct rxr_req_opt_cq_data_hdr);
+
+	if (base_hdr->flags & RXR_PKT_CONNID_HDR) {
+		connid_hdr = (struct rxr_req_opt_connid_hdr *)opt_hdr;
+		return &connid_hdr->connid;
+	}
+
+	return NULL;
+}
+
 size_t rxr_pkt_req_hdr_size(struct rxr_pkt_entry *pkt_entry)
 {
 	char *opt_hdr;
@@ -205,7 +276,13 @@ size_t rxr_pkt_req_hdr_size(struct rxr_pkt_entry *pkt_entry)
 
 	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
 	opt_hdr = (char *)pkt_entry->pkt + rxr_pkt_req_base_hdr_size(pkt_entry);
+
+	/*
+	 * It is not possible to have both optional raw addr header and optional
+	 * connid header in a packet header.
+	 */
 	if (base_hdr->flags & RXR_REQ_OPT_RAW_ADDR_HDR) {
+		assert(!(base_hdr->flags & RXR_PKT_CONNID_HDR));
 		raw_addr_hdr = (struct rxr_req_opt_raw_addr_hdr *)opt_hdr;
 		opt_hdr += sizeof(struct rxr_req_opt_raw_addr_hdr) + raw_addr_hdr->addr_len;
 	}
@@ -213,6 +290,11 @@ size_t rxr_pkt_req_hdr_size(struct rxr_pkt_entry *pkt_entry)
 	if (base_hdr->flags & RXR_REQ_OPT_CQ_DATA_HDR)
 		opt_hdr += sizeof(struct rxr_req_opt_cq_data_hdr);
 
+	if (base_hdr->flags & RXR_PKT_CONNID_HDR) {
+		assert(!(base_hdr->flags & RXR_REQ_OPT_RAW_ADDR_HDR));
+		opt_hdr += sizeof(struct rxr_req_opt_connid_hdr);
+	}
+
 	return opt_hdr - (char *)pkt_entry->pkt;
 }
 
@@ -237,13 +319,20 @@ int64_t rxr_pkt_req_cq_data(struct rxr_pkt_entry *pkt_entry)
 
 size_t rxr_pkt_req_max_header_size(int pkt_type)
 {
+	/* max_hdr_size does not include optional connid hdr length because
+	 * it is impossible to have both optional connid hdr and opt_raw_addr_hdr
+	 * in the header, and length of opt raw addr hdr is larger than
+	 * connid hdr (which is confirmed by the following assertion).
+	 */
+	assert(RXR_REQ_OPT_RAW_ADDR_HDR_SIZE >= sizeof(struct rxr_req_opt_connid_hdr));
+
 	int max_hdr_size = REQ_INF_LIST[pkt_type].base_hdr_size
-		+ sizeof(struct rxr_req_opt_raw_addr_hdr) + RXR_MAX_NAME_LENGTH
+		+ RXR_REQ_OPT_RAW_ADDR_HDR_SIZE
 		+ sizeof(struct rxr_req_opt_cq_data_hdr);
 
 	if (pkt_type == RXR_EAGER_RTW_PKT ||
 	    pkt_type == RXR_DC_EAGER_RTW_PKT ||
-	    pkt_type == RXR_LONG_RTW_PKT)
+	    pkt_type == RXR_LONGCTS_RTW_PKT)
 		max_hdr_size += RXR_IOV_LIMIT * sizeof(struct fi_rma_iov);
 
 	return max_hdr_size;
@@ -269,9 +358,10 @@ size_t rxr_pkt_max_header_size(void)
 
 size_t rxr_pkt_req_max_data_size(struct rxr_ep *ep, fi_addr_t addr, int pkt_type)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
 	peer = rxr_ep_get_peer(ep, addr);
+	assert(peer);
 
 	if (peer->is_local) {
 		assert(ep->use_shm);
@@ -286,75 +376,7 @@ size_t rxr_pkt_req_max_data_size(struct rxr_ep *ep, fi_addr_t addr, int pkt_type
  *
  *     init() functions
  */
-
-/*
- * this function is called after you have set header in pkt_entry->pkt
- */
-void rxr_pkt_data_from_tx(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
-			  struct rxr_tx_entry *tx_entry, size_t data_offset,
-			  size_t data_size)
-{
-	int tx_iov_index;
-	size_t tx_iov_offset;
-	char *data;
-	size_t hdr_size;
-	struct efa_mr *desc;
-
-	assert(pkt_entry->send);
-	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
-	assert(hdr_size > 0);
-	if (data_size == 0) {
-		pkt_entry->send->iov_count = 0;
-		pkt_entry->pkt_size = hdr_size;
-		return;
-	}
-
-	rxr_locate_iov_pos(tx_entry->iov, tx_entry->iov_count, data_offset,
-			   &tx_iov_index, &tx_iov_offset);
-	desc = tx_entry->desc[0];
-	assert(tx_iov_index < tx_entry->iov_count);
-	assert(tx_iov_offset < tx_entry->iov[tx_iov_index].iov_len);
-
-	/*
-	 * We want to go through the bounce-buffers here only when
-	 * one of the following conditions are true:
-	 * 1. The application can not register buffers (no FI_MR_LOCAL)
-	 * 2. desc.peer.iface is anything but FI_HMEM_SYSTEM
-	 * 3. prov/shm is not used for this transfer, and #1 or #2 hold true.
-	 *
-	 * In the first case, we use the pre-registered pkt_entry's MR. In the
-	 * second case, this is for the eager and medium-message protocols which
-	 * can not rendezvous and pull the data from a peer. In the third case,
-	 * the bufpool would not have been created with a registration handler,
-	 * so pkt_entry->mr will be NULL.
-	 *
-	 */
-	if (!tx_entry->desc[tx_iov_index] && pkt_entry->mr) {
-		data = (char *)pkt_entry->pkt + hdr_size;
-		data_size = ofi_copy_from_hmem_iov(data,
-					data_size,
-					desc ? desc->peer.iface : FI_HMEM_SYSTEM,
-					desc ? desc->peer.device.reserved : 0,
-					tx_entry->iov,
-					tx_entry->iov_count,
-					data_offset);
-		pkt_entry->send->iov_count = 0;
-		pkt_entry->pkt_size = hdr_size + data_size;
-		return;
-	}
-
-	assert(ep->core_iov_limit >= 2);
-	pkt_entry->send->iov[0].iov_base = pkt_entry->pkt;
-	pkt_entry->send->iov[0].iov_len = hdr_size;
-	pkt_entry->send->desc[0] = pkt_entry->mr ? fi_mr_desc(pkt_entry->mr) : NULL;
-
-	pkt_entry->send->iov[1].iov_base = (char *)tx_entry->iov[tx_iov_index].iov_base + tx_iov_offset;
-	pkt_entry->send->iov[1].iov_len = MIN(data_size, tx_entry->iov[tx_iov_index].iov_len - tx_iov_offset);
-	pkt_entry->send->desc[1] = tx_entry->desc[tx_iov_index];
-	pkt_entry->send->iov_count = 2;
-	pkt_entry->pkt_size = hdr_size + pkt_entry->send->iov[1].iov_len;
-}
-
+static inline
 void rxr_pkt_init_rtm(struct rxr_ep *ep,
 		      struct rxr_tx_entry *tx_entry,
 		      int pkt_type, uint64_t data_offset,
@@ -370,8 +392,8 @@ void rxr_pkt_init_rtm(struct rxr_ep *ep,
 
 	data_size = MIN(tx_entry->total_len - data_offset,
 			ep->mtu_size - rxr_pkt_req_hdr_size(pkt_entry));
-	rxr_pkt_data_from_tx(ep, pkt_entry, tx_entry, data_offset, data_size);
-	pkt_entry->x_entry = tx_entry;
+	rxr_pkt_init_data_from_tx_entry(ep, pkt_entry, rxr_pkt_req_hdr_size(pkt_entry),
+					tx_entry, data_offset, data_size);
 }
 
 ssize_t rxr_pkt_init_eager_msgrtm(struct rxr_ep *ep,
@@ -391,7 +413,7 @@ ssize_t rxr_pkt_init_dc_eager_msgrtm(struct rxr_ep *ep,
 
 	rxr_pkt_init_rtm(ep, tx_entry, RXR_DC_EAGER_MSGRTM_PKT, 0, pkt_entry);
 	dc_eager_msgrtm_hdr = rxr_get_dc_eager_msgrtm_hdr(pkt_entry->pkt);
-	dc_eager_msgrtm_hdr->hdr.tx_id = tx_entry->tx_id;
+	dc_eager_msgrtm_hdr->hdr.send_id = tx_entry->tx_id;
 	return 0;
 }
 
@@ -422,7 +444,7 @@ ssize_t rxr_pkt_init_dc_eager_tagrtm(struct rxr_ep *ep,
 	rxr_pkt_rtm_settag(pkt_entry, tx_entry->tag);
 
 	dc_eager_tagrtm_hdr = rxr_get_dc_eager_tagrtm_hdr(pkt_entry->pkt);
-	dc_eager_tagrtm_hdr->hdr.tx_id = tx_entry->tx_id;
+	dc_eager_tagrtm_hdr->hdr.send_id = tx_entry->tx_id;
 	return 0;
 }
 
@@ -435,8 +457,8 @@ ssize_t rxr_pkt_init_medium_msgrtm(struct rxr_ep *ep,
 	rxr_pkt_init_rtm(ep, tx_entry, RXR_MEDIUM_MSGRTM_PKT,
 			 tx_entry->bytes_sent, pkt_entry);
 	rtm_hdr = rxr_get_medium_rtm_base_hdr(pkt_entry->pkt);
-	rtm_hdr->data_len = tx_entry->total_len;
-	rtm_hdr->offset = tx_entry->bytes_sent;
+	rtm_hdr->msg_length = tx_entry->total_len;
+	rtm_hdr->seg_offset = tx_entry->bytes_sent;
 	return 0;
 }
 
@@ -450,9 +472,9 @@ ssize_t rxr_pkt_init_dc_medium_msgrtm(struct rxr_ep *ep,
 			 tx_entry->bytes_sent, pkt_entry);
 
 	dc_medium_msgrtm_hdr = rxr_get_dc_medium_msgrtm_hdr(pkt_entry->pkt);
-	dc_medium_msgrtm_hdr->hdr.data_len = tx_entry->total_len;
-	dc_medium_msgrtm_hdr->hdr.offset = tx_entry->bytes_sent;
-	dc_medium_msgrtm_hdr->hdr.tx_id = tx_entry->tx_id;
+	dc_medium_msgrtm_hdr->hdr.msg_length = tx_entry->total_len;
+	dc_medium_msgrtm_hdr->hdr.seg_offset = tx_entry->bytes_sent;
+	dc_medium_msgrtm_hdr->hdr.send_id = tx_entry->tx_id;
 	return 0;
 }
 
@@ -465,8 +487,8 @@ ssize_t rxr_pkt_init_medium_tagrtm(struct rxr_ep *ep,
 	rxr_pkt_init_rtm(ep, tx_entry, RXR_MEDIUM_TAGRTM_PKT,
 			 tx_entry->bytes_sent, pkt_entry);
 	rtm_hdr = rxr_get_medium_rtm_base_hdr(pkt_entry->pkt);
-	rtm_hdr->data_len = tx_entry->total_len;
-	rtm_hdr->offset = tx_entry->bytes_sent;
+	rtm_hdr->msg_length = tx_entry->total_len;
+	rtm_hdr->seg_offset = tx_entry->bytes_sent;
 	rtm_hdr->hdr.flags |= RXR_REQ_TAGGED;
 	rxr_pkt_rtm_settag(pkt_entry, tx_entry->tag);
 	return 0;
@@ -482,87 +504,87 @@ ssize_t rxr_pkt_init_dc_medium_tagrtm(struct rxr_ep *ep,
 			 tx_entry->bytes_sent, pkt_entry);
 
 	dc_medium_tagrtm_hdr = rxr_get_dc_medium_tagrtm_hdr(pkt_entry->pkt);
-	dc_medium_tagrtm_hdr->hdr.data_len = tx_entry->total_len;
-	dc_medium_tagrtm_hdr->hdr.offset = tx_entry->bytes_sent;
+	dc_medium_tagrtm_hdr->hdr.msg_length = tx_entry->total_len;
+	dc_medium_tagrtm_hdr->hdr.seg_offset = tx_entry->bytes_sent;
 	dc_medium_tagrtm_hdr->hdr.hdr.flags |= RXR_REQ_TAGGED;
-	dc_medium_tagrtm_hdr->hdr.tx_id = tx_entry->tx_id;
+	dc_medium_tagrtm_hdr->hdr.send_id = tx_entry->tx_id;
 	rxr_pkt_rtm_settag(pkt_entry, tx_entry->tag);
 	return 0;
 }
 
-void rxr_pkt_init_long_rtm(struct rxr_ep *ep,
+void rxr_pkt_init_longcts_rtm(struct rxr_ep *ep,
 			   struct rxr_tx_entry *tx_entry,
 			   int pkt_type,
 			   struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_long_rtm_base_hdr *rtm_hdr;
+	struct rxr_longcts_rtm_base_hdr *rtm_hdr;
 
 	rxr_pkt_init_rtm(ep, tx_entry, pkt_type, 0, pkt_entry);
-	rtm_hdr = rxr_get_long_rtm_base_hdr(pkt_entry->pkt);
-	rtm_hdr->data_len = tx_entry->total_len;
-	rtm_hdr->tx_id = tx_entry->tx_id;
+	rtm_hdr = rxr_get_longcts_rtm_base_hdr(pkt_entry->pkt);
+	rtm_hdr->msg_length = tx_entry->total_len;
+	rtm_hdr->send_id = tx_entry->tx_id;
 	rtm_hdr->credit_request = tx_entry->credit_request;
 }
 
-ssize_t rxr_pkt_init_long_msgrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_msgrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry)
 {
-	rxr_pkt_init_long_rtm(ep, tx_entry, RXR_LONG_MSGRTM_PKT, pkt_entry);
+	rxr_pkt_init_longcts_rtm(ep, tx_entry, RXR_LONGCTS_MSGRTM_PKT, pkt_entry);
 	return 0;
 }
 
-ssize_t rxr_pkt_init_dc_long_msgrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_dc_longcts_msgrtm(struct rxr_ep *ep,
 				    struct rxr_tx_entry *tx_entry,
 				    struct rxr_pkt_entry *pkt_entry)
 {
-	rxr_pkt_init_long_rtm(ep, tx_entry, RXR_DC_LONG_MSGRTM_PKT, pkt_entry);
+	rxr_pkt_init_longcts_rtm(ep, tx_entry, RXR_DC_LONGCTS_MSGRTM_PKT, pkt_entry);
 	return 0;
 }
 
-ssize_t rxr_pkt_init_long_tagrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_tagrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_base_hdr *base_hdr;
 
-	rxr_pkt_init_long_rtm(ep, tx_entry, RXR_LONG_TAGRTM_PKT, pkt_entry);
+	rxr_pkt_init_longcts_rtm(ep, tx_entry, RXR_LONGCTS_TAGRTM_PKT, pkt_entry);
 	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
 	base_hdr->flags |= RXR_REQ_TAGGED;
 	rxr_pkt_rtm_settag(pkt_entry, tx_entry->tag);
 	return 0;
 }
 
-ssize_t rxr_pkt_init_dc_long_tagrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_dc_longcts_tagrtm(struct rxr_ep *ep,
 				    struct rxr_tx_entry *tx_entry,
 				    struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_base_hdr *base_hdr;
 
-	rxr_pkt_init_long_rtm(ep, tx_entry, RXR_DC_LONG_TAGRTM_PKT, pkt_entry);
+	rxr_pkt_init_longcts_rtm(ep, tx_entry, RXR_DC_LONGCTS_TAGRTM_PKT, pkt_entry);
 	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
 	base_hdr->flags |= RXR_REQ_TAGGED;
 	rxr_pkt_rtm_settag(pkt_entry, tx_entry->tag);
 	return 0;
 }
 
-ssize_t rxr_pkt_init_read_rtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_rtm(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      int pkt_type,
 			      struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_read_rtm_base_hdr *rtm_hdr;
+	struct rxr_longread_rtm_base_hdr *rtm_hdr;
 	struct fi_rma_iov *read_iov;
 	size_t hdr_size;
 	int err;
 
 	rxr_pkt_init_req_hdr(ep, tx_entry, pkt_type, pkt_entry);
 
-	rtm_hdr = rxr_get_read_rtm_base_hdr(pkt_entry->pkt);
+	rtm_hdr = rxr_get_longread_rtm_base_hdr(pkt_entry->pkt);
 	rtm_hdr->hdr.flags |= RXR_REQ_MSG;
 	rtm_hdr->hdr.msg_id = tx_entry->msg_id;
-	rtm_hdr->data_len = tx_entry->total_len;
-	rtm_hdr->tx_id = tx_entry->tx_id;
+	rtm_hdr->msg_length = tx_entry->total_len;
+	rtm_hdr->send_id = tx_entry->tx_id;
 	rtm_hdr->read_iov_count = tx_entry->iov_count;
 
 	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
@@ -575,21 +597,21 @@ ssize_t rxr_pkt_init_read_rtm(struct rxr_ep *ep,
 	return 0;
 }
 
-ssize_t rxr_pkt_init_read_msgrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_msgrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry)
 {
-	return rxr_pkt_init_read_rtm(ep, tx_entry, RXR_READ_MSGRTM_PKT, pkt_entry);
+	return rxr_pkt_init_longread_rtm(ep, tx_entry, RXR_LONGREAD_MSGRTM_PKT, pkt_entry);
 }
 
-ssize_t rxr_pkt_init_read_tagrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_tagrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry)
 {
 	ssize_t err;
 	struct rxr_base_hdr *base_hdr;
 
-	err = rxr_pkt_init_read_rtm(ep, tx_entry, RXR_READ_TAGRTM_PKT, pkt_entry);
+	err = rxr_pkt_init_longread_rtm(ep, tx_entry, RXR_LONGREAD_TAGRTM_PKT, pkt_entry);
 	if (err)
 		return err;
 
@@ -615,7 +637,7 @@ void rxr_pkt_handle_medium_rtm_sent(struct rxr_ep *ep,
 	tx_entry->bytes_sent += rxr_pkt_req_data_size(pkt_entry);
 }
 
-void rxr_pkt_handle_long_rtm_sent(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtm_sent(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_tx_entry *tx_entry;
@@ -657,7 +679,7 @@ void rxr_pkt_handle_medium_rtm_send_completion(struct rxr_ep *ep,
 		rxr_cq_handle_tx_completion(ep, tx_entry);
 }
 
-void rxr_pkt_handle_long_rtm_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtm_send_completion(struct rxr_ep *ep,
 					     struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_tx_entry *tx_entry;
@@ -668,7 +690,7 @@ void rxr_pkt_handle_long_rtm_send_completion(struct rxr_ep *ep,
 		rxr_cq_handle_tx_completion(ep, tx_entry);
 }
 
-void rxr_pkt_handle_dc_long_rtm_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_dc_longcts_rtm_send_completion(struct rxr_ep *ep,
 						struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_tx_entry *tx_entry;
@@ -696,18 +718,18 @@ size_t rxr_pkt_rtm_total_len(struct rxr_pkt_entry *pkt_entry)
 		return rxr_pkt_req_data_size(pkt_entry);
 	case RXR_MEDIUM_MSGRTM_PKT:
 	case RXR_MEDIUM_TAGRTM_PKT:
-		return rxr_get_medium_rtm_base_hdr(pkt_entry->pkt)->data_len;
+		return rxr_get_medium_rtm_base_hdr(pkt_entry->pkt)->msg_length;
 	case RXR_DC_MEDIUM_MSGRTM_PKT:
 	case RXR_DC_MEDIUM_TAGRTM_PKT:
-		return rxr_get_dc_medium_rtm_base_hdr(pkt_entry->pkt)->data_len;
-	case RXR_LONG_MSGRTM_PKT:
-	case RXR_LONG_TAGRTM_PKT:
-	case RXR_DC_LONG_MSGRTM_PKT:
-	case RXR_DC_LONG_TAGRTM_PKT:
-		return rxr_get_long_rtm_base_hdr(pkt_entry->pkt)->data_len;
-	case RXR_READ_MSGRTM_PKT:
-	case RXR_READ_TAGRTM_PKT:
-		return rxr_get_read_rtm_base_hdr(pkt_entry->pkt)->data_len;
+		return rxr_get_dc_medium_rtm_base_hdr(pkt_entry->pkt)->msg_length;
+	case RXR_LONGCTS_MSGRTM_PKT:
+	case RXR_LONGCTS_TAGRTM_PKT:
+	case RXR_DC_LONGCTS_MSGRTM_PKT:
+	case RXR_DC_LONGCTS_TAGRTM_PKT:
+		return rxr_get_longcts_rtm_base_hdr(pkt_entry->pkt)->msg_length;
+	case RXR_LONGREAD_MSGRTM_PKT:
+	case RXR_LONGREAD_TAGRTM_PKT:
+		return rxr_get_longread_rtm_base_hdr(pkt_entry->pkt)->msg_length;
 	default:
 		assert(0 && "Unknown REQ packet type\n");
 	}
@@ -715,8 +737,25 @@ size_t rxr_pkt_rtm_total_len(struct rxr_pkt_entry *pkt_entry)
 	return 0;
 }
 
-void rxr_pkt_rtm_init_rx_entry(struct rxr_pkt_entry *pkt_entry,
-			       struct rxr_rx_entry *rx_entry)
+/*
+ * @brief Update rx_entry with the following information in RTM packet entry.
+ *            address:       this is necessary because original address in
+ *                           rx_entry can be FI_ADDR_UNSPEC
+ *            cq_entry.data: for FI_REMOTE_CQ_DATA
+ *            msg_id:        message id
+ *            total_len:     application might provide a buffer that is larger
+ *                           then incoming message size.
+ *            tag:           sender's tag can be different from receiver's tag
+ *                           becuase match only requires
+ *                           (sender_tag | ignore) == (receiver_tag | ignore)
+ *        This function is applied to both unexpected rx_entry (when they are
+ *        allocated) and expected rx_entry (when they are matched to a RTM)
+ *
+ * @param pkt_entry(input)  RTM packet entry
+ * @param rx_entry(input)   rx entry to be updated
+ */
+void rxr_pkt_rtm_update_rx_entry(struct rxr_pkt_entry *pkt_entry,
+				 struct rxr_rx_entry *rx_entry)
 {
 	struct rxr_base_hdr *base_hdr;
 
@@ -743,8 +782,7 @@ struct rxr_rx_entry *rxr_pkt_get_rtm_matched_rx_entry(struct rxr_ep *ep,
 	assert(match);
 	rx_entry = container_of(match, struct rxr_rx_entry, entry);
 	if (rx_entry->rxr_flags & RXR_MULTI_RECV_POSTED) {
-		rx_entry = rxr_ep_split_rx_entry(ep, rx_entry,
-						 NULL, pkt_entry);
+		rx_entry = rxr_msg_split_rx_entry(ep, rx_entry, NULL, pkt_entry);
 		if (OFI_UNLIKELY(!rx_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"RX entries exhausted.\n");
@@ -752,7 +790,7 @@ struct rxr_rx_entry *rxr_pkt_get_rtm_matched_rx_entry(struct rxr_ep *ep,
 			return NULL;
 		}
 	} else {
-		rxr_pkt_rtm_init_rx_entry(pkt_entry, rx_entry);
+		rxr_pkt_rtm_update_rx_entry(pkt_entry, rx_entry);
 	}
 
 	rx_entry->state = RXR_RX_MATCHED;
@@ -818,6 +856,20 @@ struct rxr_rx_entry *rxr_pkt_get_msgrtm_rx_entry(struct rxr_ep *ep,
 	dlist_func_t *match_func;
 	int pkt_type;
 
+	if ((*pkt_entry_ptr)->alloc_type == RXR_PKT_FROM_USER_BUFFER) {
+		/* If a pkt_entry is constructred from user supplied buffer,
+		 * the endpoint must be in zero copy receive mode.
+		 */
+		assert(ep->use_zcpy_rx);
+		/* In this mode, an rx_entry is always created together
+		 * with this pkt_entry, and pkt_entry->x_entry is pointing
+		 * to it. Thus we can skip the matching process, and return
+		 * pkt_entry->x_entry right away.
+		 */
+		assert((*pkt_entry_ptr)->x_entry);
+		return (*pkt_entry_ptr)->x_entry;
+	}
+
 	if (ep->util_ep.caps & FI_DIRECTED_RECV)
 		match_func = &rxr_pkt_rtm_match_recv;
 	else
@@ -827,10 +879,10 @@ struct rxr_rx_entry *rxr_pkt_get_msgrtm_rx_entry(struct rxr_ep *ep,
 	                               *pkt_entry_ptr);
 	if (OFI_UNLIKELY(!match)) {
 		/*
-		 * rxr_ep_alloc_unexp_rx_entry_for_msgrtm() might release pkt_entry,
+		 * rxr_msg_alloc_unexp_rx_entry_for_msgrtm() might release pkt_entry,
 		 * thus we have to use pkt_entry_ptr here
 		 */
-		rx_entry = rxr_ep_alloc_unexp_rx_entry_for_msgrtm(ep, pkt_entry_ptr);
+		rx_entry = rxr_msg_alloc_unexp_rx_entry_for_msgrtm(ep, pkt_entry_ptr);
 		if (OFI_UNLIKELY(!rx_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"RX entries exhausted.\n");
@@ -868,10 +920,10 @@ struct rxr_rx_entry *rxr_pkt_get_tagrtm_rx_entry(struct rxr_ep *ep,
 	                               *pkt_entry_ptr);
 	if (OFI_UNLIKELY(!match)) {
 		/*
-		 * rxr_ep_alloc_unexp_rx_entry_for_tagrtm() might release pkt_entry,
+		 * rxr_msg_alloc_unexp_rx_entry_for_tagrtm() might release pkt_entry,
 		 * thus we have to use pkt_entry_ptr here
 		 */
-		rx_entry = rxr_ep_alloc_unexp_rx_entry_for_tagrtm(ep, pkt_entry_ptr);
+		rx_entry = rxr_msg_alloc_unexp_rx_entry_for_tagrtm(ep, pkt_entry_ptr);
 		if (OFI_UNLIKELY(!rx_entry)) {
 			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
 			return NULL;
@@ -888,17 +940,17 @@ struct rxr_rx_entry *rxr_pkt_get_tagrtm_rx_entry(struct rxr_ep *ep,
 	return rx_entry;
 }
 
-ssize_t rxr_pkt_proc_matched_read_rtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_proc_matched_longread_rtm(struct rxr_ep *ep,
 				      struct rxr_rx_entry *rx_entry,
 				      struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_read_rtm_base_hdr *rtm_hdr;
+	struct rxr_longread_rtm_base_hdr *rtm_hdr;
 	struct fi_rma_iov *read_iov;
 
-	rtm_hdr = rxr_get_read_rtm_base_hdr(pkt_entry->pkt);
+	rtm_hdr = rxr_get_longread_rtm_base_hdr(pkt_entry->pkt);
 	read_iov = (struct fi_rma_iov *)((char *)pkt_entry->pkt + rxr_pkt_req_hdr_size(pkt_entry));
 
-	rx_entry->tx_id = rtm_hdr->tx_id;
+	rx_entry->tx_id = rtm_hdr->send_id;
 	rx_entry->rma_iov_count = rtm_hdr->read_iov_count;
 	memcpy(rx_entry->rma_iov, read_iov,
 	       rx_entry->rma_iov_count * sizeof(struct fi_rma_iov));
@@ -908,7 +960,7 @@ ssize_t rxr_pkt_proc_matched_read_rtm(struct rxr_ep *ep,
 	/* truncate rx_entry->iov to save memory registration pages because we
 	 * need to do memory registration for the receiving buffer.
 	 */
-	ofi_truncate_iov(rx_entry->iov, &rx_entry->iov_count, rx_entry->total_len);
+	ofi_truncate_iov(rx_entry->iov, &rx_entry->iov_count, rx_entry->total_len + ep->msg_prefix_size);
 	return rxr_read_post_remote_read_or_queue(ep, RXR_RX_ENTRY, rx_entry);
 }
 
@@ -927,25 +979,25 @@ ssize_t rxr_pkt_proc_matched_medium_rtm(struct rxr_ep *ep,
 		hdr_size = rxr_pkt_req_hdr_size(cur);
 		data = (char *)cur->pkt + hdr_size;
 		if (rx_entry->rxr_flags & RXR_DELIVERY_COMPLETE_REQUESTED)
-			offset = rxr_get_dc_medium_rtm_base_hdr(cur->pkt)->offset;
+			offset = rxr_get_dc_medium_rtm_base_hdr(cur->pkt)->seg_offset;
 		else
-			offset = rxr_get_medium_rtm_base_hdr(cur->pkt)->offset;
+			offset = rxr_get_medium_rtm_base_hdr(cur->pkt)->seg_offset;
 		data_size = cur->pkt_size - hdr_size;
 
-		/* rxr_pkt_copy_to_rx() can release rx_entry, so
+		/* rxr_pkt_copy_data_to_rx_entry() can release rx_entry, so
 		 * bytes_received must be calculated before it.
 		 */
 		rx_entry->bytes_received += data_size;
 		if (rx_entry->total_len == rx_entry->bytes_received)
 			rxr_pkt_rx_map_remove(ep, cur, rx_entry);
 
-		/* rxr_pkt_copy_to_rx() will release cur, so
+		/* rxr_pkt_copy_data_to_rx_entry() will release cur, so
 		 * cur->next must be copied out before it.
 		 */
 		nxt = cur->next;
 		cur->next = NULL;
 
-		err = rxr_pkt_copy_to_rx(ep, rx_entry, offset, cur, data, data_size);
+		err = rxr_pkt_copy_data_to_rx_entry(ep, rx_entry, offset, cur, data, data_size);
 		if (err) {
 			rxr_pkt_entry_release_rx(ep, cur);
 			ret = err;
@@ -957,6 +1009,71 @@ ssize_t rxr_pkt_proc_matched_medium_rtm(struct rxr_ep *ep,
 	return ret;
 }
 
+/**
+ * @brief process a matched eager rtm packet entry
+ *
+ * For an eager message, it will write rx completion,
+ * release packet entry and rx_entry.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	rx_entry	RX entry
+ * @param[in]	pkt_entry	packet entry
+ * @return	On success, return 0
+ * 		On failure, return libfabric error code
+ */
+ssize_t rxr_pkt_proc_matched_eager_rtm(struct rxr_ep *ep,
+				       struct rxr_rx_entry *rx_entry,
+				       struct rxr_pkt_entry *pkt_entry)
+{
+	int err;
+	char *data;
+	size_t hdr_size, data_size;
+
+	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
+
+	if (pkt_entry->alloc_type != RXR_PKT_FROM_USER_BUFFER) {
+		data = (char *)pkt_entry->pkt + hdr_size;
+		data_size = pkt_entry->pkt_size - hdr_size;
+
+		/*
+		 * On success, rxr_pkt_copy_data_to_rx_entry will write rx completion,
+		 * release pkt_entry and rx_entry
+		 */
+		err = rxr_pkt_copy_data_to_rx_entry(ep, rx_entry, 0, pkt_entry, data, data_size);
+		if (err)
+			rxr_pkt_entry_release_rx(ep, pkt_entry);
+
+		return err;
+	}
+
+	/* In this case, data is already in user provided buffer, so no need
+	 * to copy. However, we do need to make sure the packet header length
+	 * is correct. Otherwise, user will get wrong data.
+	 *
+	 * The expected header size is
+	 * 	ep->msg_prefix_size - sizeof(struct rxr_pkt_entry)
+	 * because we used the first sizeof(struct rxr_pkt_entry) to construct
+	 * a pkt_entry.
+	 */
+	if (hdr_size != ep->msg_prefix_size - sizeof(struct rxr_pkt_entry)) {
+		/* if header size is wrong, the data in user buffer is not useful.
+		 * setting rx_entry->cq_entry.len here will cause an error cq entry
+		 * to be written to application.
+		 */
+		rx_entry->cq_entry.len = 0;
+	} else {
+		assert(rx_entry->cq_entry.buf == pkt_entry->pkt - sizeof(struct rxr_pkt_entry));
+		rx_entry->cq_entry.len = pkt_entry->pkt_size + sizeof(struct rxr_pkt_entry);
+	}
+
+	rxr_cq_write_rx_completion(ep, rx_entry);
+	rxr_release_rx_entry(ep, rx_entry);
+
+	/* no need to release packet entry because it is
+	 * constructed using user supplied buffer */
+	return 0;
+}
+
 ssize_t rxr_pkt_proc_matched_rtm(struct rxr_ep *ep,
 				 struct rxr_rx_entry *rx_entry,
 				 struct rxr_pkt_entry *pkt_entry)
@@ -968,6 +1085,13 @@ ssize_t rxr_pkt_proc_matched_rtm(struct rxr_ep *ep,
 
 	assert(rx_entry->state == RXR_RX_MATCHED);
 
+	if (!rx_entry->peer) {
+		rx_entry->addr = pkt_entry->addr;
+		rx_entry->peer = rxr_ep_get_peer(ep, rx_entry->addr);
+		assert(rx_entry->peer);
+		dlist_insert_tail(&rx_entry->peer_entry, &rx_entry->peer->rx_entry_list);
+	}
+
 	/* Adjust rx_entry->cq_entry.len as needed.
 	 * Initialy rx_entry->cq_entry.len is total recv buffer size.
 	 * rx_entry->total_len is from REQ packet and is total send buffer size.
@@ -984,23 +1108,23 @@ ssize_t rxr_pkt_proc_matched_rtm(struct rxr_ep *ep,
 	    pkt_type < RXR_DC_REQ_PKT_END)
 		rx_entry->rxr_flags |= RXR_DELIVERY_COMPLETE_REQUESTED;
 
-	if (pkt_type == RXR_LONG_MSGRTM_PKT ||
-	    pkt_type == RXR_LONG_TAGRTM_PKT)
-		rx_entry->tx_id = rxr_get_long_rtm_base_hdr(pkt_entry->pkt)->tx_id;
+	if (pkt_type == RXR_LONGCTS_MSGRTM_PKT ||
+	    pkt_type == RXR_LONGCTS_TAGRTM_PKT)
+		rx_entry->tx_id = rxr_get_longcts_rtm_base_hdr(pkt_entry->pkt)->send_id;
 	else if (pkt_type == RXR_DC_EAGER_MSGRTM_PKT ||
 		 pkt_type == RXR_DC_EAGER_TAGRTM_PKT)
-		rx_entry->tx_id = rxr_get_dc_eager_rtm_base_hdr(pkt_entry->pkt)->tx_id;
+		rx_entry->tx_id = rxr_get_dc_eager_rtm_base_hdr(pkt_entry->pkt)->send_id;
 	else if (pkt_type == RXR_DC_MEDIUM_MSGRTM_PKT ||
 		 pkt_type == RXR_DC_MEDIUM_TAGRTM_PKT)
-		rx_entry->tx_id = rxr_get_dc_medium_rtm_base_hdr(pkt_entry->pkt)->tx_id;
-	else if (pkt_type == RXR_DC_LONG_MSGRTM_PKT ||
-		 pkt_type == RXR_DC_LONG_TAGRTM_PKT)
-		rx_entry->tx_id = rxr_get_long_rtm_base_hdr(pkt_entry->pkt)->tx_id;
+		rx_entry->tx_id = rxr_get_dc_medium_rtm_base_hdr(pkt_entry->pkt)->send_id;
+	else if (pkt_type == RXR_DC_LONGCTS_MSGRTM_PKT ||
+		 pkt_type == RXR_DC_LONGCTS_TAGRTM_PKT)
+		rx_entry->tx_id = rxr_get_longcts_rtm_base_hdr(pkt_entry->pkt)->send_id;
 
 	rx_entry->msg_id = rxr_get_rtm_base_hdr(pkt_entry->pkt)->msg_id;
 
-	if (pkt_type == RXR_READ_MSGRTM_PKT || pkt_type == RXR_READ_TAGRTM_PKT)
-		return rxr_pkt_proc_matched_read_rtm(ep, rx_entry, pkt_entry);
+	if (pkt_type == RXR_LONGREAD_MSGRTM_PKT || pkt_type == RXR_LONGREAD_TAGRTM_PKT)
+		return rxr_pkt_proc_matched_longread_rtm(ep, rx_entry, pkt_entry);
 
 	if (pkt_type == RXR_MEDIUM_MSGRTM_PKT ||
 	    pkt_type == RXR_MEDIUM_TAGRTM_PKT ||
@@ -1008,35 +1132,30 @@ ssize_t rxr_pkt_proc_matched_rtm(struct rxr_ep *ep,
 	    pkt_type == RXR_DC_MEDIUM_TAGRTM_PKT)
 		return rxr_pkt_proc_matched_medium_rtm(ep, rx_entry, pkt_entry);
 
+	if (pkt_type == RXR_EAGER_MSGRTM_PKT ||
+	    pkt_type == RXR_EAGER_TAGRTM_PKT ||
+	    pkt_type == RXR_DC_EAGER_MSGRTM_PKT ||
+	    pkt_type == RXR_DC_EAGER_TAGRTM_PKT) {
+		return rxr_pkt_proc_matched_eager_rtm(ep, rx_entry, pkt_entry);
+	}
+
 	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
 	data = (char *)pkt_entry->pkt + hdr_size;
 	data_size = pkt_entry->pkt_size - hdr_size;
 
 	rx_entry->bytes_received += data_size;
-	ret = rxr_pkt_copy_to_rx(ep, rx_entry, 0, pkt_entry, data, data_size);
+	ret = rxr_pkt_copy_data_to_rx_entry(ep, rx_entry, 0, pkt_entry, data, data_size);
 	if (ret) {
-		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return ret;
 	}
-
-	if (pkt_type == RXR_EAGER_MSGRTM_PKT ||
-	    pkt_type == RXR_EAGER_TAGRTM_PKT ||
-	    pkt_type == RXR_DC_EAGER_MSGRTM_PKT ||
-	    pkt_type == RXR_DC_EAGER_TAGRTM_PKT) {
-		ret = 0;
-	} else {
-		/*
-		 * long message protocol
-		 */
 #if ENABLE_DEBUG
-		dlist_insert_tail(&rx_entry->rx_pending_entry, &ep->rx_pending_list);
-		ep->rx_pending++;
+	dlist_insert_tail(&rx_entry->rx_pending_entry, &ep->rx_pending_list);
+	ep->rx_pending++;
 #endif
-		rx_entry->state = RXR_RX_RECV;
-		/* we have noticed using the default value achieve better bandwidth */
-		rx_entry->credit_request = rxr_env.tx_min_credits;
-		ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
-	}
+	rx_entry->state = RXR_RX_RECV;
+	/* we have noticed using the default value achieve better bandwidth */
+	rx_entry->credit_request = rxr_env.tx_min_credits;
+	ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
 
 	return ret;
 }
@@ -1057,10 +1176,7 @@ ssize_t rxr_pkt_proc_msgrtm(struct rxr_ep *ep,
 	if (rx_entry->state == RXR_RX_MATCHED) {
 		err = rxr_pkt_proc_matched_rtm(ep, rx_entry, pkt_entry);
 		if (OFI_UNLIKELY(err)) {
-			if (rxr_cq_handle_rx_error(ep, rx_entry, err)) {
-				assert(0 && "cannot write cq error entry");
-				efa_eq_write_error(&ep->util_ep, -err, err);
-			}
+			rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 			rxr_pkt_entry_release_rx(ep, pkt_entry);
 			rxr_release_rx_entry(ep, rx_entry);
 			return err;
@@ -1086,10 +1202,7 @@ ssize_t rxr_pkt_proc_tagrtm(struct rxr_ep *ep,
 	if (rx_entry->state == RXR_RX_MATCHED) {
 		err = rxr_pkt_proc_matched_rtm(ep, rx_entry, pkt_entry);
 		if (OFI_UNLIKELY(err)) {
-			if (rxr_cq_handle_rx_error(ep, rx_entry, err)) {
-				assert(0 && "cannot write error cq entry");
-				efa_eq_write_error(&ep->util_ep, -err, err);
-			}
+			rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 			rxr_pkt_entry_release_rx(ep, pkt_entry);
 			rxr_release_rx_entry(ep, rx_entry);
 			return err;
@@ -1113,19 +1226,19 @@ ssize_t rxr_pkt_proc_rtm_rta(struct rxr_ep *ep,
 	switch (base_hdr->type) {
 	case RXR_EAGER_MSGRTM_PKT:
 	case RXR_MEDIUM_MSGRTM_PKT:
-	case RXR_LONG_MSGRTM_PKT:
-	case RXR_READ_MSGRTM_PKT:
+	case RXR_LONGCTS_MSGRTM_PKT:
+	case RXR_LONGREAD_MSGRTM_PKT:
 	case RXR_DC_EAGER_MSGRTM_PKT:
 	case RXR_DC_MEDIUM_MSGRTM_PKT:
-	case RXR_DC_LONG_MSGRTM_PKT:
+	case RXR_DC_LONGCTS_MSGRTM_PKT:
 		return rxr_pkt_proc_msgrtm(ep, pkt_entry);
 	case RXR_EAGER_TAGRTM_PKT:
 	case RXR_MEDIUM_TAGRTM_PKT:
-	case RXR_LONG_TAGRTM_PKT:
-	case RXR_READ_TAGRTM_PKT:
+	case RXR_LONGCTS_TAGRTM_PKT:
+	case RXR_LONGREAD_TAGRTM_PKT:
 	case RXR_DC_EAGER_TAGRTM_PKT:
 	case RXR_DC_MEDIUM_TAGRTM_PKT:
-	case RXR_DC_LONG_TAGRTM_PKT:
+	case RXR_DC_LONGCTS_TAGRTM_PKT:
 		return rxr_pkt_proc_tagrtm(ep, pkt_entry);
 	case RXR_WRITE_RTA_PKT:
 		return rxr_pkt_proc_write_rta(ep, pkt_entry);
@@ -1139,65 +1252,18 @@ ssize_t rxr_pkt_proc_rtm_rta(struct rxr_ep *ep,
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 			"Unknown packet type ID: %d\n",
 		       base_hdr->type);
-		if (rxr_cq_handle_cq_error(ep, -FI_EINVAL))
-			assert(0 && "failed to write err cq entry");
-	}
-
-	return -FI_EINVAL;
-}
-
-void rxr_pkt_handle_zcpy_recv(struct rxr_ep *ep,
-			      struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rx_entry *rx_entry;
-
-	struct rxr_base_hdr *base_hdr __attribute__((unused));
-	base_hdr = rxr_get_base_hdr(pkt_entry->pkt);
-	assert(base_hdr->type >= RXR_BASELINE_REQ_PKT_BEGIN);
-	assert(base_hdr->type != RXR_MEDIUM_MSGRTM_PKT);
-	assert(base_hdr->type != RXR_MEDIUM_TAGRTM_PKT);
-	assert(base_hdr->type != RXR_DC_MEDIUM_MSGRTM_PKT);
-	assert(base_hdr->type != RXR_DC_MEDIUM_MSGRTM_PKT);
-	assert(pkt_entry->type == RXR_PKT_ENTRY_USER);
-
-	rx_entry = rxr_pkt_get_msgrtm_rx_entry(ep, &pkt_entry);
-	if (OFI_UNLIKELY(!rx_entry)) {
-		efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
+		efa_eq_write_error(&ep->util_ep, FI_EINVAL, FI_EINVAL);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
-		return;
 	}
-	pkt_entry->x_entry = rx_entry;
-	if (rx_entry->state != RXR_RX_MATCHED)
-		return;
-
-	/*
-	 * The incoming receive will always get matched to the first posted
-	 * rx_entry available, so this is a constant cost. No real tag or
-	 * address matching happens.
-	 */
-	assert(rx_entry->state == RXR_RX_MATCHED);
-
-	/*
-	 * Adjust rx_entry->cq_entry.len as needed.
-	 * Initialy rx_entry->cq_entry.len is total recv buffer size.
-	 * rx_entry->total_len is from REQ packet and is total send buffer size.
-	 * if send buffer size < recv buffer size, we adjust value of rx_entry->cq_entry.len
-	 * if send buffer size > recv buffer size, we have a truncated message and will
-	 * write error CQ entry.
-	 */
-	if (rx_entry->cq_entry.len > rx_entry->total_len)
-		rx_entry->cq_entry.len = rx_entry->total_len;
 
-	rxr_cq_write_rx_completion(ep, rx_entry);
-	rxr_pkt_entry_release_rx(ep, pkt_entry);
-	rxr_release_rx_entry(ep, rx_entry);
+	return -FI_EINVAL;
 }
 
 void rxr_pkt_handle_rtm_rta_recv(struct rxr_ep *ep,
 				 struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_base_hdr *base_hdr;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	bool need_ordering;
 	int ret, msg_id;
 
@@ -1227,6 +1293,7 @@ void rxr_pkt_handle_rtm_rta_recv(struct rxr_ep *ep,
 
 	need_ordering = false;
 	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
 
 	if (!peer->is_local) {
 		/*
@@ -1257,7 +1324,7 @@ void rxr_pkt_handle_rtm_rta_recv(struct rxr_ep *ep,
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 			"Invalid msg_id: %" PRIu32
 			" robuf->exp_msg_id: %" PRIu32 "\n",
-		       msg_id, peer->robuf->exp_msg_id);
+		       msg_id, peer->robuf.exp_msg_id);
 		efa_eq_write_error(&ep->util_ep, FI_EIO, ret);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
@@ -1286,7 +1353,7 @@ void rxr_pkt_handle_rtm_rta_recv(struct rxr_ep *ep,
 	if (OFI_UNLIKELY(ret))
 		return;
 
-	ofi_recvwin_slide(peer->robuf);
+	ofi_recvwin_slide((&peer->robuf));
 	/* process pending items in reorder buff */
 	rxr_cq_proc_pending_items_in_recvwin(ep, peer);
 }
@@ -1297,9 +1364,8 @@ void rxr_pkt_handle_rtm_rta_recv(struct rxr_ep *ep,
 void rxr_pkt_init_rtw_data(struct rxr_ep *ep,
 			   struct rxr_tx_entry *tx_entry,
 			   struct rxr_pkt_entry *pkt_entry,
-			   struct fi_rma_iov *rma_iov)
+			   struct efa_rma_iov *rma_iov)
 {
-	char *data;
 	size_t hdr_size;
 	size_t data_size;
 	int i;
@@ -1311,12 +1377,8 @@ void rxr_pkt_init_rtw_data(struct rxr_ep *ep,
 	}
 
 	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
-	data = (char *)pkt_entry->pkt + hdr_size;
-	data_size = ofi_copy_from_iov(data, ep->mtu_size - hdr_size,
-				      tx_entry->iov, tx_entry->iov_count, 0);
-
-	pkt_entry->pkt_size = hdr_size + data_size;
-	pkt_entry->x_entry = tx_entry;
+	data_size = MIN(ep->mtu_size - hdr_size, tx_entry->total_len);
+	rxr_pkt_init_data_from_tx_entry(ep, pkt_entry, hdr_size, tx_entry, 0, data_size);
 }
 
 ssize_t rxr_pkt_init_eager_rtw(struct rxr_ep *ep,
@@ -1346,72 +1408,73 @@ ssize_t rxr_pkt_init_dc_eager_rtw(struct rxr_ep *ep,
 	dc_eager_rtw_hdr->rma_iov_count = tx_entry->rma_iov_count;
 	rxr_pkt_init_req_hdr(ep, tx_entry, RXR_DC_EAGER_RTW_PKT, pkt_entry);
 	rxr_pkt_init_rtw_data(ep, tx_entry, pkt_entry,
-			      (struct fi_rma_iov *)dc_eager_rtw_hdr->rma_iov);
-	dc_eager_rtw_hdr->tx_id = tx_entry->tx_id;
+			      dc_eager_rtw_hdr->rma_iov);
+	dc_eager_rtw_hdr->send_id = tx_entry->tx_id;
 	return 0;
 }
 
-static inline void rxr_pkt_init_long_rtw_hdr(struct rxr_ep *ep,
+static inline void rxr_pkt_init_longcts_rtw_hdr(struct rxr_ep *ep,
 					     struct rxr_tx_entry *tx_entry,
 					     struct rxr_pkt_entry *pkt_entry,
 					     int pkt_type)
 {
-	struct rxr_long_rtw_hdr *rtw_hdr;
+	struct rxr_longcts_rtw_hdr *rtw_hdr;
 
-	rtw_hdr = (struct rxr_long_rtw_hdr *)pkt_entry->pkt;
+	rtw_hdr = (struct rxr_longcts_rtw_hdr *)pkt_entry->pkt;
 	rtw_hdr->rma_iov_count = tx_entry->rma_iov_count;
-	rtw_hdr->data_len = tx_entry->total_len;
-	rtw_hdr->tx_id = tx_entry->tx_id;
+	rtw_hdr->msg_length = tx_entry->total_len;
+	rtw_hdr->send_id = tx_entry->tx_id;
 	rtw_hdr->credit_request = tx_entry->credit_request;
 	rxr_pkt_init_req_hdr(ep, tx_entry, pkt_type, pkt_entry);
 }
 
-ssize_t rxr_pkt_init_long_rtw(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_rtw(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_long_rtw_hdr *rtw_hdr;
+	struct rxr_longcts_rtw_hdr *rtw_hdr;
 
 	assert(tx_entry->op == ofi_op_write);
 
-	rtw_hdr = (struct rxr_long_rtw_hdr *)pkt_entry->pkt;
-	rxr_pkt_init_long_rtw_hdr(ep, tx_entry, pkt_entry, RXR_LONG_RTW_PKT);
+	rtw_hdr = (struct rxr_longcts_rtw_hdr *)pkt_entry->pkt;
+	rxr_pkt_init_longcts_rtw_hdr(ep, tx_entry, pkt_entry, RXR_LONGCTS_RTW_PKT);
 	rxr_pkt_init_rtw_data(ep, tx_entry, pkt_entry, rtw_hdr->rma_iov);
 	return 0;
 }
 
-ssize_t rxr_pkt_init_dc_long_rtw(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_dc_longcts_rtw(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_long_rtw_hdr *rtw_hdr;
+	struct rxr_longcts_rtw_hdr *rtw_hdr;
 
 	assert(tx_entry->op == ofi_op_write);
 
-	rtw_hdr = (struct rxr_long_rtw_hdr *)pkt_entry->pkt;
-	rxr_pkt_init_long_rtw_hdr(ep, tx_entry, pkt_entry, RXR_DC_LONG_RTW_PKT);
+	rtw_hdr = (struct rxr_longcts_rtw_hdr *)pkt_entry->pkt;
+	rxr_pkt_init_longcts_rtw_hdr(ep, tx_entry, pkt_entry, RXR_DC_LONGCTS_RTW_PKT);
 	rxr_pkt_init_rtw_data(ep, tx_entry, pkt_entry, rtw_hdr->rma_iov);
 
 	return 0;
 }
 
-ssize_t rxr_pkt_init_read_rtw(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_rtw(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry)
 {
-	struct rxr_read_rtw_hdr *rtw_hdr;
-	struct fi_rma_iov *rma_iov, *read_iov;
+	struct rxr_longread_rtw_hdr *rtw_hdr;
+	struct efa_rma_iov *rma_iov;
+	struct fi_rma_iov *read_iov;
 	size_t hdr_size;
 	int i, err;
 
 	assert(tx_entry->op == ofi_op_write);
 
-	rtw_hdr = (struct rxr_read_rtw_hdr *)pkt_entry->pkt;
+	rtw_hdr = (struct rxr_longread_rtw_hdr *)pkt_entry->pkt;
 	rtw_hdr->rma_iov_count = tx_entry->rma_iov_count;
-	rtw_hdr->data_len = tx_entry->total_len;
-	rtw_hdr->tx_id = tx_entry->tx_id;
+	rtw_hdr->msg_length = tx_entry->total_len;
+	rtw_hdr->send_id = tx_entry->tx_id;
 	rtw_hdr->read_iov_count = tx_entry->iov_count;
-	rxr_pkt_init_req_hdr(ep, tx_entry, RXR_READ_RTW_PKT, pkt_entry);
+	rxr_pkt_init_req_hdr(ep, tx_entry, RXR_LONGREAD_RTW_PKT, pkt_entry);
 
 	rma_iov = rtw_hdr->rma_iov;
 	for (i = 0; i < tx_entry->rma_iov_count; ++i) {
@@ -1426,16 +1489,16 @@ ssize_t rxr_pkt_init_read_rtw(struct rxr_ep *ep,
 	if (OFI_UNLIKELY(err))
 		return err;
 
-	pkt_entry->pkt_size = hdr_size + tx_entry->iov_count * sizeof(struct fi_rma_iov);
+	pkt_entry->pkt_size = hdr_size + tx_entry->iov_count * sizeof(struct efa_rma_iov);
 	return 0;
 }
 
 /*
  *     handle_sent() functions for RTW packet types
  *
- *         rxr_pkt_handle_long_rtw_sent() is empty and is defined in rxr_pkt_type_req.h
+ *         rxr_pkt_handle_longcts_rtw_sent() is empty and is defined in rxr_pkt_type_req.h
  */
-void rxr_pkt_handle_long_rtw_sent(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtw_sent(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_tx_entry *tx_entry;
@@ -1465,7 +1528,7 @@ void rxr_pkt_handle_eager_rtw_send_completion(struct rxr_ep *ep,
 	rxr_cq_handle_tx_completion(ep, tx_entry);
 }
 
-void rxr_pkt_handle_long_rtw_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtw_send_completion(struct rxr_ep *ep,
 					     struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_tx_entry *tx_entry;
@@ -1476,7 +1539,7 @@ void rxr_pkt_handle_long_rtw_send_completion(struct rxr_ep *ep,
 		rxr_cq_handle_tx_completion(ep, tx_entry);
 }
 
-void rxr_pkt_handle_dc_long_rtw_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_dc_longcts_rtw_send_completion(struct rxr_ep *ep,
 						struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_tx_entry *tx_entry;
@@ -1498,10 +1561,8 @@ struct rxr_rx_entry *rxr_pkt_alloc_rtw_rx_entry(struct rxr_ep *ep,
 {
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_base_hdr *base_hdr;
-	struct fi_msg msg = {0};
 
-	msg.addr = pkt_entry->addr;
-	rx_entry = rxr_ep_get_rx_entry(ep, &msg, 0, ~0, ofi_op_write, 0);
+	rx_entry = rxr_ep_alloc_rx_entry(ep, pkt_entry->addr, ofi_op_write);
 	if (OFI_UNLIKELY(!rx_entry))
 		return NULL;
 
@@ -1519,7 +1580,7 @@ struct rxr_rx_entry *rxr_pkt_alloc_rtw_rx_entry(struct rxr_ep *ep,
 }
 
 void rxr_pkt_proc_eager_rtw(struct rxr_ep *ep,
-			    struct fi_rma_iov *rma_iov,
+			    struct efa_rma_iov *rma_iov,
 			    size_t rma_iov_count,
 			    struct rxr_rx_entry *rx_entry,
 			    struct rxr_pkt_entry *pkt_entry)
@@ -1556,7 +1617,7 @@ void rxr_pkt_proc_eager_rtw(struct rxr_ep *ep,
 			rx_entry->iov[0].iov_len);
 		err = FI_EINVAL;
 	} else {
-		err = rxr_pkt_copy_to_rx(ep, rx_entry, 0, pkt_entry, data, data_size);
+		err = rxr_pkt_copy_data_to_rx_entry(ep, rx_entry, 0, pkt_entry, data, data_size);
 	}
 
 	if (err) {
@@ -1607,19 +1668,19 @@ void rxr_pkt_handle_dc_eager_rtw_recv(struct rxr_ep *ep,
 
 	rx_entry->rxr_flags |= RXR_DELIVERY_COMPLETE_REQUESTED;
 	rtw_hdr = (struct rxr_dc_eager_rtw_hdr *)pkt_entry->pkt;
-	rx_entry->tx_id = rtw_hdr->tx_id;
+	rx_entry->tx_id = rtw_hdr->send_id;
 	rx_entry->iov_count = rtw_hdr->rma_iov_count;
 	rxr_pkt_proc_eager_rtw(ep,
-			       (struct fi_rma_iov *)rtw_hdr->rma_iov,
+			       rtw_hdr->rma_iov,
 			       rtw_hdr->rma_iov_count,
 			       rx_entry, pkt_entry);
 }
 
-void rxr_pkt_handle_long_rtw_recv(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtw_recv(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_rx_entry *rx_entry;
-	struct rxr_long_rtw_hdr *rtw_hdr;
+	struct rxr_longcts_rtw_hdr *rtw_hdr;
 	char *data;
 	size_t hdr_size, data_size;
 	ssize_t err;
@@ -1634,9 +1695,9 @@ void rxr_pkt_handle_long_rtw_recv(struct rxr_ep *ep,
 		return;
 	}
 
-	rtw_hdr = (struct rxr_long_rtw_hdr *)pkt_entry->pkt;
-	tx_id = rtw_hdr->tx_id;
-	if (rtw_hdr->type == RXR_DC_LONG_RTW_PKT)
+	rtw_hdr = (struct rxr_longcts_rtw_hdr *)pkt_entry->pkt;
+	tx_id = rtw_hdr->send_id;
+	if (rtw_hdr->type == RXR_DC_LONGCTS_RTW_PKT)
 		rx_entry->rxr_flags |= RXR_DELIVERY_COMPLETE_REQUESTED;
 
 	rx_entry->iov_count = rtw_hdr->rma_iov_count;
@@ -1667,7 +1728,7 @@ void rxr_pkt_handle_long_rtw_recv(struct rxr_ep *ep,
 			rx_entry->iov[0].iov_len);
 		err = FI_EINVAL;
 	} else {
-		err = rxr_pkt_copy_to_rx(ep, rx_entry, 0, pkt_entry, data, data_size);
+		err = rxr_pkt_copy_data_to_rx_entry(ep, rx_entry, 0, pkt_entry, data, data_size);
 	}
 
 	if (err) {
@@ -1687,16 +1748,16 @@ void rxr_pkt_handle_long_rtw_recv(struct rxr_ep *ep,
 	err = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
 	if (OFI_UNLIKELY(err)) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ, "Cannot post CTS packet\n");
-		rxr_cq_handle_rx_error(ep, rx_entry, err);
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 		rxr_release_rx_entry(ep, rx_entry);
 	}
 }
 
-void rxr_pkt_handle_read_rtw_recv(struct rxr_ep *ep,
+void rxr_pkt_handle_longread_rtw_recv(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_rx_entry *rx_entry;
-	struct rxr_read_rtw_hdr *rtw_hdr;
+	struct rxr_longread_rtw_hdr *rtw_hdr;
 	struct fi_rma_iov *read_iov;
 	size_t hdr_size;
 	ssize_t err;
@@ -1710,7 +1771,7 @@ void rxr_pkt_handle_read_rtw_recv(struct rxr_ep *ep,
 		return;
 	}
 
-	rtw_hdr = (struct rxr_read_rtw_hdr *)pkt_entry->pkt;
+	rtw_hdr = (struct rxr_longread_rtw_hdr *)pkt_entry->pkt;
 	rx_entry->iov_count = rtw_hdr->rma_iov_count;
 	err = rxr_rma_verified_copy_iov(ep, rtw_hdr->rma_iov, rtw_hdr->rma_iov_count,
 					FI_REMOTE_WRITE, rx_entry->iov, rx_entry->desc);
@@ -1730,7 +1791,7 @@ void rxr_pkt_handle_read_rtw_recv(struct rxr_ep *ep,
 	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
 	read_iov = (struct fi_rma_iov *)((char *)pkt_entry->pkt + hdr_size);
 	rx_entry->addr = pkt_entry->addr;
-	rx_entry->tx_id = rtw_hdr->tx_id;
+	rx_entry->tx_id = rtw_hdr->send_id;
 	rx_entry->rma_iov_count = rtw_hdr->read_iov_count;
 	memcpy(rx_entry->rma_iov, read_iov,
 	       rx_entry->rma_iov_count * sizeof(struct fi_rma_iov));
@@ -1762,9 +1823,9 @@ void rxr_pkt_init_rtr(struct rxr_ep *ep,
 	rtr_hdr = (struct rxr_rtr_hdr *)pkt_entry->pkt;
 	rtr_hdr->rma_iov_count = tx_entry->rma_iov_count;
 	rxr_pkt_init_req_hdr(ep, tx_entry, pkt_type, pkt_entry);
-	rtr_hdr->data_len = tx_entry->total_len;
-	rtr_hdr->read_req_rx_id = tx_entry->rma_loc_rx_id;
-	rtr_hdr->read_req_window = window;
+	rtr_hdr->msg_length = tx_entry->total_len;
+	rtr_hdr->recv_id = tx_entry->rma_loc_rx_id;
+	rtr_hdr->recv_length = window;
 	for (i = 0; i < tx_entry->rma_iov_count; ++i) {
 		rtr_hdr->rma_iov[i].addr = tx_entry->rma_iov[i].addr;
 		rtr_hdr->rma_iov[i].len = tx_entry->rma_iov[i].len;
@@ -1783,28 +1844,15 @@ ssize_t rxr_pkt_init_short_rtr(struct rxr_ep *ep,
 	return 0;
 }
 
-ssize_t rxr_pkt_init_long_rtr(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_rtr(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry)
 {
-	rxr_pkt_init_rtr(ep, tx_entry, RXR_LONG_RTR_PKT, tx_entry->rma_window, pkt_entry);
+	rxr_pkt_init_rtr(ep, tx_entry, RXR_LONGCTS_RTR_PKT, tx_entry->rma_window, pkt_entry);
 	return 0;
 }
 
 /*
- *     handle_sent() functions for RTR packet types
- */
-void rxr_pkt_handle_rtr_sent(struct rxr_ep *ep,
-			     struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_tx_entry *tx_entry;
-
-	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-	tx_entry->bytes_sent = 0;
-	tx_entry->state = RXR_TX_WAIT_READ_FINISH;
-}
-
-/*
  *     handle_send_completion() funciton for RTR packet
  */
 void rxr_pkt_handle_rtr_send_completion(struct rxr_ep *ep,
@@ -1827,10 +1875,8 @@ void rxr_pkt_handle_rtr_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_tx_entry *tx_entry;
 	ssize_t err;
-	struct fi_msg msg = {0};
 
-	msg.addr = pkt_entry->addr;
-	rx_entry = rxr_ep_get_rx_entry(ep, &msg, 0, ~0, ofi_op_read_rsp, 0);
+	rx_entry = rxr_ep_alloc_rx_entry(ep, pkt_entry->addr, ofi_op_read_rsp);
 	if (OFI_UNLIKELY(!rx_entry)) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"RX entries exhausted.\n");
@@ -1842,14 +1888,10 @@ void rxr_pkt_handle_rtr_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 	rx_entry->addr = pkt_entry->addr;
 	rx_entry->bytes_received = 0;
 	rx_entry->bytes_copied = 0;
-	rx_entry->cq_entry.flags |= (FI_RMA | FI_READ);
-	rx_entry->cq_entry.len = ofi_total_iov_len(rx_entry->iov, rx_entry->iov_count);
-	rx_entry->cq_entry.buf = rx_entry->iov[0].iov_base;
-	rx_entry->total_len = rx_entry->cq_entry.len;
 
 	rtr_hdr = (struct rxr_rtr_hdr *)pkt_entry->pkt;
-	rx_entry->rma_initiator_rx_id = rtr_hdr->read_req_rx_id;
-	rx_entry->window = rtr_hdr->read_req_window;
+	rx_entry->rma_initiator_rx_id = rtr_hdr->recv_id;
+	rx_entry->window = rtr_hdr->recv_length;
 	rx_entry->iov_count = rtr_hdr->rma_iov_count;
 	err = rxr_rma_verified_copy_iov(ep, rtr_hdr->rma_iov, rtr_hdr->rma_iov_count,
 					FI_REMOTE_READ, rx_entry->iov, rx_entry->desc);
@@ -1861,6 +1903,11 @@ void rxr_pkt_handle_rtr_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 		return;
 	}
 
+	rx_entry->cq_entry.flags |= (FI_RMA | FI_READ);
+	rx_entry->cq_entry.len = ofi_total_iov_len(rx_entry->iov, rx_entry->iov_count);
+	rx_entry->cq_entry.buf = rx_entry->iov[0].iov_base;
+	rx_entry->total_len = rx_entry->cq_entry.len;
+
 	tx_entry = rxr_rma_alloc_readrsp_tx_entry(ep, rx_entry);
 	if (OFI_UNLIKELY(!tx_entry)) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ, "Readrsp tx entry exhausted!\n");
@@ -1890,7 +1937,7 @@ void rxr_pkt_handle_rtr_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 ssize_t rxr_pkt_init_rta(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 			 int pkt_type, struct rxr_pkt_entry *pkt_entry)
 {
-	struct fi_rma_iov *rma_iov;
+	struct efa_rma_iov *rma_iov;
 	struct rxr_rta_hdr *rta_hdr;
 	char *data;
 	size_t hdr_size, data_size;
@@ -1901,7 +1948,6 @@ ssize_t rxr_pkt_init_rta(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	rta_hdr->rma_iov_count = tx_entry->rma_iov_count;
 	rta_hdr->atomic_datatype = tx_entry->atomic_hdr.datatype;
 	rta_hdr->atomic_op = tx_entry->atomic_hdr.atomic_op;
-	rta_hdr->tx_id = tx_entry->tx_id;
 	rxr_pkt_init_req_hdr(ep, tx_entry, pkt_type, pkt_entry);
 	rta_hdr->flags |= RXR_REQ_ATOMIC;
 	rma_iov = rta_hdr->rma_iov;
@@ -1932,14 +1978,22 @@ ssize_t rxr_pkt_init_dc_write_rta(struct rxr_ep *ep,
 				  struct rxr_tx_entry *tx_entry,
 				  struct rxr_pkt_entry *pkt_entry)
 {
+	struct rxr_rta_hdr *rta_hdr;
+
 	rxr_pkt_init_rta(ep, tx_entry, RXR_DC_WRITE_RTA_PKT, pkt_entry);
+	rta_hdr = rxr_get_rta_hdr(pkt_entry->pkt);
+	rta_hdr->send_id = tx_entry->tx_id;
 	return 0;
 }
 
 ssize_t rxr_pkt_init_fetch_rta(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry)
 {
+	struct rxr_rta_hdr *rta_hdr;
+
 	rxr_pkt_init_rta(ep, tx_entry, RXR_FETCH_RTA_PKT, pkt_entry);
+	rta_hdr = rxr_get_rta_hdr(pkt_entry->pkt);
+	rta_hdr->recv_id = tx_entry->tx_id;
 	return 0;
 }
 
@@ -1948,9 +2002,11 @@ ssize_t rxr_pkt_init_compare_rta(struct rxr_ep *ep, struct rxr_tx_entry *tx_entr
 {
 	char *data;
 	size_t data_size;
+	struct rxr_rta_hdr *rta_hdr;
 
 	rxr_pkt_init_rta(ep, tx_entry, RXR_COMPARE_RTA_PKT, pkt_entry);
-
+	rta_hdr = rxr_get_rta_hdr(pkt_entry->pkt);
+	rta_hdr->recv_id = tx_entry->tx_id;
 	/* rxr_pkt_init_rta() will copy data from tx_entry->iov to pkt entry
 	 * the following append the data to be compared
 	 */
@@ -1984,6 +2040,9 @@ int rxr_pkt_proc_write_rta(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 	op = rta_hdr->atomic_op;
 	dt = rta_hdr->atomic_datatype;
 	dtsize = ofi_datatype_size(dt);
+	if (OFI_UNLIKELY(!dtsize)) {
+		return -errno;
+	}
 
 	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
 	data = (char *)pkt_entry->pkt + hdr_size;
@@ -2006,10 +2065,8 @@ struct rxr_rx_entry *rxr_pkt_alloc_rta_rx_entry(struct rxr_ep *ep, struct rxr_pk
 {
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_rta_hdr *rta_hdr;
-	struct fi_msg msg = {0};
 
-	msg.addr = pkt_entry->addr;
-	rx_entry = rxr_ep_get_rx_entry(ep, &msg, 0, ~0, op, 0);
+	rx_entry = rxr_ep_alloc_rx_entry(ep, pkt_entry->addr, op);
 	if (OFI_UNLIKELY(!rx_entry)) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"RX entries exhausted.\n");
@@ -2028,7 +2085,6 @@ struct rxr_rx_entry *rxr_pkt_alloc_rta_rx_entry(struct rxr_ep *ep, struct rxr_pk
 	rx_entry->iov_count = rta_hdr->rma_iov_count;
 	rxr_rma_verified_copy_iov(ep, rta_hdr->rma_iov, rx_entry->iov_count,
 				  FI_REMOTE_READ, rx_entry->iov, rx_entry->desc);
-	rx_entry->tx_id = rta_hdr->tx_id;
 	rx_entry->total_len = ofi_total_iov_len(rx_entry->iov, rx_entry->iov_count);
 	/*
 	 * prepare a buffer to hold response data.
@@ -2069,7 +2125,7 @@ int rxr_pkt_proc_dc_write_rta(struct rxr_ep *ep,
 	}
 
 	rta_hdr = (struct rxr_rta_hdr *)pkt_entry->pkt;
-	rx_entry->tx_id = rta_hdr->tx_id;
+	rx_entry->tx_id = rta_hdr->send_id;
 	rx_entry->rxr_flags |= RXR_DELIVERY_COMPLETE_REQUESTED;
 
 	ret = rxr_pkt_proc_write_rta(ep, pkt_entry);
@@ -2088,8 +2144,7 @@ int rxr_pkt_proc_dc_write_rta(struct rxr_ep *ep,
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Posting of receipt packet failed! err=%s\n",
 			fi_strerror(err));
-		if (rxr_cq_handle_rx_error(ep, rx_entry, err))
-			assert(0 && "Cannot handle rx error");
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 		return err;
 	}
 
@@ -2110,9 +2165,13 @@ int rxr_pkt_proc_fetch_rta(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 		return -FI_ENOBUFS;
 	}
 
+	rx_entry->tx_id = rxr_get_rta_hdr(pkt_entry->pkt)->recv_id;
 	op = rx_entry->atomic_hdr.atomic_op;
  	dt = rx_entry->atomic_hdr.datatype;	
 	dtsize = ofi_datatype_size(rx_entry->atomic_hdr.datatype);
+	if (OFI_UNLIKELY(!dtsize)) {
+		return -errno;
+	}
 
 	data = (char *)pkt_entry->pkt + rxr_pkt_req_hdr_size(pkt_entry);
 
@@ -2126,10 +2185,8 @@ int rxr_pkt_proc_fetch_rta(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 	}
 
 	err = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_ATOMRSP_PKT, 0);
-	if (OFI_UNLIKELY(err)) {
-		if (rxr_cq_handle_rx_error(ep, rx_entry, err))
-			assert(0 && "Cannot handle rx error");
-	}
+	if (OFI_UNLIKELY(err))
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 
 	rxr_pkt_entry_release_rx(ep, pkt_entry);
 	return 0;
@@ -2150,9 +2207,16 @@ int rxr_pkt_proc_compare_rta(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 		return -FI_ENOBUFS;
 	}
 
+	rx_entry->tx_id = rxr_get_rta_hdr(pkt_entry->pkt)->recv_id;
 	op = rx_entry->atomic_hdr.atomic_op;
 	dt = rx_entry->atomic_hdr.datatype;
-       	dtsize = ofi_datatype_size(rx_entry->atomic_hdr.datatype);
+	dtsize = ofi_datatype_size(rx_entry->atomic_hdr.datatype);
+	if (OFI_UNLIKELY(!dtsize)) {
+		efa_eq_write_error(&ep->util_ep, FI_EINVAL, -errno);
+		rxr_release_rx_entry(ep, rx_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
+		return -errno;
+	}
 
 	src_data = (char *)pkt_entry->pkt + rxr_pkt_req_hdr_size(pkt_entry);
 	cmp_data = src_data + rx_entry->total_len;
diff --git a/prov/efa/src/rxr/rxr_pkt_type_req.h b/prov/efa/src/rxr/rxr_pkt_type_req.h
index a07bdfa..bf9942d 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_req.h
+++ b/prov/efa/src/rxr/rxr_pkt_type_req.h
@@ -34,63 +34,18 @@
 #ifndef _RXR_PKT_TYPE_REQ_H
 #define _RXR_PKT_TYPE_REQ_H
 
-/*
- * This file contain REQ packet type related struct and functions
- * REQ packets can be classifed into 4 categories:
- *    RTM (Request To Message) is used by message
- *    RTW (Request To Write) is used by RMA write
- *    RTR (Request To Read) is used by RMA read
- *    RTA (Request To Atomic) is used by Atomic
- *
- * For each REQ packet type need to have the following:
- *
- *     1. a header struct
- *     2. an init() function called by rxr_pkt_init_ctrl()
- *     3. a handle_sent() function called by rxr_pkt_post_ctrl()
- *     4. a handle_send_completion() function called by
- *               rxr_pkt_handle_send_completion()
- *     5. a proc() function called by
- *               rxr_pkt_proc_req()
- *
- * Some req packet types are so similar that they can share
- * some functions.
- */
+#define RXR_MSG_PREFIX_SIZE (sizeof(struct rxr_pkt_entry) + sizeof(struct rxr_eager_msgrtm_hdr) + RXR_REQ_OPT_RAW_ADDR_HDR_SIZE)
 
-/*
- * Utilities shared by all REQ packets
- *
- *     Packet Header Flags
- */
-#define RXR_REQ_OPT_RAW_ADDR_HDR	BIT_ULL(0)
-#define RXR_REQ_OPT_CQ_DATA_HDR		BIT_ULL(1)
-#define RXR_REQ_MSG			BIT_ULL(2)
-#define RXR_REQ_TAGGED			BIT_ULL(3)
-#define RXR_REQ_RMA			BIT_ULL(4)
-#define RXR_REQ_ATOMIC			BIT_ULL(5)
-
-/*
- *     Extra Feature Flags
- */
-#define RXR_REQ_FEATURE_RDMA_READ	BIT_ULL(0)
-#define RXR_REQ_FEATURE_DELIVERY_COMPLETE BIT_ULL(1)
-
-/*
- *     Utility struct and functions for
- *             REQ packet types
- */
-struct rxr_req_opt_raw_addr_hdr {
-	uint32_t addr_len;
-	char raw_addr[0];
-};
-
-struct rxr_req_opt_cq_data_hdr {
-	int64_t cq_data;
-};
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(RXR_MSG_PREFIX_SIZE % 8 == 0, "message prefix size alignment check");
+#endif
 
 void *rxr_pkt_req_raw_addr(struct rxr_pkt_entry *pkt_entry);
 
 int64_t rxr_pkt_req_cq_data(struct rxr_pkt_entry *pkt_entry);
 
+uint32_t *rxr_pkt_req_connid_ptr(struct rxr_pkt_entry *pkt_entry);
+
 size_t rxr_pkt_req_hdr_size(struct rxr_pkt_entry *pkt_entry);
 
 size_t rxr_pkt_req_base_hdr_size(struct rxr_pkt_entry *pkt_entry);
@@ -101,28 +56,6 @@ size_t rxr_pkt_max_header_size(void);
 
 size_t rxr_pkt_req_max_data_size(struct rxr_ep *ep, fi_addr_t addr, int pkt_type);
 
-/*
- * Structs and funcitons for RTM (Message) packet types
- * There are 4 message protocols
- *         Eager message protocol,
- *         Medium message protocol,
- *         Long message protocol,
- *         Read message protocol (message by read)
- * Each protocol employes two packet types: non-tagged and tagged.
- * Thus altogether there are 8 RTM packet types.
- */
-
-/*
- *   Utility structs and functions shared by all
- *   RTM packet types
- */
-struct rxr_rtm_base_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	uint32_t msg_id;
-};
-
 static inline
 struct rxr_rtm_base_hdr *rxr_get_rtm_base_hdr(void *pkt)
 {
@@ -170,86 +103,24 @@ void rxr_pkt_rtm_settag(struct rxr_pkt_entry *pkt_entry, uint64_t tag)
 	*tagptr = tag;
 }
 
-/*
- *   Header structs for each REQ packe type
- */
-struct rxr_eager_msgrtm_hdr {
-	struct rxr_rtm_base_hdr hdr;
-};
-
-struct rxr_eager_tagrtm_hdr {
-	struct rxr_rtm_base_hdr hdr;
-	uint64_t tag;
-};
-
-struct rxr_dc_eager_rtm_base_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	uint32_t msg_id;
-	uint32_t tx_id;
-	uint32_t padding;
-};
-
 static inline
 struct rxr_dc_eager_rtm_base_hdr *rxr_get_dc_eager_rtm_base_hdr(void *pkt)
 {
 	return (struct rxr_dc_eager_rtm_base_hdr *)pkt;
 }
 
-struct rxr_dc_eager_msgrtm_hdr {
-	struct rxr_dc_eager_rtm_base_hdr hdr;
-};
-
 static inline
 struct rxr_dc_eager_msgrtm_hdr *rxr_get_dc_eager_msgrtm_hdr(void *pkt)
 {
 	return (struct rxr_dc_eager_msgrtm_hdr *)pkt;
 }
 
-struct rxr_dc_eager_tagrtm_hdr {
-	struct rxr_dc_eager_rtm_base_hdr hdr;
-	uint64_t tag;
-};
-
 static inline
 struct rxr_dc_eager_tagrtm_hdr *rxr_get_dc_eager_tagrtm_hdr(void *pkt)
 {
 	return (struct rxr_dc_eager_tagrtm_hdr *)pkt;
 }
 
-struct rxr_medium_rtm_base_hdr {
-	struct rxr_rtm_base_hdr hdr;
-	uint64_t data_len;
-	uint64_t offset;
-};
-
-struct rxr_dc_medium_rtm_base_hdr {
-	struct rxr_rtm_base_hdr hdr;
-	uint32_t tx_id;
-	uint32_t padding;
-	uint64_t data_len;
-	uint64_t offset;
-};
-
-struct rxr_medium_msgrtm_hdr {
-	struct rxr_medium_rtm_base_hdr hdr;
-};
-
-struct rxr_dc_medium_msgrtm_hdr {
-	struct rxr_dc_medium_rtm_base_hdr hdr;
-};
-
-struct rxr_medium_tagrtm_hdr {
-	struct rxr_medium_rtm_base_hdr hdr;
-	uint64_t tag;
-};
-
-struct rxr_dc_medium_tagrtm_hdr {
-	struct rxr_dc_medium_rtm_base_hdr hdr;
-	uint64_t tag;
-};
-
 static inline
 struct rxr_medium_rtm_base_hdr *rxr_get_medium_rtm_base_hdr(void *pkt)
 {
@@ -274,61 +145,26 @@ struct rxr_dc_medium_tagrtm_hdr *rxr_get_dc_medium_tagrtm_hdr(void *pkt)
 	return (struct rxr_dc_medium_tagrtm_hdr *)pkt;
 }
 
-struct rxr_long_rtm_base_hdr {
-	struct rxr_rtm_base_hdr hdr;
-	uint64_t data_len;
-	uint32_t tx_id;
-	uint32_t credit_request;
-};
-
 static inline
-struct rxr_long_rtm_base_hdr *rxr_get_long_rtm_base_hdr(void *pkt)
+struct rxr_longcts_rtm_base_hdr *rxr_get_longcts_rtm_base_hdr(void *pkt)
 {
-	return (struct rxr_long_rtm_base_hdr *)pkt;
+	return (struct rxr_longcts_rtm_base_hdr *)pkt;
 }
 
-struct rxr_long_msgrtm_hdr {
-	struct rxr_long_rtm_base_hdr hdr;
-};
-
-struct rxr_long_tagrtm_hdr {
-	struct rxr_long_rtm_base_hdr hdr;
-	uint64_t tag;
-};
-
-struct rxr_read_rtm_base_hdr {
-	struct rxr_rtm_base_hdr hdr;
-	uint64_t data_len;
-	uint32_t tx_id;
-	uint32_t read_iov_count;
-};
-
 static inline
-struct rxr_read_rtm_base_hdr *rxr_get_read_rtm_base_hdr(void *pkt)
+struct rxr_longread_rtm_base_hdr *rxr_get_longread_rtm_base_hdr(void *pkt)
 {
-	return (struct rxr_read_rtm_base_hdr *)pkt;
+	return (struct rxr_longread_rtm_base_hdr *)pkt;
 }
 
-struct rxr_read_msgrtm_hdr {
-	struct rxr_read_rtm_base_hdr hdr;
-};
-
-struct rxr_read_tagrtm_hdr {
-	struct rxr_read_rtm_base_hdr hdr;
-	uint64_t tag;
-};
-
 static inline
-int rxr_read_rtm_pkt_type(int op)
+int rxr_longread_rtm_pkt_type(int op)
 {
 	assert(op == ofi_op_tagged || op == ofi_op_msg);
-	return (op == ofi_op_tagged) ? RXR_READ_TAGRTM_PKT
-				     : RXR_READ_MSGRTM_PKT;
+	return (op == ofi_op_tagged) ? RXR_LONGREAD_TAGRTM_PKT
+				     : RXR_LONGREAD_MSGRTM_PKT;
 }
 
-/*
- *  init() functions for RTM packets
- */
 ssize_t rxr_pkt_init_eager_msgrtm(struct rxr_ep *ep,
 				  struct rxr_tx_entry *tx_entry,
 				  struct rxr_pkt_entry *pkt_entry);
@@ -361,32 +197,30 @@ ssize_t rxr_pkt_init_dc_medium_tagrtm(struct rxr_ep *ep,
 				      struct rxr_tx_entry *tx_entry,
 				      struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_long_msgrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_msgrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_dc_long_msgrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_dc_longcts_msgrtm(struct rxr_ep *ep,
 				    struct rxr_tx_entry *tx_entry,
 				    struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_long_tagrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_tagrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_dc_long_tagrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_dc_longcts_tagrtm(struct rxr_ep *ep,
 				    struct rxr_tx_entry *tx_entry,
 				    struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_read_msgrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_msgrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_read_tagrtm(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_tagrtm(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry);
-/*
- *   handle_sent() functions for RTM packets
- */
+
 static inline
 void rxr_pkt_handle_eager_rtm_sent(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry)
@@ -398,41 +232,35 @@ void rxr_pkt_handle_eager_rtm_sent(struct rxr_ep *ep,
 void rxr_pkt_handle_medium_rtm_sent(struct rxr_ep *ep,
 				    struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_long_rtm_sent(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtm_sent(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry);
 
 static inline
-void rxr_pkt_handle_read_rtm_sent(struct rxr_ep *ep,
+void rxr_pkt_handle_longread_rtm_sent(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry)
 {
 }
 
-/*
- *   handle_send_completion() functions for RTM packet types
- */
 void rxr_pkt_handle_eager_rtm_send_completion(struct rxr_ep *ep,
 					      struct rxr_pkt_entry *pkt_entry);
 
 void rxr_pkt_handle_medium_rtm_send_completion(struct rxr_ep *ep,
 					       struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_long_rtm_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtm_send_completion(struct rxr_ep *ep,
 					     struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_dc_long_rtm_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_dc_longcts_rtm_send_completion(struct rxr_ep *ep,
 						struct rxr_pkt_entry *pkt_entry);
 
 static inline
-void rxr_pkt_handle_read_rtm_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_longread_rtm_send_completion(struct rxr_ep *ep,
 					     struct rxr_pkt_entry *pkt_entry)
 {
 }
 
-/*
- *   proc() functions for RTM packet types
- */
-void rxr_pkt_rtm_init_rx_entry(struct rxr_pkt_entry *pkt_entry,
-			       struct rxr_rx_entry *rx_entry);
+void rxr_pkt_rtm_update_rx_entry(struct rxr_pkt_entry *pkt_entry,
+				 struct rxr_rx_entry *rx_entry);
 
 /*         This function is called by both
  *            rxr_pkt_handle_rtm_recv() and
@@ -456,100 +284,27 @@ void rxr_pkt_handle_zcpy_recv(struct rxr_ep *ep,
 void rxr_pkt_handle_rtm_rta_recv(struct rxr_ep *ep,
 				 struct rxr_pkt_entry *pkt_entry);
 
-/* Structs and functions for RTW packet types
- * There are 3 write protocols
- *         Eager write protocol,
- *         Long write protocol and
- *         Read write protocol (write by read)
- * Each protocol correspond to a packet type
- */
-
-/*
- *     Header structs
- */
-struct rxr_rtw_base_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t rma_iov_count;
-};
-
 static inline
 struct rxr_rtw_base_hdr *rxr_get_rtw_base_hdr(void *pkt)
 {
 	return (struct rxr_rtw_base_hdr *)pkt;
 }
 
-struct efa_rma_iov {
-	uint64_t		addr;
-	size_t			len;
-	uint64_t		key;
-};
-
-struct rxr_eager_rtw_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t rma_iov_count;
-	struct fi_rma_iov rma_iov[0];
-};
-
-struct rxr_dc_eager_rtw_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t rma_iov_count;
-	/* end of rxr_rtw_base_hdr */
-	uint32_t tx_id;
-	uint32_t padding;
-	struct efa_rma_iov rma_iov[0];
-};
-
 static inline
 struct rxr_dc_eager_rtw_hdr *rxr_get_dc_eager_rtw_hdr(void *pkt)
 {
 	return (struct rxr_dc_eager_rtw_hdr *)pkt;
 }
 
-struct rxr_long_rtw_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t rma_iov_count;
-	uint64_t data_len;
-	uint32_t tx_id;
-	uint32_t credit_request;
-	struct fi_rma_iov rma_iov[0];
-};
-
-struct rxr_read_rtw_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t rma_iov_count;
-	uint64_t data_len;
-	uint32_t tx_id;
-	uint32_t read_iov_count;
-	struct fi_rma_iov rma_iov[0];
-};
-
-/*
- *     init() functions for each RTW packet types
- */
 ssize_t rxr_pkt_init_eager_rtw(struct rxr_ep *ep,
 			       struct rxr_tx_entry *tx_entry,
 			       struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_long_rtw(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_rtw(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_read_rtw(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longread_rtw(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry);
 
@@ -557,13 +312,10 @@ ssize_t rxr_pkt_init_dc_eager_rtw(struct rxr_ep *ep,
 				  struct rxr_tx_entry *tx_entry,
 				  struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_dc_long_rtw(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_dc_longcts_rtw(struct rxr_ep *ep,
 				 struct rxr_tx_entry *tx_entry,
 				 struct rxr_pkt_entry *pkt_entry);
 
-/*
- *     handle_sent() functions
- */
 static inline
 void rxr_pkt_handle_eager_rtw_sent(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry)
@@ -572,124 +324,63 @@ void rxr_pkt_handle_eager_rtw_sent(struct rxr_ep *ep,
 	return;
 }
 
-void rxr_pkt_handle_long_rtw_sent(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtw_sent(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry);
 
 static inline
-void rxr_pkt_handle_read_rtw_sent(struct rxr_ep *ep,
+void rxr_pkt_handle_longread_rtw_sent(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry)
 {
 }
 
-/*
- *     handle_send_completion() functions
- */
 void rxr_pkt_handle_eager_rtw_send_completion(struct rxr_ep *ep,
 					      struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_long_rtw_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtw_send_completion(struct rxr_ep *ep,
 					     struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_dc_long_rtw_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_dc_longcts_rtw_send_completion(struct rxr_ep *ep,
 						struct rxr_pkt_entry *pkt_entry);
 
 static inline
-void rxr_pkt_handle_read_rtw_send_completion(struct rxr_ep *ep,
+void rxr_pkt_handle_longread_rtw_send_completion(struct rxr_ep *ep,
 					     struct rxr_pkt_entry *pkt_entry)
 {
 }
 
-/*
- *     handle_recv() functions
- */
 void rxr_pkt_handle_eager_rtw_recv(struct rxr_ep *ep,
 				   struct rxr_pkt_entry *pkt_entry);
 
 void rxr_pkt_handle_dc_eager_rtw_recv(struct rxr_ep *ep,
 				      struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_long_rtw_recv(struct rxr_ep *ep,
+void rxr_pkt_handle_longcts_rtw_recv(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry);
 
-void rxr_pkt_handle_read_rtw_recv(struct rxr_ep *ep,
+void rxr_pkt_handle_longread_rtw_recv(struct rxr_ep *ep,
 				  struct rxr_pkt_entry *pkt_entry);
-
-/* Structs and functions for RTR packet types
- * There are 3 read protocols
- *         Short protocol,
- *         Long read protocol and
- *         RDMA read protocol
- * Each protocol correspond to a packet type
- */
-
-/*
- *     Header structs
- */
-struct rxr_rtr_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t rma_iov_count;
-	uint64_t data_len;
-	uint32_t read_req_rx_id;
-	uint32_t read_req_window;
-	struct fi_rma_iov rma_iov[0];
-};
-
 static inline
 struct rxr_rtr_hdr *rxr_get_rtr_hdr(void *pkt)
 {
 	return (struct rxr_rtr_hdr *)pkt;
 }
 
-/*
- *     init() functions for each RTW packet types
- */
 ssize_t rxr_pkt_init_short_rtr(struct rxr_ep *ep,
 			       struct rxr_tx_entry *tx_entry,
 			       struct rxr_pkt_entry *pkt_entry);
 
-ssize_t rxr_pkt_init_long_rtr(struct rxr_ep *ep,
+ssize_t rxr_pkt_init_longcts_rtr(struct rxr_ep *ep,
 			      struct rxr_tx_entry *tx_entry,
 			      struct rxr_pkt_entry *pkt_entry);
 
-/*
- *     handle_sent() functions
- */
 void rxr_pkt_handle_rtr_sent(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry);
 
-/*
- *     handle_send_completion() functions
- */
 void rxr_pkt_handle_rtr_send_completion(struct rxr_ep *ep,
 					struct rxr_pkt_entry *pkt_entry);
-/*
- *     handle_recv() functions
- */
 void rxr_pkt_handle_rtr_recv(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry);
 
-/* Structs and functions for RTW packet types
- * There are 2 atomic protocols
- *         write atomic protocol and, 
- *         read/compare atomic protocol and
- * Each protocol correspond to a packet type
- */
-struct rxr_rta_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	uint32_t msg_id;
-	/* end of rtm_base_hdr, atomic packet need msg_id for reordering */
-	uint32_t rma_iov_count;
-	uint32_t atomic_datatype;
-	uint32_t atomic_op;
-	uint32_t tx_id;
-	struct fi_rma_iov rma_iov[0];
-};
-
 static inline
 struct rxr_rta_hdr *rxr_get_rta_hdr(void *pkt)
 {
@@ -732,4 +423,5 @@ int rxr_pkt_proc_compare_rta(struct rxr_ep *ep,
 			     struct rxr_pkt_entry *pkt_entry);
 
 void rxr_pkt_handle_rta_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
+
 #endif
diff --git a/prov/efa/src/rxr/rxr_read.c b/prov/efa/src/rxr/rxr_read.c
index 3ba786f..5066648 100644
--- a/prov/efa/src/rxr/rxr_read.c
+++ b/prov/efa/src/rxr/rxr_read.c
@@ -110,14 +110,15 @@ ssize_t rxr_read_prepare_pkt_entry_mr(struct rxr_ep *ep, struct rxr_read_entry *
 	}
 
 	/* only ooo and unexp packet entry's memory is not registered with device */
-	assert(pkt_entry->type == RXR_PKT_ENTRY_OOO ||
-	       pkt_entry->type == RXR_PKT_ENTRY_UNEXP);
+	assert(pkt_entry->alloc_type == RXR_PKT_FROM_OOO_POOL ||
+	       pkt_entry->alloc_type == RXR_PKT_FROM_UNEXP_POOL);
 
 	pkt_offset = (char *)read_entry->rma_iov[0].addr - (char *)pkt_entry->pkt;
 	assert(pkt_offset > sizeof(struct rxr_base_hdr));
 
 	pkt_entry_copy = rxr_pkt_entry_clone(ep, ep->rx_readcopy_pkt_pool,
-					     pkt_entry, RXR_PKT_ENTRY_READ_COPY);
+					     RXR_PKT_FROM_READ_COPY_POOL,
+					     pkt_entry);
 	if (!pkt_entry_copy) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"readcopy pkt pool exhausted! Set FI_EFA_READCOPY_POOL_SIZE to a higher value!");
@@ -179,6 +180,35 @@ ssize_t rxr_read_mr_reg(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 	return 0;
 }
 
+/**
+ * @brief convert descriptor from application for lower provider to use
+ *
+ * Each provider define its descriptors format. The descriptor provided
+ * by application is in EFA provider format.
+ * This function convert it to descriptors for lower provider according
+ * to lower provider type. It also handle the case application does not
+ * provider descriptors.
+ *
+ * @param lower_ep_type[in] lower efa type, can be EFA_EP or SHM_EP.
+ * @param numdesc[in]       number of descriptors in the array
+ * @param desc_in[in]       descriptors provided by application
+ * @param desc_out[out]     descriptors for lower provider.
+ */
+static inline
+void rxr_read_copy_desc(enum rxr_lower_ep_type lower_ep_type,
+			int numdesc, void **desc_in, void **desc_out)
+{
+	if (!desc_in) {
+		memset(desc_out, 0, numdesc * sizeof(void *));
+		return;
+	}
+
+	memcpy(desc_out, desc_in, numdesc * sizeof(void *));
+	if (lower_ep_type == SHM_EP) {
+		rxr_convert_desc_for_shm(numdesc, desc_out);
+	}
+}
+
 /* rxr_read_alloc_entry allocates a read entry.
  * It is called by rxr_read_post_or_queue().
  * Input:
@@ -229,10 +259,7 @@ struct rxr_read_entry *rxr_read_alloc_entry(struct rxr_ep *ep, int entry_type, v
 		total_rma_iov_len = ofi_total_rma_iov_len(tx_entry->rma_iov, tx_entry->rma_iov_count);
 		read_entry->total_len = MIN(total_iov_len, total_rma_iov_len);
 
-		if (tx_entry->desc) {
-			memcpy(read_entry->mr_desc, tx_entry->desc,
-			       read_entry->iov_count * sizeof(void *));
-		}
+		rxr_read_copy_desc(lower_ep_type, read_entry->iov_count, tx_entry->desc, read_entry->mr_desc);
 
 	} else {
 		rx_entry = (struct rxr_rx_entry *)x_entry;
@@ -255,10 +282,7 @@ struct rxr_read_entry *rxr_read_alloc_entry(struct rxr_ep *ep, int entry_type, v
 		total_rma_iov_len = ofi_total_rma_iov_len(rx_entry->rma_iov, rx_entry->rma_iov_count);
 		read_entry->total_len = MIN(total_iov_len, total_rma_iov_len);
 
-		if (rx_entry->desc) {
-			memcpy(read_entry->mr_desc, rx_entry->desc,
-			       read_entry->iov_count * sizeof(void *));
-		}
+		rxr_read_copy_desc(lower_ep_type, read_entry->iov_count, rx_entry->desc, read_entry->mr_desc);
 	}
 
 	memset(read_entry->mr, 0, read_entry->iov_count * sizeof(struct fid_mr *));
@@ -287,7 +311,7 @@ void rxr_read_release_entry(struct rxr_ep *ep, struct rxr_read_entry *read_entry
 			err = fi_close((struct fid *)read_entry->mr[i]);
 			if (err) {
 				FI_WARN(&rxr_prov, FI_LOG_MR, "Unable to close mr\n");
-				rxr_read_handle_error(ep, read_entry, err);
+				rxr_read_write_error(ep, read_entry, -err, -err);
 			}
 		}
 	}
@@ -320,7 +344,7 @@ int rxr_read_post_or_queue(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 
 int rxr_read_post_remote_read_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry)
 {
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_read_entry *read_entry;
 	int lower_ep_type;
 
@@ -330,6 +354,7 @@ int rxr_read_post_remote_read_or_queue(struct rxr_ep *ep, int entry_type, void *
 		assert(entry_type == RXR_RX_ENTRY);
 		peer = rxr_ep_get_peer(ep, ((struct rxr_rx_entry *)x_entry)->addr);
 	}
+	assert(peer);
 
 	lower_ep_type = (peer->is_local) ? SHM_EP : EFA_EP;
 	read_entry = rxr_read_alloc_entry(ep, entry_type, x_entry, lower_ep_type);
@@ -377,8 +402,9 @@ int rxr_read_post_local_read_or_queue(struct rxr_ep *ep,
 	assert(pkt_entry->x_entry == rx_entry);
 	assert(rx_entry->desc && efa_ep_is_cuda_mr(rx_entry->desc[0]));
 	read_entry->iov_count = rx_entry->iov_count;
+	memset(read_entry->mr, 0, sizeof(*read_entry->mr) * read_entry->iov_count);
 	memcpy(read_entry->iov, rx_entry->iov, rx_entry->iov_count * sizeof(struct iovec));
-	memcpy(read_entry->mr_desc, rx_entry->desc, rx_entry->iov_count * sizeof(void *));
+	rxr_read_copy_desc(EFA_EP, rx_entry->iov_count, rx_entry->desc, read_entry->mr_desc);
 	ofi_consume_iov_desc(read_entry->iov, read_entry->mr_desc, &read_entry->iov_count, data_offset);
 	if (read_entry->iov_count == 0) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
@@ -389,7 +415,7 @@ int rxr_read_post_local_read_or_queue(struct rxr_ep *ep,
 	}
 
 	assert(efa_ep_is_cuda_mr(read_entry->mr_desc[0]));
-	err = ofi_truncate_iov(read_entry->iov, &read_entry->iov_count, data_size);
+	err = ofi_truncate_iov(read_entry->iov, &read_entry->iov_count, data_size + ep->msg_prefix_size);
 	if (err) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"data_offset %ld data_size %ld out of range\n",
@@ -407,7 +433,7 @@ int rxr_read_init_iov(struct rxr_ep *ep,
 {
 	int i, err;
 	struct fid_mr *mr;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 
 	peer = rxr_ep_get_peer(ep, tx_entry->addr);
 
@@ -426,6 +452,7 @@ int rxr_read_init_iov(struct rxr_ep *ep,
 		if (!tx_entry->mr[0]) {
 			for (i = 0; i < tx_entry->iov_count; ++i) {
 				assert(!tx_entry->mr[i]);
+				assert(peer);
 
 				if (peer->is_local)
 					err = efa_mr_reg_shm(rxr_ep_domain(ep)->rdm_domain,
@@ -466,7 +493,7 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 	struct fi_rma_iov rma_iov;
 	struct fi_msg_rma msg;
 	struct efa_ep *efa_ep;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	fi_addr_t shm_fiaddr = FI_ADDR_NOTAVAIL;
 
 	assert(read_entry->iov_count > 0);
@@ -486,32 +513,38 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 			return ret;
 	}
 
-	peer = rxr_ep_get_peer(ep, read_entry->addr);
-
-	if (read_entry->lower_ep_type == SHM_EP)
+	if (read_entry->lower_ep_type == SHM_EP) {
+		peer = rxr_ep_get_peer(ep, read_entry->addr);
+		assert(peer);
 		shm_fiaddr = peer->shm_fiaddr;
+	}
 
 	max_read_size = (read_entry->lower_ep_type == EFA_EP) ?
 				efa_max_rdma_size(ep->rdm_ep) : SIZE_MAX;
 	assert(max_read_size > 0);
 
 	ret = rxr_locate_iov_pos(read_entry->iov, read_entry->iov_count,
-				 read_entry->bytes_submitted,
+				 read_entry->bytes_submitted + ep->msg_prefix_size,
 				 &iov_idx, &iov_offset);
 	assert(ret == 0);
+	if (ret) {
+		return ret;
+	}
 
 	ret = rxr_locate_rma_iov_pos(read_entry->rma_iov, read_entry->rma_iov_count,
 				     read_entry->bytes_submitted,
 				     &rma_iov_idx, &rma_iov_offset);
 	assert(ret == 0);
-
+	if (ret) {
+		return ret;
+	}
 	total_iov_len = ofi_total_iov_len(read_entry->iov, read_entry->iov_count);
 	total_rma_iov_len = ofi_total_rma_iov_len(read_entry->rma_iov, read_entry->rma_iov_count);
 	assert(read_entry->total_len == MIN(total_iov_len, total_rma_iov_len));
 
 	while (read_entry->bytes_submitted < read_entry->total_len) {
 
-		if (ep->tx_pending == ep->max_outstanding_tx)
+		if (read_entry->lower_ep_type == EFA_EP && ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 			return -FI_EAGAIN;
 
 		assert(iov_idx < read_entry->iov_count);
@@ -536,9 +569,9 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 		 * we had to use a pkt_entry as context too
 		 */
 		if (read_entry->lower_ep_type == SHM_EP)
-			pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_shm_pool);
+			pkt_entry = rxr_pkt_entry_alloc(ep, ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 		else
-			pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_efa_pool);
+			pkt_entry = rxr_pkt_entry_alloc(ep, ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 
 		if (OFI_UNLIKELY(!pkt_entry))
 			return -FI_EAGAIN;
@@ -560,6 +593,8 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 			efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
 			msg.addr = read_entry->addr;
 			self_comm = (read_entry->context_type == RXR_READ_CONTEXT_PKT_ENTRY);
+			if (self_comm)
+				pkt_entry->flags |= RXR_PKT_ENTRY_LOCAL_READ;
 			ret = efa_rma_post_read(efa_ep, &msg, 0, self_comm);
 		}
 
@@ -568,13 +603,7 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 			return ret;
 		}
 
-		if (read_entry->context_type == RXR_READ_CONTEXT_PKT_ENTRY) {
-			assert(read_entry->lower_ep_type == EFA_EP);
-			/* read from self, no peer */
-			ep->tx_pending++;
-		} else if (read_entry->lower_ep_type == EFA_EP) {
-			rxr_ep_inc_tx_pending(ep, peer);
-		}
+		rxr_ep_record_tx_op_submitted(ep, pkt_entry);
 
 		read_entry->bytes_submitted += iov.iov_len;
 
@@ -606,22 +635,22 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 	return 0;
 }
 
-int rxr_read_handle_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry, int ret)
+void rxr_read_write_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry,
+			  int err, int prov_errno)
 {
 	struct rxr_tx_entry *tx_entry;
 	struct rxr_rx_entry *rx_entry;
 
 	if (read_entry->context_type == RXR_READ_CONTEXT_TX_ENTRY) {
 		tx_entry = read_entry->context;
-		ret = rxr_cq_handle_tx_error(ep, tx_entry, ret);
+		rxr_cq_write_tx_error(ep, tx_entry, err, prov_errno);
 	} else {
 		assert(read_entry->context_type == RXR_READ_CONTEXT_RX_ENTRY);
 		rx_entry = read_entry->context;
-		ret = rxr_cq_handle_rx_error(ep, rx_entry, ret);
+		rxr_cq_write_rx_error(ep, rx_entry, err, prov_errno);
 	}
 
 	if (read_entry->state == RXR_RDMA_ENTRY_PENDING)
 		dlist_remove(&read_entry->pending_entry);
-	return ret;
 }
 
diff --git a/prov/efa/src/rxr/rxr_read.h b/prov/efa/src/rxr/rxr_read.h
index 334648a..934eaba 100644
--- a/prov/efa/src/rxr/rxr_read.h
+++ b/prov/efa/src/rxr/rxr_read.h
@@ -127,7 +127,7 @@ int rxr_read_post_local_read_or_queue(struct rxr_ep *ep,
 
 void rxr_read_handle_read_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
-int rxr_read_handle_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry, int ret);
+void rxr_read_write_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry, int err, int prov_errno);
 
 #endif
 
diff --git a/prov/efa/src/rxr/rxr_rma.c b/prov/efa/src/rxr/rxr_rma.c
index 3cf9f6b..ba6e138 100644
--- a/prov/efa/src/rxr/rxr_rma.c
+++ b/prov/efa/src/rxr/rxr_rma.c
@@ -37,12 +37,13 @@
 #include <ofi_iov.h>
 #include "efa.h"
 #include "rxr.h"
+#include "rxr_msg.h"
 #include "rxr_rma.h"
 #include "rxr_pkt_cmd.h"
 #include "rxr_cntr.h"
 #include "rxr_read.h"
 
-int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct fi_rma_iov *rma,
+int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct efa_rma_iov *rma,
 			      size_t count, uint32_t flags,
 			      struct iovec *iov, void **desc)
 {
@@ -91,9 +92,7 @@ rxr_rma_alloc_readrsp_tx_entry(struct rxr_ep *rxr_ep,
 	}
 
 	assert(tx_entry);
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
 
 	msg.msg_iov = rx_entry->iov;
 	msg.iov_count = rx_entry->iov_count;
@@ -154,9 +153,7 @@ rxr_rma_alloc_tx_entry(struct rxr_ep *rxr_ep,
 	memcpy(tx_entry->rma_iov, msg_rma->rma_iov,
 	       sizeof(struct fi_rma_iov) * msg_rma->rma_iov_count);
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
 	return tx_entry;
 }
 
@@ -164,12 +161,13 @@ size_t rxr_rma_post_shm_write(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 {
 	struct rxr_pkt_entry *pkt_entry;
 	struct fi_msg_rma msg;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	int i, err;
 
 	assert(tx_entry->op == ofi_op_write);
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
-	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_shm_pool);
+	assert(peer);
+	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 	if (OFI_UNLIKELY(!pkt_entry))
 		return -FI_EAGAIN;
 
@@ -188,6 +186,8 @@ size_t rxr_rma_post_shm_write(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 	msg.rma_iov_count = tx_entry->rma_iov_count;
 	msg.context = pkt_entry;
 	msg.data = tx_entry->cq_entry.data;
+	msg.desc = tx_entry->desc;
+	rxr_convert_desc_for_shm(msg.iov_count, tx_entry->desc);
 
 	err = fi_writemsg(rxr_ep->shm_ep, &msg, tx_entry->fi_flags);
 	if (err)
@@ -200,18 +200,14 @@ size_t rxr_rma_post_shm_write(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 ssize_t rxr_rma_post_efa_emulated_read(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 {
 	int err, window, credits;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_rx_entry *rx_entry;
-	struct fi_msg msg = {0};
 
 	/* create a rx_entry to receve data
 	 * use ofi_op_msg for its op.
 	 * it does not write a rx completion.
 	 */
-	msg.msg_iov = tx_entry->iov;
-	msg.iov_count = tx_entry->iov_count;
-	msg.addr = tx_entry->addr;
-	rx_entry = rxr_ep_get_rx_entry(ep, &msg, 0, ~0, ofi_op_msg, 0);
+	rx_entry = rxr_ep_alloc_rx_entry(ep, tx_entry->addr, ofi_op_msg);
 	if (!rx_entry) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"RX entries exhausted for read.\n");
@@ -228,8 +224,10 @@ ssize_t rxr_rma_post_efa_emulated_read(struct rxr_ep *ep, struct rxr_tx_entry *t
 	assert(rx_entry);
 	rx_entry->tx_id = -1;
 	rx_entry->cq_entry.flags |= FI_READ;
-	rx_entry->total_len = rx_entry->cq_entry.len;
-
+	rx_entry->cq_entry.len = tx_entry->total_len;
+	rx_entry->total_len = tx_entry->total_len;
+	rx_entry->iov_count = tx_entry->iov_count;
+	memcpy(rx_entry->iov, tx_entry->iov, sizeof(*rx_entry->iov) * tx_entry->iov_count);
 	/*
 	 * there will not be a CTS for fi_read, we calculate CTS
 	 * window here, and send it via REQ.
@@ -265,9 +263,10 @@ ssize_t rxr_rma_post_efa_emulated_read(struct rxr_ep *ep, struct rxr_tx_entry *t
 	tx_entry->rma_loc_rx_id = rx_entry->rx_id;
 
 	if (tx_entry->total_len < ep->mtu_size - sizeof(struct rxr_readrsp_hdr)) {
-		err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_SHORT_RTR_PKT, 0);
+		err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_SHORT_RTR_PKT, 0, 0);
 	} else {
 		peer = rxr_ep_get_peer(ep, tx_entry->addr);
+		assert(peer);
 
 		rxr_pkt_calc_cts_window_credits(ep, peer,
 						tx_entry->total_len,
@@ -278,7 +277,7 @@ ssize_t rxr_rma_post_efa_emulated_read(struct rxr_ep *ep, struct rxr_tx_entry *t
 		rx_entry->window = window;
 		rx_entry->credit_cts = credits;
 		tx_entry->rma_window = rx_entry->window;
-		err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_LONG_RTR_PKT, 0);
+		err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_LONGCTS_RTR_PKT, 0, 0);
 	}
 
 	if (OFI_UNLIKELY(err)) {
@@ -296,7 +295,7 @@ ssize_t rxr_rma_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg, uint64_
 {
 	ssize_t err;
 	struct rxr_ep *rxr_ep;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_tx_entry *tx_entry = NULL;
 	bool use_lower_ep_read;
 
@@ -308,7 +307,7 @@ ssize_t rxr_rma_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg, uint64_
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	assert(msg->iov_count <= rxr_ep->tx_iov_limit);
 
-	rxr_perfset_start(rxr_ep, perf_rxr_tx);
+	efa_perfset_start(rxr_ep, perf_efa_tx);
 	fastlock_acquire(&rxr_ep->util_ep.lock);
 
 	if (OFI_UNLIKELY(is_tx_res_full(rxr_ep))) {
@@ -317,6 +316,7 @@ ssize_t rxr_rma_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg, uint64_
 	}
 
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 
 	if (peer->flags & RXR_PEER_IN_BACKOFF) {
 		err = -FI_EAGAIN;
@@ -361,7 +361,7 @@ out:
 		rxr_release_tx_entry(rxr_ep, tx_entry);
 
 	fastlock_release(&rxr_ep->util_ep.lock);
-	rxr_perfset_end(rxr_ep, perf_rxr_tx);
+	efa_perfset_end(rxr_ep, perf_efa_tx);
 	return err;
 }
 
@@ -403,7 +403,7 @@ ssize_t rxr_rma_read(struct fid_ep *ep, void *buf, size_t len, void *desc,
 ssize_t rxr_rma_post_write(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 {
 	ssize_t err;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct efa_domain *efa_domain;
 	bool delivery_complete_requested;
 	int ctrl_type;
@@ -414,6 +414,7 @@ ssize_t rxr_rma_post_write(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 				  util_domain.domain_fid);
 
 	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(peer);
 
 	if (peer->is_local)
 		return rxr_rma_post_shm_write(ep, tx_entry);
@@ -458,13 +459,13 @@ ssize_t rxr_rma_post_write(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 	if (tx_entry->total_len < max_rtm_data_size) {
 		ctrl_type = delivery_complete_requested ?
 			RXR_DC_EAGER_RTW_PKT : RXR_EAGER_RTW_PKT;
-		return rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, ctrl_type, 0);
+		return rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, ctrl_type, 0, 0);
 	}
 
 	if (tx_entry->total_len >= rxr_env.efa_min_read_write_size &&
 	    efa_both_support_rdma_read(ep, peer) &&
 	    (tx_entry->desc[0] || efa_is_cache_available(efa_domain))) {
-		err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_READ_RTW_PKT, 0);
+		err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_LONGREAD_RTW_PKT, 0, 0);
 		if (err != -FI_ENOMEM)
 			return err;
 		/*
@@ -478,9 +479,9 @@ ssize_t rxr_rma_post_write(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 		return err;
 
 	ctrl_type = delivery_complete_requested ?
-		RXR_DC_LONG_RTW_PKT : RXR_LONG_RTW_PKT;
+		RXR_DC_LONGCTS_RTW_PKT : RXR_LONGCTS_RTW_PKT;
 	tx_entry->rxr_flags |= RXR_LONGCTS_PROTOCOL;
-	return rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, ctrl_type, 0);
+	return rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, ctrl_type, 0, 0);
 }
 
 ssize_t rxr_rma_writemsg(struct fid_ep *ep,
@@ -488,7 +489,7 @@ ssize_t rxr_rma_writemsg(struct fid_ep *ep,
 			 uint64_t flags)
 {
 	ssize_t err;
-	struct rxr_peer *peer;
+	struct rdm_peer *peer;
 	struct rxr_ep *rxr_ep;
 	struct rxr_tx_entry *tx_entry;
 
@@ -500,10 +501,11 @@ ssize_t rxr_rma_writemsg(struct fid_ep *ep,
 	rxr_ep = container_of(ep, struct rxr_ep, util_ep.ep_fid.fid);
 	assert(msg->iov_count <= rxr_ep->tx_iov_limit);
 
-	rxr_perfset_start(rxr_ep, perf_rxr_tx);
+	efa_perfset_start(rxr_ep, perf_efa_tx);
 	fastlock_acquire(&rxr_ep->util_ep.lock);
 
 	peer = rxr_ep_get_peer(rxr_ep, msg->addr);
+	assert(peer);
 
 	if (peer->flags & RXR_PEER_IN_BACKOFF) {
 		err = -FI_EAGAIN;
@@ -523,7 +525,7 @@ ssize_t rxr_rma_writemsg(struct fid_ep *ep,
 	}
 out:
 	fastlock_release(&rxr_ep->util_ep.lock);
-	rxr_perfset_end(rxr_ep, perf_rxr_tx);
+	efa_perfset_end(rxr_ep, perf_efa_tx);
 	return err;
 }
 
diff --git a/prov/efa/src/rxr/rxr_rma.h b/prov/efa/src/rxr/rxr_rma.h
index 3514f0c..4bca1e4 100644
--- a/prov/efa/src/rxr/rxr_rma.h
+++ b/prov/efa/src/rxr/rxr_rma.h
@@ -39,7 +39,7 @@
 
 #include <rdma/fi_rma.h>
 
-int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct fi_rma_iov *rma,
+int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct efa_rma_iov *rma,
 			      size_t count, uint32_t flags,
 			      struct iovec *iov, void **desc);
 
diff --git a/prov/gni/configure.m4 b/prov/gni/configure.m4
index a368b32..19b3462 100644
--- a/prov/gni/configure.m4
+++ b/prov/gni/configure.m4
@@ -2,6 +2,8 @@ dnl
 dnl Copyright (c) 2015-2019 Cray Inc. All rights reserved.
 dnl Copyright (c) 2015-2018 Los Alamos National Security, LLC.
 dnl                         All rights reserved.
+dnl Copyright (c) 2021      Triad National Security, LLC. All rights
+dnl                         reserved.
 dnl
 dnl This software is available to you under a choice of one of two
 dnl licenses.  You may choose to be licensed under the terms of the GNU
@@ -87,7 +89,7 @@ AC_DEFUN([FI_GNI_CONFIGURE],[
                                  ],
                                  [ugni_lib_happy=0])
 
-               AS_IF([test x"$enable_ugni_static" == x"yes" && test $ugni_lib_happy -eq 1],
+               AS_IF([test x"$enable_ugni_static" = x"yes" && test $ugni_lib_happy -eq 1],
                      [gni_LDFLAGS=$(echo $gni_LDFLAGS | sed -e 's/lugni/l:libugni.a/')],[])
 
                FI_PKG_CHECK_MODULES_STATIC([CRAY_ALPS_LLI], [cray-alpslli],
@@ -119,14 +121,15 @@ AC_DEFUN([FI_GNI_CONFIGURE],[
                       [AC_DEFINE_UNQUOTED([HAVE_XPMEM], [0], [Define to 1 if xpmem available])
                       ])
 
-               gni_path_to_gni_pub=${CRAY_GNI_HEADERS_INCLUDE_OPTS:2}/gni_pub.h
-               dnl Trim the leading -I in order to provide a path
 
+               CPPFLAGS_SAVE=$CPPFLAGS
+               CPPFLAGS="$gni_CPPFLAGS $CPPFLAGS"
                AC_CHECK_TYPES([gni_ct_cqw_post_descriptor_t], [],
                               [AC_MSG_WARN([GNI provider requires CLE 5.2.UP04 or higher. Disabling gni provider.])
                                gni_header_happy=0
                               ],
-                              [[#include "$gni_path_to_gni_pub"]])
+                              [[#include "gni_pub.h"]])
+               CPPFLAGS=$CPPFLAGS_SAVE
 
                AS_IF([test -d $srcdir/prov/gni/test],
                      [AC_ARG_WITH([criterion], [AS_HELP_STRING([--with-criterion],
diff --git a/prov/gni/src/gnix_ep.c b/prov/gni/src/gnix_ep.c
index c5e1715..48e5437 100644
--- a/prov/gni/src/gnix_ep.c
+++ b/prov/gni/src/gnix_ep.c
@@ -800,7 +800,7 @@ gnix_ep_readv(struct fid_ep *ep, const struct iovec *iov, void **desc,
 	struct gnix_fid_ep *gnix_ep;
 	uint64_t flags;
 
-	if (!ep || !iov || !desc || count > GNIX_MAX_RMA_IOV_LIMIT) {
+	if (!ep || !iov || count > GNIX_MAX_RMA_IOV_LIMIT) {
 		return -FI_EINVAL;
 	}
 
@@ -810,7 +810,7 @@ gnix_ep_readv(struct fid_ep *ep, const struct iovec *iov, void **desc,
 	flags = gnix_ep->op_flags | GNIX_RMA_READ_FLAGS_DEF;
 
 	return _gnix_rma(gnix_ep, GNIX_FAB_RQ_RDMA_READ,
-			 (uint64_t)iov[0].iov_base, iov[0].iov_len, desc[0],
+			 (uint64_t)iov[0].iov_base, iov[0].iov_len, desc? desc[0] : NULL,
 			 src_addr, addr, key,
 			 context, flags, 0);
 }
@@ -820,7 +820,7 @@ gnix_ep_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg, uint64_t flags)
 {
 	struct gnix_fid_ep *gnix_ep;
 
-	if (!ep || !msg || !msg->msg_iov || !msg->rma_iov || !msg->desc ||
+	if (!ep || !msg || !msg->msg_iov || !msg->rma_iov ||
 	    msg->iov_count != 1 || msg->rma_iov_count != 1 ||
 	    msg->rma_iov[0].len > msg->msg_iov[0].iov_len) {
 		return -FI_EINVAL;
@@ -833,7 +833,7 @@ gnix_ep_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg, uint64_t flags)
 
 	return _gnix_rma(gnix_ep, GNIX_FAB_RQ_RDMA_READ,
 			 (uint64_t)msg->msg_iov[0].iov_base,
-			 msg->msg_iov[0].iov_len, msg->desc[0],
+			 msg->msg_iov[0].iov_len, msg->desc? msg->desc[0] : NULL,
 			 msg->addr, msg->rma_iov[0].addr, msg->rma_iov[0].key,
 			 msg->context, flags, msg->data);
 }
@@ -867,7 +867,7 @@ gnix_ep_writev(struct fid_ep *ep, const struct iovec *iov, void **desc,
 	struct gnix_fid_ep *gnix_ep;
 	uint64_t flags;
 
-	if (!ep || !iov || !desc || count > GNIX_MAX_RMA_IOV_LIMIT) {
+	if (!ep || !iov || count > GNIX_MAX_RMA_IOV_LIMIT) {
 		return -FI_EINVAL;
 	}
 
@@ -877,7 +877,7 @@ gnix_ep_writev(struct fid_ep *ep, const struct iovec *iov, void **desc,
 	flags = gnix_ep->op_flags | GNIX_RMA_WRITE_FLAGS_DEF;
 
 	return _gnix_rma(gnix_ep, GNIX_FAB_RQ_RDMA_WRITE,
-			 (uint64_t)iov[0].iov_base, iov[0].iov_len, desc[0],
+			 (uint64_t)iov[0].iov_base, iov[0].iov_len, desc? desc[0] : NULL,
 			 dest_addr, addr, key, context, flags, 0);
 }
 
diff --git a/prov/hook/hook_debug/configure.m4 b/prov/hook/hook_debug/configure.m4
index 58a6c63..0c2b8cf 100644
--- a/prov/hook/hook_debug/configure.m4
+++ b/prov/hook/hook_debug/configure.m4
@@ -12,7 +12,7 @@ AC_DEFUN([FI_HOOK_DEBUG_CONFIGURE],[
     # Determine if we can support the debug hooking provider
     hook_debug_happy=0
     AS_IF([test x"$enable_hook_debug" != x"no"], [hook_debug_happy=1])
-    AS_IF([test x"$hook_debug_dl" == x"1"], [
+    AS_IF([test x"$hook_debug_dl" = x"1"], [
 	hook_debug_happy=0
 	AC_MSG_ERROR([debug hooking provider cannot be compiled as DL])
     ])
diff --git a/prov/hook/perf/configure.m4 b/prov/hook/perf/configure.m4
index 82befb0..b871ed5 100644
--- a/prov/hook/perf/configure.m4
+++ b/prov/hook/perf/configure.m4
@@ -12,7 +12,7 @@ AC_DEFUN([FI_PERF_CONFIGURE],[
     # Determine if we can support the perf hooking provider
     perf_happy=0
     AS_IF([test x"$enable_perf" != x"no"], [perf_happy=1])
-    AS_IF([test x"$perf_dl" == x"1"], [
+    AS_IF([test x"$perf_dl" = x"1"], [
 	perf_happy=0
 	AC_MSG_ERROR([perf provider cannot be compiled as DL])
     ])
diff --git a/prov/hook/src/hook_domain.c b/prov/hook/src/hook_domain.c
index 004e19e..985da73 100644
--- a/prov/hook/src/hook_domain.c
+++ b/prov/hook/src/hook_domain.c
@@ -33,6 +33,7 @@
 #include <stdlib.h>
 #include <sys/uio.h>
 #include "ofi_hook.h"
+#include "ofi_util.h"
 
 static int hook_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 			   uint64_t flags, struct fid_mr **mr)
@@ -101,6 +102,85 @@ static struct fi_ops_mr hook_mr_ops = {
 	.regattr = hook_mr_regattr,
 };
 
+static ssize_t hook_credit_handler(struct fid_ep *ep_fid, size_t credits)
+{
+	/*
+	 * called from the base provider, ep_fid is the base ep, and
+	 * it's fid context is the hook ep.
+	 */
+	struct hook_ep *ep = (struct hook_ep *)ep_fid->fid.context;
+
+	return (*ep->domain->base_credit_handler)(&ep->ep, credits);
+}
+
+static void hook_set_threshold(struct fid_ep *ep_fid, size_t threshold)
+{
+	struct hook_ep *ep = container_of(ep_fid, struct hook_ep, ep);
+
+	return ep->domain->base_ops_flow_ctrl->set_threshold(ep->hep, threshold);
+}
+
+static void hook_set_send_handler(struct fid_domain *domain_fid,
+		ssize_t (*credit_handler)(struct fid_ep *ep, size_t credits))
+{
+	struct hook_domain *domain = container_of(domain_fid,
+						  struct hook_domain, domain);
+
+	domain->base_credit_handler = credit_handler;
+	domain->base_ops_flow_ctrl->set_send_handler(domain->hdomain,
+						     hook_credit_handler);
+}
+
+static int hook_enable_ep_flow_ctrl(struct fid_ep *ep_fid)
+{
+	struct hook_ep *ep = container_of(ep_fid, struct hook_ep, ep);
+
+	return ep->domain->base_ops_flow_ctrl->enable(ep->hep);
+}
+
+static void hook_add_credits(struct fid_ep *ep_fid, size_t credits)
+{
+	struct hook_ep *ep = container_of(ep_fid, struct hook_ep, ep);
+
+	return ep->domain->base_ops_flow_ctrl->add_credits(ep->hep, credits);
+}
+
+static struct ofi_ops_flow_ctrl hook_ops_flow_ctrl = {
+	.size = sizeof(struct ofi_ops_flow_ctrl),
+	.set_threshold = hook_set_threshold,
+	.add_credits = hook_add_credits,
+	.enable = hook_enable_ep_flow_ctrl,
+	.set_send_handler = hook_set_send_handler,
+};
+
+static int hook_domain_ops_open(struct fid *fid, const char *name,
+				uint64_t flags, void **ops, void *context)
+{
+	int err;
+	struct hook_domain *domain = container_of(fid, struct hook_domain,
+						  domain);
+
+	err = fi_open_ops(hook_to_hfid(fid), name, flags, ops, context);
+	if (err)
+		return err;
+
+	if (!strcasecmp(name, OFI_OPS_FLOW_CTRL)) {
+		domain->base_ops_flow_ctrl = *ops;
+		*ops = &hook_ops_flow_ctrl;
+	}
+
+	return 0;
+}
+
+struct fi_ops hook_domain_fid_ops = {
+	.size = sizeof(struct fi_ops),
+	.close = hook_close,
+	.bind = hook_bind,
+	.control = hook_control,
+	.ops_open = hook_domain_ops_open,
+};
+
+
 int hook_query_atomic(struct fid_domain *domain, enum fi_datatype datatype,
 		  enum fi_op op, struct fi_atomic_attr *attr, uint64_t flags)
 {
@@ -146,7 +226,7 @@ int hook_domain(struct fid_fabric *fabric, struct fi_info *info,
 	dom->fabric = fab;
 	dom->domain.fid.fclass = FI_CLASS_DOMAIN;
 	dom->domain.fid.context = context;
-	dom->domain.fid.ops = &hook_fid_ops;
+	dom->domain.fid.ops = &hook_domain_fid_ops;
 	dom->domain.ops = &hook_domain_ops;
 	dom->domain.mr = &hook_mr_ops;
 
diff --git a/prov/netdir/src/netdir_ndinit.c b/prov/netdir/src/netdir_ndinit.c
index a9b8dfd..1f813b6 100644
--- a/prov/netdir/src/netdir_ndinit.c
+++ b/prov/netdir/src/netdir_ndinit.c
@@ -36,7 +36,7 @@
 #include <guiddef.h>
 
 #include <ws2spi.h>
-#include <cassert>
+#include <assert.h>
 #include "ndspi.h"
 
 #include "netdir.h"
diff --git a/prov/psm3/Makefile.am b/prov/psm3/Makefile.am
index 08f214d..f0d07f1 100644
--- a/prov/psm3/Makefile.am
+++ b/prov/psm3/Makefile.am
@@ -45,6 +45,7 @@ common_srcs = \
 	shared/hmem.c \
 	shared/hmem_rocr.c \
 	shared/hmem_cuda.c \
+	shared/hmem_cuda_gdrcopy.c \
 	shared/hmem_ze.c \
 	shared/common.c \
 	shared/enosys.c \
@@ -80,6 +81,7 @@ common_srcs = \
 	util/src/rocr_mem_monitor.c
 
 if MACOS
+common_srcs += shared/osx/osd.c
 common_srcs += shared/unix/osd.c
 common_srcs += inc/osx/osd.h
 common_srcs += inc/unix/osd.h
@@ -190,7 +192,7 @@ src_libpsm3_fi_la_LDFLAGS += \
 
 chksum_srcs = $(src_libpsm3_fi_la_SOURCES)
 if HAVE_PSM3_SRC
-src_libpsm3_fi_la_SOURCES += src/psm3_revision.c
+nodist_src_libpsm3_fi_la_SOURCES += src/psm3_revision.c
 
 include psm3/Makefile.include
 src_libpsm3_fi_la_LIBADD += libpsm2.la
@@ -201,7 +203,7 @@ src_libpsm3_fi_la_LDFLAGS += -lpsm2
 endif !HAVE_PSM3_SRC
 
 if !EMBEDDED
-src_libpsm3_fi_la_LDFLAGS += -version-info 15:2:14
+src_libpsm3_fi_la_LDFLAGS += -version-info 16:1:15
 endif
 
 prov_install_man_pages = man/man7/fi_psm3.7
@@ -213,7 +215,8 @@ man_MANS = $(prov_install_man_pages)
 EXTRA_DIST += \
         libpsm3-fi.spec.in \
         config/distscript.pl \
-        $(prov_dist_man_pages)
+        $(prov_dist_man_pages) \
+        VERSION
 
 pkgconfigdir = $(libdir)/pkgconfig
 pkgconfig_DATA = libpsm3-fi.pc
diff --git a/prov/psm3/Makefile.include b/prov/psm3/Makefile.include
index 3a68998..9cc837c 100644
--- a/prov/psm3/Makefile.include
+++ b/prov/psm3/Makefile.include
@@ -23,33 +23,37 @@ _psm3_files = \
 	prov/psm3/src/psmx3_wait.c
 
 _psm3_cppflags = \
-	-I$(top_srcdir)/prov/psm3 \
-	-I$(top_srcdir)/prov/psm3/include
+	-I$(top_srcdir)/prov/psm3
 
 chksum_srcs = $(_psm3_files)
 
 if HAVE_PSM3_SRC
-_psm3_cflags = -mavx2
-#include prov/psm3/psm3/Makefile.include
+_psm3_cflags = $(psm3_ARCH_CFLAGS)
 _nodist_psm3_files = \
-	prov/psm3/src/psm3_revision.c
+	prov/psm3/src/psm3_revision.c \
+	prov/psm3/src/psm3_src_chksum.h
 
 # builddir is for nodist config headers: See nodist_libpsm3i_la_SOURCES
 _psm3_cppflags += \
+	-I$(top_srcdir)/prov/psm3/src \
 	-I$(top_srcdir)/prov/psm3/psm3 \
-	-I$(top_builddir)/prov/psm3/psm3 \
-	-I$(top_srcdir)/prov/psm3/psm3/ptl_ips/ \
-	-I$(top_srcdir)/prov/psm3/psm3/include/ \
-	-I$(top_srcdir)/prov/psm3/psm3/include/linux-i386/ \
+	-I$(top_srcdir)/prov/psm3/psm3/ptl_ips \
+	-I$(top_srcdir)/prov/psm3/psm3/include \
+	-I$(top_srcdir)/prov/psm3/psm3/include/linux-i386 \
 	-I$(top_srcdir)/prov/psm3/psm3/mpspawn \
 	-I$(top_srcdir)/prov/psm3/psm3/opa \
+	-I$(top_builddir)/prov/psm3/psm3 \
 	-D_GNU_SOURCE=1
 
-noinst_LTLIBRARIES += libopa.la libuuid.la \
-	libptl_am.la libptl_ips.la libptl_self.la \
-	libpsm_hal_gen1.la libpsm3i.la
+noinst_LTLIBRARIES += \
+	prov/psm3/psm3/libopa.la \
+	prov/psm3/psm3/libptl_am.la \
+	prov/psm3/psm3/libptl_ips.la \
+	prov/psm3/psm3/libptl_self.la \
+	prov/psm3/psm3/libpsm_hal_gen1.la \
+	prov/psm3/psm3/libpsm3i.la
 
-libptl_am_la_SOURCES = \
+prov_psm3_psm3_libptl_am_la_SOURCES = \
 	prov/psm3/psm3/ptl_am/am_config.h \
 	prov/psm3/psm3/ptl_am/am_cuda_memhandle_cache.c \
 	prov/psm3/psm3/ptl_am/am_cuda_memhandle_cache.h \
@@ -60,13 +64,13 @@ libptl_am_la_SOURCES = \
 	prov/psm3/psm3/ptl_am/psm_am_internal.h \
 	prov/psm3/psm3/ptl_am/ptl.c \
 	prov/psm3/psm3/ptl_am/ptl_fwd.h
-libptl_am_la_CPPFLAGS = \
+prov_psm3_psm3_libptl_am_la_CPPFLAGS = \
 	-I$(top_srcdir)/prov/psm3/psm3/ptl_am/ \
 	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libptl_am_la_CFLAGS = \
+prov_psm3_psm3_libptl_am_la_CFLAGS = \
 	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
 
-libptl_ips_la_SOURCES = \
+prov_psm3_psm3_libptl_ips_la_SOURCES = \
 	prov/psm3/psm3/ptl_ips/ips_config.h \
 	prov/psm3/psm3/ptl_ips/ips_crc32.c \
 	prov/psm3/psm3/ptl_ips/ips_epstate.c \
@@ -108,38 +112,24 @@ libptl_ips_la_SOURCES = \
 	prov/psm3/psm3/ptl_ips/ptl_fwd.h \
 	prov/psm3/psm3/ptl_ips/ptl_ips.h \
 	prov/psm3/psm3/ptl_ips/ptl_rcvthread.c
-libptl_ips_la_CPPFLAGS = \
+prov_psm3_psm3_libptl_ips_la_CPPFLAGS = \
 	-I$(top_srcdir)/prov/psm3/psm3/ptl_ips/ \
 	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libptl_ips_la_CFLAGS = \
+prov_psm3_psm3_libptl_ips_la_CFLAGS = \
 	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
-libptl_ips_la_DEPENDENCIES = \
-	libopa.la
+prov_psm3_psm3_libptl_ips_la_DEPENDENCIES = \
+	prov/psm3/psm3/libopa.la
 
-libptl_self_la_SOURCES = \
+prov_psm3_psm3_libptl_self_la_SOURCES = \
 	prov/psm3/psm3/ptl_self/ptl.c \
 	prov/psm3/psm3/ptl_self/ptl_fwd.h
-libptl_self_la_CPPFLAGS = \
+prov_psm3_psm3_libptl_self_la_CPPFLAGS = \
 	-I$(top_srcdir)/prov/psm3/psm3/ptl_self/ \
 	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libptl_self_la_CFLAGS = \
-	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
-
-libuuid_la_SOURCES = \
-	prov/psm3/psm3/libuuid/pack.c \
-	prov/psm3/psm3/libuuid/parse.c \
-	prov/psm3/psm3/libuuid/psm_uuid.c \
-	prov/psm3/psm3/libuuid/psm_uuid.h \
-	prov/psm3/psm3/libuuid/unpack.c \
-	prov/psm3/psm3/libuuid/unparse.c
-#   prov/psm3/psm3/libuuid/compare.c    # Omitted as it is not needed to build lib
-libuuid_la_CPPFLAGS = \
-	-I$(top_srcdir)/prov/psm3/psm3/libuuid/ \
-	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libuuid_la_CFLAGS = \
+prov_psm3_psm3_libptl_self_la_CFLAGS = \
 	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
 
-libopa_la_SOURCES = \
+prov_psm3_psm3_libopa_la_SOURCES = \
 	prov/psm3/psm3/opa/opa_debug.c \
 	prov/psm3/psm3/opa/opa_dwordcpy-x86_64.c \
 	prov/psm3/psm3/opa/opa_service.c \
@@ -156,16 +146,16 @@ libopa_la_SOURCES = \
 	prov/psm3/psm3/include/opa_udebug.h \
 	prov/psm3/psm3/include/opa_user.h \
 	prov/psm3/psm3/include/psm2_mock_testing.h \
-	prov/psm3/psm3/include/rbtree.h \
+	prov/psm3/psm3/include/psm3_rbtree.h \
 	prov/psm3/psm3/include/linux-i386/bit_ops.h \
 	prov/psm3/psm3/include/linux-i386/sysdep.h \
 	prov/psm3/psm3/mpspawn/mpspawn_stats.h
-libopa_la_CPPFLAGS = \
+prov_psm3_psm3_libopa_la_CPPFLAGS = \
 	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libopa_la_CFLAGS = \
+prov_psm3_psm3_libopa_la_CFLAGS = \
 	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
 
-libpsm_hal_gen1_la_SOURCES = \
+prov_psm3_psm3_libpsm_hal_gen1_la_SOURCES = \
 	prov/psm3/psm3/psm_hal_gen1/hfi1_deprecated_gen1.h \
 	prov/psm3/psm3/psm_hal_gen1/opa_common_gen1.h \
 	prov/psm3/psm3/psm_hal_gen1/opa_i2cflash_gen1.c \
@@ -179,13 +169,13 @@ libpsm_hal_gen1_la_SOURCES = \
 	prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1.h \
 	prov/psm3/psm3/psm_hal_gen1/psm_hal_inline_i.h \
 	prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1_spio.h
-libpsm_hal_gen1_la_CPPFLAGS = \
+prov_psm3_psm3_libpsm_hal_gen1_la_CPPFLAGS = \
 	-I$(top_srcdir)/prov/psm3/psm3/psm_hal_gen1/ \
 	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libpsm_hal_gen1_la_CFLAGS = \
+prov_psm3_psm3_libpsm_hal_gen1_la_CFLAGS = \
 	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
 
-libpsm3i_la_SOURCES = \
+prov_psm3_psm3_libpsm3i_la_SOURCES = \
 	prov/psm3/psm3/psm.c \
 	prov/psm3/psm3/psm_am.c \
 	prov/psm3/psm3/psm_am_internal.h \
@@ -239,57 +229,52 @@ libpsm3i_la_SOURCES = \
 	prov/psm3/psm3/psm2_hal_inline_t.h \
 	prov/psm3/psm3/psm2_mq.h \
 	prov/psm3/psm3/ptl.h
-libpsm3i_la_CPPFLAGS = \
+prov_psm3_psm3_libpsm3i_la_CPPFLAGS = \
 	-I$(top_srcdir)/prov/psm3/psm3/include/ \
 	$(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
-libpsm3i_la_CFLAGS = \
+prov_psm3_psm3_libpsm3i_la_CFLAGS = \
 	$(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
 
-nodist_libpsm3i_la_SOURCES = \
+nodist_prov_psm3_psm3_libpsm3i_la_SOURCES = \
 	prov/psm3/psm3/psm2_hal_inlines_i.h \
 	prov/psm3/psm3/psm2_hal_inlines_d.h
 
-libpsm3i_la_LIBADD = \
-	libopa.la \
-	libuuid.la \
-	libptl_am.la \
-	libptl_ips.la \
-	libptl_self.la \
-	libpsm_hal_gen1.la
+prov_psm3_psm3_libpsm3i_la_LIBADD = \
+	prov/psm3/psm3/libopa.la \
+	prov/psm3/psm3/libptl_am.la \
+	prov/psm3/psm3/libptl_ips.la \
+	prov/psm3/psm3/libptl_self.la \
+	prov/psm3/psm3/libpsm_hal_gen1.la
 
-libpsm3i_la_DEPENDENCIES = \
-	libopa.la \
-	libuuid.la \
-	libptl_am.la \
-	libptl_ips.la \
-	libptl_self.la \
-	libpsm_hal_gen1.la
+prov_psm3_psm3_libpsm3i_la_DEPENDENCIES = \
+	prov/psm3/psm3/libopa.la \
+	prov/psm3/psm3/libptl_am.la \
+	prov/psm3/psm3/libptl_ips.la \
+	prov/psm3/psm3/libptl_self.la \
+	prov/psm3/psm3/libpsm_hal_gen1.la
 
-EXTRA_DIST += \
-	prov/psm3/psm3/include/rbtree.c \
+_psm3_extra_dist = \
+	prov/psm3/psm3/include/psm3_rbtree.c \
 	prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1_spio.c \
 	prov/psm3/psm3/opa/opa_dwordcpy-x86_64-fast.S
+EXTRA_DIST += $(_psm3_extra_dist)
 
 chksum_srcs += \
-	$(libptl_am_la_SOURCES) $(libptl_ips_la_SOURCES) $(libptl_self_la_SOURCES) \
-	$(libuuid_la_SOURCES) $(libopa_la_SOURCES) $(libpsm_hal_gen1_la_SOURCES) \
-	$(libpsm3i_la_SOURCES) $(EXTRA_DIST)
+	$(prov_psm3_psm3_libptl_am_la_SOURCES) $(prov_psm3_psm3_libptl_ips_la_SOURCES) \
+	$(prov_psm3_psm3_libptl_self_la_SOURCES) $(prov_psm3_psm3_libopa_la_SOURCES) \
+	$(prov_psm3_psm3_libpsm_hal_gen1_la_SOURCES) $(prov_psm3_psm3_libpsm3i_la_SOURCES) \
+	$(_psm3_extra_dist)
 
-_psm3_LIBS = libpsm3i.la
-libpsm3_la_DEPENDENCIES = libpsm3i.la
+_psm3_LIBS = prov/psm3/psm3/libpsm3i.la
 
-all-local:
-	@echo "Building src checksum..."; \
-	chksum=`cat $(chksum_srcs) | sha1sum | cut -d' ' -f 1`; \
-	if ! grep -q $$chksum prov/psm3/src/psm3_revision.c 2>/dev/null; then \
-		sed -i "/define PSMX3_SRC_CHECKSUM/s/\".*\"/\"$$chksum\"/" prov/psm3/src/psm3_revision.c; \
-		echo "SRC checksum updated to $$chksum"; \
-	else \
-		echo "SRC checksum not changed: $$chksum"; \
-	fi; \
-	timestamp=`date`; \
-	sed -i "/define PSMX3_BUILD_TIMESTAMP/s/\".*\"/\"$$timestamp\"/" prov/psm3/src/psm3_revision.c; \
-	echo "Updated build timestamp: $$timestamp"
+BUILT_SOURCES = prov/psm3/src/psm3_src_chksum.h
+CLEANFILES = prov/psm3/src/psm3_src_chksum.h
+prov/psm3/src/psm3_src_chksum.h: Makefile $(chksum_srcs)
+	$(AM_V_GEN) chksum=`for file in $(chksum_srcs); do cat $(top_srcdir)/$$file; done | sha1sum | cut -d' ' -f 1`; \
+	if ! grep -q $$chksum prov/psm3/src/psm3_src_chksum.h 2>/dev/null; then \
+		echo "#define PSMX3_SRC_CHECKSUM \"$$chksum\"" > prov/psm3/src/psm3_src_chksum.h; \
+		echo "#define PSMX3_BUILD_TIMESTAMP \"`date`\"" >> prov/psm3/src/psm3_src_chksum.h; \
+	fi
 
 endif HAVE_PSM3_SRC
 
@@ -297,36 +282,44 @@ if HAVE_PSM3_DL
 pkglib_LTLIBRARIES += libpsm3-fi.la
 libpsm3_fi_la_SOURCES = $(_psm3_files) $(common_srcs)
 nodist_libpsm3_fi_la_SOURCES = $(_nodist_psm3_files)
-libpsm3_fi_la_CFLAGS = $(AM_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
+libpsm3_fi_la_CFLAGS = $(AM_CFLAGS) $(psm3_CFLAGS)
 libpsm3_fi_la_CPPFLAGS = $(AM_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
 libpsm3_fi_la_LDFLAGS = \
 	-module -avoid-version -shared -export-dynamic \
 	-export-symbols-regex ^fi_prov_ini $(psm3_LDFLAGS)
 libpsm3_fi_la_LIBADD = $(linkback) $(psm3_LIBS) $(_psm3_LIBS)
-libpsm3_fi_la_DEPENDENCIES = $(linkback)
+libpsm3_fi_la_DEPENDENCIES = $(linkback) \
+	prov/psm3/src/psm3_src_chksum.h \
+	prov/psm3/psm3/libpsm3i.la
 else !HAVE_PSM3_DL
 noinst_LTLIBRARIES += libpsm3.la
 libpsm3_la_SOURCES = $(_psm3_files)
+libpsm3_la_DEPENDENCIES = \
+	prov/psm3/src/psm3_src_chksum.h \
+	prov/psm3/psm3/libpsm3i.la
 nodist_libpsm3_la_SOURCES = $(_nodist_psm3_files)
-libpsm3_la_CFLAGS = $(src_libfabric_la_CFLAGS) $(psm3_CFLAGS) $(_psm3_cflags)
+libpsm3_la_CFLAGS = $(src_libfabric_la_CFLAGS) $(psm3_CFLAGS)
 libpsm3_la_CPPFLAGS = $(src_libfabric_la_CPPFLAGS) $(psm3_CPPFLAGS) $(_psm3_cppflags)
 libpsm3_la_LDFLAGS = $(psm3_LDFLAGS)
 libpsm3_la_LIBADD = $(psm3_LIBS) $(_psm3_LIBS)
 src_libfabric_la_LIBADD += libpsm3.la
 src_libfabric_la_DEPENDENCIES += libpsm3.la
 
-.libs/libpsm3_full.lo: $(libpsm3_la_OBJECTS) $(libpsm3_la_DEPENDENCIES) $(EXTRA_libpsm3_la_DEPENDENCIES)
-	@sed -i.bak "/dependency_libs/s/='.*'/=''/" libpsm3i.la
-	$(AM_V_CCLD)$(libpsm3_la_LINK) -r $(am_libpsm3_la_rpath) $(libpsm3_la_OBJECTS) libpsm3i.la
+if HAVE_PSM2_SRC
+prov/psm3/psm3/.libs/libpsm3_full.lo: $(libpsm3_la_OBJECTS) $(libpsm3_la_DEPENDENCIES) $(EXTRA_libpsm3_la_DEPENDENCIES)
+	@sed -i.bak "/dependency_libs/s/='.*'/=''/" prov/psm3/psm3/libpsm3i.la
+	$(AM_V_CCLD)$(libpsm3_la_LINK) -r $(am_libpsm3_la_rpath) $(libpsm3_la_OBJECTS) prov/psm3/psm3/libpsm3i.la
 
-.libs/libpsm3_exp.o: .libs/libpsm3_full.lo
-	@objcopy --keep-global-symbol=fi_psm3_ini .libs/libpsm3_full.o .libs/libpsm3_exp.o
+prov/psm3/psm3/.libs/libpsm3_exp.o: prov/psm3/psm3/.libs/libpsm3_full.lo
+	@objcopy --keep-global-symbol=fi_psm3_ini prov/psm3/psm3/.libs/libpsm3_full.o prov/psm3/psm3/.libs/libpsm3_exp.o
 
-libpsm3.la: .libs/libpsm3_exp.o
-	@mv libpsm3i.la.bak libpsm3i.la
+libpsm3.la: prov/psm3/psm3/.libs/libpsm3_exp.o
+	@mv prov/psm3/psm3/libpsm3i.la.bak prov/psm3/psm3/libpsm3i.la
 	$(AM_V_CCLD)$(libpsm3_la_LINK) $(am_libpsm3_la_rpath) $(libpsm3_la_OBJECTS) $(libpsm3_la_LIBADD) $(LIBS); \
 	rm -f .libs/libpsm3.a libpsm3.a; \
-	ar cru .libs/libpsm3.a  .libs/libpsm3_exp.o
+	$(AR) cru .libs/libpsm3.a prov/psm3/psm3/.libs/libpsm3_exp.o; \
+	$(RANLIB) .libs/libpsm3.a
+endif HAVE_PSM2_SRC
 
 endif !HAVE_PSM3_DL
 
diff --git a/prov/psm3/VERSION b/prov/psm3/VERSION
new file mode 100644
index 0000000..b435351
--- /dev/null
+++ b/prov/psm3/VERSION
@@ -0,0 +1 @@
+3_1_0_0
diff --git a/prov/psm3/configure.ac b/prov/psm3/configure.ac
index 5bc5211..8ea1671 100644
--- a/prov/psm3/configure.ac
+++ b/prov/psm3/configure.ac
@@ -6,7 +6,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ([2.60])
-AC_INIT([libpsm3-fi], [1.11.2], [ofiwg@lists.openfabrics.org])
+AC_INIT([libpsm3-fi], m4_normalize(m4_esyscmd([sed 's/_/./g' VERSION])))
 AC_CONFIG_SRCDIR([src/psmx3.h])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
@@ -14,6 +14,7 @@ AC_CONFIG_HEADERS(config.h)
 AM_INIT_AUTOMAKE([1.11 dist-bzip2 foreign -Wall -Werror subdir-objects parallel-tests tar-pax])
 m4_ifdef([AM_SILENT_RULES], [AM_SILENT_RULES([no])])
 dnl --- m4_include(config/fi_check_package.m4)
+m4_include(config/fi_strip_optflags.m4)
 AC_DEFINE([HAVE_PSM3], [1], [Build libfabric PSM3 provider])
 AC_DEFINE([HAVE_PSM3_DL], [1], [Build libfabric PSM3 provider])
 
@@ -94,7 +95,7 @@ AS_IF([test x$with_psm3_rv = xno],
 		AS_IF([test "$psm3_rv_old_header" -eq 1],
 		      [CPPFLAGS="$CPPFLAGS -DHAVE_OLD_RV_HEADER"])
 	      ])
-	AS_IF([test "$psm3_rv_happy" -eq 1], [
+	AS_IF([test "$psm3_rv_happy" -eq 1 ], [
 		AC_MSG_CHECKING([for RV support for ring.overflow_cnt])
 		AC_COMPILE_IFELSE(
 			[AC_LANG_PROGRAM(
@@ -142,6 +143,7 @@ AS_IF([test "x$enable_psm_src" == "xyes"],
 	AC_SEARCH_LIBS([shm_open], [rt], [], [AC_MSG_ERROR([unable to find shm_open() in librt])])
 	AC_SEARCH_LIBS([dlopen], [dl], [], [AC_MSG_ERROR([unable to find dlopen() in libdl])])
 	AC_SEARCH_LIBS([numa_node_of_cpu], [numa], [], [AC_MSG_ERROR([unable to find numa_node_of_cpu() in libnuma])])
+	AC_SEARCH_LIBS([uuid_parse], [uuid], [], [AC_MSG_ERROR([unable to find uuid_parse() in libuuid])])
 	AS_IF([test "x$enable_psm_ud" == "xyes"],
 	      [AC_SEARCH_LIBS([ibv_get_device_list], [ibverbs], [],
 			      [AC_MSG_ERROR([unable to find ibv_get_device_list() in libibverbs])])
@@ -171,8 +173,6 @@ AS_IF([test "x$enable_psm_src" == "xyes"],
 	AS_IF([test ! -z "$PSM_PERF"], [CPPFLAGS="$CPPFLAGS -DRDPMC_PERF_FRAMEWORK"], [])
 	AS_IF([test ! -z "$PSM_HEAP_DEBUG"], [CPPFLAGS="$CPPFLAGS -DPSM_HEAP_DEBUG"], [])
 	AS_IF([test ! -z "$PSM_PROFILE"], [CPPFLAGS="$CPPFLAGS -DPSM_PROFILE"], [])
-	AS_IF([test ! -z "$PSM_CPPFLAGS"], [CPPFLAGS="$CPPFLAGS $PSM_CPPFLAGS"], [])
-	AS_IF([test ! -z "$PSM_CFLAGS"], [CFLAGS="$CFLAGS $PSM_CFLAGS"], [])
       ])
 
 AM_CONDITIONAL([HAVE_PSM3_ADDITIONAL_GLOBALS], [test ! -z "$PSM2_ADDITIONAL_GLOBALS"], [])
@@ -656,11 +656,14 @@ AC_DEFINE_UNQUOTED([HAVE_LIBCUDA], [$have_libcuda], [Whether we have CUDA runtim
 if test $have_libcuda -eq 1; then
 	cuda_CPPFLAGS="$cuda_CPPFLAGS -DPSM_CUDA -DNVIDIA_GPU_DIRECT"
 fi
+AC_DEFINE_UNQUOTED([PSM3_CUDA], [$have_libcuda], [Whether we have CUDA runtime or not])
 
 CPPFLAGS="$CPPFLAGS $cuda_CPPFLAGS"
 LDFLAGS="$LDFLAGS $cuda_LDFLAGS"
 LIBS="$LIBS $cuda_LIBS"
 
+AS_IF([test ! -z "$PSM_CPPFLAGS"], [CPPFLAGS="$CPPFLAGS $PSM_CPPFLAGS"], [])
+AS_IF([test ! -z "$PSM_CFLAGS"], [CFLAGS="$CFLAGS $PSM_CFLAGS"], [])
 dnl Provider-specific checks
 dnl FI_PROVIDER_INIT
 dnl FI_PROVIDER_SETUP([psm3])
@@ -675,10 +678,29 @@ AM_COND_IF([HAVE_PSM3_SRC],
 	   [
 		IFS_VERSION="${RELEASE_TAG:-$(git describe --dirty --always --abbrev=8 --broken --tags 2>/dev/null \
 			|| git describe --dirty --always --abbrev=8 --broken 2>/dev/null || echo 'unknown commit')}"
+		IFS_VERSION=${IFS_VERSION//./_}
 		GIT_HASH="$(git log --oneline --format='%H' -1)"
+		RPM_RELEASE=$(echo "${IFS_VERSION}" | cut -d'_' -f5)
+		RELEASE_VER=$(echo "${IFS_VERSION}" | cut -d'_' -f1-4 | sed 's/_/./g')
+		AS_IF([test x"${RELEASE_VER}" = x"${PACKAGE_VERSION}"], [], [
+			AC_MSG_NOTICE([Release Tag does not match VERSION file])
+			AC_MSG_NOTICE([${RELEASE_VER} != ${PACKAGE_VERSION}])
+			RPM_RELEASE=999
+		])
+		PSM3_PROV_VER_MAJOR=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f1)
+		PSM3_PROV_VER_MINOR=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f2)
+		PSM3_PROV_VER_MAINT=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f3)
+		PSM3_PROV_VER_PATCH=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f4)
 	   ])
+AS_IF([test $have_libcuda -eq 1], [RPM_RELEASE=${RPM_RELEASE}cuda])
+
 AC_SUBST(IFS_VERSION)
 AC_SUBST(GIT_HASH)
+AC_SUBST(RPM_RELEASE)
+AC_SUBST(PSM3_PROV_VER_MAJOR)
+AC_SUBST(PSM3_PROV_VER_MINOR)
+AC_SUBST(PSM3_PROV_VER_MAINT)
+AC_SUBST(PSM3_PROV_VER_PATCH)
 dnl Set during Make.
 dnl AC_SUBST(BUILD_TIMESTAMP)
 dnl AC_SUBST(SRC_CHECKSUM)
diff --git a/prov/psm3/configure.m4 b/prov/psm3/configure.m4
index 6834373..c78d035 100644
--- a/prov/psm3/configure.m4
+++ b/prov/psm3/configure.m4
@@ -64,19 +64,33 @@ ifelse('
 		                 [psm3_ibv_happy=1],
 		                 [psm3_happy=0])
 
+		FI_CHECK_PACKAGE([psm3_uuid],
+		                 [uuid/uuid.h],
+		                 [uuid],
+		                 [uuid_parse],
+		                 [],
+		                 [$psm3_PREFIX],
+		                 [$psm3_LIBDIR],
+		                 [psm3_uuid_happy=1],
+		                 [psm3_happy=0])
+
 		AC_MSG_CHECKING([for -msse4.2 support])
+
+		dnl Strip other optflags to avoid conflicts when checking for instruction sets
+		FI_STRIP_OPTFLAGS($CFLAGS)
+		PSM3_STRIP_OPTFLAGS="$s_result"
+
 		save_CFLAGS=$CFLAGS
-		CFLAGS="$CFLAGS -msse4.2"
+		CFLAGS="$PSM3_STRIP_OPTFLAGS -msse4.2 -O0"
 		AC_LINK_IFELSE(
 			[AC_LANG_PROGRAM(
-				[#include <nmmintrin.h>],
-				[unsigned int crc = 0;
-				 crc = _mm_crc32_u32(crc, 0);
-				 return crc == 0;])
+				[[#include <nmmintrin.h>]],
+				[[unsigned int crc = 0;
+				  crc = _mm_crc32_u32(crc, 0);
+				  return crc == 0;]])
 			],[
 				AC_MSG_RESULT([yes])
-				psm3_crc_happy=1
-				ARCH_CFLAGS="-msse4.2"
+				PSM3_ARCH_CFLAGS="-msse4.2"
 			],[
 				psm3_happy=0
 				AC_MSG_RESULT([no])
@@ -86,18 +100,18 @@ ifelse('
 
 		AC_MSG_CHECKING([for -mavx support])
 		save_CFLAGS=$CFLAGS
-		CFLAGS="$CFLAGS -mavx"
+		CFLAGS="$PSM3_STRIP_OPTFLAGS -mavx -O0"
 		AC_LINK_IFELSE(
 			[AC_LANG_PROGRAM(
-				[#include <immintrin.h>],
-				[unsigned long long *vec_a = {1,2,3,4};
-				 __m256i *sp = (__m256i *)vec_a;
-				 __m256i vec = _mm256_load_si256(sp);
-				 return 0;])
+				[[#include <immintrin.h>]],
+				[[unsigned long long _a[4] = {1ULL,2ULL,3ULL,4ULL};
+				  __m256i vA = _mm256_loadu_si256((__m256i *)_a);
+				  __m256i vB;
+				  _mm256_store_si256(&vB, vA);
+				  return 0;]])
 			],[
 				AC_MSG_RESULT([yes])
-				psm3_256_happy=1
-				ARCH_CFLAGS="-mavx"
+				PSM3_ARCH_CFLAGS="-mavx"
 			],[
 				psm3_happy=0
 				AC_MSG_RESULT([no])
@@ -105,6 +119,29 @@ ifelse('
 			])
 		CFLAGS=$save_CFLAGS
 
+		AC_MSG_CHECKING([for -mavx2 support])
+		save_CFLAGS=$CFLAGS
+		CFLAGS="$PSM3_STRIP_OPTFLAGS -mavx2 -O0"
+		AC_LINK_IFELSE(
+			[AC_LANG_PROGRAM(
+				[[#include <immintrin.h>]],
+				[[unsigned long long _a[4] = {1ULL,2ULL,3ULL,4ULL};
+				  __m256i vA = _mm256_loadu_si256((__m256i *)_a);
+				  __m256i vB = _mm256_add_epi64(vA, vA);
+				  (void)vB;
+				  return 0;]])
+			],[
+				AC_MSG_RESULT([yes])
+				PSM3_ARCH_CFLAGS="-mavx2"
+			],[
+				AC_MSG_RESULT([no])
+			])
+		CFLAGS=$save_CFLAGS
+
+		AS_IF([test $have_libcuda -eq 1],
+		      [psm3_CPPFLAGS="$psm3_CPPFLAGS -DPSM_CUDA -DNVIDIA_GPU_DIRECT"])
+		AC_DEFINE_UNQUOTED([PSM3_CUDA], [$have_libcuda], [Whether we have CUDA runtime or not])
+
 		AS_IF([test x$with_psm3_rv = xno],
 		      [psm3_CPPFLAGS="$psm3_CPPFLAGS -URNDV_MOD"],
 		      [
@@ -153,8 +190,11 @@ ifelse('
 					[AC_LANG_PROGRAM(
 						[[#include <sys/types.h>
 						  #include <stdint.h>
-						  #include <rdma/rv_user_ioctls.h>
-						]],[[struct rv_ring_header ring; ring.overflow_cnt=0;]])
+						  #include <rdma/rv_user_ioctls.h>]],
+						[[struct rv_ring_header ring;
+						  ring.overflow_cnt=0;
+						  (void)ring;
+						  return 0;]])
 					],[
 						AC_MSG_RESULT(yes)
 					],[
@@ -172,17 +212,34 @@ ifelse('
 
 	 AS_IF([test $psm3_happy -eq 1], [$1], [$2])
 
-	 psm3_CFLAGS="$ARCH_CFLAGS"
-	 psm3_CPPFLAGS="$psm3_CPPFLAGS $psm3_rt_CPPFLAGS $psm3_dl_CPPFLAGS $psm3_numa_CPPFLAGS $psm3_ibv_CPPFLAGS"
-	 psm3_LDFLAGS="$psm3_LDFLAGS $psm3_rt_LDFLAGS $psm3_dl_LDFLAGS $psm3_numa_LDFLAGS $psm3_ibv_LDFLAGS"
-	 psm3_LIBS="$psm3_LIBS $psm3_rt_LIBS $psm3_dl_LIBS $psm3_numa_LIBS $psm3_ibv_LIBS"
+	 psm3_CFLAGS=""
+	 psm3_ARCH_CFLAGS="$PSM3_ARCH_CFLAGS"
+	 psm3_CPPFLAGS="$psm3_CPPFLAGS $psm3_rt_CPPFLAGS $psm3_dl_CPPFLAGS $psm3_numa_CPPFLAGS $psm3_ibv_CPPFLAGS $psm3_uuid_CPPFLAGS"
+	 psm3_LDFLAGS="$psm3_LDFLAGS $psm3_rt_LDFLAGS $psm3_dl_LDFLAGS $psm3_numa_LDFLAGS $psm3_ibv_LDFLAGS $psm3_uuid_LDFLAGS"
+	 psm3_LIBS="$psm3_LIBS $psm3_rt_LIBS $psm3_dl_LIBS $psm3_numa_LIBS $psm3_ibv_LIBS $psm3_uuid_LIBS"
 	 AC_SUBST(psm3_CFLAGS)
+	 AC_SUBST(psm3_ARCH_CFLAGS)
 	 AC_SUBST(psm3_CPPFLAGS)
 	 AC_SUBST(psm3_LDFLAGS)
 	 AC_SUBST(psm3_LIBS)
 	 AC_SUBST(PSM_HAL_CNT)
 	 AC_SUBST(PSM_HAL_INST)
 
+	 PSM3_IFS_VERSION=m4_normalize(m4_esyscmd([cat prov/psm3/VERSION]))
+	 AC_SUBST(PSM3_IFS_VERSION)
+	 PSM3_PROV_VER_MAJOR=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f1)
+	 PSM3_PROV_VER_MINOR=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f2)
+	 PSM3_PROV_VER_MAINT=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f3)
+	 PSM3_PROV_VER_PATCH=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f4)
+	 AC_SUBST(PSM3_PROV_VER_MAJOR)
+	 AC_SUBST(PSM3_PROV_VER_MINOR)
+	 AC_SUBST(PSM3_PROV_VER_MAINT)
+	 AC_SUBST(PSM3_PROV_VER_PATCH)
+
+	 AC_SUBST(PSM3_BUILD_TIMESTAMP, ["<Unknown>"])
+	 AC_SUBST(PSM3_SRC_CHECKSUM, ["<Unknown>"])
+	 AC_SUBST(PSM3_GIT_HASH, ["<Unknown>"])
+
 ])
 
 AC_ARG_WITH([psm3-rv],
diff --git a/prov/psm3/libpsm3-fi.spec.in b/prov/psm3/libpsm3-fi.spec.in
index aca6ef7..eee516c 100644
--- a/prov/psm3/libpsm3-fi.spec.in
+++ b/prov/psm3/libpsm3-fi.spec.in
@@ -4,7 +4,7 @@
 
 Name: lib%{provider}-fi
 Version: @VERSION@
-Release: 1%{?dist}
+Release: 179_@RPM_RELEASE@
 Summary: Dynamic %{provider_formal} provider for Libfabric
 
 Group: System Environment/Libraries
@@ -17,6 +17,17 @@ Requires: libfabric
 Provides: lib${provider}-fi1 = %{version}-%{release}
 %endif
 
+BuildRequires: libuuid-devel
+BuildRequires: rdma-core-devel
+%if 0%{?suse_version} >= 1
+BuildRequires: glibc-devel
+BuildRequires: libnuma-devel
+%endif
+%if 0%{?rhel} >= 1
+BuildRequires: glibc-headers
+BuildRequires: numactl-devel
+%endif
+
 %description
 This RPM provides the %{provider_formal} provider as a "plugin" to an existing
 libfabric installation.  This plugin will override older %{provider_formal}
@@ -52,5 +63,8 @@ rm -rf %{buildroot}
 %exclude %{_mandir}
 
 %changelog
+* Wed Mar 31 2021 Adam Goldman <adam.goldman@intel.com>
+- Include BuildRequires lines for RHEL and SLES
+
 * Wed May 24 2017 Open Fabrics Interfaces Working Group <ofiwg@lists.openfabrics.org>
 - First release of specfile for packaging a single dl provider.
diff --git a/prov/psm3/psm3/Makefile.include b/prov/psm3/psm3/Makefile.include
index 9dad7d0..51b79c1 100644
--- a/prov/psm3/psm3/Makefile.include
+++ b/prov/psm3/psm3/Makefile.include
@@ -1,12 +1,16 @@
 
 _CPPFLAGS = \
-	-I./psm3/ -I./psm3/ptl_ips/ \
-	-I./psm3/include -I./psm3/include/linux-i386 \
-	-I./psm3/mpspawn -I./psm3/opa \
+	-I$(top_srcdir)/psm3/ \
+	-I$(top_builddir)/psm3/ \
+	-I$(top_srcdir)/psm3/ptl_ips/ \
+	-I$(top_srcdir)/psm3/include \
+	-I$(top_srcdir)/psm3/include/linux-i386 \
+	-I$(top_srcdir)/psm3/mpspawn \
+	-I$(top_srcdir)/psm3/opa \
 	-D_GNU_SOURCE=1 \
 	$(AM_CPPFLAGS)
 
-noinst_LTLIBRARIES += libopa.la libuuid.la \
+noinst_LTLIBRARIES += libopa.la \
 		     libptl_am.la libptl_ips.la libptl_self.la \
 		     libpsm_hal_gen1.la libpsm2.la
 
@@ -22,7 +26,7 @@ libptl_am_la_SOURCES = \
 	psm3/ptl_am/ptl.c \
 	psm3/ptl_am/ptl_fwd.h
 libptl_am_la_CPPFLAGS = \
-	-I./psm3/ptl_am/ \
+	-I$(top_srcdir)/psm3/ptl_am/ \
 	$(_CPPFLAGS)
 
 libptl_ips_la_SOURCES = \
@@ -68,7 +72,7 @@ libptl_ips_la_SOURCES = \
 	psm3/ptl_ips/ptl_ips.h \
 	psm3/ptl_ips/ptl_rcvthread.c
 libptl_ips_la_CPPFLAGS = \
-	-I./psm3/ptl_ips/ \
+	-I$(top_srcdir)/psm3/ptl_ips/ \
 	$(_CPPFLAGS)
 libptl_ips_la_DEPENDENCIES = \
 	libopa.la
@@ -77,19 +81,7 @@ libptl_self_la_SOURCES = \
 	psm3/ptl_self/ptl.c \
 	psm3/ptl_self/ptl_fwd.h
 libptl_self_la_CPPFLAGS = \
-	-I./psm3/ptl_self/ \
-	$(_CPPFLAGS)
-
-libuuid_la_SOURCES = \
-	psm3/libuuid/pack.c \
-	psm3/libuuid/parse.c \
-	psm3/libuuid/psm_uuid.c \
-	psm3/libuuid/psm_uuid.h \
-	psm3/libuuid/unpack.c \
-	psm3/libuuid/unparse.c
-#	psm3/libuuid/compare.c    # Omitted as it is not needed to build lib
-libuuid_la_CPPFLAGS = \
-	-I./psm3/libuuid/ \
+	-I$(top_srcdir)/psm3/ptl_self/ \
 	$(_CPPFLAGS)
 
 libopa_la_SOURCES = \
@@ -112,8 +104,7 @@ libopa_la_SOURCES = \
 	psm3/include/rbtree.h \
 	psm3/include/linux-i386/bit_ops.h \
 	psm3/include/linux-i386/sysdep.h \
-	psm3/mpspawn/mpspawn_stats.h \
-	psm3/opa/opa_dwordcpy-x86_64-fast.S
+	psm3/mpspawn/mpspawn_stats.h
 libopa_la_CPPFLAGS = \
 	$(_CPPFLAGS)
 
@@ -132,7 +123,7 @@ libpsm_hal_gen1_la_SOURCES = \
 	psm3/psm_hal_gen1/psm_hal_inline_i.h \
 	psm3/psm_hal_gen1/psm_hal_gen1_spio.h
 libpsm_hal_gen1_la_CPPFLAGS = \
-	-I./psm3/psm_hal_gen1/ \
+	-I$(top_srcdir)/psm3/psm_hal_gen1/ \
 	$(_CPPFLAGS)
 
 libpsm2_la_SOURCES = \
@@ -186,17 +177,17 @@ libpsm2_la_SOURCES = \
 	psm3/psm2_am.h \
 	psm3/psm2_hal.c \
 	psm3/psm2_hal.h \
-	psm3/psm2_hal_inlines_i.h \
-	psm3/psm2_hal_inlines_d.h \
 	psm3/psm2_hal_inline_t.h \
 	psm3/psm2_mq.h \
 	psm3/ptl.h
 libpsm2_la_CPPFLAGS = \
 	$(_CPPFLAGS)
+nodist_libpsm2_la_SOURCES = \
+	psm3/psm2_hal_inlines_i.h \
+	psm3/psm2_hal_inlines_d.h
 
 libpsm2_la_LIBADD = \
 	libopa.la \
-	libuuid.la \
 	libptl_am.la \
 	libptl_ips.la \
 	libptl_self.la \
@@ -204,7 +195,6 @@ libpsm2_la_LIBADD = \
 
 libpsm2_la_DEPENDENCIES = \
 	libopa.la \
-	libuuid.la \
 	libptl_am.la \
 	libptl_ips.la \
 	libptl_self.la \
@@ -212,9 +202,10 @@ libpsm2_la_DEPENDENCIES = \
 
 EXTRA_DIST += \
 	psm3/include/rbtree.c \
-	psm3/psm_hal_gen1/psm_hal_gen1_spio.c
+	psm3/psm_hal_gen1/psm_hal_gen1_spio.c \
+	psm3/opa/opa_dwordcpy-x86_64-fast.S
 
 chksum_srcs += \
 	$(libptl_am_la_SOURCES) $(libptl_ips_la_SOURCES) $(libptl_self_la_SOURCES) \
-	$(libuuid_la_SOURCES) $(libopa_la_SOURCES) $(libpsm_hal_gen1_la_SOURCES) \
+	$(libopa_la_SOURCES) $(libpsm_hal_gen1_la_SOURCES) \
 	$(libpsm2_la_SOURCES)
diff --git a/prov/psm3/psm3/include/opa_debug.h b/prov/psm3/psm3/include/opa_debug.h
index ce6ef10..749b4ed 100644
--- a/prov/psm3/psm3/include/opa_debug.h
+++ b/prov/psm3/psm3/include/opa_debug.h
@@ -73,10 +73,7 @@
 
 #define __HFI_INFO        0x1	/* generic low verbosity stuff */
 #define __HFI_DBG         0x2	/* generic debug */
-#define __HFI_TRSAMPLE    0x8	/* generate trace buffer sample entries */
 /* leave some low verbosity spots open */
-/* Debug messages specific to UD */
-#define __HFI_UDDBG       0x10
 /* Debug messages related to the connection protocol. */
 #define __HFI_CONNDBG     0x20
 #define __HFI_VERBDBG     0x40	/* very verbose debug */
@@ -87,8 +84,10 @@
 #define __HFI_MMDBG       0x200
 /* low-level environment variables */
 #define __HFI_ENVDBG	    0x400
-#define __HFI_EPKTDBG     0x800	/* print error packet data */
-#define __HFI_CCADBG      0x1000	/* print CCA related events */
+
+#define __HFI_DEBUG_DEFAULT __HFI_INFO
+#define __HFI_DEBUG_DEFAULT_STR "0x0001"
+
 #else /* _HFI_DEBUGGING */
 
 /*
@@ -99,15 +98,15 @@
 
 #define __HFI_INFO      0x0	/* generic low verbosity stuff */
 #define __HFI_DBG       0x0	/* generic debug */
-#define __HFI_TRSAMPLE  0x0	/* generate trace buffer sample entries */
-#define __HFI_UDDBG     0x0
 #define __HFI_CONNDBG   0x0
 #define __HFI_VERBDBG   0x0	/* very verbose debug */
 #define __HFI_PKTDBG    0x0	/* print packet data */
 #define __HFI_PROCDBG   0x0	/* print process startup (init)/exit messages */
 /* print MR, mmap/nopage stuff, not using VDBG any more */
 #define __HFI_MMDBG     0x0
-#define __HFI_CCADBG    0x0	/* print CCA related events */
+
+#define __HFI_DEBUG_DEFAULT __HFI_INFO
+#define __HFI_DEBUG_DEFAULT_STR "0x0000"
 
 #endif /* _HFI_DEBUGGING */
 
diff --git a/prov/psm3/psm3/include/opa_udebug.h b/prov/psm3/psm3/include/opa_udebug.h
index ee81d6f..4a2abfa 100644
--- a/prov/psm3/psm3/include/opa_udebug.h
+++ b/prov/psm3/psm3/include/opa_udebug.h
@@ -89,9 +89,13 @@ int hfi_get_mylocalrank_count();	// -1 if unknown
 
 #if _HFI_DEBUGGING
 
-extern char *__hfi_mylabel;
+extern char __hfi_mylabel[];
 void hfi_set_mylabel(char *);
 extern FILE *__hfi_dbgout;
+extern void hfi_dump_buf(uint8_t *buf, uint32_t len);
+#ifdef PSM_CUDA
+extern void hfi_dump_gpu_buf(uint8_t *buf, uint32_t len);
+#endif
 
 #define _HFI_UNIT_ERROR(unit, fmt, ...) \
 	do { \
@@ -134,19 +138,15 @@ extern FILE *__hfi_dbgout;
 	} while (0)
 
 #define _HFI_DBG(fmt, ...) __HFI_DBG_WHICH(__HFI_DBG, fmt, ##__VA_ARGS__)
-#define _HFI_UDDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_UDDBG, fmt, ##__VA_ARGS__)
 #define _HFI_CONNDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_CONNDBG, fmt, ##__VA_ARGS__)
 #define _HFI_VDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_VERBDBG, fmt, ##__VA_ARGS__)
 #define _HFI_PDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_PKTDBG, fmt, ##__VA_ARGS__)
-#define _HFI_EPDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_EPKTDBG, fmt, ##__VA_ARGS__)
 #define _HFI_PRDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_PROCDBG, fmt, ##__VA_ARGS__)
 #define _HFI_ENVDBG(lev, fmt, ...) \
 	__HFI_DBG_WHICH_NOFUNC(					    \
-		(lev == 0) ? __HFI_INFO :				    \
-		    (lev > 1 ? __HFI_ENVDBG : (__HFI_PROCDBG|__HFI_ENVDBG)),\
+		(lev == 0) ? __HFI_INFO : __HFI_ENVDBG,\
 		"env " fmt, ##__VA_ARGS__)
 #define _HFI_MMDBG(fmt, ...) __HFI_DBG_WHICH(__HFI_MMDBG, fmt, ##__VA_ARGS__)
-#define _HFI_CCADBG(fmt, ...) __HFI_DBG_WHICH(__HFI_CCADBG, fmt, ##__VA_ARGS__)
 
 /*
  * Use these macros (_HFI_DBG_ON and _HFI_DBG_ALWAYS) together
@@ -164,9 +164,6 @@ extern FILE *__hfi_dbgout;
 			##__VA_ARGS__); \
 	} while (0)
 
-#define _HFI_UDDBG_ON unlikely(hfi_debug & __HFI_UDDBG)
-#define _HFI_UDDBG_ALWAYS(fmt, ...) _HFI_DBG_ALWAYS(fmt, ##__VA_ARGS__)
-
 #define _HFI_CONNDBG_ON unlikely(hfi_debug & __HFI_CONNDBG)
 #define _HFI_CONNDBG_ALWAYS(fmt, ...) _HFI_DBG_ALWAYS(fmt, ##__VA_ARGS__)
 
@@ -175,6 +172,10 @@ extern FILE *__hfi_dbgout;
 
 #define _HFI_PDBG_ON unlikely(hfi_debug & __HFI_PKTDBG)
 #define _HFI_PDBG_ALWAYS(fmt, ...) _HFI_DBG_ALWAYS(fmt, ##__VA_ARGS__)
+#define _HFI_PDBG_DUMP(buf, len) hfi_dump_buf(buf, len)
+#ifdef PSM_CUDA
+#define _HFI_PDBG_DUMP_GPU(buf, len) hfi_dump_gpu_buf(buf, len)
+#endif
 
 #define _HFI_PRDBG_ON unlikely(hfi_debug & __HFI_PROCDBG)
 #define _HFI_PRDBG_ALWAYS(fmt, ...) _HFI_DBG_ALWAYS(fmt, ##__VA_ARGS__)
@@ -203,25 +204,24 @@ extern FILE *__hfi_dbgout;
 
 #define _HFI_DBG(fmt, ...)
 #define _HFI_PDBG(fmt, ...)
-#define _HFI_EPDBG(fmt, ...)
 #define _HFI_PRDBG(fmt, ...)
 #define _HFI_ENVDBG(lev, fmt, ...)
-#define _HFI_UDDBG(fmt, ...)
 #define _HFI_CONNDBG(fmt, ...)
 #define _HFI_VDBG(fmt, ...)
 #define _HFI_MMDBG(fmt, ...)
-#define _HFI_CCADBG(fmt, ...)
 
 #define _HFI_DBG_ON 0
 #define _HFI_DBG_ALWAYS(fmt, ...)
-#define _HFI_UDDBG_ON 0
-#define _HFI_UDDBG_ALWAYS(fmt, ...)
 #define _HFI_CONNDBG_ON 0
 #define _HFI_CONNDBG_ALWAYS(fmt, ...)
 #define _HFI_VDBG_ON 0
 #define _HFI_VDBG_ALWAYS(fmt, ...)
 #define _HFI_PRDBG_ON 0
 #define _HFI_PRDBG_ALWAYS(fmt, ...)
+#define _HFI_PDBG_DUMP(buf, len)
+#ifdef PSM_CUDA
+#define _HFI_PDBG_DUMP_GPU(buf, len)
+#endif
 #define _HFI_CCADBG_ON 0
 #define _HFI_CCADBG_ALWAYS(fmt, ...)
 #define _HFI_INFO_ON 0
diff --git a/prov/psm3/psm3/include/opa_user.h b/prov/psm3/psm3/include/opa_user.h
index 730b619..6c15567 100644
--- a/prov/psm3/psm3/include/opa_user.h
+++ b/prov/psm3/psm3/include/opa_user.h
@@ -82,6 +82,11 @@
 #include "opa_udebug.h"
 #include "opa_service.h"
 
+#ifndef PACK_SUFFIX
+/* XXX gcc only */
+#define PACK_SUFFIX __attribute__((packed))
+#endif
+
 #define HFI_TF_NFLOWS                       32
 
 // The sender uses an RDMA Write with Immediate.  The immediate data
@@ -243,6 +248,8 @@ void hfi_vsyslog(const char *prefix, int to_console, int level,
  */
 void hfi_dwordcpy(volatile uint32_t *dest, const uint32_t *src,
 		  uint32_t ndwords);
+void hfi_qwordcpy(volatile uint64_t *dest, const uint64_t *src,
+		  uint32_t nqwords);
 
 extern uint32_t __hfi_pico_per_cycle;	/* only for use in these functions */
 
diff --git a/prov/psm3/psm3/include/psm3_rbtree.c b/prov/psm3/psm3/include/psm3_rbtree.c
new file mode 100644
index 0000000..b79f135
--- /dev/null
+++ b/prov/psm3/psm3/include/psm3_rbtree.c
@@ -0,0 +1,743 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2015 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  Intel Corporation, www.intel.com
+
+  BSD LICENSE
+
+  Copyright(c) 2015 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+/* Copyright (c) 2003-2015 Intel Corporation. All rights reserved. */
+
+/*
+ * Abstract:
+ *	Implementation of quick map, a binary tree where the caller always provides
+ *	all necessary storage.
+ *
+ * Environment:
+ *	All
+ *
+ * $Revision$
+ */
+
+
+/*****************************************************************************
+*
+* Map
+*
+* Map is an associative array.  By providing a key, the caller can retrieve
+* an object from the map.  All objects in the map have an associated key,
+* as specified by the caller when the object was inserted into the map.
+* In addition to random access, the caller can traverse the map much like
+* a linked list, either forwards from the first object or backwards from
+* the last object.  The objects in the map are always traversed in
+* order since the nodes are stored sorted.
+*
+* This implementation of Map uses a red black tree verified against
+* Cormen-Leiserson-Rivest text, McGraw-Hill Edition, fourteenth
+* printing, 1994.
+*
+*****************************************************************************/
+
+#include <string.h> /* for memset declaration */
+
+// RBTREE_CMP should be a comparator, i.e. RBTREE_CMP(a, b) should evaluate to
+// -1, 0, or 1 depending on if a < b, a == b, or a > b, respectively.
+#ifdef RBTREE_CMP
+
+#if defined(RBTREE_GET_LEFTMOST) || defined(RBTREE_GET_RIGHTMOST)
+#error Cannot define both RBTREE_CMP and RBTREE_GET_(LEFT|RIGHT)MOST
+#endif
+
+#elif !defined ( RBTREE_GET_LEFTMOST )       || \
+	! defined ( RBTREE_GET_RIGHTMOST ) || \
+	! defined ( RBTREE_MAP_COUNT )     || \
+	! defined ( RBTREE_ASSERT )
+#error "You must define RBTREE_GET_LEFTMOST and RBTREE_GET_RIGHTMOST and \
+        RBTREE_MAP_COUNT and RBTREE_ASSERT before including rbtree.c"
+
+#endif /* RBTREE_CMP */
+
+#define IN /* nothing */
+
+/******************************************************************************
+*******************************************************************************
+**************                                                     ************
+**************			 IMPLEMENTATION OF QUICK MAP       ************
+**************                                                     ************
+*******************************************************************************
+******************************************************************************/
+
+/* Forward declarations: */
+static void ips_cl_qmap_init(
+				IN	cl_qmap_t		*p_map,
+				IN	cl_map_item_t* const	root,
+				IN	cl_map_item_t* const	nil);
+static void ips_cl_qmap_insert_item(
+				IN	cl_qmap_t* const	p_map,
+				IN	cl_map_item_t* const	p_item);
+static void ips_cl_qmap_remove_item(
+				IN	cl_qmap_t* const	p_map,
+				IN	cl_map_item_t* const	p_item);
+static cl_map_item_t* ips_cl_qmap_successor(
+				IN	cl_qmap_t* const	p_map,
+				IN	const cl_map_item_t*	p_item);
+
+
+#ifndef RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR
+static cl_map_item_t* ips_cl_qmap_predecessor(
+				IN	cl_qmap_t* const	p_map,
+				IN	const cl_map_item_t*	p_item);
+#endif
+
+#if defined(RBTREE_GET_LEFTMOST)
+static cl_map_item_t* ips_cl_qmap_search(
+				IN	cl_qmap_t* const	p_map,
+				IN	unsigned long		start,
+				IN	unsigned long		end);
+#else
+static cl_map_item_t* ips_cl_qmap_searchv(
+				cl_qmap_t* const	p_map,
+				const RBTREE_MI_PL *key);
+#endif
+
+/*
+ * Get the root.
+ */
+static inline cl_map_item_t*
+__cl_map_root(
+	IN	const cl_qmap_t* const	p_map )
+{
+	RBTREE_ASSERT( p_map );
+	return( p_map->root->p_left );
+}
+
+
+/*
+ * Returns whether a given item is on the left of its parent.
+ */
+static int
+__cl_map_is_left_child(
+	IN	const cl_map_item_t* const	p_item )
+{
+	RBTREE_ASSERT( p_item );
+	RBTREE_ASSERT( p_item->p_up );
+	RBTREE_ASSERT( p_item->p_up != p_item );
+
+	return( p_item->p_up->p_left == p_item );
+}
+
+
+/*
+ * Retrieve the pointer to the parent's pointer to an item.
+ */
+static cl_map_item_t**
+__cl_map_get_parent_ptr_to_item(
+	IN	cl_map_item_t* const	p_item )
+{
+	RBTREE_ASSERT( p_item );
+	RBTREE_ASSERT( p_item->p_up );
+	RBTREE_ASSERT( p_item->p_up != p_item );
+
+	if( __cl_map_is_left_child( p_item ) )
+		return( &p_item->p_up->p_left );
+
+	RBTREE_ASSERT( p_item->p_up->p_right == p_item );
+	return( &p_item->p_up->p_right );
+}
+
+
+/*
+ * Rotate a node to the left.  This rotation affects the least number of links
+ * between nodes and brings the level of C up by one while increasing the depth
+ * of A one.  Note that the links to/from W, X, Y, and Z are not affected.
+ *
+ *	    R				      R
+ *	    |				      |
+ *	    A				      C
+ *	  /   \			        /   \
+ *	W       C			  A       Z
+ *	       / \			 / \
+ *	      B   Z			W   B
+ *	     / \			   / \
+ *	    X   Y			  X   Y
+ */
+static void
+__cl_map_rot_left(
+	IN	cl_qmap_t* const		p_map,
+	IN	cl_map_item_t* const	p_item )
+{
+	cl_map_item_t	**pp_root;
+
+	RBTREE_ASSERT( p_map );
+	RBTREE_ASSERT( p_item );
+	RBTREE_ASSERT( p_item->p_right != p_map->nil_item );
+
+	pp_root = __cl_map_get_parent_ptr_to_item( p_item );
+
+	/* Point R to C instead of A. */
+	*pp_root = p_item->p_right;
+	/* Set C's parent to R. */
+	(*pp_root)->p_up = p_item->p_up;
+
+	/* Set A's right to B */
+	p_item->p_right = (*pp_root)->p_left;
+	/*
+	 * Set B's parent to A.  We trap for B being NIL since the
+	 * caller may depend on NIL not changing.
+	 */
+	if( (*pp_root)->p_left != p_map->nil_item )
+		(*pp_root)->p_left->p_up = p_item;
+
+	/* Set C's left to A. */
+	(*pp_root)->p_left = p_item;
+	/* Set A's parent to C. */
+	p_item->p_up = *pp_root;
+}
+
+
+/*
+ * Rotate a node to the right.  This rotation affects the least number of links
+ * between nodes and brings the level of A up by one while increasing the depth
+ * of C one.  Note that the links to/from W, X, Y, and Z are not affected.
+ *
+ *	        R				     R
+ *	        |				     |
+ *	        C				     A
+ *	      /   \				   /   \
+ *	    A       Z			 W       C
+ *	   / \    				        / \
+ *	  W   B   				       B   Z
+ *	     / \				      / \
+ *	    X   Y				     X   Y
+ */
+static void
+__cl_map_rot_right(
+	IN	cl_qmap_t* const		p_map,
+	IN	cl_map_item_t* const	p_item )
+{
+	cl_map_item_t	**pp_root;
+
+	RBTREE_ASSERT( p_map );
+	RBTREE_ASSERT( p_item );
+	RBTREE_ASSERT( p_item->p_left != p_map->nil_item );
+
+	/* Point R to A instead of C. */
+	pp_root = __cl_map_get_parent_ptr_to_item( p_item );
+	(*pp_root) = p_item->p_left;
+	/* Set A's parent to R. */
+	(*pp_root)->p_up = p_item->p_up;
+
+	/* Set C's left to B */
+	p_item->p_left = (*pp_root)->p_right;
+	/*
+	 * Set B's parent to C.  We trap for B being NIL since the
+	 * caller may depend on NIL not changing.
+	 */
+	if( (*pp_root)->p_right != p_map->nil_item )
+		(*pp_root)->p_right->p_up = p_item;
+
+	/* Set A's right to C. */
+	(*pp_root)->p_right = p_item;
+	/* Set C's parent to A. */
+	p_item->p_up = *pp_root;
+}
+
+/*
+ * Balance a tree starting at a given item back to the root.
+ */
+static void
+__cl_map_ins_bal(
+	IN	cl_qmap_t* const	p_map,
+	IN	cl_map_item_t*		p_item )
+{
+	cl_map_item_t*		p_grand_uncle;
+
+	RBTREE_ASSERT( p_map );
+	RBTREE_ASSERT( p_item );
+	RBTREE_ASSERT( p_item != p_map->root );
+
+	while( p_item->p_up->color == CL_MAP_RED )
+	{
+		if( __cl_map_is_left_child( p_item->p_up ) )
+		{
+			p_grand_uncle = p_item->p_up->p_up->p_right;
+			RBTREE_ASSERT( p_grand_uncle );
+			if( p_grand_uncle->color == CL_MAP_RED )
+			{
+				p_grand_uncle->color = CL_MAP_BLACK;
+				p_item->p_up->color = CL_MAP_BLACK;
+				p_item->p_up->p_up->color = CL_MAP_RED;
+				p_item = p_item->p_up->p_up;
+				continue;
+			}
+
+			if( !__cl_map_is_left_child( p_item ) )
+			{
+				p_item = p_item->p_up;
+				__cl_map_rot_left( p_map, p_item );
+			}
+			p_item->p_up->color = CL_MAP_BLACK;
+			p_item->p_up->p_up->color = CL_MAP_RED;
+			__cl_map_rot_right( p_map, p_item->p_up->p_up );
+		}
+		else
+		{
+			p_grand_uncle = p_item->p_up->p_up->p_left;
+			RBTREE_ASSERT( p_grand_uncle );
+			if( p_grand_uncle->color == CL_MAP_RED )
+			{
+				p_grand_uncle->color = CL_MAP_BLACK;
+				p_item->p_up->color = CL_MAP_BLACK;
+				p_item->p_up->p_up->color = CL_MAP_RED;
+				p_item = p_item->p_up->p_up;
+				continue;
+			}
+
+			if( __cl_map_is_left_child( p_item ) )
+			{
+				p_item = p_item->p_up;
+				__cl_map_rot_right( p_map, p_item );
+			}
+			p_item->p_up->color = CL_MAP_BLACK;
+			p_item->p_up->p_up->color = CL_MAP_RED;
+			__cl_map_rot_left( p_map, p_item->p_up->p_up );
+		}
+	}
+}
+
+static void ips_cl_qmap_init(
+				IN	cl_qmap_t		*p_map,
+				IN	cl_map_item_t* const	root,
+				IN	cl_map_item_t* const	nil_item)
+{
+	RBTREE_ASSERT( p_map );
+	RBTREE_ASSERT( root );
+	RBTREE_ASSERT( nil_item );
+
+	memset(p_map,0,sizeof(cl_qmap_t));
+
+	p_map->root = root;
+
+	/* setup the RB tree map */
+	p_map->nil_item = nil_item;
+
+	p_map->root->p_up = p_map->root;
+	p_map->root->p_left = p_map->nil_item;
+	p_map->root->p_right = p_map->nil_item;
+	p_map->root->color = CL_MAP_BLACK;
+
+	p_map->nil_item->p_up = p_map->nil_item;
+	p_map->nil_item->p_left = p_map->nil_item;
+	p_map->nil_item->p_right = p_map->nil_item;
+	p_map->nil_item->color = CL_MAP_BLACK;
+}
+
+static void
+ips_cl_qmap_insert_item(
+	IN	cl_qmap_t* const		p_map,
+	IN	cl_map_item_t* const	p_item )
+{
+	cl_map_item_t	*p_insert_at, *p_comp_item;
+	int compare_res = 0;
+
+	RBTREE_ASSERT( p_map );
+	RBTREE_ASSERT( p_item );
+	RBTREE_ASSERT( p_map->root->p_up == p_map->root );
+	RBTREE_ASSERT( p_map->root->color != CL_MAP_RED );
+	RBTREE_ASSERT( p_map->nil_item->color != CL_MAP_RED );
+
+	/* Find the insertion location. */
+	p_insert_at = p_map->root;
+	p_comp_item = __cl_map_root( p_map );
+
+	while( p_comp_item != p_map->nil_item )
+	{
+		p_insert_at = p_comp_item;
+
+		/* Traverse the tree until the correct insertion point is found. */
+#ifdef RBTREE_GET_LEFTMOST
+		if( RBTREE_GET_LEFTMOST(&p_item->payload) < RBTREE_GET_LEFTMOST(&p_insert_at->payload) )
+#else
+		if(RBTREE_CMP(&p_item->payload, &p_insert_at->payload) < 0)
+#endif
+		{
+			p_comp_item = p_insert_at->p_left;
+			compare_res = 1;
+		} else {
+			p_comp_item = p_insert_at->p_right;
+			compare_res = -1;
+		}
+	}
+
+	RBTREE_ASSERT( p_insert_at != p_map->nil_item );
+	RBTREE_ASSERT( p_comp_item == p_map->nil_item );
+
+	/* Insert the item. */
+	p_item->p_left = p_map->nil_item;
+	p_item->p_right = p_map->nil_item;
+	p_item->color = CL_MAP_RED;
+	if( p_insert_at == p_map->root )
+	{
+		p_insert_at->p_left = p_item;
+	}
+	else if( compare_res > 0 ) /* key < p_insert_at->key */
+	{
+		p_insert_at->p_left = p_item;
+	}
+	else
+	{
+		p_insert_at->p_right = p_item;
+	}
+	/* Increase the count. */
+	RBTREE_MAP_COUNT(&p_map->payload)++;
+
+	p_item->p_up = p_insert_at;
+
+	/*
+	 * We have added depth to this section of the tree.
+	 * Rebalance as necessary as we retrace our path through the tree
+	 * and update colors.
+	 */
+	__cl_map_ins_bal( p_map, p_item );
+
+	__cl_map_root( p_map )->color = CL_MAP_BLACK;
+
+	/*
+	 * Note that it is not necessary to re-color the nil node black because all
+	 * red color assignments are made via the p_up pointer, and nil is never
+	 * set as the value of a p_up pointer.
+	 */
+}
+
+static void
+__cl_map_del_bal(
+	IN	cl_qmap_t* const	p_map,
+	IN	cl_map_item_t*		p_item )
+{
+	cl_map_item_t		*p_uncle;
+
+	while( (p_item->color != CL_MAP_RED) && (p_item->p_up != p_map->root) )
+	{
+		if( __cl_map_is_left_child( p_item ) )
+		{
+			p_uncle = p_item->p_up->p_right;
+
+			if( p_uncle->color == CL_MAP_RED )
+			{
+				p_uncle->color = CL_MAP_BLACK;
+				p_item->p_up->color = CL_MAP_RED;
+				__cl_map_rot_left( p_map, p_item->p_up );
+				p_uncle = p_item->p_up->p_right;
+			}
+
+			if( p_uncle->p_right->color != CL_MAP_RED )
+			{
+				if( p_uncle->p_left->color != CL_MAP_RED )
+				{
+					p_uncle->color = CL_MAP_RED;
+					p_item = p_item->p_up;
+					continue;
+				}
+
+				p_uncle->p_left->color = CL_MAP_BLACK;
+				p_uncle->color = CL_MAP_RED;
+				__cl_map_rot_right( p_map, p_uncle );
+				p_uncle = p_item->p_up->p_right;
+			}
+			p_uncle->color = p_item->p_up->color;
+			p_item->p_up->color = CL_MAP_BLACK;
+			p_uncle->p_right->color = CL_MAP_BLACK;
+			__cl_map_rot_left( p_map, p_item->p_up );
+			break;
+		}
+		else
+		{
+			p_uncle = p_item->p_up->p_left;
+
+			if( p_uncle->color == CL_MAP_RED )
+			{
+				p_uncle->color = CL_MAP_BLACK;
+				p_item->p_up->color = CL_MAP_RED;
+				__cl_map_rot_right( p_map, p_item->p_up );
+				p_uncle = p_item->p_up->p_left;
+			}
+
+			if( p_uncle->p_left->color != CL_MAP_RED )
+			{
+				if( p_uncle->p_right->color != CL_MAP_RED )
+				{
+					p_uncle->color = CL_MAP_RED;
+					p_item = p_item->p_up;
+					continue;
+				}
+
+				p_uncle->p_right->color = CL_MAP_BLACK;
+				p_uncle->color = CL_MAP_RED;
+				__cl_map_rot_left( p_map, p_uncle );
+				p_uncle = p_item->p_up->p_left;
+			}
+			p_uncle->color = p_item->p_up->color;
+			p_item->p_up->color = CL_MAP_BLACK;
+			p_uncle->p_left->color = CL_MAP_BLACK;
+			__cl_map_rot_right( p_map, p_item->p_up );
+			break;
+		}
+	}
+	p_item->color = CL_MAP_BLACK;
+}
+
+static void
+ips_cl_qmap_remove_item(
+	IN	cl_qmap_t* const		p_map,
+	IN	cl_map_item_t* const	p_item )
+{
+	cl_map_item_t	*p_child, *p_del_item;
+
+	RBTREE_ASSERT( p_map );
+	RBTREE_ASSERT( p_item );
+
+	if( p_item == p_map->nil_item )
+		return;
+
+	if( (p_item->p_right == p_map->nil_item) || (p_item->p_left == p_map->nil_item ) )
+	{
+		/* The item being removed has children on at most on side. */
+		p_del_item = p_item;
+	}
+	else
+	{
+		/*
+		 * The item being removed has children on both side.
+		 * We select the item that will replace it.  After removing
+		 * the substitute item and rebalancing, the tree will have the
+		 * correct topology.  Exchanging the substitute for the item
+		 * will finalize the removal.
+		 */
+		p_del_item = ips_cl_qmap_successor(p_map, p_item);
+		RBTREE_ASSERT( p_del_item != p_map->nil_item );
+	}
+
+	RBTREE_MAP_COUNT(&p_map->payload)--;
+
+	/* Get the pointer to the new root's child, if any. */
+	if( p_del_item->p_left != p_map->nil_item )
+		p_child = p_del_item->p_left;
+	else
+		p_child = p_del_item->p_right;
+
+	/*
+	 * This assignment may modify the parent pointer of the nil node.
+	 * This is inconsequential.
+	 */
+	p_child->p_up = p_del_item->p_up;
+	(*__cl_map_get_parent_ptr_to_item( p_del_item )) = p_child;
+
+	if( p_del_item->color != CL_MAP_RED )
+		__cl_map_del_bal( p_map, p_child );
+
+	/*
+	 * Note that the splicing done below does not need to occur before
+	 * the tree is balanced, since the actual topology changes are made by the
+	 * preceding code.  The topology is preserved by the color assignment made
+	 * below (reader should be reminded that p_del_item == p_item in some cases).
+	 */
+	if( p_del_item != p_item )
+	{
+		/*
+		 * Finalize the removal of the specified item by exchanging it with
+		 * the substitute which we removed above.
+		 */
+		p_del_item->p_up = p_item->p_up;
+		p_del_item->p_left = p_item->p_left;
+		p_del_item->p_right = p_item->p_right;
+		(*__cl_map_get_parent_ptr_to_item( p_item )) = p_del_item;
+		p_item->p_right->p_up = p_del_item;
+		p_item->p_left->p_up = p_del_item;
+		p_del_item->color = p_item->color;
+	}
+
+	RBTREE_ASSERT( p_map->nil_item->color != CL_MAP_RED );
+}
+
+static cl_map_item_t *
+ips_cl_qmap_successor(
+	IN	cl_qmap_t* const		p_map,
+	IN	const cl_map_item_t*		p_item )
+{
+	cl_map_item_t	*p_tmp;
+
+	p_tmp = p_item->p_right;
+	if (p_tmp != p_map->nil_item) {
+		while (p_tmp->p_left != p_map->nil_item)
+			p_tmp = p_tmp->p_left;
+		return p_tmp;
+	} else {
+		p_tmp = p_item->p_up;
+		while (p_tmp->p_right == p_item) {
+			p_item = p_tmp;
+			p_tmp = p_tmp->p_up;
+		}
+		if (p_tmp == p_map->root)
+			return p_map->nil_item;
+		return p_tmp;
+	}
+}
+
+// When includer defines RBTREE_CMP, ips_cl_qmap_search() is not emitted.
+// When this happens, ips_cl_qmap_predecessor() may not be called.
+// Combined with -Werror -Wunused-function, libpsm2 fails to build.
+// So provide macro to control emitting this function
+#ifndef RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR
+static cl_map_item_t *
+ips_cl_qmap_predecessor(
+	IN	cl_qmap_t* const		p_map,
+	IN	const cl_map_item_t*		p_item )
+{
+	cl_map_item_t	*p_tmp;
+
+	p_tmp = p_item->p_left;
+	if (p_tmp != p_map->nil_item) {
+		while (p_tmp->p_right != p_map->nil_item)
+			p_tmp = p_tmp->p_right;
+		return p_tmp;
+	} else {
+		p_tmp = p_item->p_up;
+		while (p_tmp->p_left == p_item) {
+			p_item = p_tmp;
+			p_tmp = p_tmp->p_up;
+		}
+		if (p_tmp == p_map->root)
+			return p_map->nil_item;
+		return p_tmp;
+	}
+}
+#endif /* RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR */
+
+#if defined(RBTREE_GET_LEFTMOST)
+/*
+ * return the first node with buffer overlapping or zero.
+ */
+static cl_map_item_t *
+ips_cl_qmap_search(cl_qmap_t * const p_map,
+		unsigned long start, unsigned long end)
+{
+	cl_map_item_t *p_item, *p_tmp;
+
+	RBTREE_ASSERT( p_map );
+	p_item = __cl_map_root(p_map);
+
+	while (p_item != p_map->nil_item) {
+		if (start > RBTREE_GET_LEFTMOST(&p_item->payload)) {
+			p_tmp = p_item->p_right;
+			if (p_tmp != p_map->nil_item) {
+				p_item = p_tmp;
+				continue;
+			}
+
+			/*
+			 * p_item is on immediate left side of 'start'.
+			 */
+			if (start >= RBTREE_GET_RIGHTMOST(&p_item->payload)) {
+				/*
+				 * p_item is on immediate right
+				 * side of 'start'.
+				 */
+				p_item = ips_cl_qmap_successor(p_map, p_item);
+				if (p_item != p_map->nil_item &&
+				    end <= RBTREE_GET_LEFTMOST(&p_item->payload))
+					p_item = p_map->nil_item;
+			}
+		} else if (start < RBTREE_GET_LEFTMOST(&p_item->payload)) {
+			p_tmp = p_item->p_left;
+			if (p_tmp != p_map->nil_item) {
+				p_item = p_tmp;
+				continue;
+			}
+
+			/*
+			 * p_tmp is on immediate left side of 'start'.
+			 */
+			p_tmp = ips_cl_qmap_predecessor(p_map, p_item);
+			if (p_tmp == p_map->nil_item ||
+			    (start >= RBTREE_GET_RIGHTMOST(&p_tmp->payload))) {
+				/*
+				 * p_item is on immediate right
+				 * side of 'start'.
+				 */
+				if (end <= RBTREE_GET_LEFTMOST(&p_item->payload))
+					p_item = p_map->nil_item;
+			} else
+				p_item = p_tmp;
+		}
+
+		break;
+	}
+
+
+	return p_item;
+}
+#else /* defined(...LEFTMOST) || defined(...RIGHTMOST) */
+static cl_map_item_t *
+ips_cl_qmap_searchv(cl_qmap_t * const p_map, const RBTREE_MI_PL *key)
+{
+	RBTREE_ASSERT( p_map );
+	cl_map_item_t *p_item = __cl_map_root(p_map);
+
+	while (p_item != p_map->nil_item) {
+		if (RBTREE_CMP(key, &p_item->payload) > 0) {
+			p_item = p_item->p_right;
+		} else if (RBTREE_CMP(key, &p_item->payload) < 0) {
+			p_item = p_item->p_left;
+		} else {
+			break;
+		}
+	}
+
+	return p_item;
+}
+#endif /* defined(...LEFTMOST) || defined(...RIGHTMOST) */
diff --git a/prov/psm3/psm3/include/psm3_rbtree.h b/prov/psm3/psm3/include/psm3_rbtree.h
new file mode 100644
index 0000000..13245b0
--- /dev/null
+++ b/prov/psm3/psm3/include/psm3_rbtree.h
@@ -0,0 +1,90 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2016 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  Intel Corporation, www.intel.com
+
+  BSD LICENSE
+
+  Copyright(c) 2016 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef __RBTREE_H__
+
+#define __RBTREE_H__
+
+#include <stdint.h>
+
+#ifndef RBTREE_MAP_PL
+#error "You must define RBTREE_MAP_PL before including rbtree.h"
+#endif
+
+#ifndef RBTREE_MI_PL
+#error "You must define RBTREE_MI_PL before including rbtree.h"
+#endif
+
+/*
+ * Red-Black tid cache definition.
+ */
+typedef struct _cl_map_item {
+	struct _cl_map_item	*p_left;	/* left pointer */
+	struct _cl_map_item	*p_right;	/* right pointer */
+	struct _cl_map_item	*p_up;		/* up pointer */
+	uint16_t		color;		/* red-black color */
+
+	RBTREE_MI_PL            payload;
+} cl_map_item_t;
+
+typedef struct _cl_qmap {
+	cl_map_item_t		*root;		/* root node pointer */
+	cl_map_item_t		*nil_item;	/* terminator node pointer */
+
+	RBTREE_MAP_PL            payload;
+} cl_qmap_t;
+
+#define CL_MAP_RED   0
+#define CL_MAP_BLACK 1
+
+#endif
diff --git a/prov/psm3/psm3/include/rbtree.c b/prov/psm3/psm3/include/rbtree.c
deleted file mode 100644
index b79f135..0000000
--- a/prov/psm3/psm3/include/rbtree.c
+++ /dev/null
@@ -1,743 +0,0 @@
-/*
-
-  This file is provided under a dual BSD/GPLv2 license.  When using or
-  redistributing this file, you may do so under either license.
-
-  GPL LICENSE SUMMARY
-
-  Copyright(c) 2015 Intel Corporation.
-
-  This program is free software; you can redistribute it and/or modify
-  it under the terms of version 2 of the GNU General Public License as
-  published by the Free Software Foundation.
-
-  This program is distributed in the hope that it will be useful, but
-  WITHOUT ANY WARRANTY; without even the implied warranty of
-  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-  General Public License for more details.
-
-  Contact Information:
-  Intel Corporation, www.intel.com
-
-  BSD LICENSE
-
-  Copyright(c) 2015 Intel Corporation.
-
-  Redistribution and use in source and binary forms, with or without
-  modification, are permitted provided that the following conditions
-  are met:
-
-    * Redistributions of source code must retain the above copyright
-      notice, this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright
-      notice, this list of conditions and the following disclaimer in
-      the documentation and/or other materials provided with the
-      distribution.
-    * Neither the name of Intel Corporation nor the names of its
-      contributors may be used to endorse or promote products derived
-      from this software without specific prior written permission.
-
-  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-*/
-
-/* Copyright (c) 2003-2015 Intel Corporation. All rights reserved. */
-
-/*
- * Abstract:
- *	Implementation of quick map, a binary tree where the caller always provides
- *	all necessary storage.
- *
- * Environment:
- *	All
- *
- * $Revision$
- */
-
-
-/*****************************************************************************
-*
-* Map
-*
-* Map is an associative array.  By providing a key, the caller can retrieve
-* an object from the map.  All objects in the map have an associated key,
-* as specified by the caller when the object was inserted into the map.
-* In addition to random access, the caller can traverse the map much like
-* a linked list, either forwards from the first object or backwards from
-* the last object.  The objects in the map are always traversed in
-* order since the nodes are stored sorted.
-*
-* This implementation of Map uses a red black tree verified against
-* Cormen-Leiserson-Rivest text, McGraw-Hill Edition, fourteenth
-* printing, 1994.
-*
-*****************************************************************************/
-
-#include <string.h> /* for memset declaration */
-
-// RBTREE_CMP should be a comparator, i.e. RBTREE_CMP(a, b) should evaluate to
-// -1, 0, or 1 depending on if a < b, a == b, or a > b, respectively.
-#ifdef RBTREE_CMP
-
-#if defined(RBTREE_GET_LEFTMOST) || defined(RBTREE_GET_RIGHTMOST)
-#error Cannot define both RBTREE_CMP and RBTREE_GET_(LEFT|RIGHT)MOST
-#endif
-
-#elif !defined ( RBTREE_GET_LEFTMOST )       || \
-	! defined ( RBTREE_GET_RIGHTMOST ) || \
-	! defined ( RBTREE_MAP_COUNT )     || \
-	! defined ( RBTREE_ASSERT )
-#error "You must define RBTREE_GET_LEFTMOST and RBTREE_GET_RIGHTMOST and \
-        RBTREE_MAP_COUNT and RBTREE_ASSERT before including rbtree.c"
-
-#endif /* RBTREE_CMP */
-
-#define IN /* nothing */
-
-/******************************************************************************
-*******************************************************************************
-**************                                                     ************
-**************			 IMPLEMENTATION OF QUICK MAP       ************
-**************                                                     ************
-*******************************************************************************
-******************************************************************************/
-
-/* Forward declarations: */
-static void ips_cl_qmap_init(
-				IN	cl_qmap_t		*p_map,
-				IN	cl_map_item_t* const	root,
-				IN	cl_map_item_t* const	nil);
-static void ips_cl_qmap_insert_item(
-				IN	cl_qmap_t* const	p_map,
-				IN	cl_map_item_t* const	p_item);
-static void ips_cl_qmap_remove_item(
-				IN	cl_qmap_t* const	p_map,
-				IN	cl_map_item_t* const	p_item);
-static cl_map_item_t* ips_cl_qmap_successor(
-				IN	cl_qmap_t* const	p_map,
-				IN	const cl_map_item_t*	p_item);
-
-
-#ifndef RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR
-static cl_map_item_t* ips_cl_qmap_predecessor(
-				IN	cl_qmap_t* const	p_map,
-				IN	const cl_map_item_t*	p_item);
-#endif
-
-#if defined(RBTREE_GET_LEFTMOST)
-static cl_map_item_t* ips_cl_qmap_search(
-				IN	cl_qmap_t* const	p_map,
-				IN	unsigned long		start,
-				IN	unsigned long		end);
-#else
-static cl_map_item_t* ips_cl_qmap_searchv(
-				cl_qmap_t* const	p_map,
-				const RBTREE_MI_PL *key);
-#endif
-
-/*
- * Get the root.
- */
-static inline cl_map_item_t*
-__cl_map_root(
-	IN	const cl_qmap_t* const	p_map )
-{
-	RBTREE_ASSERT( p_map );
-	return( p_map->root->p_left );
-}
-
-
-/*
- * Returns whether a given item is on the left of its parent.
- */
-static int
-__cl_map_is_left_child(
-	IN	const cl_map_item_t* const	p_item )
-{
-	RBTREE_ASSERT( p_item );
-	RBTREE_ASSERT( p_item->p_up );
-	RBTREE_ASSERT( p_item->p_up != p_item );
-
-	return( p_item->p_up->p_left == p_item );
-}
-
-
-/*
- * Retrieve the pointer to the parent's pointer to an item.
- */
-static cl_map_item_t**
-__cl_map_get_parent_ptr_to_item(
-	IN	cl_map_item_t* const	p_item )
-{
-	RBTREE_ASSERT( p_item );
-	RBTREE_ASSERT( p_item->p_up );
-	RBTREE_ASSERT( p_item->p_up != p_item );
-
-	if( __cl_map_is_left_child( p_item ) )
-		return( &p_item->p_up->p_left );
-
-	RBTREE_ASSERT( p_item->p_up->p_right == p_item );
-	return( &p_item->p_up->p_right );
-}
-
-
-/*
- * Rotate a node to the left.  This rotation affects the least number of links
- * between nodes and brings the level of C up by one while increasing the depth
- * of A one.  Note that the links to/from W, X, Y, and Z are not affected.
- *
- *	    R				      R
- *	    |				      |
- *	    A				      C
- *	  /   \			        /   \
- *	W       C			  A       Z
- *	       / \			 / \
- *	      B   Z			W   B
- *	     / \			   / \
- *	    X   Y			  X   Y
- */
-static void
-__cl_map_rot_left(
-	IN	cl_qmap_t* const		p_map,
-	IN	cl_map_item_t* const	p_item )
-{
-	cl_map_item_t	**pp_root;
-
-	RBTREE_ASSERT( p_map );
-	RBTREE_ASSERT( p_item );
-	RBTREE_ASSERT( p_item->p_right != p_map->nil_item );
-
-	pp_root = __cl_map_get_parent_ptr_to_item( p_item );
-
-	/* Point R to C instead of A. */
-	*pp_root = p_item->p_right;
-	/* Set C's parent to R. */
-	(*pp_root)->p_up = p_item->p_up;
-
-	/* Set A's right to B */
-	p_item->p_right = (*pp_root)->p_left;
-	/*
-	 * Set B's parent to A.  We trap for B being NIL since the
-	 * caller may depend on NIL not changing.
-	 */
-	if( (*pp_root)->p_left != p_map->nil_item )
-		(*pp_root)->p_left->p_up = p_item;
-
-	/* Set C's left to A. */
-	(*pp_root)->p_left = p_item;
-	/* Set A's parent to C. */
-	p_item->p_up = *pp_root;
-}
-
-
-/*
- * Rotate a node to the right.  This rotation affects the least number of links
- * between nodes and brings the level of A up by one while increasing the depth
- * of C one.  Note that the links to/from W, X, Y, and Z are not affected.
- *
- *	        R				     R
- *	        |				     |
- *	        C				     A
- *	      /   \				   /   \
- *	    A       Z			 W       C
- *	   / \    				        / \
- *	  W   B   				       B   Z
- *	     / \				      / \
- *	    X   Y				     X   Y
- */
-static void
-__cl_map_rot_right(
-	IN	cl_qmap_t* const		p_map,
-	IN	cl_map_item_t* const	p_item )
-{
-	cl_map_item_t	**pp_root;
-
-	RBTREE_ASSERT( p_map );
-	RBTREE_ASSERT( p_item );
-	RBTREE_ASSERT( p_item->p_left != p_map->nil_item );
-
-	/* Point R to A instead of C. */
-	pp_root = __cl_map_get_parent_ptr_to_item( p_item );
-	(*pp_root) = p_item->p_left;
-	/* Set A's parent to R. */
-	(*pp_root)->p_up = p_item->p_up;
-
-	/* Set C's left to B */
-	p_item->p_left = (*pp_root)->p_right;
-	/*
-	 * Set B's parent to C.  We trap for B being NIL since the
-	 * caller may depend on NIL not changing.
-	 */
-	if( (*pp_root)->p_right != p_map->nil_item )
-		(*pp_root)->p_right->p_up = p_item;
-
-	/* Set A's right to C. */
-	(*pp_root)->p_right = p_item;
-	/* Set C's parent to A. */
-	p_item->p_up = *pp_root;
-}
-
-/*
- * Balance a tree starting at a given item back to the root.
- */
-static void
-__cl_map_ins_bal(
-	IN	cl_qmap_t* const	p_map,
-	IN	cl_map_item_t*		p_item )
-{
-	cl_map_item_t*		p_grand_uncle;
-
-	RBTREE_ASSERT( p_map );
-	RBTREE_ASSERT( p_item );
-	RBTREE_ASSERT( p_item != p_map->root );
-
-	while( p_item->p_up->color == CL_MAP_RED )
-	{
-		if( __cl_map_is_left_child( p_item->p_up ) )
-		{
-			p_grand_uncle = p_item->p_up->p_up->p_right;
-			RBTREE_ASSERT( p_grand_uncle );
-			if( p_grand_uncle->color == CL_MAP_RED )
-			{
-				p_grand_uncle->color = CL_MAP_BLACK;
-				p_item->p_up->color = CL_MAP_BLACK;
-				p_item->p_up->p_up->color = CL_MAP_RED;
-				p_item = p_item->p_up->p_up;
-				continue;
-			}
-
-			if( !__cl_map_is_left_child( p_item ) )
-			{
-				p_item = p_item->p_up;
-				__cl_map_rot_left( p_map, p_item );
-			}
-			p_item->p_up->color = CL_MAP_BLACK;
-			p_item->p_up->p_up->color = CL_MAP_RED;
-			__cl_map_rot_right( p_map, p_item->p_up->p_up );
-		}
-		else
-		{
-			p_grand_uncle = p_item->p_up->p_up->p_left;
-			RBTREE_ASSERT( p_grand_uncle );
-			if( p_grand_uncle->color == CL_MAP_RED )
-			{
-				p_grand_uncle->color = CL_MAP_BLACK;
-				p_item->p_up->color = CL_MAP_BLACK;
-				p_item->p_up->p_up->color = CL_MAP_RED;
-				p_item = p_item->p_up->p_up;
-				continue;
-			}
-
-			if( __cl_map_is_left_child( p_item ) )
-			{
-				p_item = p_item->p_up;
-				__cl_map_rot_right( p_map, p_item );
-			}
-			p_item->p_up->color = CL_MAP_BLACK;
-			p_item->p_up->p_up->color = CL_MAP_RED;
-			__cl_map_rot_left( p_map, p_item->p_up->p_up );
-		}
-	}
-}
-
-static void ips_cl_qmap_init(
-				IN	cl_qmap_t		*p_map,
-				IN	cl_map_item_t* const	root,
-				IN	cl_map_item_t* const	nil_item)
-{
-	RBTREE_ASSERT( p_map );
-	RBTREE_ASSERT( root );
-	RBTREE_ASSERT( nil_item );
-
-	memset(p_map,0,sizeof(cl_qmap_t));
-
-	p_map->root = root;
-
-	/* setup the RB tree map */
-	p_map->nil_item = nil_item;
-
-	p_map->root->p_up = p_map->root;
-	p_map->root->p_left = p_map->nil_item;
-	p_map->root->p_right = p_map->nil_item;
-	p_map->root->color = CL_MAP_BLACK;
-
-	p_map->nil_item->p_up = p_map->nil_item;
-	p_map->nil_item->p_left = p_map->nil_item;
-	p_map->nil_item->p_right = p_map->nil_item;
-	p_map->nil_item->color = CL_MAP_BLACK;
-}
-
-static void
-ips_cl_qmap_insert_item(
-	IN	cl_qmap_t* const		p_map,
-	IN	cl_map_item_t* const	p_item )
-{
-	cl_map_item_t	*p_insert_at, *p_comp_item;
-	int compare_res = 0;
-
-	RBTREE_ASSERT( p_map );
-	RBTREE_ASSERT( p_item );
-	RBTREE_ASSERT( p_map->root->p_up == p_map->root );
-	RBTREE_ASSERT( p_map->root->color != CL_MAP_RED );
-	RBTREE_ASSERT( p_map->nil_item->color != CL_MAP_RED );
-
-	/* Find the insertion location. */
-	p_insert_at = p_map->root;
-	p_comp_item = __cl_map_root( p_map );
-
-	while( p_comp_item != p_map->nil_item )
-	{
-		p_insert_at = p_comp_item;
-
-		/* Traverse the tree until the correct insertion point is found. */
-#ifdef RBTREE_GET_LEFTMOST
-		if( RBTREE_GET_LEFTMOST(&p_item->payload) < RBTREE_GET_LEFTMOST(&p_insert_at->payload) )
-#else
-		if(RBTREE_CMP(&p_item->payload, &p_insert_at->payload) < 0)
-#endif
-		{
-			p_comp_item = p_insert_at->p_left;
-			compare_res = 1;
-		} else {
-			p_comp_item = p_insert_at->p_right;
-			compare_res = -1;
-		}
-	}
-
-	RBTREE_ASSERT( p_insert_at != p_map->nil_item );
-	RBTREE_ASSERT( p_comp_item == p_map->nil_item );
-
-	/* Insert the item. */
-	p_item->p_left = p_map->nil_item;
-	p_item->p_right = p_map->nil_item;
-	p_item->color = CL_MAP_RED;
-	if( p_insert_at == p_map->root )
-	{
-		p_insert_at->p_left = p_item;
-	}
-	else if( compare_res > 0 ) /* key < p_insert_at->key */
-	{
-		p_insert_at->p_left = p_item;
-	}
-	else
-	{
-		p_insert_at->p_right = p_item;
-	}
-	/* Increase the count. */
-	RBTREE_MAP_COUNT(&p_map->payload)++;
-
-	p_item->p_up = p_insert_at;
-
-	/*
-	 * We have added depth to this section of the tree.
-	 * Rebalance as necessary as we retrace our path through the tree
-	 * and update colors.
-	 */
-	__cl_map_ins_bal( p_map, p_item );
-
-	__cl_map_root( p_map )->color = CL_MAP_BLACK;
-
-	/*
-	 * Note that it is not necessary to re-color the nil node black because all
-	 * red color assignments are made via the p_up pointer, and nil is never
-	 * set as the value of a p_up pointer.
-	 */
-}
-
-static void
-__cl_map_del_bal(
-	IN	cl_qmap_t* const	p_map,
-	IN	cl_map_item_t*		p_item )
-{
-	cl_map_item_t		*p_uncle;
-
-	while( (p_item->color != CL_MAP_RED) && (p_item->p_up != p_map->root) )
-	{
-		if( __cl_map_is_left_child( p_item ) )
-		{
-			p_uncle = p_item->p_up->p_right;
-
-			if( p_uncle->color == CL_MAP_RED )
-			{
-				p_uncle->color = CL_MAP_BLACK;
-				p_item->p_up->color = CL_MAP_RED;
-				__cl_map_rot_left( p_map, p_item->p_up );
-				p_uncle = p_item->p_up->p_right;
-			}
-
-			if( p_uncle->p_right->color != CL_MAP_RED )
-			{
-				if( p_uncle->p_left->color != CL_MAP_RED )
-				{
-					p_uncle->color = CL_MAP_RED;
-					p_item = p_item->p_up;
-					continue;
-				}
-
-				p_uncle->p_left->color = CL_MAP_BLACK;
-				p_uncle->color = CL_MAP_RED;
-				__cl_map_rot_right( p_map, p_uncle );
-				p_uncle = p_item->p_up->p_right;
-			}
-			p_uncle->color = p_item->p_up->color;
-			p_item->p_up->color = CL_MAP_BLACK;
-			p_uncle->p_right->color = CL_MAP_BLACK;
-			__cl_map_rot_left( p_map, p_item->p_up );
-			break;
-		}
-		else
-		{
-			p_uncle = p_item->p_up->p_left;
-
-			if( p_uncle->color == CL_MAP_RED )
-			{
-				p_uncle->color = CL_MAP_BLACK;
-				p_item->p_up->color = CL_MAP_RED;
-				__cl_map_rot_right( p_map, p_item->p_up );
-				p_uncle = p_item->p_up->p_left;
-			}
-
-			if( p_uncle->p_left->color != CL_MAP_RED )
-			{
-				if( p_uncle->p_right->color != CL_MAP_RED )
-				{
-					p_uncle->color = CL_MAP_RED;
-					p_item = p_item->p_up;
-					continue;
-				}
-
-				p_uncle->p_right->color = CL_MAP_BLACK;
-				p_uncle->color = CL_MAP_RED;
-				__cl_map_rot_left( p_map, p_uncle );
-				p_uncle = p_item->p_up->p_left;
-			}
-			p_uncle->color = p_item->p_up->color;
-			p_item->p_up->color = CL_MAP_BLACK;
-			p_uncle->p_left->color = CL_MAP_BLACK;
-			__cl_map_rot_right( p_map, p_item->p_up );
-			break;
-		}
-	}
-	p_item->color = CL_MAP_BLACK;
-}
-
-static void
-ips_cl_qmap_remove_item(
-	IN	cl_qmap_t* const		p_map,
-	IN	cl_map_item_t* const	p_item )
-{
-	cl_map_item_t	*p_child, *p_del_item;
-
-	RBTREE_ASSERT( p_map );
-	RBTREE_ASSERT( p_item );
-
-	if( p_item == p_map->nil_item )
-		return;
-
-	if( (p_item->p_right == p_map->nil_item) || (p_item->p_left == p_map->nil_item ) )
-	{
-		/* The item being removed has children on at most on side. */
-		p_del_item = p_item;
-	}
-	else
-	{
-		/*
-		 * The item being removed has children on both side.
-		 * We select the item that will replace it.  After removing
-		 * the substitute item and rebalancing, the tree will have the
-		 * correct topology.  Exchanging the substitute for the item
-		 * will finalize the removal.
-		 */
-		p_del_item = ips_cl_qmap_successor(p_map, p_item);
-		RBTREE_ASSERT( p_del_item != p_map->nil_item );
-	}
-
-	RBTREE_MAP_COUNT(&p_map->payload)--;
-
-	/* Get the pointer to the new root's child, if any. */
-	if( p_del_item->p_left != p_map->nil_item )
-		p_child = p_del_item->p_left;
-	else
-		p_child = p_del_item->p_right;
-
-	/*
-	 * This assignment may modify the parent pointer of the nil node.
-	 * This is inconsequential.
-	 */
-	p_child->p_up = p_del_item->p_up;
-	(*__cl_map_get_parent_ptr_to_item( p_del_item )) = p_child;
-
-	if( p_del_item->color != CL_MAP_RED )
-		__cl_map_del_bal( p_map, p_child );
-
-	/*
-	 * Note that the splicing done below does not need to occur before
-	 * the tree is balanced, since the actual topology changes are made by the
-	 * preceding code.  The topology is preserved by the color assignment made
-	 * below (reader should be reminded that p_del_item == p_item in some cases).
-	 */
-	if( p_del_item != p_item )
-	{
-		/*
-		 * Finalize the removal of the specified item by exchanging it with
-		 * the substitute which we removed above.
-		 */
-		p_del_item->p_up = p_item->p_up;
-		p_del_item->p_left = p_item->p_left;
-		p_del_item->p_right = p_item->p_right;
-		(*__cl_map_get_parent_ptr_to_item( p_item )) = p_del_item;
-		p_item->p_right->p_up = p_del_item;
-		p_item->p_left->p_up = p_del_item;
-		p_del_item->color = p_item->color;
-	}
-
-	RBTREE_ASSERT( p_map->nil_item->color != CL_MAP_RED );
-}
-
-static cl_map_item_t *
-ips_cl_qmap_successor(
-	IN	cl_qmap_t* const		p_map,
-	IN	const cl_map_item_t*		p_item )
-{
-	cl_map_item_t	*p_tmp;
-
-	p_tmp = p_item->p_right;
-	if (p_tmp != p_map->nil_item) {
-		while (p_tmp->p_left != p_map->nil_item)
-			p_tmp = p_tmp->p_left;
-		return p_tmp;
-	} else {
-		p_tmp = p_item->p_up;
-		while (p_tmp->p_right == p_item) {
-			p_item = p_tmp;
-			p_tmp = p_tmp->p_up;
-		}
-		if (p_tmp == p_map->root)
-			return p_map->nil_item;
-		return p_tmp;
-	}
-}
-
-// When includer defines RBTREE_CMP, ips_cl_qmap_search() is not emitted.
-// When this happens, ips_cl_qmap_predecessor() may not be called.
-// Combined with -Werror -Wunused-function, libpsm2 fails to build.
-// So provide macro to control emitting this function
-#ifndef RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR
-static cl_map_item_t *
-ips_cl_qmap_predecessor(
-	IN	cl_qmap_t* const		p_map,
-	IN	const cl_map_item_t*		p_item )
-{
-	cl_map_item_t	*p_tmp;
-
-	p_tmp = p_item->p_left;
-	if (p_tmp != p_map->nil_item) {
-		while (p_tmp->p_right != p_map->nil_item)
-			p_tmp = p_tmp->p_right;
-		return p_tmp;
-	} else {
-		p_tmp = p_item->p_up;
-		while (p_tmp->p_left == p_item) {
-			p_item = p_tmp;
-			p_tmp = p_tmp->p_up;
-		}
-		if (p_tmp == p_map->root)
-			return p_map->nil_item;
-		return p_tmp;
-	}
-}
-#endif /* RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR */
-
-#if defined(RBTREE_GET_LEFTMOST)
-/*
- * return the first node with buffer overlapping or zero.
- */
-static cl_map_item_t *
-ips_cl_qmap_search(cl_qmap_t * const p_map,
-		unsigned long start, unsigned long end)
-{
-	cl_map_item_t *p_item, *p_tmp;
-
-	RBTREE_ASSERT( p_map );
-	p_item = __cl_map_root(p_map);
-
-	while (p_item != p_map->nil_item) {
-		if (start > RBTREE_GET_LEFTMOST(&p_item->payload)) {
-			p_tmp = p_item->p_right;
-			if (p_tmp != p_map->nil_item) {
-				p_item = p_tmp;
-				continue;
-			}
-
-			/*
-			 * p_item is on immediate left side of 'start'.
-			 */
-			if (start >= RBTREE_GET_RIGHTMOST(&p_item->payload)) {
-				/*
-				 * p_item is on immediate right
-				 * side of 'start'.
-				 */
-				p_item = ips_cl_qmap_successor(p_map, p_item);
-				if (p_item != p_map->nil_item &&
-				    end <= RBTREE_GET_LEFTMOST(&p_item->payload))
-					p_item = p_map->nil_item;
-			}
-		} else if (start < RBTREE_GET_LEFTMOST(&p_item->payload)) {
-			p_tmp = p_item->p_left;
-			if (p_tmp != p_map->nil_item) {
-				p_item = p_tmp;
-				continue;
-			}
-
-			/*
-			 * p_tmp is on immediate left side of 'start'.
-			 */
-			p_tmp = ips_cl_qmap_predecessor(p_map, p_item);
-			if (p_tmp == p_map->nil_item ||
-			    (start >= RBTREE_GET_RIGHTMOST(&p_tmp->payload))) {
-				/*
-				 * p_item is on immediate right
-				 * side of 'start'.
-				 */
-				if (end <= RBTREE_GET_LEFTMOST(&p_item->payload))
-					p_item = p_map->nil_item;
-			} else
-				p_item = p_tmp;
-		}
-
-		break;
-	}
-
-
-	return p_item;
-}
-#else /* defined(...LEFTMOST) || defined(...RIGHTMOST) */
-static cl_map_item_t *
-ips_cl_qmap_searchv(cl_qmap_t * const p_map, const RBTREE_MI_PL *key)
-{
-	RBTREE_ASSERT( p_map );
-	cl_map_item_t *p_item = __cl_map_root(p_map);
-
-	while (p_item != p_map->nil_item) {
-		if (RBTREE_CMP(key, &p_item->payload) > 0) {
-			p_item = p_item->p_right;
-		} else if (RBTREE_CMP(key, &p_item->payload) < 0) {
-			p_item = p_item->p_left;
-		} else {
-			break;
-		}
-	}
-
-	return p_item;
-}
-#endif /* defined(...LEFTMOST) || defined(...RIGHTMOST) */
diff --git a/prov/psm3/psm3/include/rbtree.h b/prov/psm3/psm3/include/rbtree.h
deleted file mode 100644
index 13245b0..0000000
--- a/prov/psm3/psm3/include/rbtree.h
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
-
-  This file is provided under a dual BSD/GPLv2 license.  When using or
-  redistributing this file, you may do so under either license.
-
-  GPL LICENSE SUMMARY
-
-  Copyright(c) 2016 Intel Corporation.
-
-  This program is free software; you can redistribute it and/or modify
-  it under the terms of version 2 of the GNU General Public License as
-  published by the Free Software Foundation.
-
-  This program is distributed in the hope that it will be useful, but
-  WITHOUT ANY WARRANTY; without even the implied warranty of
-  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-  General Public License for more details.
-
-  Contact Information:
-  Intel Corporation, www.intel.com
-
-  BSD LICENSE
-
-  Copyright(c) 2016 Intel Corporation.
-
-  Redistribution and use in source and binary forms, with or without
-  modification, are permitted provided that the following conditions
-  are met:
-
-    * Redistributions of source code must retain the above copyright
-      notice, this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright
-      notice, this list of conditions and the following disclaimer in
-      the documentation and/or other materials provided with the
-      distribution.
-    * Neither the name of Intel Corporation nor the names of its
-      contributors may be used to endorse or promote products derived
-      from this software without specific prior written permission.
-
-  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-*/
-
-#ifndef __RBTREE_H__
-
-#define __RBTREE_H__
-
-#include <stdint.h>
-
-#ifndef RBTREE_MAP_PL
-#error "You must define RBTREE_MAP_PL before including rbtree.h"
-#endif
-
-#ifndef RBTREE_MI_PL
-#error "You must define RBTREE_MI_PL before including rbtree.h"
-#endif
-
-/*
- * Red-Black tid cache definition.
- */
-typedef struct _cl_map_item {
-	struct _cl_map_item	*p_left;	/* left pointer */
-	struct _cl_map_item	*p_right;	/* right pointer */
-	struct _cl_map_item	*p_up;		/* up pointer */
-	uint16_t		color;		/* red-black color */
-
-	RBTREE_MI_PL            payload;
-} cl_map_item_t;
-
-typedef struct _cl_qmap {
-	cl_map_item_t		*root;		/* root node pointer */
-	cl_map_item_t		*nil_item;	/* terminator node pointer */
-
-	RBTREE_MAP_PL            payload;
-} cl_qmap_t;
-
-#define CL_MAP_RED   0
-#define CL_MAP_BLACK 1
-
-#endif
diff --git a/prov/psm3/psm3/opa/opa_debug.c b/prov/psm3/psm3/opa/opa_debug.c
index 3ac88e2..2b9fbda 100644
--- a/prov/psm3/psm3/opa/opa_debug.c
+++ b/prov/psm3/psm3/opa/opa_debug.c
@@ -67,10 +67,11 @@
 #include <fcntl.h>
 #include <ucontext.h>
 #include "opa_user.h"
+#include "psm_user.h"
 #include "../psm_log.h"
 
-unsigned hfi_debug = __HFI_INFO;
-char *__hfi_mylabel = NULL;
+unsigned hfi_debug = __HFI_DEBUG_DEFAULT;
+char __hfi_mylabel[1024];
 int __hfi_myrank = -1;
 int __hfi_myrank_count = -1;
 int __hfi_mylocalrank = -1;
@@ -80,7 +81,6 @@ static void init_hfi_mylabel(void) __attribute__ ((constructor));
 static void init_hfi_backtrace(void) __attribute__ ((constructor));
 static void init_hfi_dbgfile(void) __attribute__ ((constructor));
 static void fini_hfi_backtrace(void) __attribute__ ((destructor));
-static void fini_hfi_mylabel(void) __attribute__ ((destructor));
 static struct sigaction SIGSEGV_old_act;
 static struct sigaction SIGBUS_old_act;
 static struct sigaction SIGILL_old_act;
@@ -142,14 +142,13 @@ static void hfi_brake_debug(void)
 
 static void init_hfi_mylabel(void)
 {
-	char lbl[1024];
 	char hostname[80];
 	char *e;
 	/* By default, try to come up with a decent default label, it will be
 	 * overridden later.  Try getting rank, if that's not available revert to
 	 * pid. */
 	gethostname(hostname, 80);
-	lbl[0] = '\0';
+	__hfi_mylabel[0] = '\0';
 	hostname[sizeof(hostname) - 1] = '\0';
 
 #if 0
@@ -216,19 +215,14 @@ static void init_hfi_mylabel(void)
 		unsigned long val;
 		val = strtoul(e, &ep, 10);
 		if (ep != e) {	/* valid conversion */
-			snprintf(lbl, 1024, "%s:rank%lu", hostname, val);
+			snprintf(__hfi_mylabel, sizeof(__hfi_mylabel),
+				"%s:rank%lu", hostname, val);
 			__hfi_myrank = val;
 		}
 	}
-	if (lbl[0] == '\0')
-		snprintf(lbl, 1024, "%s:pid%u", hostname, getpid());
-	__hfi_mylabel = strdup(lbl);
-}
-
-static void fini_hfi_mylabel(void)
-{
-	if(__hfi_mylabel != NULL)
-		free(__hfi_mylabel);
+	if (__hfi_mylabel[0] == '\0')
+		snprintf(__hfi_mylabel, sizeof(__hfi_mylabel),
+			"%s:pid%u", hostname, getpid());
 }
 
 /* FIXME: This signal handler does not conform to the posix standards described
@@ -414,7 +408,8 @@ static void init_hfi_dbgfile(void)
 
 void hfi_set_mylabel(char *label)
 {
-	__hfi_mylabel = label;
+	strncpy(__hfi_mylabel, label, sizeof(__hfi_mylabel));
+	__hfi_mylabel[sizeof(__hfi_mylabel)-1] = '\0';
 }
 
 char *hfi_get_mylabel()
@@ -453,3 +448,32 @@ static void fini_hfi_backtrace(void)
     (void)sigaction(SIGTERM, &SIGTERM_old_act, NULL);
   }
 }
+
+void hfi_dump_buf(uint8_t *buf, uint32_t len)
+{
+	int i, j;
+	for (i=0; i<len; i += 16 ) {
+		fprintf(__hfi_dbgout, "%s: 0x%04x:", __hfi_mylabel, i);
+		for (j=0; j<16 && i+j < len; j++)
+			fprintf(__hfi_dbgout, " %02x", (unsigned)buf[i+j]);
+		fprintf(__hfi_dbgout, "\n");
+	}
+}
+
+#ifdef PSM_CUDA
+void hfi_dump_gpu_buf(uint8_t *buf, uint32_t len)
+{
+	int i, j;
+	uint8_t hbuf[1024];
+
+	for (i=0; i<len; i += 16 ) {
+		fprintf(__hfi_dbgout, "%s: 0x%04x:", __hfi_mylabel, i);
+		if (0 == i % 1024)
+			PSMI_CUDA_CALL(cuMemcpyDtoH, hbuf, (CUdeviceptr)buf,
+                                                min(len-i, 1024));
+		for (j=0; j<16 && i+j < len; j++)
+			fprintf(__hfi_dbgout, " %02x", (unsigned)hbuf[i%1024+j]);
+		fprintf(__hfi_dbgout, "\n");
+	}
+}
+#endif
diff --git a/prov/psm3/psm3/opa/opa_dwordcpy-x86_64.c b/prov/psm3/psm3/opa/opa_dwordcpy-x86_64.c
index a41a40f..342b05f 100644
--- a/prov/psm3/psm3/opa/opa_dwordcpy-x86_64.c
+++ b/prov/psm3/psm3/opa/opa_dwordcpy-x86_64.c
@@ -295,7 +295,7 @@ void MOCKABLE(psmi_mq_mtucpy)(void *vdest, const void *vsrc, uint32_t nchars)
 {
 
 #ifdef PSM_CUDA
-	if (PSMI_IS_CUDA_ENABLED && (PSMI_IS_CUDA_MEM(vdest) || PSMI_IS_CUDA_MEM((void *) vsrc))) {
+	if (nchars && PSMI_IS_CUDA_ENABLED && (PSMI_IS_CUDA_MEM(vdest) || PSMI_IS_CUDA_MEM((void *) vsrc))) {
 		PSMI_CUDA_CALL(cuMemcpy,
 			       (CUdeviceptr)vdest, (CUdeviceptr)vsrc, nchars);
 		return;
diff --git a/prov/psm3/psm3/opa/opa_syslog.c b/prov/psm3/psm3/opa/opa_syslog.c
index b52551d..88de4cb 100644
--- a/prov/psm3/psm3/opa/opa_syslog.c
+++ b/prov/psm3/psm3/opa/opa_syslog.c
@@ -64,7 +64,7 @@
 
 #define SYSLOG_MAXLEN	512
 
-extern char *__hfi_mylabel;
+extern char __hfi_mylabel[];
 
 void
 hfi_vsyslog(const char *prefix, int to_console, int level,
@@ -81,7 +81,7 @@ hfi_vsyslog(const char *prefix, int to_console, int level,
 		gethostname(hostname, sizeof(hostname));
 		hostname[sizeof(hostname) - 1] = '\0';
 
-		if (__hfi_mylabel)
+		if (__hfi_mylabel[0])
 			fprintf(stderr, "%s: ", __hfi_mylabel);
 		else
 			fprintf(stderr, "%s: ", hostname);
diff --git a/prov/psm3/psm3/psm.c b/prov/psm3/psm3/psm.c
index 49e47fa..7159722 100644
--- a/prov/psm3/psm3/psm.c
+++ b/prov/psm3/psm3/psm.c
@@ -81,27 +81,30 @@ static int psmi_refcount = PSMI_NOT_INITIALIZED;
  * will not work on an endpoint which is in a middle of closing). */
 psmi_lock_t psmi_creation_lock;
 
-sem_t *sem_affinity_shm_rw = NULL;
-int psmi_affinity_shared_file_opened = 0;
 int psmi_affinity_semaphore_open = 0;
-uint64_t *shared_affinity_ptr;
 char *sem_affinity_shm_rw_name;
+sem_t *sem_affinity_shm_rw = NULL;
+
+int psmi_affinity_shared_file_opened = 0;
 char *affinity_shm_name;
+uint64_t *shared_affinity_ptr;
 
 uint32_t psmi_cpu_model;
 
 #ifdef PSM_CUDA
 int is_cuda_enabled;
 int is_gdr_copy_enabled;
-int device_support_gpudirect;
-int gpu_p2p_supported = 0;
+int is_gpudirect_enabled = 0;
+int _device_support_unified_addr = -1; // -1 indicates "unchecked". See verify_device_support_unified_addr().
+int _device_support_gpudirect = -1; // -1 indicates "unset". See device_support_gpudirect().
+int _gpu_p2p_supported = -1; // -1 indicates "unset". see gpu_p2p_supported().
 int my_gpu_device = 0;
 int cuda_lib_version;
 int is_driver_gpudirect_enabled;
-int is_cuda_primary_context_retain = 0;
 uint32_t cuda_thresh_rndv;
-uint32_t gdr_copy_threshold_send;
-uint32_t gdr_copy_threshold_recv;
+uint32_t gdr_copy_limit_send;
+uint32_t gdr_copy_limit_recv;
+uint64_t gpu_cache_evict;	// in bytes
 
 void *psmi_cuda_lib;
 CUresult (*psmi_cuInit)(unsigned int  Flags );
@@ -117,6 +120,7 @@ CUresult (*psmi_cuDriverGetVersion)(int* driverVersion);
 CUresult (*psmi_cuDeviceGetCount)(int* count);
 CUresult (*psmi_cuStreamCreate)(CUstream* phStream, unsigned int Flags);
 CUresult (*psmi_cuStreamDestroy)(CUstream phStream);
+CUresult (*psmi_cuStreamSynchronize)(CUstream phStream);
 CUresult (*psmi_cuEventCreate)(CUevent* phEvent, unsigned int Flags);
 CUresult (*psmi_cuEventDestroy)(CUevent hEvent);
 CUresult (*psmi_cuEventQuery)(CUevent hEvent);
@@ -138,6 +142,42 @@ CUresult (*psmi_cuDevicePrimaryCtxGetState)(CUdevice dev, unsigned int* flags, i
 CUresult (*psmi_cuDevicePrimaryCtxRetain)(CUcontext* pctx, CUdevice dev);
 CUresult (*psmi_cuCtxGetDevice)(CUdevice* device);
 CUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);
+
+uint64_t psmi_count_cuInit;
+uint64_t psmi_count_cuCtxDetach;
+uint64_t psmi_count_cuCtxGetCurrent;
+uint64_t psmi_count_cuCtxSetCurrent;
+uint64_t psmi_count_cuPointerGetAttribute;
+uint64_t psmi_count_cuPointerSetAttribute;
+uint64_t psmi_count_cuDeviceCanAccessPeer;
+uint64_t psmi_count_cuDeviceGet;
+uint64_t psmi_count_cuDeviceGetAttribute;
+uint64_t psmi_count_cuDriverGetVersion;
+uint64_t psmi_count_cuDeviceGetCount;
+uint64_t psmi_count_cuStreamCreate;
+uint64_t psmi_count_cuStreamDestroy;
+uint64_t psmi_count_cuStreamSynchronize;
+uint64_t psmi_count_cuEventCreate;
+uint64_t psmi_count_cuEventDestroy;
+uint64_t psmi_count_cuEventQuery;
+uint64_t psmi_count_cuEventRecord;
+uint64_t psmi_count_cuEventSynchronize;
+uint64_t psmi_count_cuMemHostAlloc;
+uint64_t psmi_count_cuMemFreeHost;
+uint64_t psmi_count_cuMemcpy;
+uint64_t psmi_count_cuMemcpyDtoD;
+uint64_t psmi_count_cuMemcpyDtoH;
+uint64_t psmi_count_cuMemcpyHtoD;
+uint64_t psmi_count_cuMemcpyDtoHAsync;
+uint64_t psmi_count_cuMemcpyHtoDAsync;
+uint64_t psmi_count_cuIpcGetMemHandle;
+uint64_t psmi_count_cuIpcOpenMemHandle;
+uint64_t psmi_count_cuIpcCloseMemHandle;
+uint64_t psmi_count_cuMemGetAddressRange;
+uint64_t psmi_count_cuDevicePrimaryCtxGetState;
+uint64_t psmi_count_cuDevicePrimaryCtxRetain;
+uint64_t psmi_count_cuCtxGetDevice;
+uint64_t psmi_count_cuDevicePrimaryCtxRelease;
 #endif
 
 /*
@@ -192,6 +232,7 @@ int psmi_cuda_lib_load()
 		goto fail;
 	}
 
+	psmi_count_cuDriverGetVersion++;
 	psmi_cuDriverGetVersion = dlsym(psmi_cuda_lib, "cuDriverGetVersion");
 
 	if (!psmi_cuDriverGetVersion) {
@@ -218,6 +259,7 @@ int psmi_cuda_lib_load()
 	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuDeviceGetCount);
 	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuStreamCreate);
 	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuStreamDestroy);
+	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuStreamSynchronize);
 	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventCreate);
 	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventDestroy);
 	PSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventQuery);
@@ -249,129 +291,121 @@ fail:
 	return err;
 }
 
+static void psmi_cuda_stats_register()
+{
+#define PSMI_CUDA_COUNT_DECLU64(func) \
+	PSMI_STATS_DECLU64(#func, &psmi_count_##func)
+
+	struct psmi_stats_entry entries[] = {
+		PSMI_CUDA_COUNT_DECLU64(cuInit),
+		PSMI_CUDA_COUNT_DECLU64(cuCtxDetach),
+		PSMI_CUDA_COUNT_DECLU64(cuCtxGetCurrent),
+		PSMI_CUDA_COUNT_DECLU64(cuCtxSetCurrent),
+		PSMI_CUDA_COUNT_DECLU64(cuPointerGetAttribute),
+		PSMI_CUDA_COUNT_DECLU64(cuPointerSetAttribute),
+		PSMI_CUDA_COUNT_DECLU64(cuDeviceCanAccessPeer),
+		PSMI_CUDA_COUNT_DECLU64(cuDeviceGet),
+		PSMI_CUDA_COUNT_DECLU64(cuDeviceGetAttribute),
+		PSMI_CUDA_COUNT_DECLU64(cuDriverGetVersion),
+		PSMI_CUDA_COUNT_DECLU64(cuDeviceGetCount),
+		PSMI_CUDA_COUNT_DECLU64(cuStreamCreate),
+		PSMI_CUDA_COUNT_DECLU64(cuStreamDestroy),
+		PSMI_CUDA_COUNT_DECLU64(cuStreamSynchronize),
+		PSMI_CUDA_COUNT_DECLU64(cuEventCreate),
+		PSMI_CUDA_COUNT_DECLU64(cuEventDestroy),
+		PSMI_CUDA_COUNT_DECLU64(cuEventQuery),
+		PSMI_CUDA_COUNT_DECLU64(cuEventRecord),
+		PSMI_CUDA_COUNT_DECLU64(cuEventSynchronize),
+		PSMI_CUDA_COUNT_DECLU64(cuMemHostAlloc),
+		PSMI_CUDA_COUNT_DECLU64(cuMemFreeHost),
+		PSMI_CUDA_COUNT_DECLU64(cuMemcpy),
+		PSMI_CUDA_COUNT_DECLU64(cuMemcpyDtoD),
+		PSMI_CUDA_COUNT_DECLU64(cuMemcpyDtoH),
+		PSMI_CUDA_COUNT_DECLU64(cuMemcpyHtoD),
+		PSMI_CUDA_COUNT_DECLU64(cuMemcpyDtoHAsync),
+		PSMI_CUDA_COUNT_DECLU64(cuMemcpyHtoDAsync),
+		PSMI_CUDA_COUNT_DECLU64(cuIpcGetMemHandle),
+		PSMI_CUDA_COUNT_DECLU64(cuIpcOpenMemHandle),
+		PSMI_CUDA_COUNT_DECLU64(cuIpcCloseMemHandle),
+		PSMI_CUDA_COUNT_DECLU64(cuMemGetAddressRange),
+		PSMI_CUDA_COUNT_DECLU64(cuDevicePrimaryCtxGetState),
+		PSMI_CUDA_COUNT_DECLU64(cuDevicePrimaryCtxRetain),
+		PSMI_CUDA_COUNT_DECLU64(cuCtxGetDevice),
+		PSMI_CUDA_COUNT_DECLU64(cuDevicePrimaryCtxRelease),
+	};
+#undef PSMI_CUDA_COUNT_DECLU64
+
+	psmi_stats_register_type("PSM_Cuda_call_statistics",
+			PSMI_STATSTYPE_CUDA,
+			entries, PSMI_STATS_HOWMANY(entries), 0,
+			&is_cuda_enabled, NULL); /* context must != NULL */
+}
+
 int psmi_cuda_initialize()
 {
 	psm2_error_t err = PSM2_OK;
-	int num_devices, dev;
 
 	PSM2_LOG_MSG("entering");
 	_HFI_VDBG("Enabling CUDA support.\n");
 
+	psmi_cuda_stats_register();
+
 	err = psmi_cuda_lib_load();
 	if (err != PSM2_OK)
 		goto fail;
 
 	PSMI_CUDA_CALL(cuInit, 0);
 
-	/* Check if CUDA context is available. If not, we are not allowed to
-	 * launch any CUDA API calls */
-	PSMI_CUDA_CALL(cuCtxGetCurrent, &ctxt);
-	if (ctxt == NULL) {
-		_HFI_INFO("Unable to find active CUDA context\n");
-		is_cuda_enabled = 0;
-		err = PSM2_OK;
-		return err;
-	}
-
-	CUdevice current_device;
-	CUcontext primary_ctx;
-	PSMI_CUDA_CALL(cuCtxGetDevice, &current_device);
-	int is_ctx_active;
-	unsigned ctx_flags;
-	PSMI_CUDA_CALL(cuDevicePrimaryCtxGetState, current_device, &ctx_flags,
-			&is_ctx_active);
-	if (!is_ctx_active) {
-		/* There is an issue where certain CUDA API calls create
-		 * contexts but does not make it active which cause the
-		 * driver API call to fail with error 709 */
-		PSMI_CUDA_CALL(cuDevicePrimaryCtxRetain, &primary_ctx,
-				current_device);
-		is_cuda_primary_context_retain = 1;
-	}
-
-	/* Check if all devices support Unified Virtual Addressing. */
-	PSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);
-
-	device_support_gpudirect = 1;
-
-	for (dev = 0; dev < num_devices; dev++) {
-		CUdevice device;
-		PSMI_CUDA_CALL(cuDeviceGet, &device, dev);
-		int unifiedAddressing;
-		PSMI_CUDA_CALL(cuDeviceGetAttribute,
-				&unifiedAddressing,
-				CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING,
-				device);
-
-		if (unifiedAddressing !=1) {
-			_HFI_ERROR("CUDA device %d does not support Unified Virtual Addressing.\n", dev);
-			goto fail;
-		}
-
-		int major;
-		PSMI_CUDA_CALL(cuDeviceGetAttribute,
-				&major,
-				CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR,
-				device);
-		if (major < 3) {
-			device_support_gpudirect = 0;
-			_HFI_INFO("CUDA device %d does not support GPUDirect RDMA (Non-fatal error)\n", dev);
-		}
-
-		if (device != current_device) {
-			int canAccessPeer = 0;
-			PSMI_CUDA_CALL(cuDeviceCanAccessPeer, &canAccessPeer,
-					current_device, device);
-
-			if (canAccessPeer != 1)
-				_HFI_DBG("CUDA device %d does not support P2P from current device (Non-fatal error)\n", dev);
-			else
-				gpu_p2p_supported |= (1 << device);
-		} else {
-			/* Always support p2p on the same GPU */
-			my_gpu_device = device;
-			gpu_p2p_supported |= (1 << device);
-		}
-	}
-
+#ifdef RNDV_MOD
+	psm2_get_gpu_bars();
+#endif
 	union psmi_envvar_val env_enable_gdr_copy;
 	psmi_getenv("PSM3_GDRCOPY",
 				"Enable (set envvar to 1) for gdr copy support in PSM (Enabled by default)",
-				PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,
+				PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_INT,
 				(union psmi_envvar_val)1, &env_enable_gdr_copy);
 	is_gdr_copy_enabled = env_enable_gdr_copy.e_int;
 
 	union psmi_envvar_val env_cuda_thresh_rndv;
 	psmi_getenv("PSM3_CUDA_THRESH_RNDV",
-				"RNDV protocol is used for message sizes greater than the threshold \n",
-				PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,
+				"RNDV protocol is used for GPU send message sizes greater than the threshold",
+				PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_INT,
 				(union psmi_envvar_val)CUDA_THRESH_RNDV, &env_cuda_thresh_rndv);
 	cuda_thresh_rndv = env_cuda_thresh_rndv.e_int;
 
-	if (cuda_thresh_rndv < 0 || cuda_thresh_rndv > CUDA_THRESH_RNDV)
+	if (cuda_thresh_rndv < 0
+		)
 	    cuda_thresh_rndv = CUDA_THRESH_RNDV;
 
-	union psmi_envvar_val env_gdr_copy_thresh_send;
-	psmi_getenv("PSM3_GDRCOPY_THRESH_SEND",
+	union psmi_envvar_val env_gdr_copy_limit_send;
+	psmi_getenv("PSM3_GDRCOPY_LIMIT_SEND",
 				"GDR Copy is turned off on the send side"
-				" for message sizes greater than the threshold \n",
+				" for message sizes greater than the limit"
+#ifndef OPA
+				" or larger than 1 MTU\n",
+#else
+				"\n",
+#endif
 				PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,
-				(union psmi_envvar_val)GDR_COPY_THRESH_SEND, &env_gdr_copy_thresh_send);
-	gdr_copy_threshold_send = env_gdr_copy_thresh_send.e_int;
+				(union psmi_envvar_val)GDR_COPY_LIMIT_SEND, &env_gdr_copy_limit_send);
+	gdr_copy_limit_send = env_gdr_copy_limit_send.e_int;
 
-	if (gdr_copy_threshold_send < 8 || gdr_copy_threshold_send > cuda_thresh_rndv)
-		gdr_copy_threshold_send = GDR_COPY_THRESH_SEND;
+	if (gdr_copy_limit_send < 8 || gdr_copy_limit_send > cuda_thresh_rndv)
+		gdr_copy_limit_send = max(GDR_COPY_LIMIT_SEND, cuda_thresh_rndv);
 
-	union psmi_envvar_val env_gdr_copy_thresh_recv;
-	psmi_getenv("PSM3_GDRCOPY_THRESH_RECV",
+	union psmi_envvar_val env_gdr_copy_limit_recv;
+	psmi_getenv("PSM3_GDRCOPY_LIMIT_RECV",
 				"GDR Copy is turned off on the recv side"
-				" for message sizes greater than the threshold \n",
+				" for message sizes greater than the limit\n",
 				PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,
-				(union psmi_envvar_val)GDR_COPY_THRESH_RECV, &env_gdr_copy_thresh_recv);
-	gdr_copy_threshold_recv = env_gdr_copy_thresh_recv.e_int;
+				(union psmi_envvar_val)GDR_COPY_LIMIT_RECV, &env_gdr_copy_limit_recv);
+	gdr_copy_limit_recv = env_gdr_copy_limit_recv.e_int;
+
+	if (gdr_copy_limit_recv < 8)
+		gdr_copy_limit_recv = GDR_COPY_LIMIT_RECV;
 
-	if (gdr_copy_threshold_recv < 8)
-		gdr_copy_threshold_recv = GDR_COPY_THRESH_RECV;
+	if (!is_gdr_copy_enabled)
+		gdr_copy_limit_send = gdr_copy_limit_recv = 0;
 
 	PSM2_LOG_MSG("leaving");
 	return err;
@@ -487,9 +521,10 @@ psm2_error_t __psm2_init(int *major, int *minor)
 	psmi_getenv("PSM3_TRACEMASK",
 		    "Mask flags for tracing",
 		    PSMI_ENVVAR_LEVEL_USER,
-		    PSMI_ENVVAR_TYPE_ULONG_FLAGS,
-		    (union psmi_envvar_val)hfi_debug, &env_tmask);
-	hfi_debug = (long)env_tmask.e_ulong;
+		    PSMI_ENVVAR_TYPE_STR,
+		    (union psmi_envvar_val)__HFI_DEBUG_DEFAULT_STR, &env_tmask);
+	hfi_debug = psmi_parse_val_pattern(env_tmask.e_str, __HFI_DEBUG_DEFAULT,
+			__HFI_DEBUG_DEFAULT);
 
 	/* The "real thing" is done in hfi_proto.c as a constructor function, but
 	 * we getenv it here to report what we're doing with the setting */
@@ -566,7 +601,8 @@ psm2_error_t __psm2_init(int *major, int *minor)
 			"Enable (set envvar to 1) for cuda support in PSM (Disabled by default)",
 			PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_INT,
 			(union psmi_envvar_val)0, &env_enable_cuda);
-	is_cuda_enabled = env_enable_cuda.e_int;
+	// order important, always parse gpudirect
+	is_cuda_enabled = psmi_parse_gpudirect() || env_enable_cuda.e_int;
 
 	if (PSMI_IS_CUDA_ENABLED) {
 		err = psmi_cuda_initialize();
@@ -579,40 +615,56 @@ update:
 	if (psmi_parse_identify()) {
                 Dl_info info_psm;
 		char ofed_delta[100] = "";
-		strcat(strcat(ofed_delta," built for IFS OFA DELTA "),psmi_hfi_IFS_version);
-                printf("%s %s PSM3 v%d.%d%s\n"
+		strcat(strcat(ofed_delta," built for IEFS "),psmi_hfi_IFS_version);
+                printf("%s %s PSM3 v%d.%d%s%s\n"
 		       "%s %s location %s\n"
 		       "%s %s build date %s\n"
 		       "%s %s src checksum %s\n"
                        "%s %s git checksum %s\n"
 #ifdef RNDV_MOD
+#ifdef NVIDIA_GPU_DIRECT
+                       "%s %s built against rv interface v%d.%d gpu v%d.%d cuda\n"
+#else
                        "%s %s built against rv interface v%d.%d\n"
 #endif
+#endif
                        "%s %s Global Rank %d (%d total) Local Rank %d (%d total)\n"
-		       , hfi_get_mylabel(), hfi_ident_tag,
-		       PSM2_VERNO_MAJOR,PSM2_VERNO_MINOR,
-		       (strcmp(psmi_hfi_IFS_version,"") != 0) ? ofed_delta
+                       "%s %s CPU Core %d NUMA %d\n",
+		       hfi_get_mylabel(), hfi_ident_tag,
+				PSM2_VERNO_MAJOR,PSM2_VERNO_MINOR,
 #ifdef PSM_CUDA
-		       : "-cuda",
+				"-cuda",
 #else
-		       : "",
+				"",
 #endif
-		       hfi_get_mylabel(), hfi_ident_tag, dladdr(psm2_init, &info_psm) ?
-		       info_psm.dli_fname : "PSM3 path not available",
+				(strcmp(psmi_hfi_IFS_version,"") != 0) ? ofed_delta : "",
+		       hfi_get_mylabel(), hfi_ident_tag,
+				dladdr(psm2_init, &info_psm) ?
+					info_psm.dli_fname : "PSM3 path not available",
 		       hfi_get_mylabel(), hfi_ident_tag, psmi_hfi_build_timestamp,
 		       hfi_get_mylabel(), hfi_ident_tag, psmi_hfi_sources_checksum,
 		       hfi_get_mylabel(), hfi_ident_tag,
-		       (strcmp(psmi_hfi_git_checksum,"") != 0) ?
-		       psmi_hfi_git_checksum : "<not available>",
+				(strcmp(psmi_hfi_git_checksum,"") != 0) ?
+					psmi_hfi_git_checksum : "<not available>",
 #ifdef RNDV_MOD
+#ifdef NVIDIA_GPU_DIRECT
+		       hfi_get_mylabel(), hfi_ident_tag,
+				psm2_rv_get_user_major_bldtime_version(),
+				psm2_rv_get_user_minor_bldtime_version(),
+				psm2_rv_get_gpu_user_major_bldtime_version(),
+				psm2_rv_get_gpu_user_minor_bldtime_version(),
+#else
 		       hfi_get_mylabel(), hfi_ident_tag,
 				psm2_rv_get_user_major_bldtime_version(),
 				psm2_rv_get_user_minor_bldtime_version(),
 #endif
+#endif
 		       hfi_get_mylabel(), hfi_ident_tag,
 				hfi_get_myrank(), hfi_get_myrank_count(),
 				hfi_get_mylocalrank(),
-				hfi_get_mylocalrank_count()
+				hfi_get_mylocalrank_count(),
+		       hfi_get_mylabel(), hfi_ident_tag,
+				sched_getcpu(), psmi_get_current_proc_location()
 		       );
 	}
 
@@ -921,6 +973,7 @@ psm2_error_t __psm2_finalize(void)
 		psmi_sem_post(sem_affinity_shm_rw, sem_affinity_shm_rw_name);
 
 		munmap(shared_affinity_ptr, AFFINITY_SHMEMSIZE);
+		shared_affinity_ptr = NULL;
 		psmi_free(affinity_shm_name);
 		affinity_shm_name = NULL;
 		psmi_affinity_shared_file_opened = 0;
@@ -929,6 +982,7 @@ psm2_error_t __psm2_finalize(void)
 	if (psmi_affinity_semaphore_open) {
 		_HFI_VDBG("Closing and Unlinking Semaphore: %s.\n", sem_affinity_shm_rw_name);
 		sem_close(sem_affinity_shm_rw);
+		sem_affinity_shm_rw = NULL;
 		sem_unlink(sem_affinity_shm_rw_name);
 		psmi_free(sem_affinity_shm_rw_name);
 		sem_affinity_shm_rw_name = NULL;
@@ -937,16 +991,8 @@ psm2_error_t __psm2_finalize(void)
 
 	psmi_hal_finalize();
 #ifdef PSM_CUDA
-	if (is_cuda_primary_context_retain) {
-		/*
-		 * This code will be called during deinitialization, and if
-		 * CUDA is deinitialized before PSM, then
-		 * CUDA_ERROR_DEINITIALIZED will happen here
-		 */
-		CUdevice device;
-		if (psmi_cuCtxGetDevice(&device) == CUDA_SUCCESS)
-			PSMI_CUDA_CALL(cuDevicePrimaryCtxRelease, device);
-	}
+	if (PSMI_IS_CUDA_ENABLED)
+		psmi_stats_deregister_type(PSMI_STATSTYPE_CUDA, &is_cuda_enabled);
 #endif
 
 	psmi_refcount = PSMI_FINALIZED;
diff --git a/prov/psm3/psm3/psm2.h b/prov/psm3/psm3/psm2.h
index 0b88008..c37ecb1 100644
--- a/prov/psm3/psm3/psm2.h
+++ b/prov/psm3/psm3/psm2.h
@@ -56,6 +56,7 @@
 
 #include <stdint.h>
 #include <stddef.h>
+#include <uuid/uuid.h>
 
 #ifdef __cplusplus
 extern "C" {
@@ -444,7 +445,7 @@ typedef enum psm2_path_res psm2_path_res_t;
    	// In this example, we want to handle our own errors before doing init,
    	// since we don't want a fatal error if OPA is not found.
    	// Note that @ref psm2_error_register_handler
-   	// (and @ref psm2_uuid_generate and @ref psm2_get_capability_mask)
+   	// (and @ref psm2_get_capability_mask)
    	// are the only function that can be called before @ref psm2_init
 
    	int try_to_initialize_psm() {
@@ -644,9 +645,8 @@ typedef struct psm2_epaddr *psm2_epaddr_t;
  * endpoint within a particular job.  Since PSM2 does not participate in job
  * allocation and management, users are expected to generate a unique ID to
  * associate endpoints to a particular parallel or collective job.
- * @see psm2_uuid_generate
  */
-typedef uint8_t psm2_uuid_t[16];
+typedef uuid_t psm2_uuid_t;
 
 /** @brief Get Endpoint identifier's Unique Network ID */
 uint64_t psm2_epid_nid(psm2_epid_t epid);
@@ -667,15 +667,6 @@ uint64_t psm2_epid_port(psm2_epid_t epid);
  */
 psm2_error_t psm2_ep_num_devunits(uint32_t *num_units);
 
-/** @brief Utility to generate UUIDs for @ref psm2_ep_open
- *
- * This function is available as a utility for generating unique job-wide ids.
- * See discussion in @ref psm2_ep_open for further information.
- *
- * @remark This function does not require PSM2 to be initialized.
- */
-void psm2_uuid_generate(psm2_uuid_t uuid_out);
-
 /* Affinity modes for the affinity member of struct psm2_ep_open_opts */
 #define PSM2_EP_OPEN_AFFINITY_SKIP     0	/**< Disable setting affinity */
 #define PSM2_EP_OPEN_AFFINITY_SET      1	/**< Enable setting affinity unless
@@ -744,16 +735,14 @@ struct psm2_ep_open_opts {
  *
  * PSM2 does not internally verify the consistency of the uuid, it is up to the
  * user to ensure that the uid is unique enough not to collide with other
- * currently-running jobs.  Users can employ three mechanisms to obtain a uuid.
- *
- * 1. Use the supplied @ref psm2_uuid_generate utility
+ * currently-running jobs.  Users can employ two mechanisms to obtain a uuid.
  *
- * 2. Use an OS or library-specific uuid generation utility, that complies with
+ * 1. Use an OS or library-specific uuid generation utility, that complies with
  *    OSF DCE 1.1, such as @c uuid_generate on Linux or @c uuid_create on
  *    FreeBSD.
  *    (see http://www.opengroup.org/onlinepubs/009629399/uuid_create.htm)
  *
- * 3. Manually pack a 16-byte string using a utility such as /dev/random or
+ * 2. Manually pack a 16-byte string using a utility such as /dev/random or
  *    other source with enough entropy and proper seeding to prevent two nodes
  *    from generating the same uuid_t.
  *
diff --git a/prov/psm3/psm3/psm2_hal.c b/prov/psm3/psm3/psm2_hal.c
index 69ab0db..546c600 100644
--- a/prov/psm3/psm3/psm2_hal.c
+++ b/prov/psm3/psm3/psm2_hal.c
@@ -73,6 +73,7 @@ void psmi_hal_register_instance(psmi_hal_instance_t *psm_hi)
 
 	REJECT_IMPROPER_HI(hfp_close_context);
 	REJECT_IMPROPER_HI(hfp_context_open);
+	REJECT_IMPROPER_HI(hfp_context_initstats);
 
 
 	REJECT_IMPROPER_HI(hfp_finalize_);
diff --git a/prov/psm3/psm3/psm2_hal.h b/prov/psm3/psm3/psm2_hal.h
index 071a47e..cd2f9cc 100644
--- a/prov/psm3/psm3/psm2_hal.h
+++ b/prov/psm3/psm3/psm2_hal.h
@@ -113,6 +113,8 @@ typedef enum
 typedef enum
 {
 	PSM_HAL_CAP_GPUDIRECT_OT		= (1UL << 16),
+	PSM_HAL_CAP_USER_MR			= (1UL << 17),
+	PSM_HAL_CAP_EVICT			= (1UL << 18),
 } psmi_hal_capability_bits;
 
 /* The following enum constants correspond to the bits in the
@@ -229,6 +231,9 @@ struct _psmi_hal_instance
 				uint32_t cap_mask,
 				unsigned retryCnt);
 
+	/* Initialize PSM3_PRINT_STATS stats for given ep */
+	void (*hfp_context_initstats)(psm2_ep_t ep);
+
 	/* Close the context, including the device file. */
 	int (*hfp_close_context)(psmi_hal_hw_context *);
 
@@ -320,6 +325,7 @@ int psmi_hal_pre_init_cache_func(enum psmi_hal_pre_init_cache_func_krnls k, ...)
 #define psmi_hal_get_default_pkey(...)			       PSMI_HAL_DISPATCH_PI(get_default_pkey,##__VA_ARGS__)
 #define psmi_hal_get_port_subnet(...)				PSMI_HAL_DISPATCH_PI(get_port_subnet,__VA_ARGS__)
 #define psmi_hal_context_open(...)				PSMI_HAL_DISPATCH(context_open,__VA_ARGS__)
+#define psmi_hal_context_initstats(...)				PSMI_HAL_DISPATCH(context_initstats,__VA_ARGS__)
 #define psmi_hal_close_context(...)				PSMI_HAL_DISPATCH(close_context,__VA_ARGS__)
 
 
diff --git a/prov/psm3/psm3/psm2_hal_inline_t.h b/prov/psm3/psm3/psm2_hal_inline_t.h
index 6d7cc96..916c999 100644
--- a/prov/psm3/psm3/psm2_hal_inline_t.h
+++ b/prov/psm3/psm3/psm2_hal_inline_t.h
@@ -83,6 +83,8 @@ static PSMI_HAL_INLINE int PSMI_HAL_CAT_INL_SYM(context_open)
 				 psmi_context_t *psm_ctxt,
 				 uint32_t cap_mask,
 				 unsigned);
+static PSMI_HAL_INLINE void PSMI_HAL_CAT_INL_SYM(context_initstats)
+				 (psm2_ep_t ep);
 
 
 static PSMI_HAL_INLINE int PSMI_HAL_CAT_INL_SYM(get_port_rate)
diff --git a/prov/psm3/psm3/psm2_mq.h b/prov/psm3/psm3/psm2_mq.h
index 6e2afba..7267b09 100644
--- a/prov/psm3/psm3/psm2_mq.h
+++ b/prov/psm3/psm3/psm2_mq.h
@@ -1569,9 +1569,10 @@ struct psm2_mq_stats {
 	uint64_t rx_user_bytes;
 	/** Messages received into a matched user buffer */
 	uint64_t rx_user_num;
-	/** Bytes received into an unmatched system buffer */
+	/** Bytes received into an unmatched (or out of order) system buffer */
 	uint64_t rx_sys_bytes;
-	/** Messages received into an unmatched system buffer */
+	/** Messages received into an unmatched (or out of order) system buffer */
+	/** this count includes unexpected zero length eager recv */
 	uint64_t rx_sys_num;
 
 	/** Total Messages transmitted (shm and hfi) */
@@ -1580,25 +1581,57 @@ struct psm2_mq_stats {
 	uint64_t tx_eager_num;
 	/** Bytes transmitted eagerly */
 	uint64_t tx_eager_bytes;
-	/** Messages transmitted using expected TID mechanism */
+	/** Messages transmitted using any rendezvous mechanism */
 	uint64_t tx_rndv_num;
-	/** Bytes transmitted using expected TID mechanism */
+	/** Bytes transmitted using any rendezvous mechanism */
 	uint64_t tx_rndv_bytes;
 	/** Messages transmitted (shm only) */
 	uint64_t tx_shm_num;
+	/** Bytes transmitted (shm only) */
+	uint64_t tx_shm_bytes;
 	/** Messages received through shm */
 	uint64_t rx_shm_num;
+	/** Bytes received through shm */
+	uint64_t rx_shm_bytes;
 
-	/** Number of system buffers allocated  */
+	/** sysbufs are used for unexpected eager receive (and RTS payload) */
+	/** Number of messages using system buffers (not used for 0 byte msg) */
 	uint64_t rx_sysbuf_num;
-	/** Bytes allcoated for system buffers */
+	/** Bytes using system buffers */
 	uint64_t rx_sysbuf_bytes;
 
 	/** rank in MPI_COMM_WORLD, while unchanging, easiest to put here */
 	uint64_t comm_world_rank;
 
+#ifdef PSM_CUDA
+	/** Messages transmitted eagerly from CPU buffer */
+	uint64_t tx_eager_cpu_num;
+	/** Bytes transmitted eagerly from CPU buffer */
+	uint64_t tx_eager_cpu_bytes;
+	/** Messages transmitted eagerly from GPU buffer */
+	uint64_t tx_eager_gpu_num;
+	/** Bytes transmitted eagerly from GPU buffer */
+	uint64_t tx_eager_gpu_bytes;
+
+	/** Bytes copied from a system buffer into a matched CPU user buffer */
+	/** this count also includes unexpected zero length eager recv */
+	uint64_t rx_sysbuf_cpu_bytes;
+	/** Messages copied from a system buffer into a matched CPU user buffer */
+	uint64_t rx_sysbuf_cpu_num;
+	/** Bytes gdrCopied from a system buffer into a matched user GPU buffer */
+	uint64_t rx_sysbuf_gdrcopy_bytes;
+	/** Messages gdrCopied from a system buffer into a matched user GPU buffer */
+	uint64_t rx_sysbuf_gdrcopy_num;
+	/** Bytes cuCopied from a system buffer into a matched user GPU buffer */
+	uint64_t rx_sysbuf_cuCopy_bytes;
+	/** Messages cuCopied from a system buffer into a matched user GPU buffer */
+	uint64_t rx_sysbuf_cuCopy_num;
+
 	/** Internally reserved for future use */
+	uint64_t _reserved[3];
+#else
 	uint64_t _reserved[15];
+#endif
 };
 
 #define PSM2_MQ_NUM_STATS    13	/**< How many stats are currently used in @ref psm2_mq_stats */
diff --git a/prov/psm3/psm3/psm_config.h b/prov/psm3/psm3/psm_config.h
index 9e671d1..f63bc00 100644
--- a/prov/psm3/psm3/psm_config.h
+++ b/prov/psm3/psm3/psm_config.h
@@ -145,20 +145,20 @@
 #define CUDA_WINDOW_PREFETCH_DEFAULT	2
 #define GPUDIRECT_THRESH_RV 3
 
-#define GDR_COPY_THRESH_SEND 32
-#define GDR_COPY_THRESH_RECV 64000
+#define GDR_COPY_LIMIT_SEND 128
+#define GDR_COPY_LIMIT_RECV 64000
 /* All GPU transfers beyond this threshold use
  * RNDV protocol. It is mostly a send side knob.
  */
-#define CUDA_THRESH_RNDV 32768
+#define CUDA_THRESH_RNDV 8000
 #endif
 
 #define MQ_HFI_THRESH_TINY		8
-#define MQ_HFI_THRESH_EGR_SDMA_XEON	34000       /* Eager Xeon blocking */
-#define MQ_HFI_THRESH_EGR_SDMA_PHI2	200000      /* Eager Phi2 blocking */
-#define MQ_HFI_THRESH_EGR_SDMA_SQ_XEON	16384    /* Eager Xeon non-blocking */
-#define MQ_HFI_THRESH_EGR_SDMA_SQ_PHI2	65536    /* Eager Phi2 non-blocking */
 
+#define MQ_HFI_THRESH_EGR_SDMA		8192    /* Eager blocking */
+#define MQ_HFI_THRESH_EGR_SDMA_SQ	8192    /* Eager non-blocking */
+#define MQ_HFI_THRESH_GPU_EGR_SDMA	128    /* Eager blocking */
+#define MQ_HFI_THRESH_GPU_EGR_SDMA_SQ	128    /* Eager non-blocking */
 #define MQ_HFI_THRESH_RNDV_PHI2		200000
 #define MQ_HFI_THRESH_RNDV_XEON 	64000
 
diff --git a/prov/psm3/psm3/psm_context.c b/prov/psm3/psm3/psm_context.c
index b48564e..b95b03d 100644
--- a/prov/psm3/psm3/psm_context.c
+++ b/prov/psm3/psm3/psm_context.c
@@ -147,8 +147,16 @@ psmi_create_and_open_affinity_shm(psm2_uuid_t const job_key)
 	int shm_fd, ret;
 	int first_to_create = 0;
 	size_t shm_name_len = 256;
+
+	psmi_assert_always(psmi_affinity_semaphore_open);
+	if (psmi_affinity_shared_file_opened) {
+		/* opened and have our reference counted in shm */
+		psmi_assert_always(affinity_shm_name != NULL);
+		psmi_assert_always(shared_affinity_ptr != NULL);
+		return 0;
+	}
+
 	shared_affinity_ptr = NULL;
-	affinity_shm_name = NULL;
 	affinity_shm_name = (char *) psmi_malloc(PSMI_EP_NONE, UNDEFINED, shm_name_len);
 
 	psmi_assert_always(affinity_shm_name != NULL);
@@ -162,37 +170,35 @@ psmi_create_and_open_affinity_shm(psm2_uuid_t const job_key)
 		if (shm_fd < 0) {
 			_HFI_VDBG("Cannot open affinity shared mem fd:%s, errno=%d\n",
 				  affinity_shm_name, errno);
-			return shm_fd;
+			goto free_name;
 		}
-	} else if (shm_fd > 0) {
+	} else if (shm_fd >= 0) {
 		first_to_create = 1;
 	} else {
 		_HFI_VDBG("Cannot create affinity shared mem fd:%s, errno=%d\n",
 			  affinity_shm_name, errno);
+		goto free_name;
 	}
 
 	ret = ftruncate(shm_fd, AFFINITY_SHMEMSIZE);
 	if ( ret < 0 ) {
 		_HFI_VDBG("Cannot truncate affinity shared mem fd:%s, errno=%d\n",
 			affinity_shm_name, errno);
-		if (shm_fd >= 0) close(shm_fd);
-		return ret;
+		goto close_shm;
 	}
 
 	shared_affinity_ptr = (uint64_t *) mmap(NULL, AFFINITY_SHMEMSIZE, PROT_READ | PROT_WRITE,
 					MAP_SHARED, shm_fd, 0);
 	if (shared_affinity_ptr == MAP_FAILED) {
-		_HFI_VDBG("Cannot mmap affinity shared memory. errno=%d\n",
-			  errno);
-		close(shm_fd);
-		return -1;
+		_HFI_VDBG("Cannot mmap affinity shared memory: %s, errno=%d\n",
+			  affinity_shm_name, errno);
+		goto close_shm;
 	}
 	close(shm_fd);
-
-	psmi_affinity_shared_file_opened = 1;
+	shm_fd = -1;
 
 	if (first_to_create) {
-		_HFI_VDBG("Creating shm to store NIC affinity per socket\n");
+		_HFI_VDBG("Initializing shm to store NIC affinity per socket: %s\n", affinity_shm_name);
 
 		memset(shared_affinity_ptr, 0, AFFINITY_SHMEMSIZE);
 
@@ -202,7 +208,7 @@ psmi_create_and_open_affinity_shm(psm2_uuid_t const job_key)
 		 */
 		psmi_sem_post(sem_affinity_shm_rw, sem_affinity_shm_rw_name);
 	} else {
-		_HFI_VDBG("Opening shm object to read/write NIC affinity per socket\n");
+		_HFI_VDBG("Opened shm object to read/write NIC affinity per socket: %s\n", affinity_shm_name);
 	}
 
 	/*
@@ -212,15 +218,28 @@ psmi_create_and_open_affinity_shm(psm2_uuid_t const job_key)
 	 */
 	if (psmi_sem_timedwait(sem_affinity_shm_rw, sem_affinity_shm_rw_name)) {
 		_HFI_VDBG("Could not enter critical section to update shm refcount\n");
-		return -1;
+		goto unmap_shm;
 	}
 
 	shared_affinity_ptr[AFFINITY_SHM_REF_COUNT_LOCATION] += 1;
+	_HFI_VDBG("shm refcount = %"PRId64"\n",  shared_affinity_ptr[AFFINITY_SHM_REF_COUNT_LOCATION]);
 
 	/* End critical section */
 	psmi_sem_post(sem_affinity_shm_rw, sem_affinity_shm_rw_name);
 
+	psmi_affinity_shared_file_opened = 1;
+
 	return 0;
+
+unmap_shm:
+	munmap(shared_affinity_ptr, AFFINITY_SHMEMSIZE);
+	shared_affinity_ptr = NULL;
+close_shm:
+	if (shm_fd >= 0) close(shm_fd);
+free_name:
+	psmi_free(affinity_shm_name);
+	affinity_shm_name = NULL;
+	return -1;
 }
 
 /*
@@ -274,7 +293,6 @@ static void
 psmi_create_affinity_semaphores(psm2_uuid_t const job_key)
 {
 	int ret;
-	sem_affinity_shm_rw_name = NULL;
 	size_t sem_len = 256;
 
 	/*
@@ -298,7 +316,8 @@ psmi_create_affinity_semaphores(psm2_uuid_t const job_key)
 	if (ret) {
 		_HFI_VDBG("Cannot initialize semaphore: %s for read-write access to shm object.\n",
 			  sem_affinity_shm_rw_name);
-		sem_close(sem_affinity_shm_rw);
+		if (sem_affinity_shm_rw)
+			sem_close(sem_affinity_shm_rw);
 		psmi_free(sem_affinity_shm_rw_name);
 		sem_affinity_shm_rw_name = NULL;
 		return;
@@ -519,19 +538,31 @@ psmi_context_open(const psm2_ep_t ep, long unit_param, long port,
 	context->ep = (psm2_ep_t) ep;
 
 	/* Check backward compatibility bits here and save the info */
+#ifdef PSM_CUDA
+#ifndef OPA
+	gdr_copy_limit_send = min(gdr_copy_limit_send, ep->mtu);
+
+	if (PSMI_IS_CUDA_DISABLED || ! psmi_parse_gpudirect()) {
+		// when CUDA and/or PSM3_GPUDIRECT* is disabled,
+		// PSM_HALCAP_GPUDIRECT is not fetched because it doesn't matter
+		// Just be silent about this situation.
+	} else // CUDA and GPUDIRECT are enabled, check CAP_GPUDIRECT in rv
+#endif
+#endif
 	if (psmi_hal_has_cap(PSM_HAL_CAP_GPUDIRECT_OT))
 	{
 #ifdef PSM_CUDA
 		is_driver_gpudirect_enabled = 1;
 #else
-		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR, "FATAL ERROR: "
-				  "CUDA version of rendezvous driver is loaded with non-CUDA version of "
-				  "psm3 provider.\n");
+		// we can allow this combination
+		//psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR, "FATAL ERROR: "
+		//		  "CUDA version of rendezvous driver is loaded with non-CUDA version of "
+		//		  "psm3 provider.\n");
 #endif
 	}
 #ifdef PSM_CUDA
-	else
-		fprintf(stderr,"WARNING: running CUDA version of psm3 provider with non CUDA version of rendezvous driver.\n");
+	else // we warn here, later tests in ips_proto_init() will be fatal
+		_HFI_INFO("WARNING: running CUDA version of psm3 provider with non CUDA version of rendezvous driver.\n");
 #endif
 	_HFI_VDBG("hal_context_open() passed.\n");
 
@@ -540,15 +571,15 @@ psmi_context_open(const psm2_ep_t ep, long unit_param, long port,
 						|| PSMI_EPID_VERSION == PSMI_EPID_V4);
 	psmi_assert_always (ep->verbs_ep.context);
 	// TBD - if we put the verbs_ep in hw_ctxt we could push this to HAL
-	// verbs_ep_open has initialized: ep->unit_id, ep->portnum,
+	// verbs_ep_open has initialized: ep->unit_id, ep->portnum,ep->dev_name,
 	//	ep->gid_hi, ep->gid_lo
 	if (ep->verbs_ep.link_layer == IBV_LINK_LAYER_ETHERNET) {
 		char buf[INET_ADDRSTRLEN];
 		int netmask_bits = psmi_count_high_bits(ep->verbs_ep.ip_netmask);
 		if (netmask_bits < 0) {
 			err = psmi_handle_error(NULL, PSM2_EP_DEVICE_FAILURE,
-					"PSM3 invalid netmask: %s",
-					psmi_ipv4_ntop(ep->verbs_ep.ip_netmask, buf, sizeof(buf)));
+					"PSM3 invalid netmask on %s: %s",
+					ep->dev_name, psmi_ipv4_ntop(ep->verbs_ep.ip_netmask, buf, sizeof(buf)));
 			goto bail;
 		}
 		psmi_epid_ver = PSMI_EPID_V4;	// overide default based on device
diff --git a/prov/psm3/psm3/psm_ep.c b/prov/psm3/psm3/psm_ep.c
index 3f64e53..fd0bda9 100644
--- a/prov/psm3/psm3/psm_ep.c
+++ b/prov/psm3/psm3/psm_ep.c
@@ -255,7 +255,7 @@ psmi_ep_multirail(int *num_rails, uint32_t *unit, uint16_t *port)
 						unit[i], sysfs_unit_dev_name(unit[i]),
 						port[i]);
 			ret = psmi_hal_get_port_lid(unit[i], port[i]);
-			if (ret <= 0)
+			if (ret <= 0 || ret == 0xFFFF)
 				return psmi_handle_error(NULL,
 						PSM2_EP_DEVICE_FAILURE,
 						"PSM3_MULTIRAIL_MAP: Couldn't get lid for unit %d(%s):%d",
@@ -320,7 +320,7 @@ psmi_ep_multirail(int *num_rails, uint32_t *unit, uint16_t *port)
 
 		for (j = HFI_MIN_PORT; j <= HFI_MAX_PORT; j++) {
 			ret = psmi_hal_get_port_lid(i, j);
-			if (ret <= 0)
+			if (ret <= 0 || ret == 0xFFFF)
 				continue;
 			ret = psmi_hal_get_port_subnet(i, j, &gid_hi, NULL, NULL, NULL, NULL, NULL, NULL);
 			if (ret == -1)
@@ -384,7 +384,7 @@ psmi_ep_devlids(uint32_t **lids, uint32_t *num_lids_o,
 				uint32_t ipaddr = 0;
 
 				// if looking for IB/OPA lid, skip ports we can't get lid for
-				if (lid <= 0 && psmi_epid_version(my_epid) == PSMI_EPID_V3)
+				if ((lid <= 0 || lid == 0xFFFF) && psmi_epid_version(my_epid) == PSMI_EPID_V3)
 					continue;
 				// we just need subnet and addr within subnet and idx
 				ret = psmi_hal_get_port_subnet(i, j, &gid_hi, &gid_lo, &ipaddr, NULL, &idx, &actual_gid_hi, NULL);
@@ -449,11 +449,11 @@ psmi_ep_verify_pkey(psm2_ep_t ep, uint16_t pkey, uint16_t *opkey, uint16_t* oind
 // TBD - if we adjust HAL to take a hw_context for this function and
 // put the verbs_ep inside the HAL hw context, we can eliminate this ifdef
 // and simply call into HAL
-		_HFI_UDDBG("looking for pkey 0x%x\n", pkey);
+		_HFI_PRDBG("looking for pkey 0x%x\n", pkey);
 		ret = verbs_get_port_index2pkey(ep, ep->portnum, i);
 		if (ret < 0) {
 			err = psmi_handle_error(NULL, PSM2_EP_DEVICE_FAILURE,
-						"Can't get a valid pkey value from pkey table\n");
+						"Can't get a valid pkey value from pkey table on %s port %u\n", ep->dev_name, ep->portnum);
 			return err;
 		}
 		// pkey == 0 means get slot 0
@@ -467,15 +467,15 @@ psmi_ep_verify_pkey(psm2_ep_t ep, uint16_t pkey, uint16_t *opkey, uint16_t* oind
 	/* if pkey does not match */
 	if (i == 16) {
 		err = psmi_handle_error(NULL, PSM2_EP_DEVICE_FAILURE,
-					"Wrong pkey 0x%x, please use PSM3_PKEY to specify a valid pkey\n",
-					pkey);
+					"Wrong pkey 0x%x on %s port %u, please use PSM3_PKEY to specify a valid pkey\n",
+					pkey, ep->dev_name, ep->portnum);
 		return err;
 	}
 
 	if (((uint16_t)ret & 0x8000) == 0) {
 		err = psmi_handle_error(NULL, PSM2_EP_DEVICE_FAILURE,
-					"Limited Member pkey 0x%x, please use PSM3_PKEY to specify a valid pkey\n",
-					(uint16_t)ret);
+					"Limited Member pkey 0x%x on %s port %u, please use PSM3_PKEY to specify a valid pkey\n",
+					(uint16_t)ret, ep->dev_name, ep->portnum);
 		return err;
 	}
 
@@ -560,7 +560,7 @@ psm2_error_t __psm2_ep_query(int *num_of_epinfo, psm2_epinfo_t *array_of_epinfo)
 		array_of_epinfo[i].jkey = ep->jkey;
 		memcpy(array_of_epinfo[i].uuid,
 		       (void *)ep->uuid, sizeof(psm2_uuid_t));
-		psmi_uuid_unparse(ep->uuid, array_of_epinfo[i].uuid_str);
+		uuid_unparse_lower(ep->uuid, array_of_epinfo[i].uuid_str);
 		ep = ep->user_ep_next;
 	}
 	*num_of_epinfo = i;
@@ -978,12 +978,23 @@ __psm2_ep_open_internal(psm2_uuid_t const unique_job_key, int *devid_enabled,
 	// The value returned is a MR_CACHE_MODE_* selection
 	{
 		union psmi_envvar_val env_mr_cache_mode;
-
-		if (! (ep->rdmamode & IPS_PROTOEXP_FLAG_ENABLED)) {
+		if (! (ep->rdmamode & IPS_PROTOEXP_FLAG_ENABLED)
+#ifdef PSM_CUDA
+			&& (PSMI_IS_CUDA_DISABLED || ! psmi_parse_gpudirect())
+#endif
+			&& ! psmi_parse_senddma()) {
 			env_mr_cache_mode.e_uint = MR_CACHE_MODE_NONE;
 		} else if (IPS_PROTOEXP_FLAG_KERNEL_QP(ep->rdmamode)) {
 			// RDMA enabled in kernel mode.  Must use rv MR cache
 			env_mr_cache_mode.e_uint = MR_CACHE_MODE_RV;
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+		} else if (PSMI_IS_CUDA_ENABLED && psmi_parse_gpudirect()) {
+			// GPU Direct (RDMA, send DMA and/or gdrcopy) must
+			// use kernel MR cache in RV
+			env_mr_cache_mode.e_uint = MR_CACHE_MODE_KERNEL;
+#endif
+#endif
 		} else {
 			/* Behavior of user space MR Cache
 			 * when 0, we merely share MRs for concurrently used buffers
@@ -1009,8 +1020,14 @@ __psm2_ep_open_internal(psm2_uuid_t const unique_job_key, int *devid_enabled,
 				env_mr_cache_mode.e_uint = MR_CACHE_MODE_NONE;
 		}
 #ifndef RNDV_MOD
-		if (env_mr_cache_mode.e_uint == MR_CACHE_MODE_KERNEL)
+		if (env_mr_cache_mode.e_uint == MR_CACHE_MODE_KERNEL) {
+			static int logged = 0;
+			if (! logged) {
+				_HFI_INFO("WARNING: PSM built without rv module enabled, kernel MR caching unavailable\n");
+				logged = 1;
+			}
 			env_mr_cache_mode.e_uint = MR_CACHE_MODE_NONE;
+		}
 #endif
 		ep->mr_cache_mode = env_mr_cache_mode.e_uint;
 	}
@@ -1105,17 +1122,47 @@ __psm2_ep_open_internal(psm2_uuid_t const unique_job_key, int *devid_enabled,
 	/* Size of RV Cache - only used for MR_CACHE_MODE_RV or KERNEL,
 	 * otherwise ignored
 	 */
+	// RV defaults are sufficient for default PSM parameters
+	// but if user adjusts ep->hfi_num_send_rdma or mq->hfi_base_window_rv
+	// they also need to increase the cache size.  psm2_verbs_alloc_mr_cache
+	// will verify cache size is sufficient.
+	// min size is (HFI_TF_NFLOWS + ep->hfi_num_send_rdma) *
+	// chunk size (mq->hfi_base_window_rv after psmi_mq_initialize_defaults)
+	// for OPA native, actual window_rv may be smaller, but for UD it
+	// is not reduced
 	psmi_getenv("PSM3_RV_MR_CACHE_SIZE",
 			"kernel space MR cache size"
 			" (MBs, 0 lets rv module decide) [0]",
 			PSMI_ENVVAR_LEVEL_USER,
 			PSMI_ENVVAR_TYPE_UINT,
 			(union psmi_envvar_val)0, &envvar_val);
-	// TBD - min should be (HFI_TF_NFLOWS + ep->hfi_num_send_rdma) *
-	// chunk size (mq->hfi_base_window_rv after psmi_mq_initialize_defaults
-	// TBD actual window_sz may be larger than mq->hfi_base_window_rv
 	ep->rv_mr_cache_size = envvar_val.e_uint;
 
+#ifdef PSM_CUDA
+	/* Size of RV GPU Cache - only used for PSM3_CUDA=1 MR_CACHE_MODE_KERNEL,
+	 * otherwise ignored
+	 */
+	// RV defaults are sufficient for default PSM parameters
+	// but if user adjusts ep->hfi_num_send_rdma or mq->hfi_base_window_rv
+	// they also need to increase the cache size.  psm2_verbs_alloc_mr_cache
+	// will verify cache size is sufficient.
+	// min size is (HFI_TF_NFLOWS + ep->hfi_num_send_rdma) *
+	// chunk size (mq->hfi_base_window_rv after psmi_mq_initialize_defaults)
+	// for OPA native, actual window_rv may be smaller, but for UD it
+	// is not reduced
+	if (PSMI_IS_CUDA_ENABLED) {
+		psmi_getenv("PSM3_RV_GPU_CACHE_SIZE",
+				"kernel space GPU cache size"
+				" (MBs, 0 lets rv module decide) [0]",
+				PSMI_ENVVAR_LEVEL_USER,
+				PSMI_ENVVAR_TYPE_UINT,
+				(union psmi_envvar_val)0, &envvar_val);
+		ep->rv_gpu_cache_size = envvar_val.e_uint;
+	} else {
+		ep->rv_gpu_cache_size = 0;
+	}
+#endif
+
 	psmi_getenv("PSM3_RV_QP_PER_CONN",
 			"Number of sets of RC QPs per RV connection (0 lets rv module decide) [0]",
 			PSMI_ENVVAR_LEVEL_USER,
@@ -1150,7 +1197,7 @@ __psm2_ep_open_internal(psm2_uuid_t const unique_job_key, int *devid_enabled,
 		goto fail;
 
 	if (psmi_ep_device_is_enabled(ep, PTL_DEVID_IPS)) {
-		_HFI_UDDBG("my QPN=%u (0x%x)  EPID=0x%"PRIx64" %s\n",
+		_HFI_PRDBG("my QPN=%u (0x%x)  EPID=0x%"PRIx64" %s\n",
 			ep->verbs_ep.qp->qp_num, ep->verbs_ep.qp->qp_num, (uint64_t)ep->epid,
 			psmi_epaddr_fmt_addr(ep->epid));
 	}
@@ -1184,6 +1231,14 @@ __psm2_ep_open_internal(psm2_uuid_t const unique_job_key, int *devid_enabled,
 	if ((err = psmi_epid_set_hostname(psm2_epid_nid(ep->epid), buf, 0)))
 		goto fail;
 
+	if (! mq->ep)	// only call on 1st EP within MQ
+		psmi_mq_initstats(mq, ep->epid);
+
+#ifdef PSM_CUDA
+	if (PSMI_IS_CUDA_ENABLED)
+		verify_device_support_unified_addr();
+#endif
+
 	_HFI_VDBG("start ptl device init...\n");
 	if (psmi_ep_device_is_enabled(ep, PTL_DEVID_SELF)) {
 		if ((err = psmi_ptl_self.init(ep, self_ptl, &ep->ptl_self)))
@@ -1301,8 +1356,12 @@ __psm2_ep_open(psm2_uuid_t const unique_job_key,
 			setenv(pname, pvalue, 1);
 		}
 	}
-
 #ifdef PSM_CUDA
+	else {
+		// only IPS opens RV, needed for gdrcopy
+		is_gdr_copy_enabled = gdr_copy_limit_send =
+			gdr_copy_limit_recv = 0;
+	}
 	if (PSMI_IS_GDR_COPY_ENABLED)
 		hfi_gdr_open();
 #endif
@@ -1325,11 +1384,14 @@ __psm2_ep_open(psm2_uuid_t const unique_job_key,
 	ep->mctxt_master = ep;
 	mq->ep = ep;
 
-	if (show_nics)
-		printf("%s %s NIC %u (%s) Port %u\n",
+	if (show_nics) {
+		int node_id;
+		psmi_hal_get_node_id(ep->unit_id, &node_id);
+		printf("%s %s NIC %u (%s) Port %u NUMA %d\n",
 			hfi_get_mylabel(), hfi_ident_tag,
-			ep->unit_id,  sysfs_unit_dev_name(ep->unit_id),
-			ep->portnum);
+			ep->unit_id,  ep->dev_name,
+			ep->portnum, node_id);
+	}
 
 	/* Active Message initialization */
 	err = psmi_am_init_internal(ep);
@@ -1338,6 +1400,7 @@ __psm2_ep_open(psm2_uuid_t const unique_job_key,
 
 	*epo = ep;
 	*epido = epid;
+	psmi_hal_context_initstats(ep);
 
 	if (psmi_device_is_enabled(devid_enabled, PTL_DEVID_IPS)) {
 		int j;
@@ -1393,11 +1456,15 @@ __psm2_ep_open(psm2_uuid_t const unique_job_key,
 
 				/* Link slave EP after master EP. */
 				PSM_MCTXT_APPEND(ep, tmp);
-				if (j == 0 && show_nics)
-					printf("%s %s NIC %u (%s) Port %u\n",
+				if (j == 0 && show_nics) {
+					int node_id;
+					psmi_hal_get_node_id(ep->unit_id, &node_id);
+					printf("%s %s NIC %u (%s) Port %u NUMA %d\n",
 						hfi_get_mylabel(), hfi_ident_tag,
-						tmp->unit_id,  sysfs_unit_dev_name(tmp->unit_id),
-						tmp->portnum);
+						tmp->unit_id,  tmp->dev_name,
+						tmp->portnum, node_id);
+				}
+				psmi_hal_context_initstats(tmp);
 			}
 		}
 	}
@@ -1577,7 +1644,6 @@ psm2_error_t __psm2_ep_close(psm2_ep_t ep, int mode, int64_t timeout_in)
 
 		PSMI_LOCK(ep->mq->progress_lock);
 
-		PSM_MCTXT_REMOVE(ep);
 		if (psmi_ep_device_is_enabled(ep, PTL_DEVID_AMSH))
 			err =
 			    psmi_ptl_amsh.fini(ep->ptl_amsh.ptl, mode,
@@ -1588,7 +1654,7 @@ psm2_error_t __psm2_ep_close(psm2_ep_t ep, int mode, int64_t timeout_in)
 			err =
 			    psmi_ptl_ips.fini(ep->ptl_ips.ptl, mode,
 					      timeout_in);
-
+		PSM_MCTXT_REMOVE(ep);
 		/* If there's timeouts in the disconnect requests,
 		 * still make sure that we still get to close the
 		 *endpoint and mark it closed */
diff --git a/prov/psm3/psm3/psm_ep.h b/prov/psm3/psm3/psm_ep.h
index 24d3fe4..0defc11 100644
--- a/prov/psm3/psm3/psm_ep.h
+++ b/prov/psm3/psm3/psm_ep.h
@@ -60,6 +60,7 @@
 #ifndef _PSMI_EP_H
 #define _PSMI_EP_H
 
+
 #include "psm_verbs_ep.h"
 
 /*
@@ -146,6 +147,7 @@ struct psm2_ep {
 	uint16_t network_pkey;	      /**> Pkey */
 	uint16_t network_pkey_index;  /**> Pkey index */
 	int did_syslog;
+	const char *dev_name;	/* just for logging */
 	psm2_uuid_t uuid;
 	uint16_t jkey;
 	uint64_t service_id;	/* OPA service ID */
@@ -167,6 +169,9 @@ struct psm2_ep {
 	uint8_t mr_cache_mode; /** PSM3_MR_CACHE_MODE */
 	uint8_t rv_num_conn; /** PSM3_RV_QP_PER_CONN */
 	uint32_t rv_mr_cache_size; /** PSM3_RV_MR_CACHE_SIZE */
+#ifdef PSM_CUDA
+	uint32_t rv_gpu_cache_size; /** PSM3_RV_GPU_CACHE_SIZE */
+#endif
 	uint32_t rv_q_depth; /** PSM3_RV_Q_DEPTH */
 	uint32_t rv_reconnect_timeout; /* PSM3_RV_RECONNECT_TIMEOUT */
 	uint32_t rv_hb_interval; /* PSM3_RV_HEARTBEAT_INTERVAL */
diff --git a/prov/psm3/psm3/psm_ep_connect.c b/prov/psm3/psm3/psm_ep_connect.c
index 122994d..7907952 100644
--- a/prov/psm3/psm3/psm_ep_connect.c
+++ b/prov/psm3/psm3/psm_ep_connect.c
@@ -130,7 +130,9 @@ __psm2_ep_connect(psm2_ep_t ep, int num_of_epid, psm2_epid_t const *array_of_epi
 			array_of_errors[j] = PSM2_EPID_UNKNOWN;
 			array_of_epaddr[j] = NULL;
 			if (psmi_epid_version(array_of_epid[j]) !=
-						 PSMI_EPID_VERSION) {
+						 PSMI_EPID_VERSION
+				&& psmi_epid_version(array_of_epid[j]) !=
+					psmi_epid_version(ep->epid)) {
 					psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
 					  " Mismatched version of EPID - %"PRIu64"\n"
 					  "Confirm all nodes are running the same interconnect HW and PSM version\n",
@@ -479,8 +481,8 @@ psm2_error_t __psm2_ep_disconnect2(psm2_ep_t ep, int num_of_epaddr,
 		}
 		t_left = psmi_cycles_left(t_start, timeout);
 
-		if (_HFI_VDBG_ON) {
-			_HFI_VDBG_ALWAYS
+		if (_HFI_CONNDBG_ON) {
+			_HFI_CONNDBG_ALWAYS
 				("Trying to disconnect with device %s\n",
 				psmi_getdevice(ep->devid_enabled[i]));
 		}
@@ -595,8 +597,8 @@ disconnect_fail:
 				    snprintf(errbuf + len,
 					     sizeof(errbuf) - len - 1, "%c %s",
 					     j == 0 ? ':' : ',',
-					     psmi_epaddr_get_hostname
-					     (array_of_epaddr[i]->epid));
+					     array_of_epaddr[i]?psmi_epaddr_get_hostname
+					     (array_of_epaddr[i]->epid):"Unknown");
 				j++;
 			}
 		}
diff --git a/prov/psm3/psm3/psm_gdrcpy.h b/prov/psm3/psm3/psm_gdrcpy.h
index 2173f0b..eb245be 100644
--- a/prov/psm3/psm3/psm_gdrcpy.h
+++ b/prov/psm3/psm3/psm_gdrcpy.h
@@ -66,17 +66,12 @@ void hfi_gdr_open();
 
 void hfi_gdr_close();
 
+// flags=0 for send, 1 for recv
 void *
 gdr_convert_gpu_to_host_addr(int gdr_fd, unsigned long buf,
 				size_t size, int flags,
-				struct ips_proto* proto);
+				psm2_ep_t ep);
 
-int
-gdr_unmap_gpu_host_addr(int gdr_fd, const void *buf,
-                             size_t size, struct ips_proto* proto);
 
-
-uint64_t
-gdr_cache_evict();
 #endif
 #endif
diff --git a/prov/psm3/psm3/psm_hal_gen1/opa_service_gen1.c b/prov/psm3/psm3/psm_hal_gen1/opa_service_gen1.c
index 1a96860..f1572ad 100644
--- a/prov/psm3/psm3/psm_hal_gen1/opa_service_gen1.c
+++ b/prov/psm3/psm3/psm_hal_gen1/opa_service_gen1.c
@@ -149,6 +149,7 @@ int hfi_get_num_units(void)
 		char pathname[PATH_MAX];
 		struct stat st;
 		int r;
+
 		snprintf(pathname, sizeof(pathname), "/dev/infiniband/uverbs%d", ret);
 		r = stat(pathname, &st);
 		if (r) break;
@@ -163,18 +164,20 @@ int hfi_get_num_units(void)
    returns -1 when an error occurred. */
 int hfi_get_unit_active(int unit)
 {
-	int p,rv;
+	int p, lid;
 
-	for (p = HFI_MIN_PORT; p <= HFI_MAX_PORT; p++)
-		if ((rv=hfi_get_port_lid(unit, p)) > 0)
+	for (p = HFI_MIN_PORT; p <= HFI_MAX_PORT; p++) {
+		lid = hfi_get_port_lid(unit, p);
+		if (lid > 0 && lid != 0xFFFF)
 			break;
+	}
 
 	if (p <= HFI_MAX_PORT)
 	{
 		return 1;
 	}
 
-	return rv;
+	return lid;
 }
 
 /* get the number of contexts from the unit id. */
diff --git a/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c b/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c
index 6f1c307..398646d 100644
--- a/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c
+++ b/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c
@@ -74,42 +74,33 @@ int get_gdr_fd(){
 
 
 
+// flags=0 for send, 1 for recv
 void *
 gdr_convert_gpu_to_host_addr(int gdr_fd, unsigned long buf,
 							 size_t size, int flags,
-							 struct ips_proto* proto)
+							 psm2_ep_t ep)
 {
 	void *host_addr_buf;
 
 	uintptr_t pageaddr = buf & GPU_PAGE_MASK;
-// TBD - is this comment correct?  Callers may be calling for a whole
-// app buffer, especially when RECV RDMA is disabled
-	/* As size is guarenteed to be in the range of 0-8kB
-	 * there is a guarentee that buf+size-1 does not overflow
-	 * 64 bits.
-	 */
-	uint32_t pagelen = (uint32_t) (PSMI_GPU_PAGESIZE +
+	uint64_t pagelen = (uint64_t) (PSMI_GPU_PAGESIZE +
 					   ((buf + size - 1) & GPU_PAGE_MASK) -
 					   pageaddr);
 
-	_HFI_VDBG("buf=%p size=%zu pageaddr=%p pagelen=%u flags=0x%x proto=%p\n",
-		(void *)buf, size, (void *)pageaddr, pagelen, flags, proto);
+	_HFI_VDBG("buf=%p size=%zu pageaddr=%p pagelen=%"PRIu64" flags=0x%x ep=%p\n",
+		(void *)buf, size, (void *)pageaddr, pagelen, flags, ep);
 #ifdef RNDV_MOD
-	host_addr_buf = __psm2_rv_pin_and_mmap(proto->ep->verbs_ep.rv, pageaddr, pagelen);
-	if (! host_addr_buf) {
-		if (errno == ENOMEM || errno == EINVAL) {
-			/* Fatal error */
-			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-					  "Unable to PIN GPU pages(Out of BAR1 space) (errno: %d)\n", errno);
-			return NULL;
-		} else {
-			/* Fatal error */
-			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-							  "PIN/MMAP ioctl failed errno %d\n",
-							  errno);
-			return NULL;
+	ep = ep->mctxt_master;
+	host_addr_buf = __psm2_rv_pin_and_mmap(ep->verbs_ep.rv, pageaddr, pagelen, IBV_ACCESS_IS_GPU_ADDR);
+	if_pf (! host_addr_buf) {
+		if (errno == ENOMEM) {
+			if (psm2_verbs_evict_some(ep, pagelen, IBV_ACCESS_IS_GPU_ADDR) > 0)
+				host_addr_buf = __psm2_rv_pin_and_mmap(ep->verbs_ep.rv, pageaddr, pagelen, IBV_ACCESS_IS_GPU_ADDR);
 		}
+		if_pf (! host_addr_buf)
+			return NULL;
 	}
+//_HFI_ERROR("pinned buf=%p size=%zu pageaddr=%p pagelen=%u flags=0x%x ep=%p, @ %p\n", (void *)buf, size, (void *)pageaddr, pagelen, flags, ep, host_addr_buf);
 #else
 	psmi_assert_always(0);	// unimplemented, should not get here
 	host_addr_buf = NULL;
@@ -117,29 +108,6 @@ gdr_convert_gpu_to_host_addr(int gdr_fd, unsigned long buf,
 	return host_addr_buf + (buf & GPU_PAGE_OFFSET_MASK);
 }
 
-// keep this symmetrical with other functions, even though gdr_fd not used
-int
-gdr_unmap_gpu_host_addr(int gdr_fd, const void *buf,
-							 size_t size, struct ips_proto* proto)
-{
-#ifdef RNDV_MOD
-	// TBD - will we need to round size up to pagelen?
-	if (0 != __psm2_rv_munmap_and_unpin(proto->ep->verbs_ep.rv, buf, size)) {
-		/* Fatal error */
-		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-						  "UNMMAP/UNPIN ioctl failed errno %d\n",
-						  errno);
-		return -1;
-	}
-	return 0;
-#else
-	psmi_assert_always(0);	// unimplemented, should not get here
-	errno = EINVAL;
-	return -1;
-#endif
-}
-
-
 void hfi_gdr_open(){
 	return;
 }
diff --git a/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1.c b/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1.c
index bd9eb23..ec5d48a 100644
--- a/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1.c
+++ b/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1.c
@@ -79,6 +79,7 @@ static hfp_gen1_t psm_gen1_hi = {
 
 		.hfp_close_context			  = hfp_gen1_close_context,
 		.hfp_context_open			  = hfp_gen1_context_open,
+		.hfp_context_initstats			  = hfp_gen1_context_initstats,
 
 
 		.hfp_finalize_				  = hfp_gen1_finalize_,
diff --git a/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1_spio.c b/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1_spio.c
index 5f57e4b..d4a832f 100644
--- a/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1_spio.c
+++ b/prov/psm3/psm3/psm_hal_gen1/psm_hal_gen1_spio.c
@@ -138,9 +138,10 @@ ips_spio_transfer_frame(struct ips_proto *proto, struct ips_flow *flow,
 	psm2_ep_t ep = proto->ep;
 	struct ibv_send_wr wr;
 	struct ibv_send_wr *bad_wr;
-	struct ibv_sge list;
+	struct ibv_sge list[2];
 	sbuf_t sbuf;
 	struct ips_message_header *ips_lrh = &scb->ips_lrh;
+	int send_dma = ips_scb_flags(scb) & IPS_SEND_FLAG_SEND_MR;
 
 	// these defines are bit ugly, but make code below simpler with less ifdefs
 	// once we decide if USE_RC is valuable we can cleanup
@@ -159,10 +160,10 @@ ips_spio_transfer_frame(struct ips_proto *proto, struct ips_flow *flow,
 				"RC eager or any "
 				"UD packet before sending",
 				1, IPS_FAULTINJ_SENDLOST);
-		if (psmi_faultinj_is_fault(fi_sendlost))
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_sendlost, ""))
 			return PSM2_OK;
 	}
-#endif
+#endif // PSM_FI
 	PSMI_LOCK_ASSERT(proto->mq->progress_lock);
 	psmi_assert_always(! cksum_valid);	// no software checksum yet
 	// allocate a send buffer
@@ -186,25 +187,41 @@ ips_spio_transfer_frame(struct ips_proto *proto, struct ips_flow *flow,
 	// copy scb->ips_lrh to send buffer
 	_HFI_VDBG("copy lrh %p\n", ips_lrh);
 	memcpy(sbuf_to_buffer(sbuf), ips_lrh, sizeof(*ips_lrh));
-	// copy payload to send buffer, length could be zero, be safe
-	_HFI_VDBG("copy payload %p %u\n",  payload, length);
+	if (!send_dma) {
+		// copy payload to send buffer, length could be zero, be safe
+		_HFI_VDBG("copy payload %p %u\n",  payload, length);
 #ifdef PSM_CUDA
-	if (is_cuda_payload) {
-		PSMI_CUDA_CALL(cuMemcpyDtoH, sbuf_to_buffer(sbuf)+sizeof(*ips_lrh),
-			       (CUdeviceptr)payload, length);
-	} else
+		if (is_cuda_payload) {
+			//_HFI_ERROR("cuMemcpyDtoH %p %u\n", payload, length);
+			PSMI_CUDA_CALL(cuMemcpyDtoH, sbuf_to_buffer(sbuf)+sizeof(*ips_lrh),
+				(CUdeviceptr)payload, length);
+		} else
 #endif
-	{
-		memcpy(sbuf_to_buffer(sbuf)+sizeof(*ips_lrh), payload, length);
+		{
+			memcpy(sbuf_to_buffer(sbuf)+sizeof(*ips_lrh), payload, length);
+		}
 	}
-	_HFI_VDBG("%s send - opcode %x\n", qp_type_str(USE_QP),
-            _get_proto_hfi_opcode((struct  ips_message_header*)sbuf_to_buffer(sbuf)));
+	_HFI_VDBG("%s send - opcode %x dma %d MR %p\n", qp_type_str(USE_QP),
+            _get_proto_hfi_opcode((struct  ips_message_header*)sbuf_to_buffer(sbuf)), !!send_dma, scb->mr);
 	// we don't support software checksum
 	psmi_assert_always(! (proto->flags & IPS_PROTO_FLAG_CKSUM));
 	psmi_assert_always(USE_QP);	// make sure we aren't called too soon
-	list.addr = (uintptr_t)sbuf_to_buffer(sbuf);
-	list.length = sizeof(*ips_lrh)+ length ;	// note no UD_ADDITION
-	list.lkey = sbuf_lkey(ep, sbuf);
+	list[0].addr = (uintptr_t)sbuf_to_buffer(sbuf);
+	list[0].lkey = sbuf_lkey(ep, sbuf);
+	if (send_dma) {
+		list[0].length = sizeof(*ips_lrh);	// note no UD_ADDITION
+		list[1].addr = scb->mr->iova
+			+ ((uintptr_t)ips_scb_buffer(scb) - (uintptr_t)scb->mr->addr);
+		psmi_assert(ips_scb_buffer(scb) == payload);
+#ifdef RNDV_MOD
+		psmi_assert(psm2_verbs_user_space_mr(scb->mr));
+#endif
+		list[1].length = length;
+		list[1].lkey = scb->mr->lkey;
+	} else {
+		list[0].length = sizeof(*ips_lrh)+ length ;	// note no UD_ADDITION
+		list[1].length = 0;
+	}
 #ifdef PSM_FI
 	if_pf(PSMI_FAULTINJ_ENABLED_EP(ep)) {
 		PSMI_FAULTINJ_STATIC_DECL(fi_sq_lkey, "sq_lkey",
@@ -212,20 +229,20 @@ ips_spio_transfer_frame(struct ips_proto *proto, struct ips_flow *flow,
 				"RC eager or any "
 				"UD packet with bad lkey",
 				0, IPS_FAULTINJ_SQ_LKEY);
-		if (psmi_faultinj_is_fault(fi_sq_lkey)) {
-			printf("corrupting SQ lkey QP %u\n", USE_QP->qp_num );
-			fflush(stdout);
-			list.lkey = 0x55;
-		}
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_sq_lkey, " QP %u", USE_QP->qp_num ))
+			list[0].lkey = 0x55;
 	}
-#endif
+#endif // PSM_FI
 	wr.next = NULL;	// just post 1
 	psmi_assert(!((uintptr_t)sbuf & VERBS_SQ_WR_ID_MASK));
 	wr.wr_id = (uintptr_t)sbuf | VERBS_SQ_WR_ID_SEND;	// we'll get this back in completion
-		// we don't use the scb as wr_id since it seems they may be freed
+		// we don't use the scb as wr_id since for PIO they may be freed
 		// immediately after a succesful call to transfer
-	wr.sg_list = &list;
-	wr.num_sge = 1;	// size of sg_list
+	wr.sg_list = list;
+	if (send_dma)
+		wr.num_sge = 2;	// size of sg_list
+	else
+		wr.num_sge = 1;	// size of sg_list
 	wr.opcode = IBV_WR_SEND;
 	// we want to only get occasional send completions
 	// and use them to release a whole set of buffers for reuse
@@ -247,7 +264,7 @@ ips_spio_transfer_frame(struct ips_proto *proto, struct ips_flow *flow,
 	}
 
 		// for small messages, we may use IBV_SEND_INLINE for performance
-	if (list.length <= USE_MAX_INLINE)
+	if (! send_dma && list[0].length <= USE_MAX_INLINE)
 		wr.send_flags |= IBV_SEND_INLINE;
 	//wr.imm_data = 0;	// only if we use IBV_WR_SEND_WITH_IMM;
 	// ud fields are ignored for RC send (overlay fields for RDMA)
@@ -260,20 +277,21 @@ ips_spio_transfer_frame(struct ips_proto *proto, struct ips_flow *flow,
 
 	if (_HFI_PDBG_ON) {
 		_HFI_PDBG("ud_transfer_frame: len %u, remote qpn %u payload %u\n",
-			list.length,
+			list[0].length+list[1].length,
 				(USE_QP->qp_type != IBV_QPT_UD)? flow->ipsaddr->remote_qpn :
 				 wr.wr.ud.remote_qpn,
 			length);
-		__psm2_dump_buf((uint8_t*)list.addr, list.length);
+		_HFI_PDBG_DUMP((uint8_t*)list[0].addr, list[1].length);
 		_HFI_PDBG("post send: QP %p (%u)\n", USE_QP, USE_QP->qp_num);
 	}
 	if_pf (ibv_post_send(USE_QP, &wr, &bad_wr)) {
 		if (errno != EBUSY && errno != EAGAIN && errno != ENOMEM)
-			_HFI_ERROR("failed to post SQ: %s", strerror(errno));
+			_HFI_ERROR("failed to post SQ on %s: %s", ep->dev_name, strerror(errno));
+		proto->stats.post_send_fail++;
 		ret = PSM2_EP_NO_RESOURCES;
 	}
 	_HFI_VDBG("done ud_transfer_frame: len %u, remote qpn %u\n",
-		list.length,
+		list[0].length +list[1].length,
 		(USE_QP->qp_type != IBV_QPT_UD)? flow->ipsaddr->remote_qpn :
  		wr.wr.ud.remote_qpn);
 	// reap any completions
diff --git a/prov/psm3/psm3/psm_hal_gen1/psm_hal_inline_i.h b/prov/psm3/psm3/psm_hal_gen1/psm_hal_inline_i.h
index d89fd79..0119daf 100644
--- a/prov/psm3/psm3/psm_hal_gen1/psm_hal_inline_i.h
+++ b/prov/psm3/psm3/psm_hal_gen1/psm_hal_inline_i.h
@@ -239,7 +239,7 @@ static PSMI_HAL_INLINE int hfp_gen1_context_open(int unit,
 		cpu_set_t mycpuset, andcpuset;
 
 		if (hfi_get_unit_cpumask(unit, &mycpuset)) {
-			_HFI_ERROR( "Failed to get unit %d's cpu set\n", unit);
+			_HFI_ERROR( "Failed to get %s (unit %d) cpu set\n", ep->dev_name, unit);
 			//err = -PSM_HAL_ERROR_GENERAL_ERROR;
 			goto bail;
 		}
@@ -254,7 +254,7 @@ static PSMI_HAL_INLINE int hfp_gen1_context_open(int unit,
 		int cpu_and_count = CPU_COUNT(&andcpuset);
 
 		if (cpu_and_count > 0 && pthread_setaffinity_np(mythread, sizeof(andcpuset), &andcpuset)) {
-			_HFI_ERROR( "Failed to set unit %d's cpu set: %s\n", unit, strerror(errno));
+			_HFI_ERROR( "Failed to set %s (unit %d) cpu set: %s\n", ep->dev_name,  unit, strerror(errno));
 			//err = -PSM_HAL_ERROR_GENERAL_ERROR;
 			goto bail;
 		} else if (cpu_and_count == 0 && _HFI_DBG_ON) {
@@ -292,6 +292,12 @@ bail:
 	return -PSM_HAL_ERROR_GENERAL_ERROR;
 }
 
+/* hfp_gen1_context_initstats */
+static PSMI_HAL_INLINE void hfp_gen1_context_initstats(psm2_ep_t ep)
+{
+	__psm2_ep_initstats_verbs(ep);
+}
+
 
 
 
diff --git a/prov/psm3/psm3/psm_lock.h b/prov/psm3/psm3/psm_lock.h
index a7393a5..d5aad6f 100644
--- a/prov/psm3/psm3/psm_lock.h
+++ b/prov/psm3/psm3/psm_lock.h
@@ -207,7 +207,8 @@ PSMI_ALWAYS_INLINE(int psmi_sem_timedwait(sem_t *sem, const char *name))
 	ts.tv_sec += 5;
 
 	if (sem_timedwait(sem, &ts) == -1) {
-		_HFI_VDBG("Semaphore %s: Timedwait failed\n", name ? name : "NULL" );
+		_HFI_VDBG("Semaphore %s: Timedwait failed: %s (%d)\n",
+				name ? name : "NULL", strerror(errno), errno );
 		return -1;
 	}
 
diff --git a/prov/psm3/psm3/psm_mpool.c b/prov/psm3/psm3/psm_mpool.c
index 1f2a365..7da50be 100644
--- a/prov/psm3/psm3/psm_mpool.c
+++ b/prov/psm3/psm3/psm_mpool.c
@@ -68,7 +68,7 @@ struct mpool_element {
 #ifdef PSM_DEBUG
 	uint32_t me_isused;
 #endif
-} __attribute__ ((aligned(8)));
+} __attribute__ ((aligned(16)));
 
 #ifdef PSM_DEBUG
 #  define me_mark_used(me)    ((me)->me_isused = 1)
@@ -182,13 +182,16 @@ psmi_mpool_create_inner(size_t obj_size, uint32_t num_obj_per_chunk,
 	mp->mp_elm_vector_free = mp->mp_elm_vector;
 
 	if (flags & PSMI_MPOOL_ALIGN) {
+		// TBD - this is broken, mp_elm_offset is not
+		// used all the places where it needs to be
+		// fortunately this flag is not used yet
+		psmi_assert_always(0);
 		/* User wants its block to start on a PSMI_MPOOL_ALIGNMENT
 		 * boundary. */
 		hdr_size = PSMI_ALIGNUP(sizeof(struct mpool_element),
 					PSMI_MPOOL_ALIGNMENT);
 		mp->mp_obj_size = PSMI_ALIGNUP(obj_size, PSMI_MPOOL_ALIGNMENT);
 		mp->mp_elm_size = hdr_size + mp->mp_obj_size;
-
 		mp->mp_elm_offset = hdr_size - sizeof(struct mpool_element);
 	} else {
 		hdr_size = sizeof(struct mpool_element);
diff --git a/prov/psm3/psm3/psm_mq.c b/prov/psm3/psm3/psm_mq.c
index c489198..660f188 100644
--- a/prov/psm3/psm3/psm_mq.c
+++ b/prov/psm3/psm3/psm_mq.c
@@ -759,45 +759,35 @@ PSMI_API_DECL(psm2_mq_send)
  * that the provided request has been matched, and begins copying message data
  * that has already arrived to the user's buffer.  Any remaining data is copied
  * by PSM polling until the message is complete.
+ * Caller has initialized req->is_buf_gpu_mem and req->user_gpu_buffer
+ * consistently with buf/len which represent the application buffer
+ * but req->req_data.buf and req->req_data.len still point to the sysbuf
+ * where data was landed.
  */
 static psm2_error_t
 psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
 {
-	uint32_t copysz;
+	uint32_t msglen;
 
 	PSM2_LOG_MSG("entering");
 	psmi_assert(MQE_TYPE_IS_RECV(req->type));
-	psmi_mtucpy_fn_t psmi_mtucpy_fn = psmi_mq_mtucpy;
-#if defined(PSM_CUDA)
-	int converted = 0;
-	if (!req->is_buf_gpu_mem)
-		psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
-#endif // PSM_CUDA
 
 	_HFI_VDBG("(req=%p) buf=%p len=%u req.state=%u\n", req, buf, len, req->state);
 
 	switch (req->state) {
 	case MQ_STATE_COMPLETE:
 		if (req->req_data.buf != NULL) {	/* 0-byte messages don't alloc a sysbuf */
-			copysz = mq_set_msglen(req, len, req->req_data.send_msglen);
-			void *ubuf = buf;
+			msglen = mq_set_msglen(req, len, req->req_data.send_msglen);
+			psmi_mq_recv_copy(mq, req,
 #ifdef PSM_CUDA
-			if (PSMI_USE_GDR_COPY(req, len)) {
-				ubuf = gdr_convert_gpu_to_host_addr(GDR_FD, (unsigned long)buf,
-								    len, 1,
-								    mq->ep->epaddr->proto);
-				psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
-				converted = 1;
-			}
-#endif // PSM_CUDA
-			psmi_mtucpy_fn(ubuf, (const void *)req->req_data.buf, copysz);
-#if defined(PSM_CUDA)
-			if (converted) {
-				gdr_unmap_gpu_host_addr(GDR_FD, ubuf, len,
-                                    mq->ep->epaddr->proto);
-			}
-#endif // PSM_CUDA
+					req->is_buf_gpu_mem,
+#endif
+					buf, len, msglen);
 			psmi_mq_sysbuf_free(mq, req->req_data.buf);
+#ifdef PSM_CUDA
+		} else {
+			mq->stats.rx_sysbuf_cpu_num++;
+#endif
 		}
 		req->req_data.buf = buf;
 		req->req_data.buf_len = len;
@@ -805,32 +795,16 @@ psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
 		break;
 
 	case MQ_STATE_UNEXP:	/* not done yet */
-		copysz = mq_set_msglen(req, len, req->req_data.send_msglen);
+		msglen = mq_set_msglen(req, len, req->req_data.send_msglen);
 		/* Copy What's been received so far and make sure we don't receive
 		 * any more than copysz.  After that, swap system with user buffer
 		 */
-		req->recv_msgoff = min(req->recv_msgoff, copysz);
-
+		req->recv_msgoff = min(req->recv_msgoff, msglen);
+		psmi_mq_recv_copy(mq, req,
 #ifdef PSM_CUDA
-		if (PSMI_USE_GDR_COPY(req, req->req_data.send_msglen)) {
-			buf = gdr_convert_gpu_to_host_addr(GDR_FD, (unsigned long)req->user_gpu_buffer,
-							   req->req_data.send_msglen, 1,
-							   mq->ep->epaddr->proto);
-			psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
-			converted = 1;
-		}
-#endif // PSM_CUDA
-
-		if (req->recv_msgoff) {
-			psmi_mtucpy_fn(buf, (const void *)req->req_data.buf,
-				       req->recv_msgoff);
-		}
-#if defined(PSM_CUDA)
-		if (converted) {
-			gdr_unmap_gpu_host_addr(GDR_FD, buf, req->req_data.send_msglen,
-                                   mq->ep->epaddr->proto);
-		}
-#endif // PSM_CUDA
+				req->is_buf_gpu_mem,
+#endif
+				buf, len, req->recv_msgoff);
 		psmi_mq_sysbuf_free(mq, req->req_data.buf);
 
 		req->state = MQ_STATE_MATCHED;
@@ -839,16 +813,17 @@ psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
 		break;
 
 	case MQ_STATE_UNEXP_RV:	/* rendez-vous ... */
-		copysz = mq_set_msglen(req, len, req->req_data.send_msglen);
+		msglen = mq_set_msglen(req, len, req->req_data.send_msglen);
 		/* Copy What's been received so far and make sure we don't receive
 		 * any more than copysz.  After that, swap system with user buffer
 		 */
-		req->recv_msgoff = min(req->recv_msgoff, copysz);
-		if (req->recv_msgoff) {
-			psmi_mtucpy_fn(buf, (const void *)req->req_data.buf,
-				       req->recv_msgoff);
-		}
-		if (req->send_msgoff) {
+		req->recv_msgoff = min(req->recv_msgoff, msglen);
+		if (req->send_msgoff) {	// only have sysbuf if RTS w/payload
+			psmi_mq_recv_copy(mq, req,
+#ifdef PSM_CUDA
+					req->is_buf_gpu_mem,
+#endif
+					buf, len, req->recv_msgoff);
 			psmi_mq_sysbuf_free(mq, req->req_data.buf);
 		}
 
@@ -900,7 +875,7 @@ __psm2_mq_fp_msg(psm2_ep_t ep, psm2_mq_t mq, psm2_epaddr_t addr, psm2_mq_tag_t *
 		int gpu_mem = 0;
 		void *gpu_user_buffer = NULL;
 
-		if (PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(buf)) {
+		if (len && PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(buf)) {
 			psmi_cuda_set_attr_sync_memops(buf);
 
 			gpu_mem = 1;
@@ -979,7 +954,7 @@ __psm2_mq_irecv2(psm2_mq_t mq, psm2_epaddr_t src,
 #ifdef PSM_CUDA
 	int gpu_mem = 0;
 
-	if (PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(buf)) {
+	if (len && PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(buf)) {
 		psmi_cuda_set_attr_sync_memops(buf);
 
 		gpu_mem = 1;
@@ -1100,11 +1075,13 @@ __psm2_mq_imrecv(psm2_mq_t mq, uint32_t flags, void *buf, uint32_t len,
 		req->req_data.context = context;
 
 #ifdef PSM_CUDA
-		if (PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(buf)) {
+		if (len && PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(buf)) {
 			psmi_cuda_set_attr_sync_memops(buf);
 			req->is_buf_gpu_mem = 1;
+			req->user_gpu_buffer = buf;
 		} else {
 			req->is_buf_gpu_mem = 0;
+			req->user_gpu_buffer = NULL;
 		}
 #endif
 
@@ -1508,7 +1485,7 @@ void __psm2_mq_get_stats(psm2_mq_t mq, psm2_mq_stats_t *stats)
 }
 PSMI_API_DECL(psm2_mq_get_stats)
 
-static psm2_error_t psmi_mq_initstats(psm2_mq_t mq)
+psm2_error_t psmi_mq_initstats(psm2_mq_t mq, psm2_epid_t epid)
 {
 	 struct psmi_stats_entry entries[] = {
 		PSMI_STATS_DECL("COMM_WORLD_Rank",
@@ -1524,15 +1501,30 @@ static psm2_error_t psmi_mq_initstats(psm2_mq_t mq)
 		PSMI_STATS_DECLU64("Unexpected_count_recv", &mq->stats.rx_sys_num),
 		PSMI_STATS_DECLU64("Unexpected_bytes_recv", &mq->stats.rx_sys_bytes),
 		PSMI_STATS_DECLU64("shm_count_sent", &mq->stats.tx_shm_num),
+		PSMI_STATS_DECLU64("shm_bytes_sent", &mq->stats.tx_shm_bytes),
 		PSMI_STATS_DECLU64("shm_count_recv", &mq->stats.rx_shm_num),
-		PSMI_STATS_DECLU64("sysbuf_count", &mq->stats.rx_sysbuf_num),
-		PSMI_STATS_DECLU64("sysbuf_bytes", &mq->stats.rx_sysbuf_bytes),
+		PSMI_STATS_DECLU64("shm_bytes_recv", &mq->stats.rx_shm_bytes),
+		PSMI_STATS_DECLU64("sysbuf_count_recv", &mq->stats.rx_sysbuf_num),
+		PSMI_STATS_DECLU64("sysbuf_bytes_recv", &mq->stats.rx_sysbuf_bytes),
+#ifdef PSM_CUDA
+		PSMI_STATS_DECLU64("Eager_cpu_count_sent", &mq->stats.tx_eager_cpu_num),
+		PSMI_STATS_DECLU64("Eager_cpu_bytes_sent", &mq->stats.tx_eager_cpu_bytes),
+		PSMI_STATS_DECLU64("Eager_gpu_count_sent", &mq->stats.tx_eager_gpu_num),
+		PSMI_STATS_DECLU64("Eager_gpu_bytes_sent", &mq->stats.tx_eager_gpu_bytes),
+		PSMI_STATS_DECLU64("sysbuf_cpu_count_recv", &mq->stats.rx_sysbuf_cpu_num),
+		PSMI_STATS_DECLU64("sysbuf_cpu_bytes_recv", &mq->stats.rx_sysbuf_cpu_bytes),
+		PSMI_STATS_DECLU64("sysbuf_gdrcopy_count_recv", &mq->stats.rx_sysbuf_gdrcopy_num),
+		PSMI_STATS_DECLU64("sysbuf_gdrcopy_bytes_recv", &mq->stats.rx_sysbuf_gdrcopy_bytes),
+		PSMI_STATS_DECLU64("sysbuf_cuCopy_count_recv", &mq->stats.rx_sysbuf_cuCopy_num),
+		PSMI_STATS_DECLU64("sysbuf_cuCopy_bytes_recv", &mq->stats.rx_sysbuf_cuCopy_bytes),
+#endif
 	};
 
 	return psmi_stats_register_type("MPI_Statistics_Summary",
 					PSMI_STATSTYPE_MQ,
 					entries,
-					PSMI_STATS_HOWMANY(entries), 0, mq);
+					PSMI_STATS_HOWMANY(entries),
+					epid, mq, NULL);
 }
 
 psm2_error_t psmi_mq_malloc(psm2_mq_t *mqo)
@@ -1588,7 +1580,6 @@ psm2_error_t psmi_mq_malloc(psm2_mq_t *mqo)
 	err = psmi_mq_req_init(mq);
 	if (err)
 		goto fail;
-	psmi_mq_initstats(mq);
 
 	*mqo = mq;
 
@@ -1604,8 +1595,8 @@ psm2_error_t psmi_mq_initialize_defaults(psm2_mq_t mq)
 	union psmi_envvar_val env_hfitiny, env_rvwin, env_hfirv,
 		env_shmrv, env_stats;
 
-	psmi_getenv("PSM3_MQ_TINY_NIC_THRESH",
-		    "NIC tiny packet switchover (max 8, default 8)",
+	psmi_getenv("PSM3_MQ_TINY_NIC_LIMIT",
+		    "NIC tiny packet limit (max 8, default 8)",
 		    PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_UINT,
 		    (union psmi_envvar_val)mq->hfi_thresh_tiny, &env_hfitiny);
 	mq->hfi_thresh_tiny = min(env_hfitiny.e_uint, 8);
diff --git a/prov/psm3/psm3/psm_mq_internal.h b/prov/psm3/psm3/psm_mq_internal.h
index 452bf7a..e3d246b 100644
--- a/prov/psm3/psm3/psm_mq_internal.h
+++ b/prov/psm3/psm3/psm_mq_internal.h
@@ -246,12 +246,11 @@ struct psm2_mq_req {
 	psm2_verbs_mr_t	mr;	// local registered memory for app buffer
 
 #ifdef PSM_CUDA
-	uint8_t* user_gpu_buffer;
+	uint8_t* user_gpu_buffer;	/* for recv */
 	STAILQ_HEAD(sendreq_spec_, ips_cuda_hostbuf) sendreq_prefetch;
 	uint32_t prefetch_send_msgoff;
 	int cuda_hostbuf_used;
 	CUipcMemHandle cuda_ipc_handle;
-	CUevent cuda_ipc_event;
 	uint8_t cuda_ipc_handle_attached;
 	uint32_t cuda_ipc_offset;
 	/*
@@ -259,12 +258,12 @@ struct psm2_mq_req {
 	 * when send is on a device buffer
 	 */
 	uint8_t is_sendbuf_gpu_mem;
-#endif
 	/*
 	 * is_buf_gpu_mem - used to indicate if the send or receive is issued
 	 * on a device/host buffer.
 	 */
 	uint8_t is_buf_gpu_mem;
+#endif
 
 	/* PTLs get to store their own per-request data.  MQ manages the allocation
 	 * by allocating psm2_mq_req so that ptl_req_data has enough space for all
@@ -307,7 +306,7 @@ void
 mq_copy_tiny(uint32_t *dest, uint32_t *src, uint8_t len))
 {
 #ifdef PSM_CUDA
-	if (PSMI_IS_CUDA_ENABLED && (PSMI_IS_CUDA_MEM(dest) || PSMI_IS_CUDA_MEM(src))) {
+	if (len && PSMI_IS_CUDA_ENABLED && (PSMI_IS_CUDA_MEM(dest) || PSMI_IS_CUDA_MEM(src))) {
 		PSMI_CUDA_CALL(cuMemcpy, (CUdeviceptr)dest, (CUdeviceptr)src, len);
 		return;
 	}
@@ -345,6 +344,7 @@ mq_copy_tiny(uint32_t *dest, uint32_t *src, uint8_t len))
 }
 
 typedef void (*psmi_mtucpy_fn_t)(void *dest, const void *src, uint32_t len);
+typedef void (*psmi_copy_tiny_fn_t)(uint32_t *dest, uint32_t *src, uint8_t len);
 #ifdef PSM_CUDA
 
 PSMI_ALWAYS_INLINE(
@@ -535,6 +535,7 @@ MOCK_DCL_EPILOGUE(psmi_mq_req_alloc);
  */
 psm2_error_t psmi_mq_malloc(psm2_mq_t *mqo);
 psm2_error_t psmi_mq_initialize_defaults(psm2_mq_t mq);
+psm2_error_t psmi_mq_initstats(psm2_mq_t mq, psm2_epid_t epid);
 
 psm2_error_t MOCKABLE(psmi_mq_free)(psm2_mq_t mq);
 MOCK_DCL_EPILOGUE(psmi_mq_free);
@@ -548,17 +549,44 @@ MOCK_DCL_EPILOGUE(psmi_mq_free);
 
 void psmi_mq_handle_rts_complete(psm2_mq_req_t req);
 int psmi_mq_handle_data(psm2_mq_t mq, psm2_mq_req_t req,
-			uint32_t offset, const void *payload, uint32_t paylen);
+			uint32_t offset, const void *payload, uint32_t paylen
+#ifdef PSM_CUDA
+			, int use_gdrcopy,
+			psm2_ep_t ep
+#endif
+			);
 int psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
+		       struct ptl_strategy_stats *stats,
 		       uint32_t msglen, const void *payload, uint32_t paylen,
 		       int msgorder, mq_rts_callback_fn_t cb,
 		       psm2_mq_req_t *req_o);
 int psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
+			    struct ptl_strategy_stats *stats,
 			    uint32_t msglen, uint32_t offset,
 			    const void *payload, uint32_t paylen, int msgorder,
 			    uint32_t opcode, psm2_mq_req_t *req_o);
 int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t req);
 
+// perform the actual copy for a recv matching a sysbuf.  We copy from a sysbuf
+// (req->req_data.buf) to the actual user buffer (buf) and keep statistics.
+// is_buf_gpu_mem indicates if buf is a gpu buffer
+// len - recv buffer size posted, we use this for any GDR copy pinning so
+// 	can get future cache hits on other size messages in same buffer
+// not needed - msglen - negotiated total message size
+// copysz - actual amount to copy (<= msglen)
+#ifdef PSM_CUDA
+void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, uint8_t is_buf_gpu_mem,
+                                void *buf, uint32_t len, uint32_t copysz);
+#else
+PSMI_ALWAYS_INLINE(
+void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, void *buf,
+                                uint32_t len, uint32_t copysz))
+{
+	if (copysz)
+		psmi_mq_mtucpy(buf, (const void *)req->req_data.buf, copysz);
+}
+#endif
+
 #if 0   // unused code, specific to QLogic MPI
 void psmi_mq_stats_register(psm2_mq_t mq, mpspawn_stats_add_fn add_fn);
 #endif
@@ -606,18 +634,4 @@ psmi_mq_register_unexpected_callback(psm2_mq_t mq,
 				     psm_mq_unexpected_callback_fn_t fn);
 #endif
 
-PSMI_ALWAYS_INLINE(void psmi_mq_stats_rts_account(psm2_mq_req_t req))
-{
-	psm2_mq_t mq = req->mq;
-	if (MQE_TYPE_IS_SEND(req->type)) {
-		mq->stats.tx_num++;
-		mq->stats.tx_rndv_num++;
-		mq->stats.tx_rndv_bytes += req->req_data.send_msglen;
-	} else {
-		mq->stats.rx_user_num++;
-		mq->stats.rx_user_bytes += req->req_data.recv_msglen;
-	}
-	return;
-}
-
 #endif
diff --git a/prov/psm3/psm3/psm_mq_recv.c b/prov/psm3/psm3/psm_mq_recv.c
index 131c5b5..0e93807 100644
--- a/prov/psm3/psm3/psm_mq_recv.c
+++ b/prov/psm3/psm3/psm_mq_recv.c
@@ -90,8 +90,6 @@ void psmi_mq_handle_rts_complete(psm2_mq_req_t req)
 		ips_tid_mravail_callback(req->rts_peer->proto);
 	}
 
-	/* Stats on rendez-vous messages */
-	psmi_mq_stats_rts_account(req);
 	req->state = MQ_STATE_COMPLETE;
 	ips_barrier();
 	if(!psmi_is_req_internal(req))
@@ -104,7 +102,12 @@ void psmi_mq_handle_rts_complete(psm2_mq_req_t req)
 
 static void
 psmi_mq_req_copy(psm2_mq_req_t req,
-		 uint32_t offset, const void *buf, uint32_t nbytes)
+		 uint32_t offset, const void *buf, uint32_t nbytes
+#ifdef PSM_CUDA
+		, int use_gdrcopy,
+		psm2_ep_t ep
+#endif
+		)
 {
 	/* recv_msglen may be changed by unexpected receive req_data.buf. */
 	uint32_t msglen_this, end;
@@ -123,8 +126,19 @@ psmi_mq_req_copy(psm2_mq_req_t req,
 	} else {
 		msglen_this = nbytes;
 	}
-
-	psmi_mq_mtucpy(msgptr, buf, msglen_this);
+#ifdef PSM_CUDA
+	if (use_gdrcopy) {
+		void *ubuf;
+		ubuf = gdr_convert_gpu_to_host_addr(GDR_FD,
+				(unsigned long)msgptr, msglen_this, 1,
+				ep);
+		if  (! ubuf)
+			psmi_mq_mtucpy(msgptr, buf, msglen_this);
+		else
+		psmi_mq_mtucpy_host_mem(ubuf, buf, msglen_this);
+	} else
+#endif
+		psmi_mq_mtucpy(msgptr, buf, msglen_this);
 
 	if (req->recv_msgoff < end) {
 		req->recv_msgoff = end;
@@ -135,9 +149,17 @@ psmi_mq_req_copy(psm2_mq_req_t req,
 }
 
 // This handles eager and LONG_DATA payload and completion for receiver
+// For ips eager, the caller will have already prepared for gdrcopy
+// For ips, LONG_DATA will not be used for GPU buffers unless RDMA disabled
+// So no need/opportunity to take advantage of gdrcopy here.
 int
 psmi_mq_handle_data(psm2_mq_t mq, psm2_mq_req_t req,
-		    uint32_t offset, const void *buf, uint32_t nbytes)
+		    uint32_t offset, const void *buf, uint32_t nbytes
+#ifdef PSM_CUDA
+		    , int use_gdrcopy,
+		    psm2_ep_t ep
+#endif
+		)
 {
 	psmi_assert(req != NULL);
 	int rc;
@@ -146,10 +168,17 @@ psmi_mq_handle_data(psm2_mq_t mq, psm2_mq_req_t req,
 		rc = MQ_RET_MATCH_OK;
 	else {
 		psmi_assert(req->state == MQ_STATE_UNEXP);
+		// TBD - will be sysbuf, could tell psmi_mq_req_copy to
+		// use psmi_mq_mtucpy_host_mem by passing a func arg
+		// but limited benefit for eager/long protocol
 		rc = MQ_RET_UNEXP_OK;
 	}
 
+#ifdef PSM_CUDA
+	psmi_mq_req_copy(req, offset, buf, nbytes, use_gdrcopy, ep);
+#else
 	psmi_mq_req_copy(req, offset, buf, nbytes);
+#endif
 
 	/*
 	 * the reason to use >= is because send_msgoff
@@ -267,6 +296,7 @@ mq_req_match(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag, int remove)
  */
 int
 psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
+		   struct ptl_strategy_stats *stats,
 		   uint32_t send_msglen, const void *payload, uint32_t paylen,
 		   int msgorder, mq_rts_callback_fn_t cb, psm2_mq_req_t *req_o)
 {
@@ -291,6 +321,16 @@ psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		if (paylen) {
 			// payload of RTS can contain a single packet synchronous MPI msg
 			psmi_mq_mtucpy(req->req_data.buf, payload, paylen);
+#ifdef PSM_CUDA
+			if (req->is_buf_gpu_mem) {
+				stats->rndv_rts_cuCopy_recv++;
+				stats->rndv_rts_cuCopy_recv_bytes += paylen;
+			} else
+#endif
+			{
+				stats->rndv_rts_cpu_recv++;
+				stats->rndv_rts_cpu_recv_bytes += paylen;
+			}
 		}
 		req->recv_msgoff = req->send_msgoff = paylen;
 		*req_o = req;	/* yes match */
@@ -339,9 +379,13 @@ psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		if (paylen) {
 			req->req_data.buf = psmi_mq_sysbuf_alloc(mq, paylen);
 			psmi_assert(paylen == 0 || req->req_data.buf != NULL);
-			mq->stats.rx_sysbuf_num++;
-			mq->stats.rx_sysbuf_bytes += paylen;
+#ifdef PSM_CUDA
+			psmi_mq_mtucpy_host_mem(req->req_data.buf, payload, paylen);
+#else
 			psmi_mq_mtucpy(req->req_data.buf, payload, paylen);
+#endif
+			stats->rndv_rts_sysbuf_recv++;
+			stats->rndv_rts_sysbuf_recv_bytes += paylen;
 		}
 		req->recv_msgoff = req->send_msgoff = paylen;
 
@@ -374,6 +418,7 @@ psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
  */
 int
 psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
+			struct ptl_strategy_stats *stats,
 			uint32_t send_msglen, uint32_t offset,
 			const void *payload, uint32_t paylen, int msgorder,
 			uint32_t opcode, psm2_mq_req_t *req_o)
@@ -382,7 +427,7 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 	uint32_t msglen;
 	psmi_mtucpy_fn_t psmi_mtucpy_fn;
 #if defined(PSM_CUDA)
-	int converted = 0;
+	int use_gdrcopy = 0;
 #endif // PSM_CUDA
 
 	if (msgorder && (req = mq_req_match(mq, src, tag, 1))) {
@@ -403,20 +448,31 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		case MQ_MSG_TINY:
 			/* mq_copy_tiny() can handle zero byte */
 #ifdef PSM_CUDA
-			if (PSMI_USE_GDR_COPY(req, msglen)) {
-				user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
+			if (!req->is_buf_gpu_mem) {
+				mq_copy_tiny_host_mem((uint32_t *) user_buffer, (uint32_t *) payload, msglen);
+				stats->tiny_cpu_recv++;
+				stats->tiny_cpu_recv_bytes += msglen;
+			// conversion will round up to 64K so just use
+			// msglen here to protect against huge buf_len
+			} else if (PSMI_USE_GDR_COPY_RECV(msglen) &&
+				NULL != (user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
 								(unsigned long)req->req_data.buf,
-								msglen, 1, src->proto);
-				converted = 1;
-			}
+								msglen, 1, mq->ep))) {
+				mq_copy_tiny_host_mem((uint32_t *) user_buffer, (uint32_t *) payload, msglen);
+				stats->tiny_gdrcopy_recv++;
+				stats->tiny_gdrcopy_recv_bytes += msglen;
+			} else {
+				user_buffer = req->req_data.buf;
 #endif
-			mq_copy_tiny((uint32_t *) user_buffer, (uint32_t *) payload, msglen);
-#if defined(PSM_CUDA)
-			if (converted) {
-				gdr_unmap_gpu_host_addr(GDR_FD, user_buffer, msglen,
-                                    src->proto);
+				mq_copy_tiny((uint32_t *) user_buffer, (uint32_t *) payload, msglen);
+#ifdef PSM_CUDA
+				stats->tiny_cuCopy_recv++;
+				stats->tiny_cuCopy_recv_bytes += msglen;
 			}
-#endif // PSM_CUDA
+#else
+			stats->tiny_cpu_recv++;
+			stats->tiny_cpu_recv_bytes += msglen;
+#endif
 
 			req->state = MQ_STATE_COMPLETE;
 			ips_barrier();
@@ -426,13 +482,29 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		case MQ_MSG_SHORT:	/* message fits in 1 payload */
 			psmi_mtucpy_fn = psmi_mq_mtucpy;
 #ifdef PSM_CUDA
-			if (PSMI_USE_GDR_COPY(req, msglen)) {
-				user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
+			if (!req->is_buf_gpu_mem) {
+				psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
+				stats->short_cpu_recv++;
+				stats->short_cpu_recv_bytes += msglen;
+			// conversion will round up to 64K so just use
+			// msglen here to protect against huge buf_len
+			} else if (PSMI_USE_GDR_COPY_RECV(msglen) &&
+				NULL != (user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
 							(unsigned long)req->req_data.buf,
-							msglen, 1, src->proto);
+							msglen, 1, mq->ep))) {
 				psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
-				converted = 1;
+				stats->short_gdrcopy_recv++;
+				stats->short_gdrcopy_recv_bytes += msglen;
+			} else {
+				user_buffer = req->req_data.buf;
+#endif
+#ifdef PSM_CUDA
+				stats->short_cuCopy_recv++;
+				stats->short_cuCopy_recv_bytes += msglen;
 			}
+#else
+			stats->short_cpu_recv++;
+			stats->short_cpu_recv_bytes += msglen;
 #endif
 			if (msglen <= paylen) {
 				psmi_mtucpy_fn(user_buffer, payload, msglen);
@@ -446,12 +518,6 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 				mq_copy_tiny((uint32_t *)((uint8_t *)user_buffer + paylen),
 					(uint32_t *)&offset, msglen & 0x3);
 			}
-#if defined(PSM_CUDA)
-			if (converted) {
-				gdr_unmap_gpu_host_addr(GDR_FD, user_buffer, msglen,
-                                    src->proto);
-			}
-#endif // PSM_CUDA
 			req->state = MQ_STATE_COMPLETE;
 			ips_barrier();
 			mq_qq_append(&mq->completed_q, req);
@@ -464,23 +530,32 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 			STAILQ_INSERT_TAIL(&mq->eager_q, req, nextq);
 			_HFI_VDBG("exp MSG_EAGER of length %d bytes pay=%d\n",
 				  msglen, paylen);
+			// !offset -> only count recv msgs on 1st pkt in msg
 #ifdef PSM_CUDA
-			if (PSMI_USE_GDR_COPY(req, req->req_data.send_msglen)) {
-				req->req_data.buf = gdr_convert_gpu_to_host_addr(GDR_FD,
-						(unsigned long)req->user_gpu_buffer,
-						req->req_data.send_msglen, 1, src->proto);
-				converted = 1;
+			if (!req->is_buf_gpu_mem) {
+				if (!offset) stats->eager_cpu_recv++;
+				stats->eager_cpu_recv_bytes += paylen;
+			} else if (PSMI_USE_GDR_COPY_RECV(paylen)) {
+				req->req_data.buf = req->user_gpu_buffer;
+				use_gdrcopy = 1;
+				if (!offset) stats->eager_gdrcopy_recv++;
+				stats->eager_gdrcopy_recv_bytes += paylen;
+			} else {
+				req->req_data.buf = req->user_gpu_buffer;
+				if (!offset) stats->eager_cuCopy_recv++;
+				stats->eager_cuCopy_recv_bytes += paylen;
 			}
+#else
+			if (!offset) stats->eager_cpu_recv++;
+			stats->eager_cpu_recv_bytes += paylen;
 #endif
 			if (paylen > 0)
 				psmi_mq_handle_data(mq, req, offset, payload,
+#ifdef PSM_CUDA
+						    paylen, use_gdrcopy, mq->ep);
+#else
 						    paylen);
-#if defined(PSM_CUDA)
-			if (converted) {
-				gdr_unmap_gpu_host_addr(GDR_FD, req->req_data.buf,
-									req->req_data.send_msglen, src->proto);
-			}
-#endif // PSM_CUDA
+#endif
 			break;
 
 		default:
@@ -528,6 +603,7 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 	req->req_data.peer = src;
 	req->req_data.tag = *tag;
 	req->recv_msgoff = 0;
+	// don't yet know recv buffer size, so use send_msglen for now
 	req->req_data.recv_msglen = req->req_data.send_msglen = req->req_data.buf_len = msglen =
 	    send_msglen;
 
@@ -541,32 +617,52 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		if (msglen > 0) {
 			req->req_data.buf = psmi_mq_sysbuf_alloc(mq, msglen);
 			psmi_assert(msglen == 0 || req->req_data.buf != NULL);
-			mq->stats.rx_sysbuf_num++;
-			mq->stats.rx_sysbuf_bytes += paylen;
+#ifdef PSM_CUDA
+			mq_copy_tiny_host_mem((uint32_t *) req->req_data.buf,
+				     (uint32_t *) payload, msglen);
+#else
 			mq_copy_tiny((uint32_t *) req->req_data.buf,
 				     (uint32_t *) payload, msglen);
-		} else
+#endif
+			stats->tiny_sysbuf_recv++;
+			stats->tiny_sysbuf_recv_bytes += msglen;
+		} else {
 			req->req_data.buf = NULL;
+			stats->tiny_sysbuf_recv++;	// 0 length
+		}
 		req->state = MQ_STATE_COMPLETE;
 		break;
 
 	case MQ_MSG_SHORT:
 		req->req_data.buf = psmi_mq_sysbuf_alloc(mq, msglen);
 		psmi_assert(msglen == 0 || req->req_data.buf != NULL);
-		mq->stats.rx_sysbuf_num++;
-		mq->stats.rx_sysbuf_bytes += paylen;
 		if (msglen <= paylen) {
+#ifdef PSM_CUDA
+			psmi_mq_mtucpy_host_mem(req->req_data.buf, payload, msglen);
+#else
 			psmi_mq_mtucpy(req->req_data.buf, payload, msglen);
+#endif
 		} else {
 			psmi_assert((msglen & ~0x3) == paylen);
+#ifdef PSM_CUDA
+			psmi_mq_mtucpy_host_mem(req->req_data.buf, payload, paylen);
+#else
 			psmi_mq_mtucpy(req->req_data.buf, payload, paylen);
+#endif
 			/*
 			 * there are nonDW bytes attached in header,
 			 * copy after the DW payload.
 			 */
+#ifdef PSM_CUDA
+			mq_copy_tiny_host_mem((uint32_t *)(req->req_data.buf+paylen),
+				(uint32_t *)&offset, msglen & 0x3);
+#else
 			mq_copy_tiny((uint32_t *)(req->req_data.buf+paylen),
 				(uint32_t *)&offset, msglen & 0x3);
+#endif
 		}
+		stats->short_sysbuf_recv++;
+		stats->short_sysbuf_recv_bytes += msglen;
 		req->state = MQ_STATE_COMPLETE;
 		break;
 
@@ -574,15 +670,19 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		req->send_msgoff = 0;
 		req->req_data.buf = psmi_mq_sysbuf_alloc(mq, msglen);
 		psmi_assert(msglen == 0 || req->req_data.buf != NULL);
-		mq->stats.rx_sysbuf_num++;
-		mq->stats.rx_sysbuf_bytes += paylen;
 		req->state = MQ_STATE_UNEXP;
 		req->type |= MQE_TYPE_EAGER_QUEUE;
 		STAILQ_INSERT_TAIL(&mq->eager_q, req, nextq);
 		_HFI_VDBG("unexp MSG_EAGER of length %d bytes pay=%d\n",
 			  msglen, paylen);
 		if (paylen > 0)
+#ifdef PSM_CUDA
+			psmi_mq_handle_data(mq, req, offset, payload, paylen, 0, NULL);
+#else
 			psmi_mq_handle_data(mq, req, offset, payload, paylen);
+#endif
+		stats->eager_sysbuf_recv++;
+		stats->eager_sysbuf_recv_bytes += paylen;
 		break;
 
 	default:
@@ -602,6 +702,51 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 	return MQ_RET_UNEXP_OK;
 }
 
+#ifdef PSM_CUDA	// declared inline in psm_mq_internal.h for non-CUDA
+// perform the actual copy for an psmi_mq_irecv_inner.  We copy from a sysbuf
+// (req->req_data.buf) to the actual user buffer (buf) and keep statistics.
+// is_buf_gpu_mem indicates if buf is a gpu buffer
+// len - recv buffer size posted, we use this for any GDR copy pinning so
+// 	can get future cache hits on other size messages in same buffer
+// not needed - msglen - negotiated total message size
+// copysz - actual amount to copy (<= msglen)
+void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, uint8_t is_buf_gpu_mem,
+				void *buf, uint32_t len, uint32_t copysz)
+{
+	psmi_mtucpy_fn_t psmi_mtucpy_fn = psmi_mq_mtucpy;
+	void *ubuf = buf;
+	if (! copysz) {
+		mq->stats.rx_sysbuf_cpu_num++; // zero length
+		return;
+	}
+	if (!is_buf_gpu_mem) {
+		psmi_assert(!PSMI_IS_CUDA_MEM(buf));
+		mq->stats.rx_sysbuf_cpu_num++;
+		mq->stats.rx_sysbuf_cpu_bytes += copysz;
+		psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
+	// len could be huge, so limit ourselves to gdr_copy_limit_recv
+	// Note to get here copysz <= gdr_copy_limit_recv
+	} else if (PSMI_USE_GDR_COPY_RECV(copysz) &&
+		NULL != (ubuf = gdr_convert_gpu_to_host_addr(GDR_FD, (unsigned long)buf,
+						    min(gdr_copy_limit_recv, len), 1,
+						    mq->ep))) {
+		psmi_assert(PSMI_IS_CUDA_MEM(buf));
+		psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
+		mq->stats.rx_sysbuf_gdrcopy_num++;
+		mq->stats.rx_sysbuf_gdrcopy_bytes += copysz;
+	} else {
+		psmi_assert(PSMI_IS_CUDA_MEM(buf));
+		ubuf = buf;
+		mq->stats.rx_sysbuf_cuCopy_num++;
+		mq->stats.rx_sysbuf_cuCopy_bytes += copysz;
+	}
+	if (copysz)
+		psmi_mtucpy_fn(ubuf, (const void *)req->req_data.buf, copysz);
+}
+#endif // PSM_CUDA
+
+// we landed an out of order message in a sysbuf and can now process it
+// ureq is where we landed it.  If found, ereq is the user posted receive.
 int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 {
 	psm2_mq_req_t ereq;
@@ -621,9 +766,17 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 	switch (ureq->state) {
 	case MQ_STATE_COMPLETE:
 		if (ureq->req_data.buf != NULL) {	/* 0-byte don't alloc a sysreq_data.buf */
-			psmi_mq_mtucpy(ereq->req_data.buf, (const void *)ureq->req_data.buf,
-				       msglen);
+			psmi_mq_recv_copy(mq, ureq,
+#ifdef PSM_CUDA
+					ereq->is_buf_gpu_mem,
+#endif
+					ereq->req_data.buf,
+					ereq->req_data.buf_len, msglen);
 			psmi_mq_sysbuf_free(mq, ureq->req_data.buf);
+#ifdef PSM_CUDA
+		} else {
+			mq->stats.rx_sysbuf_cpu_num++; // zero length
+#endif
 		}
 		ereq->state = MQ_STATE_COMPLETE;
 		ips_barrier();
@@ -635,11 +788,12 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 		ereq->ptl_req_ptr = ureq->ptl_req_ptr;
 		ereq->send_msgoff = ureq->send_msgoff;
 		ereq->recv_msgoff = min(ureq->recv_msgoff, msglen);
-		if (ereq->recv_msgoff) {
-			psmi_mq_mtucpy(ereq->req_data.buf,
-				       (const void *)ureq->req_data.buf,
-				       ereq->recv_msgoff);
-		}
+		psmi_mq_recv_copy(mq, ureq,
+#ifdef PSM_CUDA
+				ereq->is_buf_gpu_mem,
+#endif
+				ereq->req_data.buf,
+			 	ereq->req_data.buf_len, ereq->recv_msgoff);
 		psmi_mq_sysbuf_free(mq, ureq->req_data.buf);
 		ereq->type = ureq->type;
 		STAILQ_INSERT_AFTER(&mq->eager_q, ureq, ereq, nextq);
@@ -651,12 +805,14 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 		ereq->rts_sbuf = ureq->rts_sbuf;
 		ereq->send_msgoff = ureq->send_msgoff;
 		ereq->recv_msgoff = min(ureq->recv_msgoff, msglen);
-		if (ereq->recv_msgoff) {
-			psmi_mq_mtucpy(ereq->req_data.buf,
-				       (const void *)ureq->req_data.buf,
-				       ereq->recv_msgoff);
-		}
-		if (ereq->send_msgoff) {
+		if (ereq->send_msgoff) { // only have sysbuf if RTS w/payload
+			psmi_mq_recv_copy(mq, ureq,
+#ifdef PSM_CUDA
+					ereq->is_buf_gpu_mem,
+#endif
+					ereq->req_data.buf,
+			 		ereq->req_data.buf_len,
+					ereq->recv_msgoff);
 			psmi_mq_sysbuf_free(mq, ureq->req_data.buf);
 		}
 		ereq->rts_callback = ureq->rts_callback;
diff --git a/prov/psm3/psm3/psm_mq_utils.c b/prov/psm3/psm3/psm_mq_utils.c
index be05a46..7ea5bb7 100644
--- a/prov/psm3/psm3/psm_mq_utils.c
+++ b/prov/psm3/psm3/psm_mq_utils.c
@@ -75,7 +75,6 @@ psm2_mq_req_t MOCKABLE(psmi_mq_req_alloc)(psm2_mq_t mq, uint32_t type)
 
 	if_pt(req != NULL) {
 		memset(req, 0, sizeof(struct psm2_mq_req));
-
 		req->type = type;
 		req->state = MQ_STATE_FREE;
 		req->mq = mq;
@@ -98,19 +97,6 @@ psm2_mq_req_t MOCKABLE(psmi_mq_req_alloc)(psm2_mq_t mq, uint32_t type)
 }
 MOCK_DEF_EPILOGUE(psmi_mq_req_alloc);
 
-#ifdef PSM_CUDA
-void psmi_cuda_recvreq_alloc_func(int is_alloc, void* context, void* obj) {
-	psm2_mq_req_t recvreq = (psm2_mq_req_t)obj;
-	if (PSMI_IS_CUDA_ENABLED) {
-		if (is_alloc)
-			PSMI_CUDA_CALL(cuEventCreate, &recvreq->cuda_ipc_event, CU_EVENT_DEFAULT);
-		else
-			PSMI_CUDA_CALL(cuEventDestroy, recvreq->cuda_ipc_event);
-	}
-	return;
-}
-#endif
-
 psm2_error_t psmi_mq_req_init(psm2_mq_t mq)
 {
 	psm2_mq_req_t warmup_req;
@@ -149,29 +135,6 @@ psm2_error_t psmi_mq_req_init(psm2_mq_t mq)
 		if ((err =
 		     psmi_parse_mpool_env(mq, 0, &rlim, &maxsz, &chunksz)))
 			goto fail;
-		/* Have a callback function for receive req mpool which creates
-		 * and destroy events.
-		 */
-#ifdef PSM_CUDA
-		if (PSMI_IS_CUDA_ENABLED) {
-			if ((mq->rreq_pool =
-	                     psmi_mpool_create_for_cuda(sizeof(struct psm2_mq_req), chunksz,
-                                       maxsz, 0, DESCRIPTORS, NULL,
-                                       NULL, psmi_cuda_recvreq_alloc_func, NULL)) == NULL) {
-				err = PSM2_NO_MEMORY;
-				goto fail;
-			}
-		}
-		else {
-			if ((mq->rreq_pool =
-				psmi_mpool_create(sizeof(struct psm2_mq_req), chunksz,
-                                       maxsz, 0, DESCRIPTORS, NULL,
-                                       NULL)) == NULL) {
-				err = PSM2_NO_MEMORY;
-				goto fail;
-			}
-		}
-#else
 		if ((mq->rreq_pool =
 			psmi_mpool_create(sizeof(struct psm2_mq_req), chunksz,
 				       maxsz, 0, DESCRIPTORS, NULL,
@@ -179,7 +142,6 @@ psm2_error_t psmi_mq_req_init(psm2_mq_t mq)
 			err = PSM2_NO_MEMORY;
 			goto fail;
 		}
-#endif
 	}
 
 	/* Warm up the allocators */
diff --git a/prov/psm3/psm3/psm_rndv_mod.c b/prov/psm3/psm3/psm_rndv_mod.c
index 80b598c..f21aa6c 100644
--- a/prov/psm3/psm3/psm_rndv_mod.c
+++ b/prov/psm3/psm3/psm_rndv_mod.c
@@ -65,10 +65,9 @@
 #include <ctype.h>		/* isalpha */
 //#include <netdb.h>
 #include <infiniband/verbs.h>
-//#include <infiniband/ib.h>	// for AF_IB structures
-//#include <rdma/rdma_verbs.h>
 #include "psm_user.h"	// get psmi_calloc and free
 #include "psm_rndv_mod.h"
+#include "ips_config.h"
 
 #include <sys/ioctl.h>
 #include <fcntl.h>
@@ -99,6 +98,130 @@ struct irdma_mem_reg_req {
 //#define my_calloc(nmemb, size) (psmi_calloc(PSMI_EP_NONE, NETWORK_BUFFERS, (nmemb), (size)))
 #define my_free(p) (psmi_free(p))
 
+#ifdef PSM_CUDA
+static int gpu_pin_check;	// PSM3_GPU_PIN_CHECK
+static uint64_t *gpu_bars;
+static int num_gpu_bars = 0;
+static uint64_t min_gpu_bar_size;
+
+// The second BAR address is where the GPU will map GPUDirect memory.
+// The beginning of this BAR is reserved for non-GPUDirect uses.
+// However, it has been observed that in some multi-process
+// pinning failures, HED-2035, the nvidia_p2p_get_pages can foul up
+// it's IOMMU after which the next successful pin will incorrectly
+// return the 1st physical address of the BAR for the pinned pages.
+// In this case it will report this same physical address for other GPU virtual
+// addresses and cause RDMA to use the wrong memory.
+// As a workaround, we gather the Region 1 BAR address start for each
+// GPU and if we see this address returned as the phys_addr of a mmapped
+// GPUDirect Copy or the iova of a GPU MR we fail the job before it can
+// corrupt any more application data.
+static uint64_t get_nvidia_bar_addr(int domain, int bus, int slot)
+{
+	char sysfs[100];
+	int ret;
+	FILE *f;
+	unsigned long long start_addr, end_addr, bar_size;
+
+	ret = snprintf(sysfs, sizeof(sysfs),
+		"/sys/class/pci_bus/%04x:%02x/device/%04x:%02x:%02x.0/resource",
+		domain, bus, domain, bus, slot);
+	psmi_assert_always(ret < sizeof(sysfs));
+	f = fopen(sysfs, "r");
+	if (! f) {
+		if (gpu_pin_check) {
+			_HFI_ERROR("Unable to open %s for GPU BAR Address: %s\n",
+				sysfs, strerror(errno));
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
+				"Unable to get GPU BAR address\n");
+		}
+		return 0;
+	}
+	// for each BAR region, start, end and flags are listed in hex
+	// nVidia uses the 2nd BAR region (aka Region #1) to map peer to peer
+	// accesses into it's potentially larger GPU local memory space
+	ret = fscanf(f, "%*x %*x %*x %llx %llx", &start_addr, &end_addr);
+	if (ret != 2) {
+		if (gpu_pin_check) {
+			_HFI_ERROR("Unable to get GPU BAR Address from %s: %s\n",
+				sysfs, strerror(errno));
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
+				"Unable to get GPU BAR address\n");
+		}
+		fclose(f);
+		return 0;
+	}
+	fclose(f);
+
+	bar_size = (end_addr - start_addr) + 1;
+	_HFI_DBG("GPU BAR Addr from %s is 0x%llx - 0x%llx (size 0x%llx)\n", sysfs, start_addr, end_addr, bar_size);
+	if (! min_gpu_bar_size || bar_size < min_gpu_bar_size)
+		min_gpu_bar_size = bar_size;
+	return start_addr;
+}
+
+void psm2_get_gpu_bars(void)
+{
+	int num_devices, dev;
+	union psmi_envvar_val env;
+
+	psmi_getenv("PSM3_GPU_PIN_CHECK",
+			"Enable sanity check of physical addresses mapped into GPU BAR space (Enabled by default)",
+			PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,
+			(union psmi_envvar_val)1, &env);
+	gpu_pin_check = env.e_int;
+
+	PSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);
+	gpu_bars = psmi_calloc(PSMI_EP_NONE, UNDEFINED, num_devices, sizeof(gpu_bars[0]));
+	if (! gpu_bars)
+		return;	// psmi_calloc will have exited for Out of Memory
+
+	if (gpu_pin_check)
+		num_gpu_bars = num_devices;
+
+	for (dev = 0; dev < num_devices; dev++) {
+		CUdevice device;
+		int domain, bus, slot;
+
+		PSMI_CUDA_CALL(cuDeviceGet, &device, dev);
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&domain,
+				CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID,
+				device);
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&bus,
+				CU_DEVICE_ATTRIBUTE_PCI_BUS_ID,
+				device);
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&slot,
+				CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID,
+				device);
+		gpu_bars[dev] = get_nvidia_bar_addr(domain, bus, slot);
+	}
+}
+
+static psm2_error_t psm2_check_phys_addr(uint64_t phys_addr)
+{
+	int i;
+	for (i=0; i < num_gpu_bars; i++) {
+		if (phys_addr == gpu_bars[i]) {
+			_HFI_ERROR("Incorrect Physical Address (0x%"PRIx64") returned by nVidia driver.  PSM3 exiting to avoid data corruption.  Job may be rerun with PSM3_GPUDIRECT=0 to avoid this issue.\n",
+				phys_addr);
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
+				"Incorrect Physical Address returned by nVidia driver\n");
+			psmi_assert_always(0);
+			return PSM2_INTERNAL_ERR;
+		}
+	}
+	return PSM2_OK;
+}
+
+uint64_t __psm2_min_gpu_bar_size(void)
+{
+	return min_gpu_bar_size;
+}
+#endif
+
 static int rv_map_event_ring(psm2_rv_t rv, struct rv_event_ring* ring,
 				int entries, int offset)
 {
@@ -127,7 +250,7 @@ static void rv_unmap_event_ring(psm2_rv_t rv, struct rv_event_ring* ring)
 {
 	if (ring->hdr)
 		if(munmap(ring->hdr, ring->len))
-			printf("munmap event ring failed:%s (%d)\n", strerror(errno),errno);
+			_HFI_ERROR("rv munmap event ring failed:%s (%d)\n", strerror(errno),errno);
 	ring->hdr = NULL;
 	ring->len = 0;
 	ring->num = 0;
@@ -153,22 +276,47 @@ psm2_rv_t __psm2_rv_open(const char *devname, struct local_info *loc_info)
 	rv->fd = open(RV_FILE_NAME, O_RDWR);
 	if (rv->fd == -1) {
 		save_errno = errno;
-		printf("fd open failed %s: %s\n", RV_FILE_NAME, strerror(errno));
+		_HFI_ERROR("fd open failed %s: %s\n", RV_FILE_NAME, strerror(errno));
 		goto fail;
 	}
 
 	if ((ret = ioctl(rv->fd, RV_IOCTL_QUERY, &qparams)) != 0) {
 		save_errno = errno;
-		printf("query ioctl failed ret:%s (%d)\n", strerror(errno), ret);
+		_HFI_ERROR("rv query ioctl failed ret:%s (%d)\n", strerror(errno), ret);
 		goto fail;
 	}
 	loc_info->major_rev = qparams.major_rev;
 	loc_info->minor_rev = qparams.minor_rev;
 	loc_info->capability = qparams.capability;
 
+#ifdef PSM_CUDA
+	loc_info->gpu_major_rev = qparams.gpu_major_rev;
+	loc_info->gpu_minor_rev = qparams.gpu_minor_rev;
+	if (loc_info->rdma_mode & RV_RDMA_MODE_GPU) {
+		if (!(qparams.capability & RV_CAP_GPU_DIRECT)) {
+			// caller will warn and avoid GPUDirect use
+			loc_info->rdma_mode &= ~(RV_RDMA_MODE_GPU|RV_RDMA_MODE_UPSIZE_GPU);
+		}
+		if (!(qparams.capability & RV_CAP_EVICT)) {
+			save_errno = ENOTSUP;
+			_HFI_ERROR("Error: rv lacks EVICT ioctl, needed for GPU Support\n");
+			goto fail;
+		}
+	}
+#endif
+	if ((loc_info->rdma_mode & RV_RDMA_MODE_MASK) == RV_RDMA_MODE_USER
+		&& !(qparams.capability & RV_CAP_USER_MR)) {
+		save_errno = ENOTSUP;
+		_HFI_ERROR("Error: rv lacks enable_user_mr capability\n");
+		goto fail;
+	}
+
 	memset(&aparams, 0, sizeof(aparams));
 	snprintf(aparams.in.dev_name, RV_MAX_DEV_NAME_LEN, "%s", devname);
 	aparams.in.mr_cache_size = loc_info->mr_cache_size;
+#ifdef PSM_CUDA
+	aparams.in.gpu_cache_size = loc_info->gpu_cache_size;
+#endif
 	aparams.in.rdma_mode = loc_info->rdma_mode;
 	aparams.in.port_num = loc_info->port_num;
 	aparams.in.num_conn = loc_info->num_conn;
@@ -179,7 +327,7 @@ psm2_rv_t __psm2_rv_open(const char *devname, struct local_info *loc_info)
 
 	if (loc_info->job_key_len > sizeof(aparams.in.job_key)) {
 		save_errno = EINVAL;
-		printf("job_key_len too long\n");
+		_HFI_ERROR("Error: job_key_len too long\n");
 		goto fail;
 	}
 	aparams.in.job_key_len = loc_info->job_key_len;
@@ -198,20 +346,32 @@ psm2_rv_t __psm2_rv_open(const char *devname, struct local_info *loc_info)
 
 	if ((ret = ioctl(rv->fd, RV_IOCTL_ATTACH, &aparams)) != 0) {
 		save_errno = errno;
-		printf("attach ioctl failed ret:%s (%d)\n", strerror(errno), ret);
+		_HFI_ERROR("rv attach ioctl failed (mode 0x%x) ret:%s (%d)\n", loc_info->rdma_mode, strerror(errno), ret);
 		goto fail;
 	}
 
-	loc_info->rv_index = aparams.out.rv_index;
-	loc_info->mr_cache_size = aparams.out.mr_cache_size;
-	loc_info->q_depth = aparams.out.q_depth;
-	loc_info->reconnect_timeout = aparams.out.reconnect_timeout;
+#ifdef PSM_CUDA
+	if (loc_info->rdma_mode & RV_RDMA_MODE_GPU) {
+		loc_info->rv_index = aparams.out_gpu.rv_index;
+		loc_info->mr_cache_size = aparams.out_gpu.mr_cache_size;
+		loc_info->q_depth = aparams.out_gpu.q_depth;
+		loc_info->reconnect_timeout = aparams.out_gpu.reconnect_timeout;
+		loc_info->gpu_cache_size = aparams.out_gpu.gpu_cache_size;
+	} else {
+#endif
+		loc_info->rv_index = aparams.out.rv_index;
+		loc_info->mr_cache_size = aparams.out.mr_cache_size;
+		loc_info->q_depth = aparams.out.q_depth;
+		loc_info->reconnect_timeout = aparams.out.reconnect_timeout;
+#ifdef PSM_CUDA
+	}
+#endif
 
 	//printf("XXXX 0x%lx %s fd:%d\n", pthread_self(), __FUNCTION__, rv->fd);
 	if (loc_info->cq_entries) {
 		if (rv_map_event_ring(rv, &rv->events, loc_info->cq_entries, 0)) {
 			save_errno = errno;
-			printf("mmap event ring failed:%s (%d)\n", strerror(errno), errno);
+			_HFI_ERROR("rv mmap event ring failed:%s (%d)\n", strerror(errno), errno);
 			goto fail;
 		}
 	}
@@ -258,7 +418,7 @@ int __psm2_rv_get_cache_stats(psm2_rv_t rv, struct psm2_rv_cache_stats *stats)
 	memset(&sparams, 0, sizeof(sparams));
 	if ((ret = ioctl(rv->fd, RV_IOCTL_GET_CACHE_STATS, &sparams)) != 0) {
 		save_errno = errno;
-		printf("get_cache_stats failed ret:%d: %s\n", ret, strerror(errno));
+		_HFI_ERROR("rv get_cache_stats failed ret:%d: %s\n", ret, strerror(errno));
 		goto fail;
 	}
 	stats->cache_size = sparams.cache_size;
@@ -268,6 +428,8 @@ int __psm2_rv_get_cache_stats(psm2_rv_t rv, struct psm2_rv_cache_stats *stats)
 	stats->max_count = sparams.max_count;
 	stats->inuse = sparams.inuse;
 	stats->max_inuse = sparams.max_inuse;
+	stats->inuse_bytes = sparams.inuse_bytes;
+	stats->max_inuse_bytes = sparams.max_inuse_bytes;
 	stats->max_refcount = sparams.max_refcount;
 	stats->hit = sparams.hit;
 	stats->miss = sparams.miss;
@@ -281,6 +443,90 @@ fail:
 	return -1;
 }
 
+#ifdef PSM_CUDA
+int __psm2_rv_gpu_get_cache_stats(psm2_rv_t rv, struct psm2_rv_gpu_cache_stats *stats)
+{
+	struct rv_gpu_cache_stats_params_out sparams;
+	int ret;
+	int save_errno;
+
+	memset(&sparams, 0, sizeof(sparams));
+	if ((ret = ioctl(rv->fd, RV_IOCTL_GPU_GET_CACHE_STATS, &sparams)) != 0) {
+		save_errno = errno;
+		_HFI_ERROR("rv gpu_get_cache_stats failed ret:%d: %s\n", ret, strerror(errno));
+		goto fail;
+	}
+	stats->cache_size = sparams.cache_size;
+	stats->cache_size_reg = sparams.cache_size_reg;
+	stats->cache_size_mmap = sparams.cache_size_mmap;
+	stats->cache_size_both = sparams.cache_size_both;
+	stats->max_cache_size = sparams.max_cache_size;
+	stats->max_cache_size_reg = sparams.max_cache_size_reg;
+	stats->max_cache_size_mmap = sparams.max_cache_size_mmap;
+	stats->max_cache_size_both = sparams.max_cache_size_both;
+	stats->limit_cache_size = sparams.limit_cache_size;
+	stats->count = sparams.count;
+	stats->count_reg = sparams.count_reg;
+	stats->count_mmap = sparams.count_mmap;
+	stats->count_both = sparams.count_both;
+	stats->max_count = sparams.max_count;
+	stats->max_count_reg = sparams.max_count_reg;
+	stats->max_count_mmap = sparams.max_count_mmap;
+	stats->max_count_both = sparams.max_count_both;
+	stats->inuse = sparams.inuse;
+	stats->inuse_reg = sparams.inuse_reg;
+	stats->inuse_mmap = sparams.inuse_mmap;
+	stats->inuse_both = sparams.inuse_both;
+	stats->max_inuse = sparams.max_inuse;
+	stats->max_inuse_reg = sparams.max_inuse_reg;
+	stats->max_inuse_mmap = sparams.max_inuse_mmap;
+	stats->max_inuse_both = sparams.max_inuse_both;
+	stats->max_refcount = sparams.max_refcount;
+	stats->max_refcount_reg = sparams.max_refcount_reg;
+	stats->max_refcount_mmap = sparams.max_refcount_mmap;
+	stats->max_refcount_both = sparams.max_refcount_both;
+	stats->inuse_bytes = sparams.inuse_bytes;
+	stats->inuse_bytes_reg = sparams.inuse_bytes_reg;
+	stats->inuse_bytes_mmap = sparams.inuse_bytes_mmap;
+	stats->inuse_bytes_both = sparams.inuse_bytes_both;
+	stats->max_inuse_bytes = sparams.max_inuse_bytes;
+	stats->max_inuse_bytes_reg = sparams.max_inuse_bytes_reg;
+	stats->max_inuse_bytes_mmap = sparams.max_inuse_bytes_mmap;
+	stats->max_inuse_bytes_both = sparams.max_inuse_bytes_both;
+	stats->hit = sparams.hit;
+	stats->hit_reg = sparams.hit_reg;
+	stats->hit_add_reg = sparams.hit_add_reg;
+	stats->hit_mmap = sparams.hit_mmap;
+	stats->hit_add_mmap = sparams.hit_add_mmap;
+	stats->miss = sparams.miss;
+	stats->miss_reg = sparams.miss_reg;
+	stats->miss_mmap = sparams.miss_mmap;
+	stats->full = sparams.full;
+	stats->full_reg = sparams.full_reg;
+	stats->full_mmap = sparams.full_mmap;
+	stats->failed_pin = sparams.failed_pin;
+	stats->failed_reg = sparams.failed_reg;
+	stats->failed_mmap = sparams.failed_mmap;
+	stats->remove = sparams.remove;
+	stats->remove_reg = sparams.remove_reg;
+	stats->remove_mmap = sparams.remove_mmap;
+	stats->remove_both = sparams.remove_both;
+	stats->evict = sparams.evict;
+	stats->evict_reg = sparams.evict_reg;
+	stats->evict_mmap = sparams.evict_mmap;
+	stats->evict_both = sparams.evict_both;
+	stats->inval_mr = sparams.inval_mr;
+	stats->post_write = sparams.post_write;
+	stats->post_write_bytes = sparams.post_write_bytes;
+	stats->gpu_post_write = sparams.gpu_post_write;
+	stats->gpu_post_write_bytes = sparams.gpu_post_write_bytes;
+	return 0;
+fail:
+	errno = save_errno;
+	return -1;
+}
+#endif
+
 // we have a little dance here to hide the RV connect REQ and RSP from PSM
 // without needing a callback into PSM.
 // We do this by creating the rv_conn object with the remote addressing
@@ -449,7 +695,7 @@ int __psm2_rv_get_conn_count(psm2_rv_t rv, psm2_rv_conn_t conn,
 
 	if ((ret = ioctl(rv->fd, RV_IOCTL_CONN_GET_CONN_COUNT, &params)) != 0) {
 		save_errno = errno;
-		printf("get_conn_count failed ret:%d: %s\n", ret, strerror(errno));
+		_HFI_ERROR("rv get_conn_count failed ret:%d: %s\n", ret, strerror(errno));
 		goto fail;
 	}
 	*count = params.out.count;
@@ -472,7 +718,7 @@ int __psm2_rv_get_conn_stats(psm2_rv_t rv, psm2_rv_conn_t conn,
 	sparams.in.index = index;
 	if ((ret = ioctl(rv->fd, RV_IOCTL_CONN_GET_STATS, &sparams)) != 0) {
 		save_errno = errno;
-		printf("get_conn_stats failed ret:%d: %s\n", ret, strerror(errno));
+		_HFI_ERROR("rv get_conn_stats failed ret:%d: %s\n", ret, strerror(errno));
 		goto fail;
 	}
 	stats->index = sparams.out.index;
@@ -548,7 +794,7 @@ int __psm2_rv_get_event_stats(psm2_rv_t rv, struct psm2_rv_event_stats *stats)
 	memset(&sparams, 0, sizeof(sparams));
 	if ((ret = ioctl(rv->fd, RV_IOCTL_GET_EVENT_STATS, &sparams)) != 0) {
 		save_errno = errno;
-		printf("get_event_stats failed ret:%d: %s\n", ret, strerror(errno));
+		_HFI_ERROR("rv get_event_stats failed ret:%d: %s\n", ret, strerror(errno));
 		goto fail;
 	}
 	stats->send_write_cqe = sparams.send_write_cqe;
@@ -609,6 +855,20 @@ psm2_rv_mr_t __psm2_rv_reg_mem(psm2_rv_t rv, int cmd_fd_int, struct ibv_pd *pd,
 		goto fail;
 	}
 
+#ifdef PSM_CUDA
+#ifdef PSM_FI
+	if_pf((access & IBV_ACCESS_IS_GPU_ADDR) && PSMI_FAULTINJ_ENABLED()) {
+                PSMI_FAULTINJ_STATIC_DECL(fi_gpu_reg_mr, "gpu_reg_mr",
+                                          "fail GPU reg_mr",
+                                           1, IPS_FAULTINJ_GPU_REG_MR);
+                if_pf(PSMI_FAULTINJ_IS_FAULT(fi_gpu_reg_mr, "")) {
+                        errno = ENOMEM;
+                        return NULL;
+                }
+        }
+#endif
+#endif
+
 	mr = (psm2_rv_mr_t)my_calloc(1, sizeof(struct psm2_rv_mr));
 	if (! mr) {
 		save_errno = ENOMEM;
@@ -632,6 +892,14 @@ psm2_rv_mr_t __psm2_rv_reg_mem(psm2_rv_t rv, int cmd_fd_int, struct ibv_pd *pd,
 		save_errno = errno;
 		goto fail;
 	}
+#ifdef PSM_CUDA
+	if ((access & IBV_ACCESS_IS_GPU_ADDR)
+		&& PSM2_OK != psm2_check_phys_addr(mparams.out.iova)) {
+		(void)__psm2_rv_dereg_mem(rv, mr);
+		errno = EFAULT;
+		return NULL;
+	}
+#endif
 	mr->addr = (uint64_t)addr;
 	mr->length = length;
 	mr->access = access;
@@ -673,58 +941,217 @@ int __psm2_rv_dereg_mem(psm2_rv_t rv, psm2_rv_mr_t mr)
 
 #ifdef PSM_CUDA
 
-void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr, uint32_t pagelen)
+void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr,
+				uint64_t pagelen, int access)
 {
 	struct rv_gpu_mem_params params;
 	int ret;
-	int save_errno;
+
+#ifdef PSM_FI
+	if_pf(PSMI_FAULTINJ_ENABLED()) {
+                PSMI_FAULTINJ_STATIC_DECL(fi_gdrmmap, "gdrmmap",
+                                          "fail GPU gdrcopy mmap",
+                                           1, IPS_FAULTINJ_GDRMMAP);
+                if_pf(PSMI_FAULTINJ_IS_FAULT(fi_gdrmmap, "")) {
+                        errno = ENOMEM;
+                        return NULL;
+                }
+        }
+#endif
 
 	memset(&params, 0, sizeof(params));
-	/* XXX: Add the version field once it is restored */
 	params.in.gpu_buf_addr = pageaddr;
 	params.in.gpu_buf_size = pagelen;
+	params.in.access = access;
 
-	if ((ret = ioctl(rv->fd, RV_IOCTL_GPU_PIN_MMAP, &params)) != 0) {
-		save_errno = errno;
-		perror("gpu_pin_mmap failed\n");
-		errno = save_errno;
+	if ((ret = ioctl(rv->fd, RV_IOCTL_GPU_PIN_MMAP, &params)) != 0)
 		return NULL;
-	}
 
+	if (PSM2_OK != psm2_check_phys_addr(params.out.phys_addr)) {
+		(void)__psm2_rv_evict_exact(rv, (void*)pageaddr, pagelen, access);
+		errno = EFAULT;
+		return NULL;
+	}
 	// return mapped host address or NULL with errno set
 	return (void*)(uintptr_t)params.out.host_buf_addr;
 }
+#endif /* PSM_CUDA */
 
-int __psm2_rv_munmap_and_unpin(psm2_rv_t rv, const void *buf, uint32_t size)
+// addr, length, access are what was used in a previous call to
+// __psm_rv_reg_mem or __psm2_rv_pin_and_mmap
+// this will remove from the cache the matching entry if it's
+// refcount is 0.  In the case of reg_mem, a matching call
+// to dereg_mem is required for this to be able to evict the entry
+// return number of bytes evicted (> 0) on success or -1 with errno
+// Reports ENOENT if entry not found in cache (may already be evicted)
+int64_t __psm2_rv_evict_exact(psm2_rv_t rv, void *addr, uint64_t length, int access)
 {
-	struct rv_gpu_mem_params params;
+#ifdef RV_IOCTL_EVICT
+	struct rv_evict_params params;
 	int ret;
 	int save_errno;
 
 	memset(&params, 0, sizeof(params));
-	/* XXX: Add the version field once it is restored */
-	params.in.gpu_buf_addr = (uintptr_t)buf;
-	params.in.gpu_buf_size = size;
-
-	// buf is what was returned from a previous call to __psm2_rv_pin_and_mmap
-	// size is app buffer size, not rounded up to page size (could do that in caller if needed)
-	// this should reduce reference count but continue to cache the mmap & pin
-	// pages for future use in a later pin_and_mmap call (or perhaps even a
-	// later reg_mr?).  Note we can even keep the pages mmaped still as caller
-	// should not use the pointer again until after a future pin_and_mmap call
-	// return 0 on success or -1 with errno
-
-	if ((ret = ioctl(rv->fd, RV_IOCTL_GPU_MUNMAP_UNPIN, &params)) != 0) {
-		save_errno = errno;
-		perror("gpu_unpin_munmap failed\n");
-		errno = save_errno;
+	params.in.type = RV_EVICT_TYPE_SEARCH_EXACT;
+	params.in.search.addr = (uint64_t)addr;
+	params.in.search.length = length;
+	params.in.search.access = access;
+
+	if ((ret = ioctl(rv->fd, RV_IOCTL_EVICT, &params)) != 0) {
+		if (errno != ENOENT) {
+			save_errno = errno;
+			perror("rv_evict_exact failed\n");
+			errno = save_errno;
+		}
 		return ret;
 	}
 
-	return 0;
+	return params.out.bytes;
+#else
+	errno = EINVAL;
+	return -1;
+#endif
 }
 
-#endif /* PSM_CUDA */
+// this will remove from the cache all entries which include
+// addresses between addr and addr+length-1 inclusive if it's
+// refcount is 0.  In the case of reg_mem, a matching call
+// to dereg_mem is required for this to be able to evict the entry
+// return number of bytes evicted (> 0) on success or -1 with errno
+// Reports ENOENT if no matching entries found in cache (may already be evicted)
+int64_t __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length)
+{
+#ifdef RV_IOCTL_EVICT
+	struct rv_evict_params params;
+	int ret;
+	int save_errno;
+
+	memset(&params, 0, sizeof(params));
+	params.in.type = RV_EVICT_TYPE_SEARCH_RANGE;
+	params.in.search.addr = (uint64_t)addr;
+	params.in.search.length = length;
+
+	if ((ret = ioctl(rv->fd, RV_IOCTL_EVICT, &params)) != 0) {
+		if (errno != ENOENT) {
+			save_errno = errno;
+			perror("rv_evict_range failed\n");
+			errno = save_errno;
+		}
+		return ret;
+	}
+
+	return params.out.bytes;
+#else
+	errno = EINVAL;
+	return -1;
+#endif
+}
+
+#ifdef PSM_CUDA
+// this will remove from the GPU cache all entries which include
+// addresses between addr and addr+length-1 inclusive if it's
+// refcount is 0.  In the case of reg_mem, a matching call
+// to dereg_mem is required for this to be able to evict the entry
+// return number of bytes evicted (> 0) on success or -1 with errno
+// Reports ENOENT if no matching entries found in cache (may already be evicted)
+int64_t __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr, uint64_t length)
+{
+#ifdef RV_IOCTL_EVICT
+	struct rv_evict_params params;
+	int ret;
+	int save_errno;
+
+	memset(&params, 0, sizeof(params));
+	params.in.type = RV_EVICT_TYPE_GPU_SEARCH_RANGE;
+	params.in.search.addr = addr;
+	params.in.search.length = length;
+
+	if ((ret = ioctl(rv->fd, RV_IOCTL_EVICT, &params)) != 0) {
+		if (errno != ENOENT) {
+			save_errno = errno;
+			perror("rv_evict_gpu_range failed\n");
+			errno = save_errno;
+		}
+		return ret;
+	}
+
+	return params.out.bytes;
+#else
+	errno = EINVAL;
+	return -1;
+#endif
+}
+#endif // PSM_CUDA
+
+// this will remove from the cache up to the amount specified
+// Only entries with a refcount of 0 are removed.
+// In the case of reg_mem, a matching call
+// to dereg_mem is required for this to be able to evict the entry
+// return number of bytes evicted (> 0) on success or -1 with errno
+// Reports ENOENT if no entries could be evicted
+int64_t __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
+{
+#ifdef RV_IOCTL_EVICT
+	struct rv_evict_params params;
+	int ret;
+	int save_errno;
+
+	memset(&params, 0, sizeof(params));
+	params.in.type = RV_EVICT_TYPE_AMOUNT;
+	params.in.amount.bytes = bytes;
+	params.in.amount.count = count;
+
+	if ((ret = ioctl(rv->fd, RV_IOCTL_EVICT, &params)) != 0) {
+		if (errno != ENOENT) {
+			save_errno = errno;
+			perror("rv_evict_amount failed\n");
+			errno = save_errno;
+		}
+		return ret;
+	}
+
+	return params.out.bytes;
+#else
+	errno = EINVAL;
+	return -1;
+#endif
+}
+
+#ifdef PSM_CUDA
+// this will remove from the GPU cache up to the amount specified
+// Only entries with a refcount of 0 are removed.
+// In the case of reg_mem, a matching call
+// to dereg_mem is required for this to be able to evict the entry
+// return number of bytes evicted (> 0) on success or -1 with errno
+// Reports ENOENT if no entries could be evicted
+int64_t __psm2_rv_evict_gpu_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
+{
+#ifdef RV_IOCTL_EVICT
+	struct rv_evict_params params;
+	int ret;
+	int save_errno;
+
+	memset(&params, 0, sizeof(params));
+	params.in.type = RV_EVICT_TYPE_GPU_AMOUNT;
+	params.in.amount.bytes = bytes;
+	params.in.amount.count = count;
+
+	if ((ret = ioctl(rv->fd, RV_IOCTL_EVICT, &params)) != 0) {
+		if (errno != ENOENT) {
+			save_errno = errno;
+			perror("rv_evict_gpu_amount failed\n");
+			errno = save_errno;
+		}
+		return ret;
+	}
+
+	return params.out.bytes;
+#else
+	errno = EINVAL;
+	return -1;
+#endif
+}
+#endif // PSM_CUDA
 
 int __psm2_rv_post_rdma_write_immed(psm2_rv_t rv, psm2_rv_conn_t conn,
 				void *loc_buf, psm2_rv_mr_t loc_mr,
diff --git a/prov/psm3/psm3/psm_rndv_mod.h b/prov/psm3/psm3/psm_rndv_mod.h
index 75f9075..f5fa322 100644
--- a/prov/psm3/psm3/psm_rndv_mod.h
+++ b/prov/psm3/psm3/psm_rndv_mod.h
@@ -68,6 +68,9 @@
 
 struct local_info {
 	uint32_t mr_cache_size;	// in MBs
+#ifdef PSM_CUDA
+	uint32_t gpu_cache_size;	// in MBs
+#endif
 	uint8_t rdma_mode;	// RV_RDMA_MODE_*
 
 	// additional information for RV_RDMA_MODE_KERNEL
@@ -92,6 +95,10 @@ struct local_info {
 	// output from RNDV driver
 	uint16_t major_rev;		// driver ABI rev
 	uint16_t minor_rev;		// driver ABI rev
+#ifdef PSM_CUDA
+	uint16_t gpu_major_rev;		// driver GPU ABI rev
+	uint16_t gpu_minor_rev;		// driver GPU ABI rev
+#endif
 	uint64_t capability;
 	uint32_t rv_index;		// unique within job on given NIC
 };
@@ -135,6 +142,10 @@ typedef struct psm2_rv_mr *psm2_rv_mr_t;
 
 #define psm2_rv_cache_stats rv_cache_stats_params_out
 
+#ifdef PSM_CUDA
+#define psm2_rv_gpu_cache_stats rv_gpu_cache_stats_params_out
+#endif
+
 #define psm2_rv_conn_stats rv_conn_get_stats_params_out
 
 #define psm2_rv_event_stats rv_event_stats_params_out
@@ -149,6 +160,20 @@ static inline uint16_t psm2_rv_get_user_minor_bldtime_version(void)
 	return RV_ABI_VER_MINOR;
 }
 
+#ifdef NVIDIA_GPU_DIRECT
+static inline uint16_t psm2_rv_get_gpu_user_major_bldtime_version(void)
+{
+	return RV_GPU_ABI_VER_MAJOR;
+}
+
+static inline uint16_t psm2_rv_get_gpu_user_minor_bldtime_version(void)
+{
+	return RV_GPU_ABI_VER_MINOR;
+}
+
+extern uint64_t __psm2_min_gpu_bar_size(void);
+#endif
+
 extern psm2_rv_t __psm2_rv_open(const char *devname, struct local_info *loc_info);
 
 extern int __psm2_rv_close(psm2_rv_t rv);
@@ -156,6 +181,11 @@ extern int __psm2_rv_close(psm2_rv_t rv);
 extern int __psm2_rv_get_cache_stats(psm2_rv_t rv,
 									struct psm2_rv_cache_stats *stats);
 
+#ifdef PSM_CUDA
+extern int __psm2_rv_gpu_get_cache_stats(psm2_rv_t rv,
+									struct psm2_rv_gpu_cache_stats *stats);
+#endif
+
 extern psm2_rv_conn_t __psm2_rv_create_conn(psm2_rv_t rv,
 		struct ibv_ah_attr *ah_attr, // for remote node
 		uint32_t rem_addr);  // for simple compare to loc_addr
@@ -183,9 +213,23 @@ extern psm2_rv_mr_t __psm2_rv_reg_mem(psm2_rv_t rv, int cmd_fd, struct ibv_pd *p
 
 extern int __psm2_rv_dereg_mem(psm2_rv_t rv, psm2_rv_mr_t mr);
 
-extern void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr, uint32_t pagelen);
+extern void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr,
+			uint64_t pagelen, int access);
+
+extern int64_t __psm2_rv_evict_exact(psm2_rv_t rv, void *addr,
+			uint64_t length, int access);
 
-extern int __psm2_rv_munmap_and_unpin(psm2_rv_t rv, const void *buf, uint32_t size);
+extern int64_t __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length);
+
+extern int64_t __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count);
+
+#ifdef PSM_CUDA
+extern int64_t __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr,
+			uint64_t length);
+
+extern int64_t __psm2_rv_evict_gpu_amount(psm2_rv_t rv, uint64_t bytes,
+			uint32_t count);
+#endif
 
 extern int __psm2_rv_post_rdma_write_immed(psm2_rv_t rv, psm2_rv_conn_t conn,
 				void *loc_buf, psm2_rv_mr_t loc_mr,
diff --git a/prov/psm3/psm3/psm_stats.c b/prov/psm3/psm3/psm_stats.c
index 439046f..684b9f6 100644
--- a/prov/psm3/psm3/psm_stats.c
+++ b/prov/psm3/psm3/psm_stats.c
@@ -55,6 +55,7 @@
 
 #include "psm_user.h"
 #include "psm_mq_internal.h"
+#include <sys/syscall.h>
 
 struct psmi_stats_type {
 	STAILQ_ENTRY(psmi_stats_type) next;
@@ -65,6 +66,8 @@ struct psmi_stats_type {
 	uint32_t statstype;
 	uint64_t id;	// identifier to include in output, typically epid
 	void *context;
+	char *info;
+	pid_t tid;	// thread id, useful for multi-ep
 };
 
 static STAILQ_HEAD(, psmi_stats_type) psmi_stats =
@@ -106,6 +109,8 @@ psmi_stats_deregister_type_internal(uint32_t statstype,
 		if (type->statstype == statstype && type->context == context) {
 			STAILQ_REMOVE(&psmi_stats, type, psmi_stats_type, next);
 			psmi_free(type->entries);
+			if (type->info)
+				psmi_free(type->info);
 			psmi_free(type);
 			return PSM2_OK;
 		}
@@ -118,7 +123,7 @@ psmi_stats_register_type_internal(const char *heading,
 			 uint32_t statstype,
 			 const struct psmi_stats_entry *entries_i,
 			 int num_entries, uint64_t id, void *context,
-			 bool rereg)
+			 const char *info, bool rereg)
 {
 	struct psmi_stats_entry *entries;
 	struct psmi_stats_type *type;
@@ -142,6 +147,13 @@ psmi_stats_register_type_internal(const char *heading,
 	type->id = id;
 	type->context = context;
 	type->heading = heading;
+	if (info)
+		type->info = psmi_strdup(NULL, info);
+#ifdef SYS_gettid
+	type->tid = (long int)syscall(SYS_gettid); // gettid();
+#else
+	type->tid = 0;
+#endif
 
 	for (i = 0; i < num_entries; i++) {
 		type->entries[i].desc = entries_i[i].desc;
@@ -160,8 +172,11 @@ psmi_stats_register_type_internal(const char *heading,
 fail:
 	if (entries)
 		psmi_free(entries);
-	if (type)
+	if (type) {
+		if (type->info)
+			psmi_free(type->info);
 		psmi_free(type);
+	}
 	return err;
 }
 
@@ -169,20 +184,22 @@ psm2_error_t
 psmi_stats_register_type(const char *heading,
 			 uint32_t statstype,
 			 const struct psmi_stats_entry *entries_i,
-			 int num_entries, uint64_t id, void *context)
+			 int num_entries, uint64_t id, void *context,
+			 const char* info)
 {
 	return psmi_stats_register_type_internal(heading, statstype, entries_i,
-			 num_entries, id, context, 0);
+			 num_entries, id, context, info, 0);
 }
 
 psm2_error_t
 psmi_stats_reregister_type(const char *heading,
 			 uint32_t statstype,
 			 const struct psmi_stats_entry *entries_i,
-			 int num_entries, uint64_t id, void *context)
+			 int num_entries, uint64_t id, void *context,
+			 const char *info)
 {
 	return psmi_stats_register_type_internal(heading, statstype, entries_i,
-			 num_entries, id, context, 1);
+			 num_entries, id, context, info, 1);
 }
 
 void psmi_stats_show(uint32_t statsmask)
@@ -190,7 +207,6 @@ void psmi_stats_show(uint32_t statsmask)
 	struct psmi_stats_type *type;
 	time_t now;
 	char buf[100];
-	int first=1;
 
 	pthread_spin_lock(&psmi_stats_lock);
 	psmi_open_stats_fd();
@@ -199,9 +215,8 @@ void psmi_stats_show(uint32_t statsmask)
 
 	now = time(NULL);
 
-	if (print_stats_freq > 0)
-		fprintf(perf_stats_fd, "Time Delta %u seconds %s\n",
-			(unsigned)(now - stats_start), ctime_r(&now, buf));
+	fprintf(perf_stats_fd, "Time Delta %u seconds %s",
+		(unsigned)(now - stats_start), ctime_r(&now, buf));
 
 	STAILQ_FOREACH(type, &psmi_stats, next) {
 		int i;
@@ -209,15 +224,17 @@ void psmi_stats_show(uint32_t statsmask)
 
 		if (! (type->statstype & statsmask))
 			continue;
-		if (print_stats_freq <= 0 && first) {
-			fprintf(perf_stats_fd, "Time Delta %u seconds %s\n",
-				(unsigned)(now - stats_start), ctime_r(&now, buf));
-			first = 0;
-		}
+		// when id == 0, we expect 1 report of given type per
+		// process, so we also omit tid.  In which case info probably
+		// NULL but show it if provided when stats_register called.
 		if (type->id)
-			fprintf(perf_stats_fd, " %s id 0x%"PRIx64"\n", type->heading, type->id);
+			fprintf(perf_stats_fd, " %s id 0x%"PRIx64"%s%s tid %d\n",
+				type->heading, type->id, type->info?" ":"",
+				type->info?type->info:"", type->tid);
 		else
-			fprintf(perf_stats_fd, " %s\n", type->heading);
+			fprintf(perf_stats_fd, " %s%s%s\n",
+				type->heading, type->info?" ":"",
+				type->info?type->info:"");
 		for (i=0, entry=&type->entries[0]; i<type->num_entries; i++, entry++) {
 			uint64_t value;
 			value = (entry->getfn != NULL)? entry->getfn(type->context)
@@ -229,6 +246,7 @@ void psmi_stats_show(uint32_t statsmask)
 			entry->old_value = value;
 		}
 	}
+	fprintf(perf_stats_fd, "\n");
 	fflush(perf_stats_fd);
 unlock:
 	pthread_spin_unlock(&psmi_stats_lock);
@@ -254,6 +272,8 @@ psm2_error_t psmi_stats_deregister_all(void)
 	while ((type = STAILQ_FIRST(&psmi_stats)) != NULL) {
 		STAILQ_REMOVE_HEAD(&psmi_stats, next);
 		psmi_free(type->entries);
+		if (type->info)
+			psmi_free(type->info);
 		psmi_free(type);
 	}
 	pthread_spin_unlock(&psmi_stats_lock);
@@ -764,8 +784,11 @@ void stats_register_mem_stats(psm2_ep_t ep)
 		_SDECL("Other_(max)", m_undefined_max),
 	};
 
+	// TBD - these are global, should only call once and not provide
+	// ep nor device name
 	psmi_stats_register_type("PSM_memory_allocation_statistics",
 				 PSMI_STATSTYPE_MEMORY,
-				 entries, PSMI_STATS_HOWMANY(entries), ep);
+				 entries, PSMI_STATS_HOWMANY(entries), ep,
+				 ep->dev_name);
 }
 #endif // 0   // unused code, specific to QLogic MPI
diff --git a/prov/psm3/psm3/psm_stats.h b/prov/psm3/psm3/psm_stats.h
index 0060c26..3143af4 100644
--- a/prov/psm3/psm3/psm_stats.h
+++ b/prov/psm3/psm3/psm_stats.h
@@ -63,6 +63,9 @@
 #include "mpspawn_stats.h"
 
 #define PSMI_STATSTYPE_MQ	    	0x00001
+#ifdef PSM_CUDA
+#define PSMI_STATSTYPE_CUDA	    	0x00002 /* count of cuda calls */
+#endif
 #define PSMI_STATSTYPE_RCVTHREAD    0x00100	/* num_wakups, ratio, etc. */
 #define PSMI_STATSTYPE_IPSPROTO	    0x00200	/* acks,naks,err_chks */
 #define PSMI_STATSTYPE_TIDS	    	0x00400
@@ -138,14 +141,16 @@ psm2_error_t
 psmi_stats_register_type(const char *heading,
 			 uint32_t statstype,
 			 const struct psmi_stats_entry *entries,
-			 int num_entries, uint64_t id, void *context);
+			 int num_entries, uint64_t id, void *context,
+			 const char *info);
 
 /* deregister old copy and register a new one in it's place */
 psm2_error_t
 psmi_stats_reregister_type(const char *heading,
 			 uint32_t statstype,
 			 const struct psmi_stats_entry *entries,
-			 int num_entries, uint64_t id, void *context);
+			 int num_entries, uint64_t id, void *context,
+			 const char *info);
 
 psm2_error_t psmi_stats_deregister_type(uint32_t statstype, void *context);
 
diff --git a/prov/psm3/psm3/psm_sysbuf.c b/prov/psm3/psm3/psm_sysbuf.c
index 48fc06e..234ba8f 100644
--- a/prov/psm3/psm3/psm_sysbuf.c
+++ b/prov/psm3/psm3/psm_sysbuf.c
@@ -109,6 +109,9 @@ void psmi_mq_sysbuf_init(psm2_mq_t mq)
         ptr = psmi_mq_sysbuf_alloc(mq, block_sizes[i]);
         psmi_mq_sysbuf_free(mq, ptr);
     }
+    // undo counters from psmi_mq_sysbuf_alloc during init
+    mq->stats.rx_sysbuf_num = 0;
+    mq->stats.rx_sysbuf_bytes  = 0;
 }
 
 void psmi_mq_sysbuf_fini(psm2_mq_t mq)  // free all buffers that is currently not used
diff --git a/prov/psm3/psm3/psm_user.h b/prov/psm3/psm3/psm_user.h
index a0d9e5a..d376515 100644
--- a/prov/psm3/psm3/psm_user.h
+++ b/prov/psm3/psm3/psm_user.h
@@ -314,12 +314,14 @@ void psmi_profile_reblock(int did_no_progress) __attribute__ ((weak));
 
 extern int is_cuda_enabled;
 extern int is_gdr_copy_enabled;
-extern int device_support_gpudirect;
-extern int gpu_p2p_supported;
+extern int is_gpudirect_enabled; // only for use during parsing of other params
+extern int _device_support_unified_addr;
+extern int _device_support_gpudirect;
+extern int _gpu_p2p_supported;
 extern int my_gpu_device;
 extern int cuda_lib_version;
 
-extern CUcontext ctxt;
+extern CUcontext cu_ctxt;
 extern void *psmi_cuda_lib;
 
 extern CUresult (*psmi_cuInit)(unsigned int  Flags );
@@ -335,6 +337,7 @@ extern CUresult (*psmi_cuDriverGetVersion)(int* driverVersion);
 extern CUresult (*psmi_cuDeviceGetCount)(int* count);
 extern CUresult (*psmi_cuStreamCreate)(CUstream* phStream, unsigned int Flags);
 extern CUresult (*psmi_cuStreamDestroy)(CUstream phStream);
+extern CUresult (*psmi_cuStreamSynchronize)(CUstream phStream);
 extern CUresult (*psmi_cuEventCreate)(CUevent* phEvent, unsigned int Flags);
 extern CUresult (*psmi_cuEventDestroy)(CUevent hEvent);
 extern CUresult (*psmi_cuEventQuery)(CUevent hEvent);
@@ -357,14 +360,73 @@ extern CUresult (*psmi_cuDevicePrimaryCtxRetain)(CUcontext* pctx, CUdevice dev);
 extern CUresult (*psmi_cuCtxGetDevice)(CUdevice* device);
 extern CUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);
 
+extern uint64_t psmi_count_cuInit;
+extern uint64_t psmi_count_cuCtxDetach;
+extern uint64_t psmi_count_cuCtxGetCurrent;
+extern uint64_t psmi_count_cuCtxSetCurrent;
+extern uint64_t psmi_count_cuPointerGetAttribute;
+extern uint64_t psmi_count_cuPointerSetAttribute;
+extern uint64_t psmi_count_cuDeviceCanAccessPeer;
+extern uint64_t psmi_count_cuDeviceGet;
+extern uint64_t psmi_count_cuDeviceGetAttribute;
+extern uint64_t psmi_count_cuDriverGetVersion;
+extern uint64_t psmi_count_cuDeviceGetCount;
+extern uint64_t psmi_count_cuStreamCreate;
+extern uint64_t psmi_count_cuStreamDestroy;
+extern uint64_t psmi_count_cuStreamSynchronize;
+extern uint64_t psmi_count_cuEventCreate;
+extern uint64_t psmi_count_cuEventDestroy;
+extern uint64_t psmi_count_cuEventQuery;
+extern uint64_t psmi_count_cuEventRecord;
+extern uint64_t psmi_count_cuEventSynchronize;
+extern uint64_t psmi_count_cuMemHostAlloc;
+extern uint64_t psmi_count_cuMemFreeHost;
+extern uint64_t psmi_count_cuMemcpy;
+extern uint64_t psmi_count_cuMemcpyDtoD;
+extern uint64_t psmi_count_cuMemcpyDtoH;
+extern uint64_t psmi_count_cuMemcpyHtoD;
+extern uint64_t psmi_count_cuMemcpyDtoHAsync;
+extern uint64_t psmi_count_cuMemcpyHtoDAsync;
+extern uint64_t psmi_count_cuIpcGetMemHandle;
+extern uint64_t psmi_count_cuIpcOpenMemHandle;
+extern uint64_t psmi_count_cuIpcCloseMemHandle;
+extern uint64_t psmi_count_cuMemGetAddressRange;
+extern uint64_t psmi_count_cuDevicePrimaryCtxGetState;
+extern uint64_t psmi_count_cuDevicePrimaryCtxRetain;
+extern uint64_t psmi_count_cuCtxGetDevice;
+extern uint64_t psmi_count_cuDevicePrimaryCtxRelease;
+
+static int check_set_cuda_ctxt(void)
+{
+	CUresult err;
+	CUcontext tmpctxt = {0};
+
+	if (unlikely(!psmi_cuCtxGetCurrent || !psmi_cuCtxSetCurrent))
+		return 0;
+
+	err = psmi_cuCtxGetCurrent(&tmpctxt);
+	if (likely(!err)) {
+		if (unlikely(!tmpctxt && cu_ctxt)) {
+			err = psmi_cuCtxSetCurrent(cu_ctxt);
+			return !!err;
+		} else if (unlikely(tmpctxt && !cu_ctxt)) {
+			cu_ctxt = tmpctxt;
+		}
+	}
+	return 0;
+}
+
+
 #define PSMI_CUDA_CALL(func, args...) do {				\
 		CUresult cudaerr;					\
+		if (unlikely(check_set_cuda_ctxt())) {			\
+			psmi_handle_error(PSMI_EP_NORETURN,		\
+			PSM2_INTERNAL_ERR, "Failed to set/synchronize"	\
+			" CUDA context.\n");				\
+		}							\
+		psmi_count_##func++;					\
 		cudaerr = psmi_##func(args);				\
 		if (cudaerr != CUDA_SUCCESS) {				\
-			if (ctxt == NULL)				\
-				_HFI_ERROR(				\
-				"Check if CUDA is initialized"	\
-				"before psm2_ep_open call \n");		\
 			_HFI_ERROR(					\
 				"CUDA failure: %s() (at %s:%d)"		\
 				"returned %d\n",			\
@@ -375,6 +437,117 @@ extern CUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);
 		}							\
 	} while (0)
 
+PSMI_ALWAYS_INLINE(
+void verify_device_support_unified_addr())
+{
+	if (likely(_device_support_unified_addr > -1)) return;
+
+	int num_devices, dev;
+
+	/* Check if all devices support Unified Virtual Addressing. */
+	PSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);
+
+	_device_support_unified_addr = 1;
+
+	for (dev = 0; dev < num_devices; dev++) {
+		CUdevice device;
+		PSMI_CUDA_CALL(cuDeviceGet, &device, dev);
+		int unifiedAddressing;
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&unifiedAddressing,
+				CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING,
+				device);
+
+		if (unifiedAddressing !=1) {
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_EP_DEVICE_FAILURE,
+				"CUDA device %d does not support Unified Virtual Addressing.\n",
+				dev);
+		}
+	}
+
+	return;
+}
+
+PSMI_ALWAYS_INLINE(
+int device_support_gpudirect())
+{
+	if (likely(_device_support_gpudirect > -1)) return _device_support_gpudirect;
+
+	int num_devices, dev;
+
+	/* Check if all devices support Unified Virtual Addressing. */
+	PSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);
+
+	_device_support_gpudirect = 1;
+
+	for (dev = 0; dev < num_devices; dev++) {
+		CUdevice device;
+		PSMI_CUDA_CALL(cuDeviceGet, &device, dev);
+
+		int major;
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&major,
+				CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR,
+				device);
+		if (major < 3) {
+			_device_support_gpudirect = 0;
+			_HFI_INFO("CUDA device %d does not support GPUDirect RDMA (Non-fatal error)\n", dev);
+		}
+	}
+
+	return _device_support_gpudirect;
+}
+
+PSMI_ALWAYS_INLINE(
+int gpu_p2p_supported())
+{
+	if (likely(_gpu_p2p_supported > -1)) return _gpu_p2p_supported;
+
+	if (unlikely(!is_cuda_enabled)) {
+		_gpu_p2p_supported=0;
+		return 0;
+	}
+
+	int num_devices, dev;
+	CUcontext c;
+
+	/* Check which devices the current device has p2p access to. */
+	CUdevice current_device;
+	PSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);
+	_gpu_p2p_supported = 0;
+
+	if (num_devices > 1) {
+		PSMI_CUDA_CALL(cuCtxGetCurrent, &c);
+		if (c == NULL) {
+			_HFI_INFO("Unable to find active CUDA context, assuming P2P not supported\n");
+			return 0;
+		}
+		PSMI_CUDA_CALL(cuCtxGetDevice, &current_device);
+	}
+
+	for (dev = 0; dev < num_devices; dev++) {
+		CUdevice device;
+		PSMI_CUDA_CALL(cuDeviceGet, &device, dev);
+
+		if (num_devices > 1 && device != current_device) {
+			int canAccessPeer = 0;
+			PSMI_CUDA_CALL(cuDeviceCanAccessPeer, &canAccessPeer,
+					current_device, device);
+
+			if (canAccessPeer != 1)
+				_HFI_DBG("CUDA device %d does not support P2P from current device (Non-fatal error)\n", dev);
+			else
+				_gpu_p2p_supported |= (1 << device);
+		} else {
+			/* Always support p2p on the same GPU */
+			my_gpu_device = device;
+			_gpu_p2p_supported |= (1 << device);
+		}
+	}
+
+	return _gpu_p2p_supported;
+}
+
 /**
  * Similar to PSMI_CUDA_CALL() except does not error out
  * if func(args) returns CUDA_SUCCESS or except_err
@@ -386,10 +559,16 @@ extern CUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);
  * As except_err is an allowed value, message is printed at
  * DBG level.
  */
-#define PSMI_CUDA_CALL_EXCEPT(except_err, func, args...) do { \
+#define PSMI_CUDA_CALL_EXCEPT(except_err, func, args...) do {		\
+		if (unlikely(check_set_cuda_ctxt())) {			\
+			psmi_handle_error(PSMI_EP_NORETURN,		\
+				PSM2_INTERNAL_ERR, "Failed to "		\
+				"set/synchronize CUDA context.\n");	\
+		}							\
+		psmi_count_##func++;					\
 		cudaerr = psmi_##func(args);				\
 		if (cudaerr != CUDA_SUCCESS && cudaerr != except_err) {	\
-			if (ctxt == NULL)				\
+			if (cu_ctxt == NULL)				\
 				_HFI_ERROR(				\
 				"Check if CUDA is initialized"	\
 				"before psm2_ep_open call \n");		\
@@ -409,6 +588,7 @@ extern CUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);
 	} while (0)
 
 #define PSMI_CUDA_CHECK_EVENT(event, cudaerr) do {			\
+		psmi_count_cuEventQuery++;				\
 		cudaerr = psmi_cuEventQuery(event);			\
 		if ((cudaerr != CUDA_SUCCESS) &&			\
 		    (cudaerr != CUDA_ERROR_NOT_READY)) {		\
@@ -438,9 +618,11 @@ _psmi_is_cuda_mem(const void *ptr))
 	CUresult cres;
 	CUmemorytype mt;
 	unsigned uvm = 0;
+	psmi_count_cuPointerGetAttribute++;
 	cres = psmi_cuPointerGetAttribute(
 		&mt, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, (CUdeviceptr) ptr);
 	if ((cres == CUDA_SUCCESS) && (mt == CU_MEMORYTYPE_DEVICE)) {
+		psmi_count_cuPointerGetAttribute++;
 		cres = psmi_cuPointerGetAttribute(
 			&uvm, CU_POINTER_ATTRIBUTE_IS_MANAGED, (CUdeviceptr) ptr);
 		if ((cres == CUDA_SUCCESS) && (uvm == 0))
@@ -464,6 +646,7 @@ _psmi_is_gdr_copy_enabled())
 #define PSMI_IS_GDR_COPY_ENABLED _psmi_is_gdr_copy_enabled()
 
 #define PSMI_IS_CUDA_MEM(p) _psmi_is_cuda_mem(p)
+extern void psm2_get_gpu_bars(void);
 
 struct ips_cuda_hostbuf {
 	STAILQ_ENTRY(ips_cuda_hostbuf) req_next;
@@ -495,23 +678,26 @@ void psmi_cuda_hostbuf_alloc_func(int is_alloc, void *context, void *obj);
 	    .mode[PSMI_MEMMODE_LARGE]   = {  32, 512 }		\
 	}
 
-extern uint32_t gpudirect_send_threshold;
-extern uint32_t gpudirect_recv_threshold;
+extern uint32_t gpudirect_send_limit;
+extern uint32_t gpudirect_recv_limit;
 extern uint32_t cuda_thresh_rndv;
-/* This threshold dictates when the sender turns off
- * GDR Copy. The threshold needs to be less than
+/* This limit dictates when the sender turns off
+ * GDR Copy and uses SDMA. The limit needs to be less than equal
  * CUDA RNDV threshold.
+ * set to 0 if GDR Copy disabled
  */
-extern uint32_t gdr_copy_threshold_send;
-/* This threshold dictates when the reciever turns off
- * GDR Copy. The threshold needs to be less than
+extern uint32_t gdr_copy_limit_send;
+/* This limit dictates when the reciever turns off
+ * GDR Copy. The limit needs to be less than equal
  * CUDA RNDV threshold.
+ * set to 0 if GDR Copy disabled
  */
-extern uint32_t gdr_copy_threshold_recv;
+extern uint32_t gdr_copy_limit_recv;
+
+uint64_t gpu_cache_evict;
 
-#define PSMI_USE_GDR_COPY(req, len) req->is_buf_gpu_mem &&       \
-				    PSMI_IS_GDR_COPY_ENABLED  && \
-				    len >=1 && len <= gdr_copy_threshold_recv
+// Only valid if called for a GPU buffer
+#define PSMI_USE_GDR_COPY_RECV(len) ((len) >=1 && (len) <= gdr_copy_limit_recv)
 
 enum psm2_chb_match_type {
 	/* Complete data found in a single chb */
diff --git a/prov/psm3/psm3/psm_utils.c b/prov/psm3/psm3/psm_utils.c
index bd2965b..20f640a 100644
--- a/prov/psm3/psm3/psm_utils.c
+++ b/prov/psm3/psm3/psm_utils.c
@@ -59,7 +59,6 @@
 #include "psm_mq_internal.h"
 #include "ips_proto_params.h"
 #include <netinet/in.h>  // for sockaddr
-#include <infiniband/ib.h>  // for AF_IB structures
 #include <fnmatch.h>
 
 
@@ -449,15 +448,6 @@ const char *psmi_sockaddr_ntop(struct sockaddr* addr, char *dst, socklen_t size)
 		snprintf(dst+strlen(dst), size-strlen(dst), " %u", be16toh(in_addr->sin6_port));
 		return dst;
 	}
-	case AF_IB:
-	{
-		struct sockaddr_ib* ib_addr = ((struct sockaddr_ib*)addr);
-		// we show the GID sid and pkey.
-		// Could also output sid_mask and sib_scope_id
-		inet_ntop(AF_INET6, &ib_addr->sib_addr, dst, size);
-		snprintf(dst+strlen(dst), size-strlen(dst), " 0x%016"PRIx64" 0x%04"PRIx16, be64toh(ib_addr->sib_sid), be16toh(ib_addr->sib_pkey));
-		return dst;
-	}
 	default:
 		snprintf(dst, size, "Unsupported");
 		return dst;
@@ -497,8 +487,6 @@ socklen_t psmi_sockaddr_len(struct sockaddr* addr)
 		return (sizeof(struct sockaddr_in));
 	case AF_INET6:
 		return (sizeof(struct sockaddr_in6));
-	case AF_IB:
-		return (sizeof(struct sockaddr_ib));
 	default:
 		// unknown
 		return 0;	// be conservative
@@ -584,7 +572,7 @@ uintptr_t psmi_getpagesize(void)
  * if nothing provided or doesn't match current process, def is returned
  * if syntax error, def_syntax is returned
  */
-static int psmi_parse_val_pattern(const char *env, int def, int def_syntax)
+int psmi_parse_val_pattern(const char *env, int def, int def_syntax)
 {
 	int ret = def;
 
@@ -926,6 +914,104 @@ int psmi_parse_memmode(void)
 	}
 }
 
+#ifdef PSM_CUDA
+// we need GPUDIRECT config early to influence rdmamode defaults,
+// MR Cache mode and whether we need to open RV.
+// These functions are later used to confirm and finalize config for
+// ips_proto_init
+
+// value returned is 0/1 (disable/enable)
+unsigned psmi_parse_gpudirect(void)
+{
+	union psmi_envvar_val envval;
+	static int have_value = 0;
+	static unsigned saved;
+
+	// only parse once so doesn't appear in PSM3_VERBOSE_ENV multiple times
+	if (have_value)
+		return saved;
+
+	psmi_getenv("PSM3_GPUDIRECT",
+		"Use GPUDirect DMA and RDMA support to allow the NIC to directly read"
+		" from the GPU for send DMA and write to the GPU for recv RDMA."
+		" Also enable GPUDirect copy for more efficient CPU to/from GPU copies."
+		" Requires rv module support.(default is disabled i.e. 0)",
+		PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
+		(union psmi_envvar_val)0, /* Disabled by default */
+		&envval);
+
+	saved = envval.e_uint;
+	have_value = 1;
+	return saved;
+}
+
+// value returned is limit >= 0, (0 disables GPUDIRECT Send RDMA)
+unsigned psmi_parse_gpudirect_send_limit(void)
+{
+	union psmi_envvar_val envval;
+	static int have_value = 0;
+	static unsigned saved;
+
+	// only parse once so doesn't appear in PSM3_VERBOSE_ENV multiple times
+	if (have_value)
+		return saved;
+
+	/* Default Send threshold for Gpu-direct set to 30000 */
+	psmi_getenv("PSM3_GPUDIRECT_SEND_LIMIT",
+		    "GPUDirect feature on send side will be switched off for messages larger than limit.",
+		    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
+		    (union psmi_envvar_val)30000, &envval);
+
+	saved = envval.e_uint;
+	have_value = 1;
+	return saved;
+}
+
+// value returned is limit >= 0, (0 disables GPUDIRECT Recv RDMA)
+unsigned psmi_parse_gpudirect_recv_limit(void)
+{
+	union psmi_envvar_val envval;
+	static int have_value = 0;
+	static unsigned saved;
+
+	// only parse once so doesn't appear in PSM3_VERBOSE_ENV multiple times
+	if (have_value)
+		return saved;
+
+	/* Default Send threshold for Gpu-direct set to 30000 */
+	psmi_getenv("PSM3_GPUDIRECT_RECV_LIMIT",
+		    "GPUDirect feature on receive side will be switched off for messages larger than limit.",
+		    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
+		    (union psmi_envvar_val)UINT_MAX, &envval);
+
+	saved = envval.e_uint;
+	have_value = 1;
+	return saved;
+}
+
+#endif	// PSM_CUDA
+
+/* Send DMA Enable */
+unsigned psmi_parse_senddma(void)
+{
+	union psmi_envvar_val envval;
+	static int have_value = 0;
+	static unsigned saved;
+
+	// only parse once so doesn't appear in PSM3_VERBOSE_ENV multiple times
+	if (have_value)
+		return saved;
+
+	psmi_getenv("PSM3_SDMA",
+		"UD send dma flags (0 disables send dma, 1 enables), default 0",
+		PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
+		(union psmi_envvar_val)0, &envval);
+	saved = envval.e_uint;
+	have_value = 1;
+	return saved;
+}
+
+
 /* RDMA mode */
 // we need this early when setting defaults for RV thresholds in psmi_mq_malloc
 // and also want this available when creating the verbs_ep since it may affect
@@ -935,12 +1021,30 @@ int psmi_parse_memmode(void)
 unsigned psmi_parse_rdmamode(void)
 {
 	union psmi_envvar_val env_rdma;
-	static unsigned saved_rdmamode = 0xffffffff;
+	static int have_value = 0;
+	static unsigned saved_rdmamode;
+	unsigned default_rdma;
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+	int gpudirect = 0;
+#endif
+#endif
 
 	// only parse once so doesn't appear in PSM3_VERBOSE_ENV multiple times
-	if (saved_rdmamode != 0xffffffff)
+	if (have_value)
 		return saved_rdmamode;
 
+	default_rdma = IPS_PROTOEXP_FLAGS_DEFAULT;
+
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+	gpudirect = PSMI_IS_CUDA_ENABLED && psmi_parse_gpudirect();
+	// GPUDIRECT causes default of RDMA=1
+	if (gpudirect)
+		default_rdma = (default_rdma & ~IPS_PROTOEXP_FLAG_RDMA_MASK)
+				| IPS_PROTOEXP_FLAG_RDMA_KERNEL;
+#endif
+#endif
 	psmi_getenv("PSM3_RDMA",
 		    "RDMA proto control (0-no RDMA,"
 #ifdef RNDV_MOD
@@ -951,13 +1055,30 @@ unsigned psmi_parse_rdmamode(void)
 			// IPS_PROTOEXP_FLAG_TID_DEBUG (0x4)      N/A
 			,
 		    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
-		    (union psmi_envvar_val)IPS_PROTOEXP_FLAGS_DEFAULT,
+		    (union psmi_envvar_val)default_rdma,
 		    &env_rdma);
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+#if 1 // remove this code when RV is ready for RDMA=2, 3 w/GPU Direct
+	if (gpudirect && IPS_PROTOEXP_FLAG_USER_RC_QP(env_rdma.e_uint)) {
+		_HFI_INFO("WARNING: GPUDIRECT only allowed with PSM3_RDMA=0 or 1, using %u\n", default_rdma);
+		env_rdma.e_uint = default_rdma;
+	}
+#endif
+#endif
+#endif
 #ifndef RNDV_MOD
-	if (IPS_PROTOEXP_FLAG_KERNEL_QP(env_rdma.e_uint))
+	if (IPS_PROTOEXP_FLAG_KERNEL_QP(env_rdma.e_uint)) {
+		static int logged = 0;
+		if (! logged) {
+			_HFI_INFO("WARNING: PSM built without rv module enabled, RDMA mode %d unavailable\n", IPS_PROTOEXP_FLAG_RDMA_KERNEL);
+			logged = 1;
+		}
 		env_rdma.e_uint = 0;
+	}
 #endif
 	saved_rdmamode = env_rdma.e_uint;
+	have_value = 1;
 	return saved_rdmamode;
 }
 
@@ -966,10 +1087,11 @@ unsigned psmi_parse_rdmamode(void)
 int psmi_parse_identify(void)
 {
 	union psmi_envvar_val myenv;
-	static int saved_identify = -1;
+	static int have_value;
+	static int saved_identify;
 
 	// only parse once so doesn't appear in PSM3_VERBOSE_ENV multiple times
-	if (saved_identify >= 0)
+	if (have_value)
 		return saved_identify;
 
 	psmi_getenv("PSM3_IDENTIFY", "Identify PSM version being run "
@@ -983,6 +1105,7 @@ int psmi_parse_identify(void)
 		    	PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_STR,
 		    	(union psmi_envvar_val)"0", &myenv);
 	saved_identify = psmi_parse_val_pattern(myenv.e_str, 0, 0);
+	have_value = 1;
 
 	return saved_identify;
 }
@@ -1086,7 +1209,7 @@ psmi_syslog(psm2_ep_t ep, int to_console, int level, const char *format, ...)
 		ep->did_syslog = 1;
 
 		memset(&uuid_str, 0, sizeof(uuid_str));
-		psmi_uuid_unparse(ep->uuid, uuid_str);
+		uuid_unparse(ep->uuid, uuid_str);
 		hfi_syslog("PSM", 0, LOG_WARNING,
 			   "uuid_key=%s,unit=%d"
 			   ,
@@ -1164,19 +1287,6 @@ void psmi_multi_ep_init()
 
 #ifdef PSM_FI
 
-struct psmi_faultinj_spec {
-	STAILQ_ENTRY(psmi_faultinj_spec) next;
-	char spec_name[PSMI_FAULTINJ_SPEC_NAMELEN];
-
-	uint64_t num_faults;
-	uint64_t num_calls;
-
-	struct drand48_data drand48_data;
-	int num;
-	int denom;
-	long int initial_seed;
-};
-
 int psmi_faultinj_enabled = 0;
 int psmi_faultinj_verbose = 0;
 char *psmi_faultinj_outfile = NULL;
@@ -1247,7 +1357,7 @@ static void psmi_faultinj_reregister_stats()
 	}
 
 	psmi_stats_reregister_type("Fault_Injection", PSMI_STATSTYPE_FAULTINJ,
-		entries, num_entries, 0, &psmi_faultinj_head);
+		entries, num_entries, 0, &psmi_faultinj_head, NULL);
 	psmi_free(entries);
 }
 
@@ -1393,10 +1503,6 @@ int psmi_faultinj_is_fault(struct psmi_faultinj_spec *fi)
 	lrand48_r(&fi->drand48_data, &rnum);
 	if (((int) (rnum % INT_MAX)) % fi->denom <= fi->num) {
 		fi->num_faults++;
-		if (psmi_faultinj_verbose) {
-			printf("%s: injecting fault: %s\n", hfi_get_mylabel(), fi->spec_name);
-			fflush(stdout);
-		}
 		return 1;
 	} else
 		return 0;
@@ -1473,7 +1579,7 @@ void psmi_mem_stats_register(void)
 		psmi_stats_register_type("PSM_memory_allocation_statistics",
                     PSMI_STATSTYPE_MEMORY,
                     entries,
-                    PSMI_STATS_HOWMANY(entries), 0, &psmi_stats_memory);
+                    PSMI_STATS_HOWMANY(entries), 0, &psmi_stats_memory, NULL);
 	}
 }
 
diff --git a/prov/psm3/psm3/psm_utils.h b/prov/psm3/psm3/psm_utils.h
index 4428865..a27bb81 100644
--- a/prov/psm3/psm3/psm_utils.h
+++ b/prov/psm3/psm3/psm_utils.h
@@ -269,7 +269,13 @@ psm2_error_t psmi_parse_mpool_env(const psm2_mq_t mq, int level,
 				 uint32_t *valo, uint32_t *chunkszo);
 int psmi_parse_memmode(void);
 int psmi_parse_identify(void);
+unsigned psmi_parse_senddma(void);
 unsigned psmi_parse_rdmamode(void);
+#ifdef PSM_CUDA
+unsigned psmi_parse_gpudirect(void);
+unsigned psmi_parse_gpudirect_send_limit(void);
+unsigned psmi_parse_gpudirect_recv_limit(void);
+#endif
 
 /*
  * Parsing environment variables
@@ -307,6 +313,7 @@ MOCKABLE(psmi_getenv)(const char *name, const char *descr, int level,
 		int type, union psmi_envvar_val defval,
 		union psmi_envvar_val *newval);
 MOCK_DCL_EPILOGUE(psmi_getenv);
+int psmi_parse_val_pattern(const char *env, int def, int def_syntax);
 /*
  * Misc functionality
  */
@@ -315,8 +322,6 @@ uint64_t psmi_cycles_left(uint64_t start_cycles, int64_t timeout_ns);
 uint32_t psmi_get_ipv4addr();
 void psmi_syslog(psm2_ep_t ep, int to_console, int level,
 		 const char *format, ...);
-void psmi_uuid_unparse(const psm2_uuid_t uuid, char *out);
-int psmi_uuid_compare(const psm2_uuid_t uuA, const psm2_uuid_t uuB);
 void *psmi_memcpyo(void *dst, const void *src, size_t n);
 uint32_t psmi_crc(unsigned char *buf, int len);
 
@@ -385,19 +390,36 @@ void psmi_multi_ep_init();
  *		reg_mr - register MR failure (ENOMEM)
  *		nonpri_reg_mr - non-priority register MR failure (ENOMEM)
  *		pri_reg_mr - priority register MR failure (ENOMEM)
+ *		gdrmmap - GPU gdrcopy pin and mmap failure
  */
-struct psmi_faultinj_spec;
 int psmi_faultinj_enabled;	/* use macro to test */
+int psmi_faultinj_verbose;	/* use IS_FAULT macro to test */
 int psmi_faultinj_sec_rail;	/* faults only on secondary rails or EPs */
-#if 1				/* possible to disable at compile time */
+
+struct psmi_faultinj_spec {
+	STAILQ_ENTRY(psmi_faultinj_spec) next;
+	char spec_name[PSMI_FAULTINJ_SPEC_NAMELEN];
+
+	uint64_t num_faults;
+	uint64_t num_calls;
+
+	struct drand48_data drand48_data;
+	int num;
+	int denom;
+	long int initial_seed;
+};
+
 #define PSMI_FAULTINJ_ENABLED()	(!!psmi_faultinj_enabled)
 #define PSMI_FAULTINJ_ENABLED_EP(ep)	(PSMI_FAULTINJ_ENABLED() \
-		&& (!psmi_faultinj_sec_rail || \
-		 	(psmi_opened_endpoint && (ep) != psmi_opened_endpoint)))
-#else
-#define PSMI_FAULTINJ_ENABLED()	0
-#define PSMI_FAULTINJ_ENABLED_EP(ep)	0
-#endif
+		&& (!psmi_faultinj_sec_rail || ((ep)->mctxt_master != (ep))))
+
+int psmi_faultinj_is_fault(struct psmi_faultinj_spec *fi); // use macro instead
+#define PSMI_FAULTINJ_IS_FAULT(fi, fmt, ...) \
+	(psmi_faultinj_is_fault(fi)? \
+			psmi_faultinj_verbose? \
+				(printf("%s: injecting fault: %s" fmt "\n", hfi_get_mylabel(), fi->spec_name, ##__VA_ARGS__), fflush(stdout), 1) \
+				: 1 \
+			: 0)
 
 void psmi_faultinj_init();
 void psmi_faultinj_fini();
@@ -406,10 +428,14 @@ struct psmi_faultinj_spec *psmi_faultinj_getspec(const char *spec_name,
 						 int num, int denom);
 #define PSMI_FAULTINJ_STATIC_DECL(var, spec_name, help, num, denom)	\
 	static struct psmi_faultinj_spec *var;			\
-	if (PSMI_FAULTINJ_ENABLED() && (var) == NULL)			\
+	if_pf(PSMI_FAULTINJ_ENABLED() && (var) == NULL)			\
 	    (var) = psmi_faultinj_getspec((spec_name), (help), (num), (denom));
-int psmi_faultinj_is_fault(struct psmi_faultinj_spec *spec);
 
+#else
+#define PSMI_FAULTINJ_ENABLED()	0
+#define PSMI_FAULTINJ_ENABLED_EP(ep)	0
+#define PSMI_FAULTINJ_IS_FAULT(fi, fmt, ...) 0
+#define PSMI_FAULTINJ_STATIC_DECL(var, spec_name, help, num, denom)
 #endif /* #ifdef PSM_FI */
 /*
  * PSM core component set/get options
diff --git a/prov/psm3/psm3/psm_verbs_ep.c b/prov/psm3/psm3/psm_verbs_ep.c
index 7985665..15d957b 100644
--- a/prov/psm3/psm3/psm_verbs_ep.c
+++ b/prov/psm3/psm3/psm_verbs_ep.c
@@ -59,7 +59,7 @@
 #include <sched.h>		/* cpu_set */
 #include <ctype.h>		/* isalpha */
 #include <netdb.h>
-#include <infiniband/ib.h> // for AF_IB
+//#include <infiniband/verbs.h>
 #include <ifaddrs.h>
 #include <sys/socket.h>
 #include <netinet/in.h>
@@ -153,11 +153,11 @@ __psm2_ep_open_verbs(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key
 
 	ep->verbs_ep.qkey = *(uint32_t*)job_key;	// use 1st 32 bits of job_key
 
-	if (_HFI_UDDBG_ON) {
+	if (_HFI_PRDBG_ON) {
 		char uuid_str[64];
 		memset(&uuid_str, 0, sizeof(uuid_str));
-		psmi_uuid_unparse(job_key, uuid_str);
-		_HFI_UDDBG("job key %s qkey=0x%x\n", uuid_str, ep->verbs_ep.qkey);
+		uuid_unparse(job_key, uuid_str);
+		_HFI_PRDBG("job key %s qkey=0x%x\n", uuid_str, ep->verbs_ep.qkey);
 	}
 
 	if (PSM2_OK != verbs_open_dev(ep, unit, port, job_key)) {
@@ -177,7 +177,7 @@ __psm2_ep_open_verbs(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key
 	ep->verbs_ep.pd = ibv_alloc_pd(ep->verbs_ep.context);
 	if (! ep->verbs_ep.pd) {
 		_HFI_ERROR( "Unable to alloc PD on %s: %s\n",
-						ep->verbs_ep.ib_devname, strerror(errno));
+						ep->dev_name, strerror(errno));
 		goto fail;
 	}
 
@@ -192,7 +192,7 @@ __psm2_ep_open_verbs(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key
 	ep->verbs_ep.send_cq = ibv_create_cq(ep->verbs_ep.context, ep->hfi_num_send_wqes+ep->hfi_num_send_rdma + 1000, (void*)ep, NULL, 0);
 	if (! ep->verbs_ep.send_cq) {
 		_HFI_ERROR( "Unable to create send CQ of size %u on %s: %s\n",
-						ep->hfi_num_send_wqes+1000, ep->verbs_ep.ib_devname,
+						ep->hfi_num_send_wqes+1000, ep->dev_name,
 						strerror(errno));
 		goto fail;
 	}
@@ -200,20 +200,20 @@ __psm2_ep_open_verbs(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key
 	ep->verbs_ep.recv_comp_channel = ibv_create_comp_channel(ep->verbs_ep.context);
 	if (! ep->verbs_ep.recv_comp_channel) {
 		_HFI_ERROR( "Unable to create recv CQ completion channel on %s: %s\n",
-						ep->verbs_ep.ib_devname, strerror(errno));
+						ep->dev_name, strerror(errno));
 		goto fail;
 	}
 	// change completion channel to non-blocking
 	flags = fcntl( ep->verbs_ep.recv_comp_channel->fd, F_GETFL);
 	if (0 > fcntl( ep->verbs_ep.recv_comp_channel->fd, F_SETFL, flags | O_NONBLOCK)) {
 		_HFI_ERROR( "Unable to change file descriptor of completion event channel for %s: %s\n",
-					ep->verbs_ep.ib_devname, strerror(errno));
+					ep->dev_name, strerror(errno));
 		goto fail;
 	}
 	// this gets done by __psm2_ep_poll_type
 	//if (ibv_req_notify_cq(ep->verbs_ep.recv_cq, 0)) {
 	//	_HFI_ERROR("Can't request RQ events from %s: %s\n",
-	//					ep->verbs_ep.ib_devname, strerror(errno));
+	//					ep->dev_name, strerror(errno));
 	//	goto fail;
 	//}
 
@@ -237,14 +237,14 @@ __psm2_ep_open_verbs(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key
 						 (void*)ep,  ep->verbs_ep.recv_comp_channel, 0);
 	if (! ep->verbs_ep.recv_cq) {
 		_HFI_ERROR( "Unable to create recv CQ of size %u on %s: %s\n",
-					ep->hfi_num_recv_cqes, ep->verbs_ep.ib_devname,
+					ep->hfi_num_recv_cqes, ep->dev_name,
 					strerror(errno));
 		goto fail;
 	}
 
 	ep->verbs_ep.qp = ud_qp_create(ep);
 	if (! ep->verbs_ep.qp) {
-		_HFI_ERROR( "Unable to create QP\n");
+		_HFI_ERROR( "Unable to create UD QP on %s\n", ep->dev_name);
 		goto fail;
 	}
 
@@ -282,7 +282,7 @@ __psm2_ep_initialize_queues(psm2_ep_t ep)
 	}
 
 	ep->verbs_ep.send_reap_thresh = min(ep->hfi_send_reap_thresh, ep->verbs_ep.send_pool.send_total/2);
-	_HFI_UDDBG("reaping when %u posted.\n", ep->verbs_ep.send_reap_thresh);
+	_HFI_PRDBG("reaping when %u posted.\n", ep->verbs_ep.send_reap_thresh);
 
 	if (PSM2_OK != psm_verbs_alloc_recv_pool(ep, ep->verbs_ep.qp, &ep->verbs_ep.recv_pool, 
 				min(ep->hfi_num_recv_wqes, ep->verbs_ep.qp_cap.max_recv_wr),
@@ -300,7 +300,7 @@ __psm2_ep_initialize_queues(psm2_ep_t ep)
 	}
 
 	if (PSM2_OK != __psm2_ep_verbs_prepost_recv(&ep->verbs_ep.recv_pool)) {
-		_HFI_ERROR( "Unable to prepost recv buffers on QP\n");
+		_HFI_ERROR( "Unable to prepost recv buffers on QP for %s port %u\n", ep->dev_name, ep->portnum);
 		goto fail;
 	}
 
@@ -311,7 +311,7 @@ __psm2_ep_initialize_queues(psm2_ep_t ep)
 	if(PSM2_OK != modify_ud_qp_to_rts(ep, ep->verbs_ep.qp)) {
 		goto fail;
 	}
-	_HFI_UDDBG("created QP %p (%u)\n", ep->verbs_ep.qp, ep->verbs_ep.qp->qp_num);
+	_HFI_PRDBG("created QP %p (%u)\n", ep->verbs_ep.qp, ep->verbs_ep.qp->qp_num);
 	return PSM2_OK;
 
 fail:
@@ -325,10 +325,10 @@ int __psm2_ep_poll_type(int poll_type, psm2_ep_t ep)
 	//if (poll_type == PSMI_HAL_POLL_TYPE_URGENT) {
 	if (poll_type) {
 		// set for event on solicted recv
-		_HFI_UDDBG("enable solicited event\n");
+		_HFI_PRDBG("enable solicited event\n");
 		if (0 != ibv_req_notify_cq(ep->verbs_ep.recv_cq, 1)) {
 			_HFI_ERROR("Can't request solicitied RQ events on %s: %s\n",
-							ep->verbs_ep.ib_devname, strerror(errno));
+							ep->dev_name, strerror(errno));
 			return -1;
 		}
 #if 0
@@ -337,13 +337,13 @@ int __psm2_ep_poll_type(int poll_type, psm2_ep_t ep)
 		psmi_assert_always(0);	// not used by PSM
 		if (0 != ibv_req_notify_cq(ep->verbs_ep.recv_cq, 0)) {
 			_HFI_ERROR("Can't request all RQ events on %s: %s\n",
-							ep->verbs_ep.ib_devname, strerror(errno));
+							ep->dev_name, strerror(errno));
 			return -1;
 		}
 #endif
 	} else {
 		// no events for solicted and unsolictited recv
-		_HFI_UDDBG("disable solicited event - noop\n");
+		_HFI_PRDBG("disable solicited event - noop\n");
 		// this is only done once during PSM shutdown of rcvthread.
 		// Verbs events are one-shots.  No way to disable.  However once
 		// PSM stops rcvthread shortly after this call, no one will be
@@ -356,12 +356,12 @@ int __psm2_ep_poll_type(int poll_type, psm2_ep_t ep)
 // free reources in ep->verbs_ep portion of the ep
 void __psm2_ep_free_verbs(psm2_ep_t ep)
 {
-	psm_verbs_free_send_pool(&ep->verbs_ep.send_pool);
-	psm_verbs_free_recv_pool(&ep->verbs_ep.recv_pool);
 	if (ep->verbs_ep.qp) {
 		ibv_destroy_qp(ep->verbs_ep.qp);
 		ep->verbs_ep.qp = NULL;
 	}
+	psm_verbs_free_send_pool(&ep->verbs_ep.send_pool);
+	psm_verbs_free_recv_pool(&ep->verbs_ep.recv_pool);
 	if (ep->verbs_ep.recv_cq) {
 		ibv_destroy_cq(ep->verbs_ep.recv_cq);
 		ep->verbs_ep.recv_cq = NULL;
@@ -393,9 +393,9 @@ void __psm2_ep_free_verbs(psm2_ep_t ep)
 		ibv_close_device(ep->verbs_ep.context);
 		ep->verbs_ep.context = NULL;
 	}
-	if (ep->verbs_ep.ib_devname) {
-		psmi_free(ep->verbs_ep.ib_devname);
-		ep->verbs_ep.ib_devname = NULL;
+	if (ep->dev_name) {
+		psmi_free((char*)ep->dev_name);
+		ep->dev_name = NULL;
 	}
 }
 
@@ -425,7 +425,7 @@ psm2_error_t psm_verbs_alloc_send_pool(psm2_ep_t ep, struct ibv_pd *pd,
 			goto fail;
 		}
 
-		_HFI_UDDBG("send pool: buffers: %p size %u\n",  pool->send_buffers, pool->send_buffer_size);
+		_HFI_PRDBG("send pool: buffers: %p size %u\n",  pool->send_buffers, pool->send_buffer_size);
 		pool->send_bufs = (struct verbs_sbuf *)psmi_calloc(ep, NETWORK_BUFFERS,
 							 pool->send_total*sizeof(struct verbs_sbuf), 1);
 		if (! pool->send_bufs) {
@@ -438,7 +438,7 @@ psm2_error_t psm_verbs_alloc_send_pool(psm2_ep_t ep, struct ibv_pd *pd,
 			pool->send_bufs[i].next = pool->send_free;
 			pool->send_free = &(pool->send_bufs[i]);
 		}
-		_HFI_UDDBG("%u Send Buffers of %u bytes each allocated at %p.\n", pool->send_total, pool->send_buffer_size,
+		_HFI_PRDBG("%u Send Buffers of %u bytes each allocated at %p.\n", pool->send_total, pool->send_buffer_size,
 			pool->send_buffers);
 
 		// UD doesn't support RDMA, so we just need local NIC to be able to
@@ -450,7 +450,7 @@ psm2_error_t psm_verbs_alloc_send_pool(psm2_ep_t ep, struct ibv_pd *pd,
 						IBV_ACCESS_LOCAL_WRITE);
 		if (! pool->send_buffer_mr) {
 			_HFI_ERROR( "Unable to alloc send buffer MR on %s: %s\n",
-							ep->verbs_ep.ib_devname, strerror(errno));
+							ep->dev_name, strerror(errno));
 			goto fail;
 		}
 	}
@@ -486,9 +486,7 @@ psm2_error_t psm_verbs_alloc_recv_pool(psm2_ep_t ep, struct ibv_qp *qp,
 	memset(pool,0,sizeof(*pool));
 
 	pool->qp = qp;	// save a reference
-#ifdef PSM_FI
 	pool->ep = ep;
-#endif
 	pool->recv_total = recv_total;
 
 	if (recv_total ) {
@@ -521,7 +519,7 @@ psm2_error_t psm_verbs_alloc_recv_pool(psm2_ep_t ep, struct ibv_qp *qp,
 				pool->recv_bufs[i].buffer = &(pool->recv_buffers[recv_buffer_start(pool, i)]);
 				pool->recv_bufs[i].pool = pool;
 			}
-			_HFI_UDDBG("%u Recv Buffers of %u bytes each allocated at %p.\n", pool->recv_total, pool->recv_buffer_size,
+			_HFI_PRDBG("%u Recv Buffers of %u bytes each allocated at %p.\n", pool->recv_total, pool->recv_buffer_size,
 				pool->recv_buffers);
 
 			// UD doesn't support RDMA, so we just need local NIC to be able to
@@ -532,7 +530,7 @@ psm2_error_t psm_verbs_alloc_recv_pool(psm2_ep_t ep, struct ibv_qp *qp,
 							IBV_ACCESS_LOCAL_WRITE);
 			if (! pool->recv_buffer_mr) {
 				_HFI_ERROR( "Unable to alloc recv buffer MR on %s: %s\n",
-								ep->verbs_ep.ib_devname, strerror(errno));
+								ep->dev_name, strerror(errno));
 				goto fail;
 			}
 		} else {
@@ -547,7 +545,7 @@ psm2_error_t psm_verbs_alloc_recv_pool(psm2_ep_t ep, struct ibv_qp *qp,
 			}
 			// prepare rbuf handle for use as wr_id
 			pool->recv_bufs->pool = pool;
-			_HFI_UDDBG("%u Recv Buffers of %u bytes each allocated.\n", pool->recv_total, pool->recv_buffer_size);
+			_HFI_PRDBG("%u Recv Buffers of %u bytes each allocated.\n", pool->recv_total, pool->recv_buffer_size);
 		}
 #if VERBS_RECV_QP_COALLESCE > 1
 		// prebuild as much as we can
@@ -705,15 +703,15 @@ psm2_error_t __psm2_ep_verbs_post_recv(
 					"or RC "
 					"RQ WQE with bad lkey",
 					0, IPS_FAULTINJ_RQ_LKEY);
-			if (psmi_faultinj_is_fault(fi_rq_lkey))
+			if_pf(PSMI_FAULTINJ_IS_FAULT(fi_rq_lkey, " QP %u", pool->qp->qp_num))
 				wr->sg_list->lkey = 55;
 		} else
 			wr->sg_list->lkey = pool->recv_buffer_mr->lkey;
-#endif
+#endif // PSM_FI
 		if_pf (++pool->next_recv_wqe >= VERBS_RECV_QP_COALLESCE) {
 			// we have a batch ready to post
 			if_pf (ibv_post_recv(pool->qp, pool->recv_wr_list, &bad_wr)) {
-				_HFI_ERROR("failed to post RQ: %s", strerror(errno));
+				_HFI_ERROR("failed to post RQ on %s port %u: %s", pool->ep->dev_name, pool->ep->portnum, strerror(errno));
 				return PSM2_INTERNAL_ERR;
 			}
 			//_HFI_VDBG("posted RQ, including buffer %u\n", index);
@@ -732,17 +730,17 @@ psm2_error_t __psm2_ep_verbs_post_recv(
 					"or RC "
 					"RQ WQE with bad lkey",
 					0, IPS_FAULTINJ_RQ_LKEY);
-			if (psmi_faultinj_is_fault(fi_rq_lkey))
+			if_pf(PSMI_FAULTINJ_IS_FAULT(fi_rq_lkey, " QP %u", pool->qp->qp_num))
 				list.lkey = 55;
 		}
-#endif
+#endif // PSM_FI
 		wr.next = NULL;	// just post 1
 		wr.wr_id = (uintptr_t)buf;	// we'll get this back in completion
 		wr.sg_list = &list;
 		wr.num_sge = 1;	// size of sg_list
 
 		if_pf (ibv_post_recv(pool->qp, &wr, &bad_wr)) {
-			_HFI_ERROR("failed to post RQ: %s", strerror(errno));
+			_HFI_ERROR("failed to post RQ on %s port %u: %s", pool->ep->dev_name, pool->ep->portnum, strerror(errno));
 			return PSM2_INTERNAL_ERR;
 		}
 		//_HFI_VDBG("posted RQ, buffer %u\n", index);
@@ -756,7 +754,7 @@ psm2_error_t __psm2_ep_verbs_post_recv(
 		if_pf (++pool->next_recv_wqe >= VERBS_RECV_QP_COALLESCE) {
 			// we have a batch ready to post
 			if_pf (ibv_post_recv(pool->qp, pool->recv_wr_list, &bad_wr)) {
-				_HFI_ERROR("failed to post RQ: %s", strerror(errno));
+				_HFI_ERROR("failed to post RQ on %s on port %u: %s", pool->ep->dev_name, pool->ep->portnum, strerror(errno));
 				return PSM2_INTERNAL_ERR;
 			}
 			//_HFI_VDBG("posted RQ\n");
@@ -771,7 +769,7 @@ psm2_error_t __psm2_ep_verbs_post_recv(
 		wr.num_sge = 0;	// size of sg_list
 
 		if_pf (ibv_post_recv(pool->qp, &wr, &bad_wr)) {
-			_HFI_ERROR("failed to post RQ: %s", strerror(errno));
+			_HFI_ERROR("failed to post RQ on %s on port %u: %s", pool->ep->dev_name, pool->ep->portnum, strerror(errno));
 			return PSM2_INTERNAL_ERR;
 		}
 		//_HFI_VDBG("posted RQ\n");
@@ -796,7 +794,7 @@ psm2_error_t __psm2_ep_verbs_prepost_recv(
 			buf = pool->recv_bufs;	// only 1, just to find pool and qp
 		if (PSM2_OK != __psm2_ep_verbs_post_recv(
 							buf)) {
-			_HFI_ERROR( "Unable to post RQ\n");
+			_HFI_ERROR( "Unable to post RQ on %s port %u\n", pool->ep->dev_name, pool->ep->portnum);
 			return PSM2_INTERNAL_ERR;
 		}
 	}
@@ -825,13 +823,10 @@ psm2_error_t psm2_verbs_post_rdma_write_immed(psm2_ep_t ep, struct ibv_qp *qp,
 		PSMI_FAULTINJ_STATIC_DECL(fi_rc_rdma_lkey, "rc_rdma_lkey",
 				"post RC RDMA Write WQE with bad lkey",
 				0, IPS_FAULTINJ_RC_RDMA_LKEY);
-		if (psmi_faultinj_is_fault(fi_rc_rdma_lkey)) {
-			printf("corrupting RC RDMA lkey QP %u\n", qp->qp_num);
-			fflush(stdout);
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_rc_rdma_lkey, " QP %u", qp->qp_num))
 			list.lkey = 55;
-		}
 	}
-#endif
+#endif // PSM_FI
 	wr.next = NULL; // just post 1
 	psmi_assert(! (wr_id & VERBS_SQ_WR_ID_MASK));
 	wr.wr_id = wr_id | VERBS_SQ_WR_ID_RDMA_WRITE;
@@ -846,13 +841,10 @@ psm2_error_t psm2_verbs_post_rdma_write_immed(psm2_ep_t ep, struct ibv_qp *qp,
 		PSMI_FAULTINJ_STATIC_DECL(fi_rc_rdma_rkey, "rc_rdma_rkey",
 				"post RC RDMA Write WQE with bad rkey",
 				0, IPS_FAULTINJ_RC_RDMA_RKEY);
-		if (psmi_faultinj_is_fault(fi_rc_rdma_rkey)) {
-			printf("corrupting RC RDMA rkey QP %u\n", qp->qp_num);
-			fflush(stdout);
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_rc_rdma_rkey, " QP %u", qp->qp_num))
 			wr.wr.rdma.rkey = 55;
-		}
 	}
-#endif
+#endif // PSM_FI
 	// RDMA Writes will tend to be larger and we want the completion
 	// to reflect the RDMA for a given CTS is completed
 	wr.send_flags = IBV_SEND_SIGNALED;  // get a completion
@@ -861,8 +853,8 @@ psm2_error_t psm2_verbs_post_rdma_write_immed(psm2_ep_t ep, struct ibv_qp *qp,
 	ep->verbs_ep.send_rdma_outstanding++;
 	if_pf (ibv_post_send(qp, &wr, &bad_wr)) {
 		if (errno != EBUSY && errno != EAGAIN && errno != ENOMEM)
-			_HFI_ERROR("failed to post RC SQ on %s: %s",
-					ep->verbs_ep.ib_devname, strerror(errno));
+			_HFI_ERROR("failed to post RC SQ on %s port %u: %s",
+					ep->dev_name, ep->portnum, strerror(errno));
 		// caller will try again later when next send buffer freed
 		// or timer expires
 		ret = PSM2_TIMEOUT;
@@ -903,17 +895,17 @@ psm2_error_t psm2_verbs_post_rv_rdma_write_immed(psm2_ep_t ep,
 		PSMI_FAULTINJ_STATIC_DECL(fi_rv_rdma_len, "rv_rdma_len",
 				"post RV RDMA Write with bad len (may want RV build with RNDV_LOCAL_ERR_TEST)",
 				0, IPS_FAULTINJ_RV_RDMA_LEN);
-		if (psmi_faultinj_is_fault(fi_rv_rdma_len))
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_rv_rdma_len, ""))
 			len += 1000000000;
 	}
 	if_pf(PSMI_FAULTINJ_ENABLED_EP(ep)) {
 		PSMI_FAULTINJ_STATIC_DECL(fi_rv_rdma_rkey, "rv_rdma_rkey",
 				"post RV RDMA Write with bad rkey",
 				1, IPS_FAULTINJ_RV_RDMA_RKEY);
-		if (psmi_faultinj_is_fault(fi_rv_rdma_rkey))
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_rv_rdma_rkey, ""))
 			rkey = 55;
 	}
-#endif
+#endif // PSM_FI
 	if (__psm2_rv_post_rdma_write_immed(ep->verbs_ep.rv, conn,
                 loc_buf, loc_mr->mr.rv_mr,
                 rem_buf, rkey,
@@ -938,8 +930,8 @@ psm2_error_t psm2_verbs_post_rv_rdma_write_immed(psm2_ep_t ep,
 			break;
 		}
 		if (errno != EBUSY && errno != EAGAIN && errno != ENOMEM) {
-			_HFI_ERROR("failed to post RV RC SQ on %s: %s",
-					ep->verbs_ep.ib_devname, strerror(errno));
+			_HFI_ERROR("failed to post RV RC SQ on %s port %u: %s",
+					ep->dev_name, ep->portnum, strerror(errno));
 			psmi_assert_always(errno != EINVAL);
 		}
 		ep->verbs_ep.send_rdma_outstanding--;
@@ -993,9 +985,9 @@ psm2_verbs_completion_update(psm2_ep_t ep)
 			psmi_assert_always(wc[i].wr_id);
 			if_pf (wc[i].status) {
 				if (wc[i].status != IBV_WC_WR_FLUSH_ERR)
-					_HFI_ERROR("failed %s on %s status: '%s' (%d) QP %u\n",
+					_HFI_ERROR("failed %s on %s port %u status: '%s' (%d) QP %u\n",
 						VERBS_SQ_WR_OP_STR(wc[i].wr_id),
-						ep->verbs_ep.ib_devname,
+						ep->dev_name, ep->portnum,
 						ibv_wc_status_str(wc[i].status), (int)wc[i].status,
 						wc[i].qp_num);
 				// For user space RC QP, the QP is now in QPS_ERROR and we
@@ -1025,8 +1017,8 @@ psm2_verbs_completion_update(psm2_ep_t ep)
 							 wc[i].wr_id & ~VERBS_SQ_WR_ID_MASK);
 				break;
 			default:
-				_HFI_ERROR("unexpected send completion on %s opcode %d QP %u\n",
-							ep->verbs_ep.ib_devname,
+				_HFI_ERROR("unexpected send completion on %s port %u opcode %d QP %u\n",
+							ep->dev_name, ep->portnum,
 							wc[i].opcode, wc[i].qp_num);
 				break;
 			}
@@ -1042,29 +1034,17 @@ psm2_verbs_completion_update(psm2_ep_t ep)
 	return PSM2_OK;
 }
 
-void __psm2_dump_buf(uint8_t *buf, uint32_t len)
-{
-	int i, j;
-	for (i=0; i<len; i += 16 ) {
-		printf("0x%04x:", i);
-		for (j=0; j<16; j++)
-			printf(" %02x", (unsigned)buf[i+j]);
-		printf("\n");
-	}
-}
-
 int verbs_get_port_index2pkey(psm2_ep_t ep, int port, int index)
 {
 	__be16 pkey;
 
 	psmi_assert_always(ep->verbs_ep.context);
 	if (0 != ibv_query_pkey(ep->verbs_ep.context, port, index, &pkey)) {
-		_HFI_ERROR( "Can't query pkey index %d on %s: %s\n", index,
-				ep->verbs_ep.ib_devname, strerror(errno));
+		_HFI_ERROR( "Can't query pkey index %d on %s port %u: %s\n", index,
+				ep->dev_name, port, strerror(errno));
 		return -1;
 	}
-	_HFI_UDDBG("got pkey 0x%x on %s\n", __be16_to_cpu(pkey),
-						ep->verbs_ep.ib_devname);
+	_HFI_PRDBG("got pkey 0x%x on %s port %u\n", __be16_to_cpu(pkey), ep->dev_name, port);
 	return __be16_to_cpu(pkey);
 }
 
@@ -1241,7 +1221,7 @@ static void register_rv_conn_stats(psm2_ep_t ep)
 					PSMI_STATSTYPE_RV_RDMA,
 					entries,
 					PSMI_STATS_HOWMANY(entries),
-					ep->epid, ep);
+					ep->epid, ep, ep->dev_name);
 }
 
 static void deregister_rv_conn_stats(psm2_ep_t ep)
@@ -1298,7 +1278,7 @@ static void register_rv_event_stats(psm2_ep_t ep)
 					PSMI_STATSTYPE_RV_EVENT,
 					entries,
 					PSMI_STATS_HOWMANY(entries),
-					ep->epid, ep);
+					ep->epid, ep, ep->dev_name);
 }
 
 static void deregister_rv_event_stats(psm2_ep_t ep)
@@ -1310,12 +1290,39 @@ static psm2_error_t open_rv(psm2_ep_t ep, psm2_uuid_t const job_key)
 {
 	struct local_info loc_info = { 0 };
 
+	// we always fill in everything we might need in loc_info
+	// in some modes, some of the fields are not used by RV
 	loc_info.mr_cache_size = ep->rv_mr_cache_size;
+#ifdef PSM_CUDA
+	/* gpu_cache_size ignored unless RV_RDMA_MODE_GPU */
+	loc_info.gpu_cache_size = ep->rv_gpu_cache_size;
+#endif
 	loc_info.rdma_mode = IPS_PROTOEXP_FLAG_KERNEL_QP(ep->rdmamode)?
 					RV_RDMA_MODE_KERNEL: RV_RDMA_MODE_USER;
+#ifdef PSM_CUDA
+	if (PSMI_IS_CUDA_ENABLED) {
+		// when Cuda is enabled we will have larger window_sz and
+		// need to upsize the caches we will use for priority MRs
+		if (ep->rdmamode & IPS_PROTOEXP_FLAG_ENABLED) {
+			// priority window_sz reg_mr for CPU
+			loc_info.rdma_mode |= RV_RDMA_MODE_UPSIZE_CPU;
+		}
+ 		if (psmi_parse_gpudirect()) {
+			// When GPU Direct is enabled we need a GPU Cache
+			loc_info.rdma_mode |= RV_RDMA_MODE_GPU;
+			if ((ep->rdmamode & IPS_PROTOEXP_FLAG_ENABLED)
+				&& (psmi_parse_gpudirect_send_limit()
+				|| psmi_parse_gpudirect_recv_limit())) {
+				// priority window_sz reg_mr for GPU memory
+				loc_info.rdma_mode |= RV_RDMA_MODE_UPSIZE_GPU;
+			}
+		}
+	}
+#endif
 
-	// the rest of loc_info is really only needed for RV_RDMA_MODE_KERNEL
+	// need portnum for rdma_mode KERNEL or USER|GPU
 	loc_info.port_num = ep->portnum;
+	// the rest of loc_info is really only needed for RV_RDMA_MODE_KERNEL
 	loc_info.num_conn = ep->rv_num_conn;
 	// caller computes our local EPID, but loc_addr must == PSMI_EPID_GET_LID
 	// for what will be established as our local epid by psmi_context_open
@@ -1344,18 +1351,12 @@ static psm2_error_t open_rv(psm2_ep_t ep, psm2_uuid_t const job_key)
 	loc_info.reconnect_timeout = ep->rv_reconnect_timeout;
 	loc_info.hb_interval = ep->rv_hb_interval;
 
-	ep->verbs_ep.rv =__psm2_rv_open(ep->verbs_ep.ib_devname, &loc_info);
+	ep->verbs_ep.rv =__psm2_rv_open(ep->dev_name, &loc_info);
 	if (! ep->verbs_ep.rv) {
 		return PSM2_INTERNAL_ERR;
 	}
-	if (psmi_parse_identify()) {
-		printf("%s %s run-time rv interface v%d.%d\n",
-		       hfi_get_mylabel(), hfi_ident_tag,
-		       loc_info.major_rev,
-		       loc_info.minor_rev);
-	}
 	// parallel psm_hal_gen1/psm_hal_inline_i.h handling HFI1_CAP_GPUDIRECT_OT
-	// psm_context.c will detect a CUDA driver w/non-CUDA PSM as fatal error
+	// for OPA psm_context.c, treats CUDA driver w/non-CUDA PSM as fatal
 #ifndef RV_CAP_GPU_DIRECT
 #ifdef PSM_CUDA
 #error "Inconsistent build.  RV_CAP_GPU_DIRECT must be defined for CUDA builds."
@@ -1364,21 +1365,63 @@ static psm2_error_t open_rv(psm2_ep_t ep, psm2_uuid_t const job_key)
 #define RV_CAP_GPU_DIRECT (1UL << 63)
 #endif
 #endif
+	if (psmi_parse_identify()) {
+		if (loc_info.capability & RV_CAP_GPU_DIRECT)
+#ifdef PSM_CUDA
+			printf("%s %s run-time rv interface v%d.%d%s gpu v%d.%d cuda\n",
+			       hfi_get_mylabel(), hfi_ident_tag,
+			       loc_info.major_rev,
+			       loc_info.minor_rev,
+			       (loc_info.capability & RV_CAP_USER_MR)?" mr":"",
+			       loc_info.gpu_major_rev,
+			       loc_info.gpu_minor_rev);
+#else
+			printf("%s %s run-time rv interface v%d.%d%s cuda\n",
+			       hfi_get_mylabel(), hfi_ident_tag,
+			       loc_info.major_rev,
+			       loc_info.minor_rev,
+			       (loc_info.capability & RV_CAP_USER_MR)?" mr":"");
+#endif
+		else
+			printf("%s %s run-time rv interface v%d.%d%s\n",
+			       hfi_get_mylabel(), hfi_ident_tag,
+			       loc_info.major_rev,
+			       loc_info.minor_rev,
+			       (loc_info.capability & RV_CAP_USER_MR)?" mr":"");
+	}
+	if (loc_info.capability & RV_CAP_USER_MR)
+		psmi_hal_add_cap(PSM_HAL_CAP_USER_MR);
+	if (loc_info.capability & RV_CAP_EVICT)
+		psmi_hal_add_cap(PSM_HAL_CAP_EVICT);
 	if (loc_info.capability & RV_CAP_GPU_DIRECT)
 		psmi_hal_add_cap(PSM_HAL_CAP_GPUDIRECT_OT);
 	ep->verbs_ep.rv_index = loc_info.rv_index;
 	ep->rv_mr_cache_size = loc_info.mr_cache_size;
+#ifdef PSM_CUDA
+	ep->rv_gpu_cache_size = loc_info.gpu_cache_size;
+#endif
 	ep->rv_q_depth = loc_info.q_depth;
 	ep->rv_reconnect_timeout = loc_info.reconnect_timeout;
 
-	if (IPS_PROTOEXP_FLAG_KERNEL_QP(ep->rdmamode)) {
-		register_rv_conn_stats(ep);
+	return PSM2_OK;
+}
+#endif // RNDV_MOD
+
+// initialize verbs specific statistics
+void
+__psm2_ep_initstats_verbs(psm2_ep_t ep)
+{
+#ifdef RNDV_MOD
+	if (ep->verbs_ep.rv && IPS_PROTOEXP_FLAG_KERNEL_QP(ep->rdmamode)) {
+		// only one set of conn stats per job_dev, so
+		// no use gathering for any extra QPs we open
+		if (ep->mctxt_master == ep)
+			register_rv_conn_stats(ep);
 		register_rv_event_stats(ep);
 	}
+#endif
 
-	return PSM2_OK;
 }
-#endif // RNDV_MOD
 
 static psm2_error_t verbs_open_dev(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key)
 {
@@ -1407,8 +1450,8 @@ static psm2_error_t verbs_open_dev(psm2_ep_t ep, int unit, int port, psm2_uuid_t
 	}
 	dev_name++; // Inc past last '/'
 
-	ep->verbs_ep.ib_devname = psmi_strdup(ep, dev_name);
-	if (! ep->verbs_ep.ib_devname) {
+	ep->dev_name = psmi_strdup(ep, dev_name);
+	if (! ep->dev_name) {
 		_HFI_ERROR( "can't alloc devname");
 		return PSM2_INTERNAL_ERR;
 	}
@@ -1427,44 +1470,44 @@ static psm2_error_t verbs_open_dev(psm2_ep_t ep, int unit, int port, psm2_uuid_t
 	}
 
 	for (i = 0; i < num_of_devices; i++) {
-		if (!strcmp(ibv_get_device_name(dev_list[i]), ep->verbs_ep.ib_devname))
+		if (!strcmp(ibv_get_device_name(dev_list[i]), ep->dev_name))
 			break;
 	}
 	if (i >= num_of_devices) {
 		_HFI_ERROR("Unit Id [%d] name %s not found, number of devices is %d\n",
-				   unit, ep->verbs_ep.ib_devname, num_of_devices);
+				   unit, ep->dev_name, num_of_devices);
 		err = PSM2_INTERNAL_ERR;
 		goto fail;
 	}
 	ep->unit_id = unit;
-	_HFI_UDDBG("Using unit_id[%d] %s.\n", ep->unit_id, ep->verbs_ep.ib_devname);
+	_HFI_PRDBG("Using unit_id[%d] %s.\n", ep->unit_id, ep->dev_name);
 
 	ib_dev = dev_list[i];	// device list order may differ from unit order
 	ep->verbs_ep.context = ibv_open_device(ib_dev);
 	if (! ep->verbs_ep.context) {
-		_HFI_ERROR( "Unable to open %s: %s\n", ep->verbs_ep.ib_devname,
+		_HFI_ERROR( "Unable to open %s: %s\n", ep->dev_name,
 						strerror(errno));
 		err = PSM2_INTERNAL_ERR;
 		goto fail;
 	} else {
-		_HFI_UDDBG("Opened %s.\n",ep->verbs_ep.ib_devname);
+		_HFI_PRDBG("Opened %s.\n",ep->dev_name);
 	}
 	// change async events to non-blocking
 	flags = fcntl( ep->verbs_ep.context->async_fd, F_GETFL);
 	if (0 > fcntl( ep->verbs_ep.context->async_fd, F_SETFL, flags | O_NONBLOCK)) {
 		_HFI_ERROR( "Unable to change file descriptor of async events for %s: %s\n",
-					ep->verbs_ep.ib_devname, strerror(errno));
+					ep->dev_name, strerror(errno));
 		err = PSM2_INTERNAL_ERR;
 		goto fail;
 	}
 
 	if (ibv_query_port(ep->verbs_ep.context, ep->portnum, &ep->verbs_ep.port_attr)) {
 		_HFI_ERROR( "Unable to query port %u of %s: %s\n", ep->portnum,
-						ep->verbs_ep.ib_devname, strerror(errno));
+						ep->dev_name, strerror(errno));
 		err = PSM2_INTERNAL_ERR;
 		goto fail;
 	} else {
-		_HFI_UDDBG("Queried %s.\n",ep->verbs_ep.ib_devname);
+		_HFI_PRDBG("Queried %s.\n",ep->dev_name);
 	}
 
 	if (0 != psmi_hal_get_port_subnet(ep->unit_id, ep->portnum,
@@ -1472,27 +1515,33 @@ static psm2_error_t verbs_open_dev(psm2_ep_t ep, int unit, int port, psm2_uuid_t
 			&ep->verbs_ep.ip_addr, &ep->verbs_ep.ip_netmask,	// if eth
 			&ep->verbs_ep.lgid_index, &hi, &lo)) {
 		_HFI_ERROR( "Unable to get subnet for port %u of %s: %s\n", ep->portnum,
-						ep->verbs_ep.ib_devname, strerror(errno));
+						ep->dev_name, strerror(errno));
 		err = PSM2_INTERNAL_ERR;
 		goto fail;
 	} else {
 		ep->verbs_ep.lgid.global.subnet_prefix = __cpu_to_be64(hi);
 		ep->verbs_ep.lgid.global.interface_id = __cpu_to_be64(lo);
-		_HFI_UDDBG("Subnet for port %u of %s: 0x%"PRIx64" addr 0x%"PRIx64" gid 0x%"PRIx64":0x%"PRIx64"\n",
-					ep->portnum, ep->verbs_ep.ib_devname,
+		_HFI_PRDBG("Subnet for port %u of %s: 0x%"PRIx64" addr 0x%"PRIx64" gid 0x%"PRIx64":0x%"PRIx64"\n",
+					ep->portnum, ep->dev_name,
 					ep->gid_hi, ep->gid_lo, hi, lo);
 	}
 
 #ifdef RNDV_MOD
 	if (IPS_PROTOEXP_FLAG_KERNEL_QP(ep->rdmamode)
 		|| ep->mr_cache_mode == MR_CACHE_MODE_KERNEL ) {
-		// cache mode is only set when rdmamode is enabled (eg. kernel or user)
-		psmi_assert(ep->rdmamode & IPS_PROTOEXP_FLAG_ENABLED);
 		// open rendezvous module for the same port as our verbs device
 		err = open_rv(ep, job_key);
 		if (err != PSM2_OK) {
 			_HFI_ERROR( "Unable to open rendezvous module for port %u of %s.\n",
-				ep->portnum, ep->verbs_ep.ib_devname);
+				ep->portnum, ep->dev_name);
+			// TBD - could ignore error and proceed with UD mode
+			//err = PSM2_OK;
+			err = PSM2_INTERNAL_ERR;
+			goto fail;
+		}
+		if (ep->mr_cache_mode == MR_CACHE_MODE_KERNEL
+			&& ! psmi_hal_has_cap(PSM_HAL_CAP_USER_MR)) {
+			_HFI_ERROR( "Rendezvous module lacks enable_user_mr capability.\n");
 			// TBD - could ignore error and proceed with UD mode
 			//err = PSM2_OK;
 			err = PSM2_INTERNAL_ERR;
@@ -1511,9 +1560,9 @@ fail:
 		ibv_close_device(ep->verbs_ep.context);
 		ep->verbs_ep.context = NULL;
 	}
-	if (ep->verbs_ep.ib_devname) {
-		psmi_free(ep->verbs_ep.ib_devname);
-		ep->verbs_ep.ib_devname = NULL;
+	if (ep->dev_name) {
+		psmi_free((char*)ep->dev_name);
+		ep->dev_name = NULL;
 	}
 	goto done;
 }
@@ -1525,23 +1574,23 @@ check_port_state(psm2_ep_t ep)
 
 	active_mtu = MTU_SIZE(ep->verbs_ep.port_attr.active_mtu);
 	if (ep->verbs_ep.port_attr.link_layer == IBV_LINK_LAYER_ETHERNET) {
-		_HFI_UDDBG("running on ethernet at %d MTU\n", active_mtu);
+		_HFI_PRDBG("running on ethernet at %d MTU\n", active_mtu);
 	} else {
-		_HFI_UDDBG( "running on %s at %d MTU\n", link_layer_str(ep->verbs_ep.port_attr.link_layer), active_mtu);
+		_HFI_PRDBG( "running on %s at %d MTU\n", link_layer_str(ep->verbs_ep.port_attr.link_layer), active_mtu);
 	}
 	if (strcmp("Unknown", link_layer_str(ep->verbs_ep.port_attr.link_layer)) == 0) {
 		_HFI_ERROR( "Link layer on port %d of %s is Unknown\n", ep->portnum,
-						ep->verbs_ep.ib_devname);
+						ep->dev_name);
 		return PSM2_INTERNAL_ERR;
 	}
 	ep->verbs_ep.link_layer = ep->verbs_ep.port_attr.link_layer;
 
 	if (ep->verbs_ep.port_attr.state != IBV_PORT_ACTIVE) {
 		_HFI_ERROR( " Port state is not active for %s port %d: %d\n",
-						ep->verbs_ep.ib_devname, ep->portnum,
+						ep->dev_name, ep->portnum,
 						ep->verbs_ep.port_attr.state);
 		//_HFI_ERROR( " Port number %d on %s state is %s\n",
-				//params->ib_port, ep->verbs_ep.ib_devname,
+				//params->ib_port, ep->dev_name,
 				//portStates[ep->verbs_ep.port_attr.state]);
 		return PSM2_INTERNAL_ERR;
 	}
@@ -1552,7 +1601,7 @@ check_port_state(psm2_ep_t ep)
 	// However all UD QP payloads (including PSM headers) are
 	// counted toward MTU in UD verbs.  So need to discount by PSM header size
 	ep->mtu = active_mtu - MAX_PSM_HEADER;
-	_HFI_UDDBG("Max PSM payload (aka MTU): %u\n", ep->mtu);
+	_HFI_PRDBG("Max PSM payload (aka MTU): %u\n", ep->mtu);
 	// TBD - *act_mtu = defined constant, we can use an eager RC message size
 	// for PSM which is larger than packet MTU
 	ep->verbs_ep.active_rate = verbs_get_rate(
@@ -1572,7 +1621,7 @@ static struct ibv_qp* ud_qp_create(psm2_ep_t ep)
 	attr.recv_cq = ep->verbs_ep.recv_cq;
 	// one extra WQE to be safe in case verbs needs a spare WQE
 	attr.cap.max_send_wr  = ep->hfi_num_send_wqes+1;
-	attr.cap.max_send_sge = 1;
+	attr.cap.max_send_sge = 2;
 	attr.cap.max_inline_data = ep->hfi_imm_size;
 
 	attr.srq = NULL;
@@ -1584,7 +1633,7 @@ static struct ibv_qp* ud_qp_create(psm2_ep_t ep)
 	qp = ibv_create_qp(ep->verbs_ep.pd, &attr);
 	if (qp == NULL && errno == ENOMEM) {
 		_HFI_ERROR( "Unable to create UD QP on %s: %s\n",
-					ep->verbs_ep.ib_devname, strerror(errno));
+					ep->dev_name, strerror(errno));
 		_HFI_ERROR( "Requested QP size might be too big. Try reducing TX depth and/or inline size.\n");
 		_HFI_ERROR( "Requested TX depth was %u and RX depth was %u .\n",
 					ep->hfi_num_send_wqes+1, ep->hfi_num_recv_wqes);
@@ -1595,29 +1644,29 @@ static struct ibv_qp* ud_qp_create(psm2_ep_t ep)
 
 	// QP adjusted values due to HW limits
 	if (ep->hfi_imm_size > attr.cap.max_inline_data) {
-		_HFI_UDDBG( "Limited to inline size of %d, requested %u\n",
+		_HFI_PRDBG( "Limited to inline size of %d, requested %u\n",
 			attr.cap.max_inline_data, ep->hfi_imm_size);
 	} else {
-		_HFI_UDDBG("Inline Size: %u\n", attr.cap.max_inline_data);
+		_HFI_PRDBG("Inline Size: %u\n", attr.cap.max_inline_data);
 	}
 	if (ep->hfi_num_send_wqes+1 > attr.cap.max_send_wr) {
-		_HFI_UDDBG( "Limited to %d SQ WQEs, requested %u\n",
+		_HFI_PRDBG( "Limited to %d SQ WQEs, requested %u\n",
 			attr.cap.max_send_wr, ep->hfi_num_send_wqes+1);
 	} else {
-		_HFI_UDDBG("SQ WQEs: %u\n", attr.cap.max_send_wr);
+		_HFI_PRDBG("SQ WQEs: %u\n", attr.cap.max_send_wr);
 	}
-	if (1 > attr.cap.max_send_sge) {
-		_HFI_UDDBG( "Limited to %d SQ SGEs\n",
+	if (2 > attr.cap.max_send_sge) {
+		_HFI_PRDBG( "Limited to %d SQ SGEs\n",
 			attr.cap.max_send_sge);
 	}
 	if (ep->hfi_num_recv_wqes > attr.cap.max_recv_wr) {
-		_HFI_UDDBG( "Limited to %d RQ WQEs, requested %u\n",
+		_HFI_PRDBG( "Limited to %d RQ WQEs, requested %u\n",
 			attr.cap.max_recv_wr, ep->hfi_num_recv_wqes);
 	} else {
-		_HFI_UDDBG("RQ WQEs: %u\n", attr.cap.max_recv_wr);
+		_HFI_PRDBG("RQ WQEs: %u\n", attr.cap.max_recv_wr);
 	}
 	if (1 > attr.cap.max_recv_sge) {
-		_HFI_UDDBG( "Limited to %d RQ SGEs\n",
+		_HFI_PRDBG( "Limited to %d RQ SGEs\n",
 			attr.cap.max_recv_sge);
 	}
 
@@ -1638,7 +1687,7 @@ static psm2_error_t modify_ud_qp_to_init(psm2_ep_t ep, struct ibv_qp *qp)
 
 	if (ibv_modify_qp(qp, &attr,flags)) {
 		_HFI_ERROR( "Failed to modify UD QP to INIT on %s: %s\n",
-					ep->verbs_ep.ib_devname,strerror(errno));
+					ep->dev_name, strerror(errno));
 		return PSM2_INTERNAL_ERR;
 	}
 	return PSM2_OK;
@@ -1653,7 +1702,7 @@ static psm2_error_t modify_ud_qp_to_rtr(psm2_ep_t ep,struct ibv_qp *qp)
 
 	if (ibv_modify_qp(qp, &attr, flags)) {
 		_HFI_ERROR( "Failed to modify UD QP to RTR on %s: %s\n",
-					ep->verbs_ep.ib_devname,strerror(errno));
+					ep->dev_name, strerror(errno));
 		return PSM2_INTERNAL_ERR;
 	}
 	return PSM2_OK;
@@ -1669,7 +1718,7 @@ static psm2_error_t modify_ud_qp_to_rts(psm2_ep_t ep, struct ibv_qp *qp)
 
 	if (ibv_modify_qp(qp, &attr, flags)) {
 		_HFI_ERROR( "Failed to modify UD QP to RTS on %s: %s\n",
-					ep->verbs_ep.ib_devname,strerror(errno));
+					ep->dev_name, strerror(errno));
 		return PSM2_INTERNAL_ERR;
 	}
 	return PSM2_OK;
@@ -1691,7 +1740,7 @@ struct ibv_qp* rc_qp_create(psm2_ep_t ep, void *context, struct ibv_qp_cap *cap)
 		// need to be prepared in case all sends posted to same RC QP, so
 		// match the number of send buffers we plan to allocate
 		attr.cap.max_send_wr  = ep->hfi_num_send_wqes+ep->hfi_num_send_rdma+1;
-		attr.cap.max_send_sge = 1;
+		attr.cap.max_send_sge = 2;
 		// inline data helps latency and message rate for small sends
 		// Later we may explore use of
 		// send SGEs pointing to application buffers, somewhat like WFR send DMA
@@ -1713,7 +1762,7 @@ struct ibv_qp* rc_qp_create(psm2_ep_t ep, void *context, struct ibv_qp_cap *cap)
 	qp = ibv_create_qp(ep->verbs_ep.pd, &attr);
 	if (qp == NULL) {
 		_HFI_ERROR( "Unable to create RC QP on %s: %s\n",
-					ep->verbs_ep.ib_devname, strerror(errno));
+					ep->dev_name, strerror(errno));
 		_HFI_ERROR( "Requested QP size might be too big. Try reducing TX depth and/or inline size.\n");
 		_HFI_ERROR( "Requested TX depth was %u and RX depth was %u .\n",
 					ep->hfi_num_send_wqes+1,
@@ -1725,48 +1774,48 @@ struct ibv_qp* rc_qp_create(psm2_ep_t ep, void *context, struct ibv_qp_cap *cap)
 	if ((ep->rdmamode&IPS_PROTOEXP_FLAG_RDMA_MASK) == IPS_PROTOEXP_FLAG_RDMA_USER_RC) {
 		// QP adjusted values due to HW limits
 		if (ep->hfi_imm_size > attr.cap.max_inline_data) {
-			_HFI_UDDBG( "Limited to inline size of %d, requested %u\n",
+			_HFI_PRDBG( "Limited to inline size of %d, requested %u\n",
 				attr.cap.max_inline_data, ep->hfi_imm_size);
 		} else {
-			_HFI_UDDBG("Inline Size: %u\n", attr.cap.max_inline_data);
+			_HFI_PRDBG("Inline Size: %u\n", attr.cap.max_inline_data);
 		}
 		if (ep->hfi_num_send_wqes+ep->hfi_num_send_rdma+1 > attr.cap.max_send_wr) {
-			_HFI_UDDBG( "Limited to %d SQ WQEs, requested %u\n",
+			_HFI_PRDBG( "Limited to %d SQ WQEs, requested %u\n",
 				attr.cap.max_send_wr, ep->hfi_num_send_wqes+ep->hfi_num_send_rdma+1);
 		} else {
-			_HFI_UDDBG("SQ WQEs: %u\n", attr.cap.max_send_wr);
+			_HFI_PRDBG("SQ WQEs: %u\n", attr.cap.max_send_wr);
 		}
-		if (1 > attr.cap.max_send_sge) {
-			_HFI_UDDBG( "Limited to %d SQ SGEs\n",
+		if (2 > attr.cap.max_send_sge) {
+			_HFI_PRDBG( "Limited to %d SQ SGEs\n",
 				attr.cap.max_send_sge);
 		}
 		if (ep->hfi_num_recv_wqes/VERBS_RECV_QP_FRACTION > attr.cap.max_recv_wr) {
-			_HFI_UDDBG( "Limited to %d RQ WQEs, requested %u\n",
+			_HFI_PRDBG( "Limited to %d RQ WQEs, requested %u\n",
 				attr.cap.max_recv_wr, ep->hfi_num_recv_wqes/VERBS_RECV_QP_FRACTION);
 		} else {
-			_HFI_UDDBG("RQ WQEs: %u\n", attr.cap.max_recv_wr);
+			_HFI_PRDBG("RQ WQEs: %u\n", attr.cap.max_recv_wr);
 		}
 		if (1 > attr.cap.max_recv_sge) {
-			_HFI_UDDBG( "Limited to %d RQ SGEs\n",
+			_HFI_PRDBG( "Limited to %d RQ SGEs\n",
 				attr.cap.max_recv_sge);
 		}
 	} else {
 		// QP adjusted values due to HW limits
 		if (ep->hfi_num_send_rdma+1 > attr.cap.max_send_wr) {
-			_HFI_UDDBG( "Limited to %d SQ WQEs, requested %u\n",
+			_HFI_PRDBG( "Limited to %d SQ WQEs, requested %u\n",
 				attr.cap.max_send_wr, ep->hfi_num_send_rdma+1);
 		} else {
-			_HFI_UDDBG("SQ WQEs: %u\n", attr.cap.max_send_wr);
+			_HFI_PRDBG("SQ WQEs: %u\n", attr.cap.max_send_wr);
 		}
 		if (1 > attr.cap.max_send_sge) {
-			_HFI_UDDBG( "Limited to %d SQ SGEs\n",
+			_HFI_PRDBG( "Limited to %d SQ SGEs\n",
 				attr.cap.max_send_sge);
 		}
 		if (HFI_TF_NFLOWS+1 > attr.cap.max_recv_wr) {
-			_HFI_UDDBG( "Limited to %d RQ WQEs, requested %u\n",
+			_HFI_PRDBG( "Limited to %d RQ WQEs, requested %u\n",
 				attr.cap.max_recv_wr, HFI_TF_NFLOWS+1);
 		} else {
-			_HFI_UDDBG("RQ WQEs: %u\n", attr.cap.max_recv_wr);
+			_HFI_PRDBG("RQ WQEs: %u\n", attr.cap.max_recv_wr);
 		}
 	}
 
@@ -1799,7 +1848,7 @@ psm2_error_t modify_rc_qp_to_init(psm2_ep_t ep, struct ibv_qp *qp)
 
 	if (ibv_modify_qp(qp, &attr, flags)) {
 		_HFI_ERROR( "Failed to modify RC QP to INIT on %s: %s\n",
-					ep->verbs_ep.ib_devname, strerror(errno));
+					ep->dev_name, strerror(errno));
 		return PSM2_INTERNAL_ERR;
 	}
 	_HFI_MMDBG("moved %d to INIT\n", qp->qp_num);
@@ -1828,13 +1877,13 @@ psm2_error_t modify_rc_qp_to_rtr(psm2_ep_t ep, struct ibv_qp *qp,
 	attr.rq_psn = initpsn;
 	flags |= (IBV_QP_PATH_MTU | IBV_QP_DEST_QPN | IBV_QP_RQ_PSN);
 
-	_HFI_UDDBG("set max_dest_rd_atomic to %u\n", attr.max_dest_rd_atomic);
+	_HFI_PRDBG("set max_dest_rd_atomic to %u\n", attr.max_dest_rd_atomic);
 	attr.min_rnr_timer = 12;	// TBD well known
 	flags |= (IBV_QP_MIN_RNR_TIMER | IBV_QP_MAX_DEST_RD_ATOMIC);
 
 	if (ibv_modify_qp(qp, &attr, flags)) {
 		_HFI_ERROR( "Failed to modify RC QP to RTR on %s: %s\n",
-					ep->verbs_ep.ib_devname, strerror(errno));
+					ep->dev_name, strerror(errno));
 		return PSM2_INTERNAL_ERR;
 	}
 	_HFI_MMDBG("moved %d to RTR\n", qp->qp_num);
@@ -1855,7 +1904,7 @@ psm2_error_t modify_rc_qp_to_rts(psm2_ep_t ep, struct ibv_qp *qp,
 	attr.sq_psn = initpsn;	// value we told other side
 	flags |= IBV_QP_SQ_PSN;
 
-	_HFI_UDDBG("set max_rd_atomic to %u\n", attr.max_rd_atomic);
+	_HFI_PRDBG("set max_rd_atomic to %u\n", attr.max_rd_atomic);
 	flags |=  IBV_QP_MAX_QP_RD_ATOMIC;
 
 	attr.retry_cnt = ep->hfi_qp_retry;
@@ -1866,7 +1915,7 @@ psm2_error_t modify_rc_qp_to_rts(psm2_ep_t ep, struct ibv_qp *qp,
 	_HFI_MMDBG("moving %d to RTS\n", qp->qp_num);
 	if (ibv_modify_qp(qp, &attr, flags)) {
 		_HFI_ERROR( "Failed to modify RC QP to RTS on %s: %s\n",
-						ep->verbs_ep.ib_devname, strerror(errno));
+						ep->dev_name, strerror(errno));
 		return PSM2_INTERNAL_ERR;
 	}
 	//__psm2_dump_verbs_qp(qp);
@@ -1916,7 +1965,7 @@ __psm2_dump_verbs_ep(psm2_ep_t ep, unsigned igid)
 	struct psm2_verbs_ep *vep = &(ep->verbs_ep);
 	union ibv_gid gid;
 
-	printf("ib_devname = %s\n", vep->ib_devname);
+	printf("ib_devname = %s\n", ep->dev_name);
 	printf("qp_num     = %u\n", vep->qp->qp_num);
 	printf("GID        = ");
 	if (0 == ibv_query_gid(vep->context, ep->portnum, igid, &gid)) {
@@ -2058,6 +2107,19 @@ static enum psm_ibv_rate verbs_get_rate(uint8_t width, uint8_t speed)
 				_HFI_ERROR( "unknown link speed 0x%x\n", speed);
 				return PSM_IBV_RATE_100_GBPS;
 		}
+	case 16: /* 2x */
+		switch (speed) {
+		case 1: return PSM_IBV_RATE_5_GBPS;
+		case 2: return PSM_IBV_RATE_10_GBPS;
+		case 4: /* fall through */
+		case 8: return PSM_IBV_RATE_20_GBPS;
+		case 16: return PSM_IBV_RATE_28_GBPS;
+		case 32: return PSM_IBV_RATE_50_GBPS;
+		case 64: return PSM_IBV_RATE_100_GBPS;
+		default:
+				_HFI_ERROR( "unknown link speed 0x%x\n", speed);
+				return PSM_IBV_RATE_100_GBPS;
+		}
 	default:
 		_HFI_ERROR( "unknown link width 0x%x\n", width);
 		return PSM_IBV_RATE_100_GBPS;
diff --git a/prov/psm3/psm3/psm_verbs_ep.h b/prov/psm3/psm3/psm_verbs_ep.h
index 360e298..5e4749b 100644
--- a/prov/psm3/psm3/psm_verbs_ep.h
+++ b/prov/psm3/psm3/psm_verbs_ep.h
@@ -260,9 +260,7 @@ typedef struct psm2_verbs_send_allocator *psm2_verbs_send_allocator_t;
 // when USE_RC, we need a separate recv pool per QP so we can prepost bufs.
 struct psm2_verbs_recv_pool {
 	struct ibv_qp *qp;	// secondary reference to QP these buffers are for
-#ifdef PSM_FI
 	psm2_ep_t ep;
-#endif
 	// our preregistered recv buffers
 	uint32_t recv_buffer_size;
 	uint32_t recv_total;
@@ -287,7 +285,6 @@ typedef struct psm2_verbs_recv_pool *psm2_verbs_recv_pool_t;
 // TODO - later could optimize cache hit rates by putting some of the less
 // frequently used fields in a different part of psm2_ep struct
 struct psm2_verbs_ep {
-	char   *ib_devname;
 	//struct ibv_device *ib_dev;
 	struct ibv_context *context;
 	struct ibv_port_attr port_attr;
@@ -337,6 +334,7 @@ struct psm2_verbs_ep {
 #define recv_buffer_index(pool, buf) (((buf)-(pool)->recv_buffers)/(pool)->recv_buffer_size)
 
 extern psm2_error_t __psm2_ep_open_verbs(psm2_ep_t ep, int unit, int port, psm2_uuid_t const job_key);
+extern void __psm2_ep_initstats_verbs(psm2_ep_t ep);
 extern void __psm2_ep_free_verbs(psm2_ep_t ep);
 extern psm2_error_t __psm2_ep_initialize_queues(psm2_ep_t ep);
 extern struct ibv_qp* rc_qp_create(psm2_ep_t ep, void *context,
@@ -384,7 +382,6 @@ extern psm2_error_t psm2_verbs_post_rv_rdma_write_immed(psm2_ep_t ep,
 
 extern psm2_error_t psm2_verbs_completion_update(psm2_ep_t ep);
 
-extern void __psm2_dump_buf(uint8_t *buf, uint32_t len);
 extern int __psm2_nonzero_gid(const union ibv_gid *gid);
 extern char *__psm2_dump_gid(union ibv_gid *gid, char *buf, size_t bufsize);
 extern void __psm2_dump_verbs_qp(struct ibv_qp *qp);
diff --git a/prov/psm3/psm3/psm_verbs_mr.c b/prov/psm3/psm3/psm_verbs_mr.c
index 845f46c..bb77cbb 100644
--- a/prov/psm3/psm3/psm_verbs_mr.c
+++ b/prov/psm3/psm3/psm_verbs_mr.c
@@ -68,6 +68,7 @@
 #ifdef RNDV_MOD
 #include "psm_rndv_mod.h"
 #endif
+#include "psm2_hal.h"
 #ifdef PSM_FI
 #include "ips_config.h"
 #endif
@@ -113,7 +114,7 @@ struct psm2_mr_cache_map_pl {
 // cl_map_item_t and cl_qmap_t structures
 #define RBTREE_MI_PL  struct psm2_verbs_mr
 #define RBTREE_MAP_PL struct psm2_mr_cache_map_pl
-#include "rbtree.h"
+#include "psm3_rbtree.h"
 
 struct psm2_mr_cache {
 	uint32_t max_entries;
@@ -121,12 +122,13 @@ struct psm2_mr_cache {
 	uint32_t limit_inuse;
 	uint64_t limit_inuse_bytes;
 #ifdef RNDV_MOD
+#ifdef PSM_CUDA
+	uint64_t limit_gpu_inuse_bytes;
+#endif
 	psm2_rv_t rv;
 	int cmd_fd;
 #endif
-#ifdef PSM_FI
 	psm2_ep_t ep;
-#endif
 	uint8_t cache_mode;	// MR_CACHE_MODE_*
 	cl_qmap_t map;
 	cl_map_item_t root;
@@ -149,11 +151,21 @@ struct psm2_mr_cache {
 	uint32_t max_inuse;
 	uint64_t inuse_bytes;
 	uint64_t max_inuse_bytes;
+#ifdef RNDV_MOD
+#ifdef PSM_CUDA
+	uint64_t gpu_inuse_bytes;
+	uint64_t max_gpu_inuse_bytes;
+#endif
+#endif
 	uint32_t max_nelems;
 	uint32_t max_refcount;
 #ifdef RNDV_MOD
 	struct psm2_rv_cache_stats rv_stats;	// statistics from rv module
 									// will remain 0 if rv not open
+#ifdef PSM_CUDA
+	struct psm2_rv_gpu_cache_stats rv_gpu_stats;	// GPU statistics from rv module
+									// will remain 0 if rv not open
+#endif
 #endif
 };
 
@@ -189,7 +201,7 @@ static int mr_cache_key_cmp(const struct psm2_verbs_mr *a,
 #define RBTREE_ASSERT                     psmi_assert
 #define RBTREE_MAP_COUNT(PAYLOAD_PTR)     ((PAYLOAD_PTR)->nelems)
 #define RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR
-#include "rbtree.c"
+#include "psm3_rbtree.c"
 
 // TBD - move to a utility macro header
 // taken fron IbAccess imath.h and imath.c
@@ -318,6 +330,146 @@ static uint64_t mr_cache_rv_miss_rate(void *context)
 	else
 		return 0;
 }
+
+#ifdef PSM_CUDA
+static uint64_t mr_cache_rv_gpu_size(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	if (cache->rv && PSMI_IS_CUDA_ENABLED ) {
+		// this is a little sly, we know the stats processing routines will
+		// call the accessors in the order from the entries list
+		// so we use the 1st of the rv statistics accessors to get
+		// the statistics from rv into the cache structure so other accessors
+		// can simply return the relevant value
+		(void)__psm2_rv_gpu_get_cache_stats(cache->rv, &cache->rv_gpu_stats);
+	}
+	return cache->rv_gpu_stats.cache_size/MEGABYTE;
+}
+
+#define CACHE_RV_GPU_STAT_FUNC(func, stat) \
+    static uint64_t func(void *context) \
+    { \
+		psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats); \
+		return cache->rv_gpu_stats.stat; \
+    }
+
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_reg, cache_size_reg/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_mmap, cache_size_mmap/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_both, cache_size_both/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size, max_cache_size/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size_reg, max_cache_size_reg/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size_mmap, max_cache_size_mmap/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size_both, max_cache_size_both/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_limit_size, limit_cache_size)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_nelems, count)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_nelems_reg, count_reg)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_nelems_mmap, count_mmap)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_nelems_both, count_both)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_nelems, max_count)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_nelems_reg, max_count_reg)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_nelems_mmap, max_count_mmap)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_nelems_both, max_count_both)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_inuse, inuse)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_inuse_reg, inuse_reg)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_inuse_mmap, inuse_mmap)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_inuse_both, inuse_both)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_inuse, max_inuse)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_inuse_reg, max_inuse_reg)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_inuse_mmap, max_inuse_mmap)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_inuse_both, max_inuse_both)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_refcount, max_refcount)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_refcount_reg, max_refcount_reg)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_refcount_mmap, max_refcount_mmap)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_refcount_both, max_refcount_both)
+#undef CACHE_RV_GPU_STAT_FUNC
+
+/* any hit which found an entry, even if partial */
+static uint64_t mr_cache_rv_gpu_hit_rate(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	if (cache->rv_gpu_stats.miss)	// all entries start with a miss, then get hits
+		return((cache->rv_gpu_stats.hit*100)/(cache->rv_gpu_stats.miss+cache->rv_gpu_stats.hit));
+	else
+		return 0;
+}
+
+/* pure hit, want MR and found entry w/ MR */
+static uint64_t mr_cache_rv_gpu_hit_rate_reg(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	// all entries start with a miss or add_reg, then get hits
+	if (cache->rv_gpu_stats.miss_reg || cache->rv_gpu_stats.hit_add_reg)
+		return((cache->rv_gpu_stats.hit_reg*100)/(cache->rv_gpu_stats.miss_reg+cache->rv_gpu_stats.hit_reg+cache->rv_gpu_stats.hit_add_reg));
+	else
+		return 0;
+}
+
+/* partial hit, want MR and found pinned entry w/o MR */
+static uint64_t mr_cache_rv_gpu_hit_rate_add_reg(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	// all entries start with a miss or add_reg, then get hits
+	if (cache->rv_gpu_stats.miss_reg || cache->rv_gpu_stats.hit_add_reg)
+		return((cache->rv_gpu_stats.hit_add_reg*100)/(cache->rv_gpu_stats.miss_reg+cache->rv_gpu_stats.hit_reg+cache->rv_gpu_stats.hit_add_reg));
+	else
+		return 0;
+}
+
+/* pure hit, want mmap and found entry w/ mmap */
+static uint64_t mr_cache_rv_gpu_hit_rate_mmap(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	// all entries start with a miss or add_mmap, then get hits
+	if (cache->rv_gpu_stats.miss_mmap || cache->rv_gpu_stats.hit_add_mmap)
+		return((cache->rv_gpu_stats.hit_mmap*100)/(cache->rv_gpu_stats.miss_mmap+cache->rv_gpu_stats.hit_mmap+cache->rv_gpu_stats.hit_add_mmap));
+	else
+		return 0;
+}
+
+/* partial hit, want MR and found pinned entry w/o mmap */
+static uint64_t mr_cache_rv_gpu_hit_rate_add_mmap(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	// all entries start with a miss or add_mmap, then get hits
+	if (cache->rv_gpu_stats.miss_mmap || cache->rv_gpu_stats.hit_add_mmap)
+		return((cache->rv_gpu_stats.hit_add_mmap*100)/(cache->rv_gpu_stats.miss_mmap+cache->rv_gpu_stats.hit_mmap+cache->rv_gpu_stats.hit_add_mmap));
+	else
+		return 0;
+}
+
+/* complete miss, no entry found */
+static uint64_t mr_cache_rv_gpu_miss_rate(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	if (cache->rv_gpu_stats.miss)	// all entries start with a miss, then get hits
+		return((cache->rv_gpu_stats.miss*100)/(cache->rv_gpu_stats.miss+cache->rv_gpu_stats.hit));
+	else
+		return 0;
+}
+
+/* no entry found when want MR */
+static uint64_t mr_cache_rv_gpu_miss_rate_reg(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	// all entries start with a miss or add_reg, then get hits
+	if (cache->rv_gpu_stats.miss_reg || cache->rv_gpu_stats.hit_add_reg)
+		return((cache->rv_gpu_stats.miss_reg*100)/(cache->rv_gpu_stats.miss_reg+cache->rv_gpu_stats.hit_reg+cache->rv_gpu_stats.hit_add_reg));
+	else
+		return 0;
+}
+
+/* no entry found when want mmap */
+static uint64_t mr_cache_rv_gpu_miss_rate_mmap(void *context)
+{
+	psm2_mr_cache_t cache = container_of(context, struct psm2_mr_cache, rv_gpu_stats);
+	// all entries start with a miss or add_reg, then get hits
+	if (cache->rv_gpu_stats.miss_mmap || cache->rv_gpu_stats.hit_add_mmap)
+		return((cache->rv_gpu_stats.miss_mmap*100)/(cache->rv_gpu_stats.miss_mmap+cache->rv_gpu_stats.hit_mmap+cache->rv_gpu_stats.hit_add_mmap));
+	else
+		return 0;
+}
+#endif // PSM_CUDA
+
 #endif // RNDV_MOD
 
 #define INC_STAT(cache, stat, max_stat) \
@@ -336,7 +488,11 @@ static uint64_t mr_cache_rv_miss_rate(void *context)
 // ep is used for RNDV_MOD, memory tracking and stats
 psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 							uint32_t max_entries, uint8_t cache_mode,
-							uint32_t pri_entries, uint64_t pri_size)
+							uint32_t pri_entries, uint64_t pri_size
+#ifdef PSM_CUDA
+							, uint64_t gpu_pri_size
+#endif
+							)
 {
 	struct psm2_mr_cache *cache;
 
@@ -351,18 +507,45 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 	cache->cache_mode = cache_mode;
 	// we leave headroom for priority registrations
 	cache->limit_inuse = max_entries - pri_entries;
-#ifdef PSM_FI
 	cache->ep = ep;
-#endif
 #ifdef RNDV_MOD
 	if (cache->cache_mode == MR_CACHE_MODE_KERNEL
 		|| cache->cache_mode == MR_CACHE_MODE_RV) {
-		if (ep->rv_mr_cache_size*MEGABYTE < pri_size) {
+		// TBD - could make this a warning and set limit_inuse_bytes=0
+		// then depend on transfers queuing and retrying until
+		// reg_mr cache space is available
+		if ((uint64_t)ep->rv_mr_cache_size*MEGABYTE < pri_size) {
 			_HFI_ERROR("PSM3_RV_MR_CACHE_SIZE=%u too small, require >= %"PRIu64"\n",
 				ep->rv_mr_cache_size, (pri_size + MEGABYTE-1)/MEGABYTE);
 			return NULL;
 		}
-		cache->limit_inuse_bytes = ep->rv_mr_cache_size*MEGABYTE - pri_size;
+		cache->limit_inuse_bytes = (uint64_t)ep->rv_mr_cache_size*MEGABYTE - pri_size;
+#ifdef PSM_CUDA
+		if (PSMI_IS_CUDA_ENABLED) {
+			// For GPU, due to GdrCopy, we can't undersize cache.
+			// Otherwise RDMA MRs could consume all the
+			// cache space and leave a gdrcopy pin/mmap stuck
+			// retrying indefinitely.  If we want to allow undersize
+			// GPU cache, we need to have gdrcopy pin/mmap failures
+			// also invoke progress functions to release MRs
+			if (__psm2_min_gpu_bar_size()) {
+				uint64_t max_recommend = __psm2_min_gpu_bar_size() - 32*MEGABYTE;
+				if ((uint64_t)ep->rv_gpu_cache_size*MEGABYTE >= max_recommend) {
+					_HFI_INFO("Warning: PSM3_RV_GPU_CACHE_SIZE=%u too large for smallest GPU's BAR size of %"PRIu64" (< %"PRIu64" total of endpoint-rail-qp recommended)\n",
+						ep->rv_gpu_cache_size,
+						(__psm2_min_gpu_bar_size() + MEGABYTE-1)/MEGABYTE,
+						max_recommend/MEGABYTE);
+				}
+			}
+			if ((uint64_t)ep->rv_gpu_cache_size*MEGABYTE < gpu_pri_size) {
+				_HFI_ERROR("PSM3_RV_GPU_CACHE_SIZE=%u too small, require >= %"PRIu64"\n",
+					ep->rv_gpu_cache_size, (gpu_pri_size + MEGABYTE-1)/MEGABYTE);
+				return NULL;
+			}
+			cache->limit_gpu_inuse_bytes = (uint64_t)ep->rv_gpu_cache_size*MEGABYTE - gpu_pri_size;
+		}
+		_HFI_MMDBG("CPU cache %u GPU cache %u\n", ep->rv_mr_cache_size, ep->rv_gpu_cache_size);
+#endif
 	} else
 #endif // RNDV_MOD
 		cache->limit_inuse_bytes = UINT64_MAX;	// no limit, just count inuse
@@ -370,9 +553,16 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 	cache->rv = ep->verbs_ep.rv;
 	cache->cmd_fd = ep->verbs_ep.context->cmd_fd;
 #endif // RNDV_MOD
+#if defined(RNDV_MOD) && defined(PSM_CUDA)
+	_HFI_MMDBG("cache alloc: max_entries=%u limit_inuse=%u limit_inuse_bytes=%"PRIu64" limit_gpu_inuse_bytes=%"PRIu64", pri_entries=%u pri_size=%"PRIu64" gpu_pri_size=%"PRIu64"\n",
+			cache->max_entries, cache->limit_inuse,
+			cache->limit_inuse_bytes, cache->limit_gpu_inuse_bytes,
+			pri_entries, pri_size, gpu_pri_size);
+#else
 	_HFI_MMDBG("cache alloc: max_entries=%u limit_inuse=%u limit_inuse_bytes=%"PRIu64", pri_entries=%u pri_size=%"PRIu64"\n",
 			cache->max_entries, cache->limit_inuse,
 			cache->limit_inuse_bytes, pri_entries, pri_size);
+#endif
 	// max_entries must be power of 2>= obj per chunk which is also a power of 2
 	cache->mr_pool = psmi_mpool_create(sizeof(cl_map_item_t),
 						min(128, max_entries), max_entries, 0,
@@ -402,6 +592,15 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 				NULL, &cache->limit_inuse_bytes),
 		PSMI_STATS_DECLU64("inuse_bytes", &cache->inuse_bytes),
 		PSMI_STATS_DECLU64("max_inuse_bytes", &cache->max_inuse_bytes),
+#ifdef RNDV_MOD
+#ifdef PSM_CUDA
+		PSMI_STATS_DECL("limit_gpu_inuse_bytes",
+				MPSPAWN_STATS_REDUCTION_ALL,
+				NULL, &cache->limit_gpu_inuse_bytes),
+		PSMI_STATS_DECLU64("gpu_inuse_bytes", &cache->gpu_inuse_bytes),
+		PSMI_STATS_DECLU64("max_gpu_inuse_bytes", &cache->max_gpu_inuse_bytes),
+#endif
+#endif
 		PSMI_STATS_DECL_FUNC("max_refcount", mr_cache_max_refcount),
 		PSMI_STATS_DECLU64("hit", &cache->hit),
 		PSMI_STATS_DECL("hit_%",MPSPAWN_STATS_REDUCTION_ALL,
@@ -424,10 +623,10 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 		PSMI_STATS_DECLU64("rv_max_inuse_bytes", (uint64_t*)&cache->rv_stats.max_inuse_bytes),
 		PSMI_STATS_DECL_FUNC("rv_max_refcount", mr_cache_rv_max_refcount),
 		PSMI_STATS_DECLU64("rv_hit", (uint64_t*)&cache->rv_stats.hit),
-		PSMI_STATS_DECL("rv_hit %", MPSPAWN_STATS_REDUCTION_ALL,
+		PSMI_STATS_DECL("rv_hit_%", MPSPAWN_STATS_REDUCTION_ALL,
 				mr_cache_rv_hit_rate, NULL),
 		PSMI_STATS_DECLU64("rv_miss", (uint64_t*)&cache->rv_stats.miss),
-		PSMI_STATS_DECL("rv_miss %", MPSPAWN_STATS_REDUCTION_ALL,
+		PSMI_STATS_DECL("rv_miss_%", MPSPAWN_STATS_REDUCTION_ALL,
 				mr_cache_rv_miss_rate, NULL),
 		PSMI_STATS_DECLU64("rv_full", (uint64_t*)&cache->rv_stats.full),
 		PSMI_STATS_DECLU64("rv_failed", (uint64_t*)&cache->rv_stats.failed),
@@ -439,18 +638,300 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 					PSMI_STATSTYPE_MR_CACHE,
 					entries,
 					PSMI_STATS_HOWMANY(entries),
-					ep->epid, cache);
+					ep->epid, cache, ep->dev_name);
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+	struct psmi_stats_entry gpu_entries[] = {
+		PSMI_STATS_DECL_FUNC("rv_gpu_size", mr_cache_rv_gpu_size),
+		PSMI_STATS_DECL_FUNC("rv_gpu_size_reg", mr_cache_rv_gpu_size_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_size_mmap", mr_cache_rv_gpu_size_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_size_both", mr_cache_rv_gpu_size_both),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_size", mr_cache_rv_gpu_max_size),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_size_reg", mr_cache_rv_gpu_max_size_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_size_mmap", mr_cache_rv_gpu_max_size_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_size_both", mr_cache_rv_gpu_max_size_both),
+		PSMI_STATS_DECL_FUNC("rv_gpu_limit", mr_cache_rv_gpu_limit_size),
+		PSMI_STATS_DECL_FUNC("rv_gpu_nelems", mr_cache_rv_gpu_nelems),
+		PSMI_STATS_DECL_FUNC("rv_gpu_nelems_reg", mr_cache_rv_gpu_nelems_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_nelems_mmap", mr_cache_rv_gpu_nelems_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_nelems_both", mr_cache_rv_gpu_nelems_both),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_nelems", mr_cache_rv_gpu_max_nelems),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_nelems_reg", mr_cache_rv_gpu_max_nelems_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_nelems_mmap", mr_cache_rv_gpu_max_nelems_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_nelems_both", mr_cache_rv_gpu_max_nelems_both),
+		PSMI_STATS_DECL_FUNC("rv_gpu_inuse", mr_cache_rv_gpu_inuse),
+		PSMI_STATS_DECL_FUNC("rv_gpu_inuse_reg", mr_cache_rv_gpu_inuse_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_inuse_mmap", mr_cache_rv_gpu_inuse_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_inuse_both", mr_cache_rv_gpu_inuse_both),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_inuse", mr_cache_rv_gpu_max_inuse),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_inuse_reg", mr_cache_rv_gpu_max_inuse_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_inuse_mmap", mr_cache_rv_gpu_max_inuse_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_inuse_both", mr_cache_rv_gpu_max_inuse_both),
+		PSMI_STATS_DECLU64("rv_gpu_inuse_bytes", (uint64_t*)&cache->rv_gpu_stats.inuse_bytes),
+		PSMI_STATS_DECLU64("rv_gpu_inuse_bytes_reg", (uint64_t*)&cache->rv_gpu_stats.inuse_bytes_reg),
+		PSMI_STATS_DECLU64("rv_gpu_inuse_bytes_mmap", (uint64_t*)&cache->rv_gpu_stats.inuse_bytes_mmap),
+		PSMI_STATS_DECLU64("rv_gpu_inuse_bytes_both", (uint64_t*)&cache->rv_gpu_stats.inuse_bytes_both),
+		PSMI_STATS_DECLU64("rv_gpu_max_inuse_bytes", (uint64_t*)&cache->rv_gpu_stats.max_inuse_bytes),
+		PSMI_STATS_DECLU64("rv_gpu_max_inuse_bytes_reg", (uint64_t*)&cache->rv_gpu_stats.max_inuse_bytes_reg),
+		PSMI_STATS_DECLU64("rv_gpu_max_inuse_bytes_mmap", (uint64_t*)&cache->rv_gpu_stats.max_inuse_bytes_mmap),
+		PSMI_STATS_DECLU64("rv_gpu_max_inuse_bytes_both", (uint64_t*)&cache->rv_gpu_stats.max_inuse_bytes_both),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_refcount", mr_cache_rv_gpu_max_refcount),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_refcount_reg", mr_cache_rv_gpu_max_refcount_reg),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_refcount_mmap", mr_cache_rv_gpu_max_refcount_mmap),
+		PSMI_STATS_DECL_FUNC("rv_gpu_max_refcount_both", mr_cache_rv_gpu_max_refcount_both),
+		PSMI_STATS_DECLU64("rv_gpu_hit", (uint64_t*)&cache->rv_gpu_stats.hit),
+		PSMI_STATS_DECL("rv_gpu_hit_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_hit_rate, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_hit_reg", (uint64_t*)&cache->rv_gpu_stats.hit_reg),
+		PSMI_STATS_DECL("rv_gpu_hit_reg_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_hit_rate_reg, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_hit_add_reg", (uint64_t*)&cache->rv_gpu_stats.hit_add_reg),
+		PSMI_STATS_DECL("rv_gpu_hit_add_reg_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_hit_rate_add_reg, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_hit_mmap", (uint64_t*)&cache->rv_gpu_stats.hit_mmap),
+		PSMI_STATS_DECL("rv_gpu_hit_mmap_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_hit_rate_mmap, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_hit_add_mmap", (uint64_t*)&cache->rv_gpu_stats.hit_add_mmap),
+		PSMI_STATS_DECL("rv_gpu_hit_add_mmap_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_hit_rate_add_mmap, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_miss", (uint64_t*)&cache->rv_gpu_stats.miss),
+		PSMI_STATS_DECL("rv_gpu_miss_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_miss_rate, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_miss_reg", (uint64_t*)&cache->rv_gpu_stats.miss_reg),
+		PSMI_STATS_DECL("rv_gpu_miss_reg_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_miss_rate_reg, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_miss_mmap", (uint64_t*)&cache->rv_gpu_stats.miss_mmap),
+		PSMI_STATS_DECL("rv_gpu_miss_mmap_%", MPSPAWN_STATS_REDUCTION_ALL,
+				mr_cache_rv_gpu_miss_rate_mmap, NULL),
+		PSMI_STATS_DECLU64("rv_gpu_full", (uint64_t*)&cache->rv_gpu_stats.full),
+		PSMI_STATS_DECLU64("rv_gpu_full_reg", (uint64_t*)&cache->rv_gpu_stats.full_reg),
+		PSMI_STATS_DECLU64("rv_gpu_full_mmap", (uint64_t*)&cache->rv_gpu_stats.full_mmap),
+		PSMI_STATS_DECLU64("rv_gpu_failed_pin", (uint64_t*)&cache->rv_gpu_stats.failed_pin),
+		PSMI_STATS_DECLU64("rv_gpu_failed_reg", (uint64_t*)&cache->rv_gpu_stats.failed_reg),
+		PSMI_STATS_DECLU64("rv_gpu_failed_mmap", (uint64_t*)&cache->rv_gpu_stats.failed_mmap),
+		PSMI_STATS_DECLU64("rv_gpu_remove", (uint64_t*)&cache->rv_gpu_stats.remove),
+		PSMI_STATS_DECLU64("rv_gpu_remove_reg", (uint64_t*)&cache->rv_gpu_stats.remove_reg),
+		PSMI_STATS_DECLU64("rv_gpu_remove_mmap", (uint64_t*)&cache->rv_gpu_stats.remove_mmap),
+		PSMI_STATS_DECLU64("rv_gpu_remove_both", (uint64_t*)&cache->rv_gpu_stats.remove_both),
+		PSMI_STATS_DECLU64("rv_gpu_evict", (uint64_t*)&cache->rv_gpu_stats.evict),
+		PSMI_STATS_DECLU64("rv_gpu_evict_reg", (uint64_t*)&cache->rv_gpu_stats.evict_reg),
+		PSMI_STATS_DECLU64("rv_gpu_evict_mmap", (uint64_t*)&cache->rv_gpu_stats.evict_mmap),
+		PSMI_STATS_DECLU64("rv_gpu_evict_both", (uint64_t*)&cache->rv_gpu_stats.evict_both),
+		PSMI_STATS_DECLU64("rv_gpu_inval_mr", (uint64_t*)&cache->rv_gpu_stats.inval_mr),
+		PSMI_STATS_DECLU64("rv_post_write", (uint64_t*)&cache->rv_gpu_stats.post_write),
+		PSMI_STATS_DECLU64("rv_post_write_bytes", (uint64_t*)&cache->rv_gpu_stats.post_write_bytes),
+		PSMI_STATS_DECLU64("rv_gpu_post_write", (uint64_t*)&cache->rv_gpu_stats.gpu_post_write),
+		PSMI_STATS_DECLU64("rv_gpu_post_write_bytes", (uint64_t*)&cache->rv_gpu_stats.gpu_post_write_bytes),
+	};
+	if (cache->rv && PSMI_IS_CUDA_ENABLED)
+		psmi_stats_register_type("MR_GPU_Cache_Statistics",
+					PSMI_STATSTYPE_MR_CACHE,
+					gpu_entries,
+					PSMI_STATS_HOWMANY(gpu_entries),
+					ep->epid, &cache->rv_gpu_stats,
+					ep->dev_name);
+#endif
+#endif
 
 	return cache;
 }
 
+int psm2_verbs_mr_cache_allows_user_mr(psm2_mr_cache_t cache)
+{
+	if (!cache)
+		return 0;
+	switch (cache->cache_mode) {
+		case MR_CACHE_MODE_NONE:
+			return 0;
+		case MR_CACHE_MODE_KERNEL:
+			return psmi_hal_has_cap(PSM_HAL_CAP_USER_MR);
+		case MR_CACHE_MODE_USER:
+			return 1;
+		case MR_CACHE_MODE_RV:
+			return psmi_hal_has_cap(PSM_HAL_CAP_USER_MR);
+		default:	// unexpected
+			return 0;
+	}
+}
+
+static void update_stats_inc_inuse(psm2_mr_cache_t cache, uint64_t length,
+					int access)
+{
+	INC_STAT(cache, inuse, max_inuse);
+#ifdef RNDV_MOD
+#ifdef PSM_CUDA
+	if (access & IBV_ACCESS_IS_GPU_ADDR)
+		ADD_STAT(cache, length, gpu_inuse_bytes, max_gpu_inuse_bytes);
+	else
+#endif
+#endif
+		ADD_STAT(cache, length, inuse_bytes, max_inuse_bytes);
+}
+
+static void update_stats_dec_inuse(psm2_mr_cache_t cache, uint64_t length,
+					int access)
+{
+	cache->inuse--;
+#ifdef RNDV_MOD
+#ifdef PSM_CUDA
+	if (access & IBV_ACCESS_IS_GPU_ADDR)
+		cache->gpu_inuse_bytes -= length;
+	else
+#endif
+#endif
+		cache->inuse_bytes -= length;
+}
+
 // checks for space for a non-priority registration
-static inline int have_space(psm2_mr_cache_t cache, uint64_t length)
+static inline int have_space(psm2_mr_cache_t cache, uint64_t length, int access)
 {
-	return (cache->inuse < cache->limit_inuse
+#ifdef RNDV_MOD
+#ifdef PSM_CUDA
+	if (access & IBV_ACCESS_IS_GPU_ADDR)
+		return (cache->inuse < cache->limit_inuse
+			&& cache->gpu_inuse_bytes + length < cache->limit_gpu_inuse_bytes);
+	else
+#endif
+#endif
+		return (cache->inuse < cache->limit_inuse
 			&& cache->inuse_bytes + length < cache->limit_inuse_bytes);
 }
 
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+// given an ep this returns the "next one".
+// It loops through all the multi-rail/multi-QP EPs in a given user opened EP
+// 1st, then it goes to the next user opened EP (multi-EP) and loops through
+// it's multi-rail/mult-QP EPs.
+// When it hits the last rail of the last user opened EP, it goes back to
+// the 1st rail of the 1st user opened EP.
+// caller must hold creation_lock
+static psm2_ep_t next_ep(psm2_ep_t ep)
+{
+       //mctxt_next is the circular list of rails/QPs in a given user EP
+       //mctxt_master is the 1st in the list, when we get back to the 1st
+       //go to the next user EP
+       ep = ep->mctxt_next;
+       if (ep->mctxt_master != ep)
+               return ep;
+       //user_ep_next is a linked list of user opened EPs.  End of list is NULL
+       //when hit end of list, go back to 1st (psmi_opened_endpoint)
+       //for each user opened EP, the entry on this list is the 1st rail within
+       //the EP
+       ep = ep->user_ep_next;
+       if (ep)
+               return ep;
+       else
+               return psmi_opened_endpoint;
+}
+
+// determine if ep is still valid (can't dereference or trust ep given)
+// caller must hold creation_lock
+static int valid_ep(psm2_ep_t ep)
+{
+	psm2_ep_t e1 = psmi_opened_endpoint;
+
+	while (e1) {	// user opened ep's - linear list ending in NULL
+		psm2_ep_t e2 = e1;
+		//check mtcxt list (multi-rail within user opened ep)
+		do {
+			if (e2 == ep)
+				return 1;
+			e2 = e2->mctxt_next;
+		} while (e2 != e1);	// circular list
+		e1 = e1->user_ep_next;
+	}
+	return 0;	// not found
+}
+
+// advance ep to the next.  However it's possible ep is stale and
+// now closed/freed, so make sure it's good.  good_ep is at least one
+// known good_ep and lets us avoid search some of the time (or if only 1 EP)
+// caller must hold creation_lock
+static psm2_ep_t next_valid_ep(psm2_ep_t ep, psm2_ep_t good_ep)
+{
+	if (ep == good_ep || valid_ep(ep))
+		return next_ep(ep);
+	else
+		return good_ep;
+}
+
+/*
+ * Evict some space in given cache (only GPU needs this)
+ * If nvidia_p2p_get_pages reports out of BAR space (perhaps prematurely),
+ * we need to evict from other EPs too.
+ * So we rotate among all eps (rails or QPs) in our user opened EP for eviction.
+ * length - amount attempted in pin/register which just failed
+ * access - indicates if IS_GPU_ADDR or not (rest ignored)
+ * returns:
+ * 	>0 bytes evicted if some evicted
+ * 	-1 if nothing evicted (errno == ENOENT means nothing evictable found)
+ * 	ENOENT also used when access is not for GPU
+ * The caller will have the progress_lock, we need the creation_lock
+ * to walk the list of EPs outside our own MQ.  However creation_lock
+ * is above the progress_lock in lock heirarchy, so we use a LOCK_TRY
+ * to avoid deadlock in the rare case where another thread
+ * has creation_lock and is trying to get progress_lock (such as during
+ * open_ep, close_ep or rcvthread).
+ */
+int64_t psm2_verbs_evict_some(psm2_ep_t ep, uint64_t length, int access)
+{
+	static __thread psm2_ep_t last_evict_ep;	// among all eps
+	static __thread psm2_ep_t last_evict_myuser_ep;	// in my user ep
+	int64_t evicted = 0;
+	int ret;
+
+	if (! (access & IBV_ACCESS_IS_GPU_ADDR)) {
+		errno = ENOENT;
+		return -1;	// only need evictions on GPU addresses
+	}
+	if (! last_evict_ep) {	// first call only
+		last_evict_ep = ep;
+		last_evict_myuser_ep = ep;
+	}
+	// 1st try to evict from 1st rail/QP in our opened EP (gdrcopy and MRs)
+	ret = __psm2_rv_evict_gpu_amount(ep->mctxt_master->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+	if (ret > 0)
+		evicted = ret;
+
+	// next rotate among other rails/QPs in our opened ep (MRs)
+	last_evict_myuser_ep = last_evict_myuser_ep->mctxt_next;
+	if (last_evict_myuser_ep != ep->mctxt_master) {
+		ret = __psm2_rv_evict_gpu_amount(last_evict_myuser_ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+		if (ret > 0)
+			evicted += ret;
+	}
+	if (evicted >= length)
+		return evicted;
+
+	// now try other opened EPs
+	if (PSMI_LOCK_TRY(psmi_creation_lock))
+		goto done;
+	// last_evict_ep could point to an ep which has since been closed/freed
+ 	last_evict_ep = next_valid_ep(last_evict_ep, ep);
+	if (last_evict_ep->mctxt_master != ep->mctxt_master) {
+		if (!PSMI_LOCK_TRY(last_evict_ep->mq->progress_lock)) {
+			ret = __psm2_rv_evict_gpu_amount(last_evict_ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+			PSMI_UNLOCK(last_evict_ep->mq->progress_lock);
+			if (ret > 0)
+				evicted += ret;
+		}
+	} else {
+		ret = __psm2_rv_evict_gpu_amount(last_evict_ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+		if (ret > 0 )
+			evicted += ret;
+	}
+	PSMI_UNLOCK(psmi_creation_lock);
+done:
+	if (! evicted) {
+		errno = ENOENT;
+		return -1;
+	}
+	return evicted;
+}
+#endif
+#endif
+
 // each attempt will increment exactly one of: hit, miss, rejected, full, failed
 struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 				bool priority, struct ibv_pd *pd,
@@ -463,7 +944,7 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 		PSMI_FAULTINJ_STATIC_DECL(fi_reg_mr, "reg_mr",
 				"MR cache full, any request type",
 				1, IPS_FAULTINJ_REG_MR);
-		if (psmi_faultinj_is_fault(fi_reg_mr)) {
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_reg_mr, "")) {
 			cache->failed++;
 			errno = ENOMEM;
 			return NULL;
@@ -473,7 +954,7 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 		PSMI_FAULTINJ_STATIC_DECL(fi_nonpri_reg_mr, "nonpri_reg_mr",
 				"MR cache full, non-priority request",
 				1, IPS_FAULTINJ_NONPRI_REG_MR);
-		if (psmi_faultinj_is_fault(fi_nonpri_reg_mr)) {
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_nonpri_reg_mr, "")) {
 			cache->failed++;
 			errno = ENOMEM;
 			return NULL;
@@ -483,13 +964,13 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 		PSMI_FAULTINJ_STATIC_DECL(fi_pri_reg_mr, "pri_reg_mr",
 				"MR cache full, priority request",
 				1, IPS_FAULTINJ_PRI_REG_MR);
-		if (psmi_faultinj_is_fault(fi_pri_reg_mr)) {
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_pri_reg_mr, "")) {
 			cache->failed++;
 			errno = ENOMEM;
 			return NULL;
 		}
 	}
-#endif
+#endif // PSM_FI
 	access |= IBV_ACCESS_LOCAL_WRITE;	// manditory flag
 #ifndef RNDV_MOD
 	if (access & IBV_ACCESS_IS_GPU_ADDR) {
@@ -498,23 +979,30 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 		errno = EINVAL;
 		return NULL;
 	}
+#else
+#ifdef PSM_CUDA
+	psmi_assert(!!(access & IBV_ACCESS_IS_GPU_ADDR) == (PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(addr)));
+#endif
 #endif
 	struct psm2_verbs_mr key = { // our search key
 		.addr = addr,
 		.length = length,
-		// only 8 bits in mrc for access
-		.access = (access & ~(IBV_ACCESS_IS_GPU_ADDR
+		.access = access
+	};
+	// for user QPs, can share entries with send DMA and send RDMA
 #ifdef RNDV_MOD
-								|IBV_ACCESS_KERNEL
+	if (cache->cache_mode != MR_CACHE_MODE_RV)
+		key.access &= ~IBV_ACCESS_RDMA;
+#else
+	key.access &= ~IBV_ACCESS_RDMA;
 #endif
-								))
-	};
+
 	cl_map_item_t *p_item = ips_cl_qmap_searchv(&cache->map, &key);
 	if (p_item->payload.mr.mr_ptr) {
 		psmi_assert(p_item != cache->map.nil_item);
 		mrc = &p_item->payload;
 		if (! mrc->refcount) {
-			if (! priority && ! have_space(cache, length)) {
+			if (! priority && ! have_space(cache, length, access)) {
 				_HFI_MMDBG("cache has no headroom for non-priority hit addr %p len %"PRIu64" access 0x%x ptr %p\n",
 						addr, length, access, mrc);
 				cache->rejected++;
@@ -523,18 +1011,17 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 			}
 			// it was an entry on avail_list, take off list
 			TAILQ_REMOVE(&cache->avail_list, mrc, next);
-			INC_STAT(cache, inuse, max_inuse);
-			ADD_STAT(cache, length, inuse_bytes, max_inuse_bytes);
+			update_stats_inc_inuse(cache, length, access);
 		}
 		cache->hit++;
-		_HFI_MMDBG("cache hit MR addr %p len %"PRIu64" access 0x%x ptr %p\n",
-						addr, length, mrc->access, mrc);
 		mrc->refcount++;
+		_HFI_MMDBG("cache hit MR addr %p len %"PRIu64" access 0x%x ref %d ptr %p\n",
+			addr, length, access, mrc->refcount, mrc);
 		cache->max_refcount = max(cache->max_refcount, mrc->refcount);
 		return mrc;
 	}
 	psmi_assert(p_item == cache->map.nil_item);
-	if (! priority && ! have_space(cache, length)) {
+	if (! priority && ! have_space(cache, length, access)) {
 		_HFI_MMDBG("cache has no headroom for non-priority miss addr %p len %"PRIu64" access 0x%x\n",
 			addr, length, access);
 		cache->rejected++;
@@ -557,7 +1044,7 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 		psmi_assert(mrc->mr.mr_ptr);
 		psmi_assert(! mrc->refcount);
 		_HFI_MMDBG("reuse avail MR addr %p len %"PRIu64" access 0x%x ptr %p\n",
-					addr, length, mrc->access, mrc);
+					addr, length, access, mrc);
 		ips_cl_qmap_remove_item(&mrc->cache->map, p_item);
 		TAILQ_REMOVE(&cache->avail_list, mrc, next);
 #ifdef RNDV_MOD
@@ -568,7 +1055,7 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 #endif
 			ret = ibv_dereg_mr(mrc->mr.ibv_mr);
 		if (ret) {
-			_HFI_ERROR("unexpected dreg_mr failure: %s", strerror(errno));
+			_HFI_ERROR("unexpected dreg_mr failure: %s\n", strerror(errno));
 			cache->failed++;
 			errno = EIO;
 			// MR is fouled up, we leak the MR and free the cache entry
@@ -593,13 +1080,18 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 #ifdef RNDV_MOD
 	/* need cmd_fd for access to ucontext when converting user pd into kernel pd */
 	if (cache->cache_mode == MR_CACHE_MODE_KERNEL) {
-		mrc->mr.rv_mr = __psm2_rv_reg_mem(cache->rv, cache->cmd_fd, pd, addr, length, access);
+		// user space QPs for everything, drop IBV_ACCESS_RDMA flag
+		mrc->mr.rv_mr = __psm2_rv_reg_mem(cache->rv, cache->cmd_fd, pd, addr, length, access & ~IBV_ACCESS_RDMA);
 		if (! mrc->mr.rv_mr) {
 			int save_errno = errno;
 			if (errno == ENOMEM) {
 				cache->full++;
+#ifdef PSM_CUDA
+				if (priority)
+					(void)psm2_verbs_evict_some(cache->ep, length, access);
+#endif
 			} else {
-				_HFI_ERROR("reg_mr failed; %s", strerror(errno));
+				_HFI_ERROR("reg_mr failed; %s acc 0x%x\n", strerror(errno), access);
 				cache->failed++;
 			}
 			psmi_mpool_put(p_item);
@@ -610,13 +1102,18 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 		mrc->lkey = mrc->mr.rv_mr->lkey;
 		mrc->rkey = mrc->mr.rv_mr->rkey;
 	} else if (cache->cache_mode == MR_CACHE_MODE_RV) {
-		mrc->mr.rv_mr = __psm2_rv_reg_mem(cache->rv, cache->cmd_fd, NULL, addr, length, access|IBV_ACCESS_KERNEL);
+		// kernel QP for RDMA, user QP for send DMA
+		mrc->mr.rv_mr = __psm2_rv_reg_mem(cache->rv, cache->cmd_fd, (access&IBV_ACCESS_RDMA)?NULL:pd, addr, length, access);
 		if (! mrc->mr.rv_mr) {
 			int save_errno = errno;
 			if (errno == ENOMEM) {
 				cache->full++;
+#ifdef PSM_CUDA
+				if (priority)
+					(void)psm2_verbs_evict_some(cache->ep, length, access);
+#endif
 			} else {
-				_HFI_ERROR("reg_mr failed; %s", strerror(errno));
+				_HFI_ERROR("reg_mr failed; %s acc 0x%x\n", strerror(errno), access);
 				cache->failed++;
 			}
 			psmi_mpool_put(p_item);
@@ -629,13 +1126,14 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 	} else
 #endif
 	{
-		mrc->mr.ibv_mr = ibv_reg_mr(pd, addr, length, access);
+		// user space QPs for everything, drop IBV_ACCESS_RDMA flag
+		mrc->mr.ibv_mr = ibv_reg_mr(pd, addr, length, access & ~IBV_ACCESS_RDMA);
 		if (! mrc->mr.ibv_mr) {
 			int save_errno = errno;
 			if (errno == ENOMEM) {
 				cache->full++;
 			} else {
-				_HFI_ERROR("reg_mr failed; %s", strerror(errno));
+				_HFI_ERROR("reg_mr failed; %s acc 0x%x\n", strerror(errno), access);
 				cache->failed++;
 			}
 			psmi_mpool_put(p_item);
@@ -651,13 +1149,18 @@ struct psm2_verbs_mr * psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 	mrc->refcount = 1;
 	mrc->addr = addr;
 	mrc->length = length;
-	mrc->access = access;
+#ifdef RNDV_MOD
+	if (cache->cache_mode != MR_CACHE_MODE_RV)
+		mrc->access = access & ~IBV_ACCESS_RDMA;
+	else
+		mrc->access = access;
+#else
+	mrc->access = access & ~IBV_ACCESS_RDMA;
+#endif
 	ips_cl_qmap_insert_item(&cache->map, p_item);
-	INC_STAT(cache, inuse, max_inuse);
-	ADD_STAT(cache, length, inuse_bytes, max_inuse_bytes);
-	_HFI_MMDBG("registered new MR pri %d addr %p len %"PRIu64" access 0x%x ptr %p nelems %u\n",
-					priority, addr, length, mrc->access, mrc,
-					cache->map.payload.nelems);
+	update_stats_inc_inuse(cache, length, access);
+	_HFI_MMDBG("registered new MR pri %d addr %p len %"PRIu64" access 0x%x ref %u ptr %p nelems %u\n",
+		priority, addr, length, access, mrc->refcount, mrc, cache->map.payload.nelems);
 	return mrc;
 }
 
@@ -672,23 +1175,19 @@ int psm2_verbs_release_mr(struct psm2_verbs_mr *mrc)
 		errno = ENXIO;
 		return -1;
 	}
-	_HFI_MMDBG("releasing MR addr %p len %"PRIu64" access 0x%x ref %u ptr %p\n",
-				mrc->addr, mrc->length, mrc->access,
-				mrc->refcount, mrc);
-	if (mrc->cache->cache_mode == MR_CACHE_MODE_USER) {
-		// if refcount now zero, put on avail_list to be reclaimed if needed
-		if (! --(mrc->refcount)) {
-			mrc->cache->inuse--;
-			mrc->cache->inuse_bytes -= mrc->length;
+	_HFI_MMDBG("releasing MR addr %p len %"PRIu64" access 0x%x ref %u-- ptr %p\n",
+		mrc->addr, mrc->length, mrc->access, mrc->refcount, mrc);
+	mrc->refcount--;
+	if (!mrc->refcount) {
+		if (mrc->cache->cache_mode == MR_CACHE_MODE_USER) {
+			// if refcount now zero, put on avail_list to be reclaimed if needed
+			update_stats_dec_inuse(mrc->cache, mrc->length, mrc->access);
 			TAILQ_INSERT_TAIL(&mrc->cache->avail_list, mrc, next);
-		}
-	} else {
-		if (! --(mrc->refcount)) {
+		} else {
 			_HFI_MMDBG("freeing MR addr %p len %"PRIu64" access 0x%x ref %u ptr %p nelems %u\n",
-						mrc->addr, mrc->length, mrc->access,
-						mrc->refcount, mrc, mrc->cache->map.payload.nelems);
-			mrc->cache->inuse--;
-			mrc->cache->inuse_bytes -= mrc->length;
+				mrc->addr, mrc->length, mrc->access, mrc->refcount, mrc,
+				mrc->cache->map.payload.nelems);
+			update_stats_dec_inuse(mrc->cache, mrc->length, mrc->access);
 			cl_map_item_t *p_item = container_of(mrc, cl_map_item_t, payload);
 			ips_cl_qmap_remove_item(&mrc->cache->map, p_item);
 #ifdef RNDV_MOD
@@ -701,7 +1200,7 @@ int psm2_verbs_release_mr(struct psm2_verbs_mr *mrc)
 			if (ret) {
 				// nasty choice, do we leak the MR or leak the cache entry
 				// we chose to leak the MR and free the cache entry
-				_HFI_ERROR("unexpected dreg_mr failure: %s", strerror(errno));
+				_HFI_ERROR("unexpected dreg_mr failure on %s: %s\n", mrc->cache->ep->dev_name, strerror(errno));
 				errno = EIO;
 				ret = -1;
 			}
@@ -714,6 +1213,13 @@ int psm2_verbs_release_mr(struct psm2_verbs_mr *mrc)
 
 void psm2_verbs_free_mr_cache(psm2_mr_cache_t cache)
 {
+#ifdef PSM_CUDA
+#ifdef RNDV_MOD
+	if (cache->rv && PSMI_IS_CUDA_ENABLED)
+		psmi_stats_deregister_type(PSMI_STATSTYPE_MR_CACHE,
+					&cache->rv_gpu_stats);
+#endif
+#endif
 	psmi_stats_deregister_type(PSMI_STATSTYPE_MR_CACHE, cache);
 	while (cache->map.payload.nelems) {
 		cl_map_item_t *p_item = __cl_map_root(&cache->map);
@@ -723,10 +1229,12 @@ void psm2_verbs_free_mr_cache(psm2_mr_cache_t cache)
 		if (mrc->mr.mr_ptr) {
 			int ret;
 			_HFI_MMDBG("free MR addr %p len %"PRIu64" access 0x%x ref %u ptr %p\n",
-					mrc->addr, mrc->length, mrc->access,
-					mrc->refcount, mrc);
-			if (mrc->refcount)
-				_HFI_ERROR("unreleased MR in psm2_verbs_free_mr_cache addr %p len %"PRIu64" access 0x%x\n", mrc->addr, mrc->length, mrc->access);
+				mrc->addr, mrc->length, mrc->access, mrc->refcount, mrc);
+			if (mrc->refcount) {
+				_HFI_ERROR("unreleased MR in psm2_verbs_free_mr_cache addr %p len %"PRIu64" access 0x%x\n",
+					mrc->addr, mrc->length, mrc->access);
+				return; // leak the rest, let process exit cleanup
+			}
 			mrc->refcount = 0;
 			cl_map_item_t *p_item = container_of(mrc, cl_map_item_t, payload);
 			ips_cl_qmap_remove_item(&cache->map, p_item);
@@ -739,7 +1247,7 @@ void psm2_verbs_free_mr_cache(psm2_mr_cache_t cache)
 #endif
 				ret = ibv_dereg_mr(mrc->mr.ibv_mr);
 			if (ret)
-				_HFI_ERROR("unexpected dreg_mr failure: %s", strerror(errno));
+				_HFI_ERROR("unexpected dreg_mr failure on %s: %s\n", mrc->cache->ep->dev_name, strerror(errno));
 			mrc->mr.mr_ptr = NULL;
 			psmi_mpool_put(p_item);
 		}
diff --git a/prov/psm3/psm3/psm_verbs_mr.h b/prov/psm3/psm3/psm_verbs_mr.h
index 8d26bf0..98f4ada 100644
--- a/prov/psm3/psm3/psm_verbs_mr.h
+++ b/prov/psm3/psm3/psm_verbs_mr.h
@@ -64,6 +64,11 @@
 #include <infiniband/verbs.h>
 #ifdef RNDV_MOD
 #include <psm_rndv_mod.h>
+#define IBV_ACCESS_RDMA IBV_ACCESS_KERNEL
+#else
+#define IBV_ACCESS_RDMA 0
+// pick a flag value unused by verbs.h
+#define IBV_ACCESS_IS_GPU_ADDR 0x10000000
 #endif
 
 #define MR_CACHE_MODE_NONE 0	// user space MRs, but no caching
@@ -73,7 +78,7 @@
 #define MR_CACHE_MODE_VALID(mode) ((unsigned)(mode) <= 3)
 
 // This performs memory registration for RDMA Rendezvous when PSM3_RDMA enabled
-// Priority registration calls ere those immediately before the data transfer
+// Priority registration calls are those immediately before the data transfer
 // hence delaying their registration directly delays IOs.
 // Non-priority calls are those registering the whole IO
 // prior to sending/receiving the CTS.  Delays in non-priority
@@ -92,6 +97,15 @@
 // track current inuse entries and only allow non-priority registrations when
 // we have a reasonable amount of headroom.  This way most priority
 // registrations will succeed.
+//
+// access indicates the purpose and permissions for the MR
+// IBV_ACCESS_RDMA - send RDMA
+// IBV_ACCESS_RDMA|IBV_ACCESS_REMOTE_WRITE - recv RDMA
+// 0 - send DMA
+// | IBV_ACCESS_IS_GPU_ADDR - GPU variation of any of the above 3
+// When using RV kernel QPs, IBV_ACCESS_RDMA (== IBV_ACCESS_KERNEL) is passed
+// to RV.  Otherwise, it's is omitted and a user space accessible MR is created.
+// When using only user space QPs, we allow send RDMA and send DMA to share MRs.
 
 // the pointer to psm2_verbs_mr itself is the handle for subsequenent release
 struct psm2_verbs_mr {
@@ -119,7 +133,7 @@ struct psm2_verbs_mr {
 	// also addr is used in callers to translate remote addr returned in CTS
 	void *addr;
 	uint64_t length;
-	uint8_t access;
+	uint32_t access;
 	// below is for queue of cache entries available for reuse (refcount==0)
 	// only used when cache_mode==1
 	TAILQ_ENTRY(psm2_verbs_mr) next;
@@ -132,18 +146,37 @@ typedef struct psm2_mr_cache *psm2_mr_cache_t;
 
 extern psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 				uint32_t num_entries, uint8_t cache_mode,
-				uint32_t pri_entries, uint64_t pri_size);
-// pick a flag value unused by verbs.h
-#define IBV_ACCESS_IS_GPU_ADDR 0x10000000
+				uint32_t pri_entries, uint64_t pri_size
+#ifdef PSM_CUDA
+				, uint64_t gpu_pri_size
+#endif
+				);
+extern int psm2_verbs_mr_cache_allows_user_mr(psm2_mr_cache_t cache);
+
+#ifdef PSM_CUDA
+extern int64_t psm2_verbs_evict_some(psm2_ep_t ep, uint64_t length, int access);
+#endif
+
 // pd can be the verbs_ep.pd or NULL to use the RV module's kernel pd
 extern psm2_verbs_mr_t psm2_verbs_reg_mr(psm2_mr_cache_t cache,
 				bool priority, struct ibv_pd *pd,
 				void *addr, uint64_t length, int access);
 static inline psm2_verbs_mr_t psm2_verbs_ref_mr(psm2_verbs_mr_t mr) {
 	mr->refcount++;
+	_HFI_MMDBG("cache hit MR addr %p len %"PRIu64" access 0x%x ref %d ptr %p\n",
+		mr->addr, mr->length, mr->access, mr->refcount, mr);
 	return mr;
 }
 extern int psm2_verbs_release_mr(psm2_verbs_mr_t mrc);
+#ifdef RNDV_MOD
+// can the given MR be used for user space send DMA
+static inline int psm2_verbs_user_space_mr(struct psm2_verbs_mr *mrc)
+{
+	psmi_assert(mrc);
+	psmi_assert(mrc->refcount);
+	return ! (mrc->access & IBV_ACCESS_KERNEL);
+}
+#endif
 extern void psm2_verbs_free_mr_cache(psm2_mr_cache_t cache);
 void ips_tid_mravail_callback(struct ips_proto *proto);
 
diff --git a/prov/psm3/psm3/ptl.h b/prov/psm3/psm3/ptl.h
index 23dca3a..0c2f148 100644
--- a/prov/psm3/psm3/ptl.h
+++ b/prov/psm3/psm3/ptl.h
@@ -133,6 +133,186 @@ struct ptl_arg {
 	};
 } PACK_SUFFIX ptl_arg_t;
 
+/* can be tracked per protocol, only fully tracked and reported
+ * for ips_proto at this time but by defining here we can later track
+ * for shm and maybe self protocols too and we avoid a branch in
+ * psmi_mq_handle_envelope
+ */
+struct ptl_strategy_stats {
+	uint64_t tiny_cpu_isend;
+	uint64_t tiny_cpu_isend_bytes;
+#ifdef PSM_CUDA
+	uint64_t tiny_gdrcopy_isend;
+	uint64_t tiny_gdrcopy_isend_bytes;
+	uint64_t tiny_cuCopy_isend;
+	uint64_t tiny_cuCopy_isend_bytes;
+#endif
+	uint64_t tiny_cpu_send;
+	uint64_t tiny_cpu_send_bytes;
+#ifdef PSM_CUDA
+	uint64_t tiny_gdrcopy_send;
+	uint64_t tiny_gdrcopy_send_bytes;
+	uint64_t tiny_cuCopy_send;
+	uint64_t tiny_cuCopy_send_bytes;
+#endif
+
+	uint64_t tiny_cpu_recv;
+	uint64_t tiny_cpu_recv_bytes;
+	uint64_t tiny_sysbuf_recv;	/* to unexpected Q sysbuf */ /* incl 0 byte */
+	uint64_t tiny_sysbuf_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t tiny_gdrcopy_recv;
+	uint64_t tiny_gdrcopy_recv_bytes;
+	uint64_t tiny_cuCopy_recv;
+	uint64_t tiny_cuCopy_recv_bytes;
+#endif
+
+	uint64_t short_copy_cpu_isend;
+	uint64_t short_copy_cpu_isend_bytes;
+	uint64_t short_dma_cpu_isend;
+	uint64_t short_dma_cpu_isend_bytes;
+#ifdef PSM_CUDA
+	uint64_t short_gdrcopy_isend;
+	uint64_t short_gdrcopy_isend_bytes;
+	uint64_t short_cuCopy_send;
+	uint64_t short_cuCopy_send_bytes;
+	uint64_t short_gdr_send;
+	uint64_t short_gdr_send_bytes;
+#endif
+	uint64_t short_copy_cpu_send;
+	uint64_t short_copy_cpu_send_bytes;
+	uint64_t short_dma_cpu_send;
+	uint64_t short_dma_cpu_send_bytes;
+
+#ifdef PSM_CUDA
+	uint64_t short_gdrcopy_send;
+	uint64_t short_gdrcopy_send_bytes;
+	uint64_t short_cuCopy_isend;
+	uint64_t short_cuCopy_isend_bytes;
+	uint64_t short_gdr_isend;
+	uint64_t short_gdr_isend_bytes;
+#endif
+
+	uint64_t short_cpu_recv;
+	uint64_t short_cpu_recv_bytes;
+	uint64_t short_sysbuf_recv;	/* to unexpected Q sysbuf */
+	uint64_t short_sysbuf_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t short_gdrcopy_recv;
+	uint64_t short_gdrcopy_recv_bytes;
+	uint64_t short_cuCopy_recv;
+	uint64_t short_cuCopy_recv_bytes;
+#endif
+
+	uint64_t eager_copy_cpu_isend;
+	uint64_t eager_copy_cpu_isend_bytes;
+	uint64_t eager_dma_cpu_isend;
+	uint64_t eager_dma_cpu_isend_bytes;
+	uint64_t eager_sysbuf_recv;	/* to unexpected Q sysbuf */
+	uint64_t eager_sysbuf_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t eager_cuCopy_isend;
+	uint64_t eager_cuCopy_isend_bytes;
+	uint64_t eager_gdr_isend;
+	uint64_t eager_gdr_isend_bytes;
+#endif
+	uint64_t eager_copy_cpu_send;
+	uint64_t eager_copy_cpu_send_bytes;
+	uint64_t eager_dma_cpu_send;
+	uint64_t eager_dma_cpu_send_bytes;
+#ifdef PSM_CUDA
+	uint64_t eager_cuCopy_send;
+	uint64_t eager_cuCopy_send_bytes;
+	uint64_t eager_gdr_send;
+	uint64_t eager_gdr_send_bytes;
+#endif
+
+	uint64_t eager_cpu_recv;
+	uint64_t eager_cpu_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t eager_gdrcopy_recv;
+	uint64_t eager_gdrcopy_recv_bytes;
+	uint64_t eager_cuCopy_recv;
+	uint64_t eager_cuCopy_recv_bytes;
+#endif
+
+	uint64_t rndv_cpu_isend;
+	uint64_t rndv_cpu_isend_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_gpu_isend;
+	uint64_t rndv_gpu_isend_bytes;
+#endif
+	uint64_t rndv_cpu_send;
+	uint64_t rndv_cpu_send_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_gpu_send;
+	uint64_t rndv_gpu_send_bytes;
+#endif
+
+	/* Payload in RTS for small sync send */
+	uint64_t rndv_rts_cpu_recv;
+	uint64_t rndv_rts_cpu_recv_bytes;
+	uint64_t rndv_rts_sysbuf_recv;
+	uint64_t rndv_rts_sysbuf_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_rts_cuCopy_recv;
+	uint64_t rndv_rts_cuCopy_recv_bytes;
+#endif
+
+	/* Payload in RTS approach used by sender */
+	/* this approach uses a LONG DATA CTS, but sends no more data */
+	uint64_t rndv_rts_copy_cpu_send;	/* per CTS  (1 per RTS) */
+	uint64_t rndv_rts_copy_cpu_send_bytes;
+
+	/* LONG DATA approach selected by receiver */
+	uint64_t rndv_long_cpu_recv;	/* per RTS */
+	uint64_t rndv_long_cpu_recv_bytes;
+	uint64_t rndv_long_gpu_recv;	/* per RTS */
+	uint64_t rndv_long_gpu_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_long_cuCopy_recv;
+	uint64_t rndv_long_cuCopy_recv_bytes;
+	uint64_t rndv_long_gdr_recv;
+	uint64_t rndv_long_gdr_recv_bytes;
+#endif
+
+	/* LONG DATA approach used by sender after LONG selected by receiver */
+	/* LONG DATA only uses 1 CTS per RTS */
+	uint64_t rndv_long_copy_cpu_send;	/* per CTS  (1 per RTS) */
+	uint64_t rndv_long_copy_cpu_send_bytes;
+	uint64_t rndv_long_dma_cpu_send;	/* per CTS  (1 per RTS) */
+	uint64_t rndv_long_dma_cpu_send_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_long_cuCopy_send;	/* per CTS  (1 per RTS) */
+	uint64_t rndv_long_cuCopy_send_bytes;
+	uint64_t rndv_long_gdrcopy_send;	/* per CTS  (1 per RTS) */
+	uint64_t rndv_long_gdrcopy_send_bytes;
+	uint64_t rndv_long_gdr_send;	/* per CTS  (1 per RTS) */ /* SDMA */
+	uint64_t rndv_long_gdr_send_bytes;		/* SDMA */
+#endif
+
+	/* RDMA approach selected by receiver */
+	uint64_t rndv_rdma_cpu_recv;	/* per RTS */
+	uint64_t rndv_rdma_cpu_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_rdma_gdr_recv;	/* per RTS */
+	uint64_t rndv_rdma_gdr_recv_bytes;
+	uint64_t rndv_rdma_hbuf_recv;	/* per RTS */
+	uint64_t rndv_rdma_hbuf_recv_bytes;
+#endif
+
+	/* RDMA approach used by sender after RDMA selected by receiver */
+	/* RDMA may use >= 1 CTS per RTS */
+	uint64_t rndv_rdma_cpu_send;	/* per CTS */
+	uint64_t rndv_rdma_cpu_send_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_rdma_gdr_send;	/* per CTS */
+	uint64_t rndv_rdma_gdr_send_bytes;
+	uint64_t rndv_rdma_hbuf_send;	/* per CTS */
+	uint64_t rndv_rdma_hbuf_send_bytes;
+#endif
+};
+
 #include "ptl_self/ptl_fwd.h"
 #include "ptl_ips/ptl_fwd.h"
 #include "ptl_am/ptl_fwd.h"
diff --git a/prov/psm3/psm3/ptl_am/am_cuda_memhandle_cache.c b/prov/psm3/psm3/ptl_am/am_cuda_memhandle_cache.c
index a3801ea..cfc68f2 100644
--- a/prov/psm3/psm3/ptl_am/am_cuda_memhandle_cache.c
+++ b/prov/psm3/psm3/ptl_am/am_cuda_memhandle_cache.c
@@ -126,8 +126,8 @@ static int cuda_cache_key_cmp(const cuda_cache_item *a, const cuda_cache_item *b
 #define RBTREE_MAP_COUNT(PAYLOAD_PTR)     ((PAYLOAD_PTR)->nelems)
 #define RBTREE_NO_EMIT_IPS_CL_QMAP_PREDECESSOR
 
-#include "rbtree.h"
-#include "rbtree.c"
+#include "psm3_rbtree.h"
+#include "psm3_rbtree.c"
 
 /*
  * Convenience rbtree cruft
diff --git a/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c b/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c
index 2f135a8..3990f9f 100644
--- a/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c
+++ b/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c
@@ -182,7 +182,7 @@ void amsh_atexit()
 		ptl = (struct ptl_am *)(ep->ptl_amsh.ptl);
 		if (ptl->self_nodeinfo &&
 		    ptl->amsh_keyname != NULL) {
-			_HFI_VDBG("unlinking shm file %s\n",
+			_HFI_PRDBG("unlinking shm file %s\n",
 				  ptl->amsh_keyname);
 			shm_unlink(ptl->amsh_keyname);
 		}
@@ -520,8 +520,8 @@ psm2_error_t psmi_shm_map_remote(ptl_t *ptl_gen, psm2_epid_t epid, uint16_t *shm
 		while (*is_init == 0)
 			usleep(1);
 		ips_sync_reads();
-		_HFI_PRDBG("Got a published remote dirpage page at "
-			   "%p, size=%dn", dest_mapptr, (int)segsz);
+		_HFI_CONNDBG("Got a published remote dirpage page at "
+			   "%p, size=%d\n", dest_mapptr, (int)segsz);
 	}
 
 	shmidx = -1;
@@ -563,10 +563,10 @@ psm2_error_t psmi_shm_map_remote(ptl_t *ptl_gen, psm2_epid_t epid, uint16_t *shm
 			} else
 				psmi_shm_mq_rv_thresh =
 				    PSMI_MQ_RV_THRESH_NO_KASSIST;
-			_HFI_PRDBG("KASSIST MODE: %s\n",
+			_HFI_CONNDBG("KASSIST MODE: %s\n",
 				   psmi_kassist_getmode(ptl->psmi_kassist_mode));
 			shmidx = *shmidx_o = i;
-			_HFI_PRDBG("Mapped epid %lx into shmidx %d\n", epid, shmidx);
+			_HFI_CONNDBG("Mapped epid 0x%"PRIx64" into shmidx %d\n", epid, shmidx);
 			ptl->am_ep[i].amsh_shmbase = (uintptr_t) dest_mapptr;
 			ptl->am_ep[i].amsh_qsizes = dest_nodeinfo->amsh_qsizes;
 			if (i > ptl->max_ep_idx)
@@ -679,7 +679,7 @@ psm2_error_t psmi_shm_detach(ptl_t *ptl_gen)
 	if (ptl->self_nodeinfo == NULL)
 		return err;
 
-	_HFI_VDBG("unlinking shm file %s\n", ptl->amsh_keyname + 1);
+	_HFI_PRDBG("unlinking shm file %s\n", ptl->amsh_keyname + 1);
 	shmbase = ptl->self_nodeinfo->amsh_shmbase;
 	shm_unlink(ptl->amsh_keyname);
 	psmi_free(ptl->amsh_keyname);
@@ -813,8 +813,8 @@ amsh_epaddr_add(ptl_t *ptl_gen, psm2_epid_t epid, uint16_t shmidx, psm2_epaddr_t
 	/* Finally, add to table */
 	if ((err = psmi_epid_add(ptl->ep, epid, epaddr)))
 		goto fail;
-	_HFI_VDBG("epaddr=%s added to ptl=%p\n",
-		  psmi_epaddr_get_name(epid), ptl);
+	_HFI_CONNDBG("epaddr=%p %s added to ptl=%p\n",
+		  epaddr, psmi_epaddr_get_name(epid), ptl);
 	*epaddr_o = epaddr;
 	return PSM2_OK;
 fail:
@@ -938,8 +938,7 @@ amsh_ep_connreq_init(ptl_t *ptl_gen, int op, /* connect, disconnect or abort */
 				continue;
 			}
 
-			_HFI_VDBG("looking at epid %llx\n",
-				  (unsigned long long)epid);
+			_HFI_CONNDBG("Connect epid 0x%"PRIx64"\n", epid);
 			epaddr = psmi_epid_lookup(ptl->ep, epid);
 			if (epaddr != NULL) {
 				if (epaddr->ptlctl->ptl != ptl_gen) {
@@ -969,6 +968,8 @@ amsh_ep_connreq_init(ptl_t *ptl_gen, int op, /* connect, disconnect or abort */
 				continue;
 
 			psmi_assert(epaddr != NULL);
+			_HFI_CONNDBG("Disconnect force=%d epid 0x%"PRIx64"\n",
+				(op == PTL_OP_ABORT), epaddr->epid);
 			cstate = ((am_epaddr_t *) epaddr)->cstate_outgoing;
 			if (cstate == AMSH_CSTATE_OUTGOING_ESTABLISHED) {
 				req->epid_mask[i] = AMSH_CMASK_PREREQ;
@@ -985,8 +986,10 @@ amsh_ep_connreq_init(ptl_t *ptl_gen, int op, /* connect, disconnect or abort */
 	if (req->numep_left == 0) {	/* nothing to do */
 		psmi_free(req->epid_mask);
 		psmi_free(req);
-		_HFI_VDBG("Nothing to connect, bump up phase\n");
-		ptl->connect_phase++;
+		if (op != PTL_OP_ABORT) {
+			_HFI_CONNDBG("Nothing to connect, bump up phase\n");
+			ptl->connect_phase++;
+		}
 		*req_o = NULL;
 		return PSM2_OK;
 	} else {
@@ -1039,8 +1042,11 @@ amsh_ep_connreq_poll(ptl_t *ptl_gen, struct ptl_connection_req *req)
 				psmi_assert(shmidx != (uint16_t)-1);
 				req->args[2].u32w0 = create_extra_ep_data();
 				req->args[2].u32w1 = PSM2_OK;
-				req->args[3].u64w0 =
-				    (uint64_t) (uintptr_t) &req->errors[i];
+				if (req->op != PTL_OP_ABORT)
+					req->args[3].u64w0 =
+					    (uint64_t) (uintptr_t) &req->errors[i];
+				else
+					req->args[3].u64w0 = 0;
 				psmi_amsh_short_request(ptl_gen, epaddr,
 							amsh_conn_handler_hidx,
 							req->args, 4, NULL, 0,
@@ -1186,7 +1192,7 @@ amsh_ep_connreq_poll(ptl_t *ptl_gen, struct ptl_connection_req *req)
 							amsh_conn_handler_hidx,
 							req->args, 4, NULL, 0,
 							0);
-				_HFI_PRDBG("epaddr=%p, epid=%" PRIx64
+				_HFI_CONNDBG("epaddr=%p, epid=0x%" PRIx64
 					   " at shmidx=%d\n", epaddr, epid,
 					   shmidx);
 			}
@@ -1216,12 +1222,15 @@ amsh_ep_connreq_fini(ptl_t *ptl_gen, struct ptl_connection_req *req)
 		return PSM2_OK;
 
 	/* This prevents future connect replies from referencing data structures
-	 * that disappeared */
-	ptl->connect_phase++;
+	 * that disappeared.  For abort we aren't waiting for DISC_REP so
+	 * we want to keep same phase so we accept them after this function */
+	if (req->op != PTL_OP_ABORT)
+		ptl->connect_phase++;
 
 	/* First process any leftovers in postreq or prereq */
 	for (i = 0; i < req->numep; i++) {
-		if (req->epid_mask[i] == AMSH_CMASK_NONE)
+		if (req->epid_mask[i] == AMSH_CMASK_NONE
+			|| req->op == PTL_OP_ABORT)
 			continue;
 		else if (req->epid_mask[i] == AMSH_CMASK_POSTREQ) {
 			int cstate;
@@ -1249,6 +1258,11 @@ amsh_ep_connreq_fini(ptl_t *ptl_gen, struct ptl_connection_req *req)
 	for (i = 0; i < req->numep; i++) {
 		if (req->epid_mask[i] == AMSH_CMASK_NONE)
 			continue;
+		if (req->op == PTL_OP_ABORT
+			 && req->epid_mask[i] != AMSH_CMASK_DONE) {
+			req->epid_mask[i] = AMSH_CMASK_DONE;
+			continue;
+		}
 		psmi_assert(req->epid_mask[i] == AMSH_CMASK_DONE);
 
 		err = psmi_error_cmp(err, req->errors[i]);
@@ -1307,6 +1321,24 @@ amsh_ep_connreq_wrap(ptl_t *ptl_gen, int op,
 					 * there was an error */
 		return err;
 
+	if (op == PTL_OP_ABORT) {
+		int i;
+		/* loop a couple times only, ignore timeout */
+		/* this will move from PREREQ to POSTREQ and check once
+		 * for reply, but not wait for reply
+		 */
+		for (i=0; i < 2; i++) {
+			psmi_poll_internal(ptl->ep, 1);
+			err = amsh_ep_connreq_poll(ptl_gen, req);
+			if (err != PSM2_OK && err != PSM2_OK_NO_PROGRESS) {
+				psmi_free(req->epid_mask);
+				psmi_free(req);
+				goto fail;
+			}
+		}
+		goto fini;
+	}
+
 	/* Poll until either
 	 * 1. We time out
 	 * 2. We are done with connecting
@@ -1330,6 +1362,7 @@ amsh_ep_connreq_wrap(ptl_t *ptl_gen, int op,
 	}
 	while (psmi_cycles_left(t_start, timeout_ns));
 
+fini:
 	err = amsh_ep_connreq_fini(ptl_gen, req);
 
 fail:
@@ -2021,7 +2054,7 @@ amsh_mq_rndv(ptl_t *ptl, psm2_mq_t mq, psm2_mq_req_t req,
 	mq->stats.tx_num++;
 	mq->stats.tx_shm_num++;
 	mq->stats.tx_rndv_num++;
-	mq->stats.tx_rndv_bytes += len;
+	// tx_rndv_bytes tabulated when get CTS
 
 	return err;
 }
@@ -2079,6 +2112,7 @@ amsh_mq_send_inner_eager(psm2_mq_t mq, psm2_mq_req_t req, psm2_epaddr_t epaddr,
 
 	mq->stats.tx_num++;
 	mq->stats.tx_shm_num++;
+	mq->stats.tx_shm_bytes += len;
 	mq->stats.tx_eager_num++;
 	mq->stats.tx_eager_bytes += len;
 
@@ -2100,7 +2134,7 @@ amsh_mq_send_inner(psm2_mq_t mq, psm2_mq_req_t req, psm2_epaddr_t epaddr,
 
 #ifdef PSM_CUDA
 	int gpu_mem = 0;
-	int ep_supports_p2p = (1 << ((am_epaddr_t *) epaddr)->gpuid) & gpu_p2p_supported;
+	int ep_supports_p2p = (1 << ((am_epaddr_t *) epaddr)->gpuid) & gpu_p2p_supported();
 
 	if (PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(ubuf)) {
 		gpu_mem = 1;
@@ -2301,13 +2335,13 @@ amsh_conn_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 	psmi_assert_always(buf == NULL && len == 0);
 	read_extra_ep_data(args[2].u32w0, &pid, &gpuid);
 
-	_HFI_VDBG("Conn op=%d, phase=%d, epid=%llx, err=%d\n",
+	_HFI_CONNDBG("Conn op=%d, phase=%d, epid=0x%llx, err=%d\n",
 		  op, phase, (unsigned long long)epid, err);
 
 	switch (op) {
 	case PSMI_AM_CONN_REQ:
-		_HFI_VDBG("Connect from %d:%d\n",
-			  (int)psm2_epid_nid(epid), (int)psm2_epid_context(epid));
+		_HFI_CONNDBG("Connect from 0x%"PRIx64":%"PRIu64"\n",
+			  psm2_epid_nid(epid), psm2_epid_context(epid));
 		epaddr = psmi_epid_lookup(ptl->ep, epid);
 		if (epaddr && ((am_epaddr_t *) epaddr)->pid != pid) {
 			/* If old pid is unknown consider new pid the correct one */
@@ -2368,7 +2402,7 @@ amsh_conn_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 
 	case PSMI_AM_CONN_REP:
 		if (ptl->connect_phase != phase) {
-			_HFI_VDBG("Out of phase connect reply\n");
+			_HFI_CONNDBG("Out of phase connect reply exp %d got %d\n", ptl->connect_phase, phase);
 			return;
 		}
 		epaddr = ptl->am_ep[shmidx].epaddr;
@@ -2386,14 +2420,14 @@ amsh_conn_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 			= AMSH_CSTATE_OUTGOING_REPLIED;
 		((am_epaddr_t *) epaddr)->return_shmidx = return_shmidx;
 		ptl->connect_outgoing++;
-		_HFI_VDBG("CCC epaddr=%s connected to ptl=%p\n",
+		_HFI_CONNDBG("CCC epaddr=%s connected to ptl=%p\n",
 			  psmi_epaddr_get_name(epaddr->epid), ptl);
 		break;
 
 	case PSMI_AM_DISC_REQ:
 		epaddr = psmi_epid_lookup(ptl->ep, epid);
 		if (!epaddr) {
-			_HFI_VDBG("Dropping disconnect request from an epid that we are not connected to\n");
+			_HFI_CONNDBG("Dropping disconnect request from an epid that we are not connected to 0x%"PRIx64"\n", epid);
 			return;
 		}
 		args[0].u16w0 = PSMI_AM_DISC_REP;
@@ -2428,10 +2462,11 @@ amsh_conn_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 
 	case PSMI_AM_DISC_REP:
 		if (ptl->connect_phase != phase) {
-			_HFI_VDBG("Out of phase disconnect reply\n");
+			_HFI_CONNDBG("Out of phase disconnect reply exp %d got %d\n", ptl->connect_phase, phase);
 			return;
 		}
-		*perr = err;
+		if (perr)
+			*perr = err;
 		epaddr = tok->tok.epaddr_incoming;
 		((am_epaddr_t *) epaddr)->cstate_outgoing =
 			AMSH_CSTATE_OUTGOING_DISC_REPLIED;
@@ -2567,14 +2602,14 @@ amsh_init(psm2_ep_t ep, ptl_t *ptl_gen, ptl_ctl_t *ctl)
 	union psmi_envvar_val env_memcache_enabled;
 	psmi_getenv("PSM3_CUDA_MEMCACHE_ENABLED",
 		    "PSM cuda ipc memhandle cache enabled (default is enabled)",
-		     PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
+		     PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_UINT,
 		     (union psmi_envvar_val)
 		      1, &env_memcache_enabled);
 	if (PSMI_IS_CUDA_ENABLED && env_memcache_enabled.e_uint) {
 		union psmi_envvar_val env_memcache_size;
 		psmi_getenv("PSM3_CUDA_MEMCACHE_SIZE",
 			    "Size of the cuda ipc memhandle cache ",
-			    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
+			    PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_UINT,
 			    (union psmi_envvar_val)
 			    CUDA_MEMHANDLE_CACHE_SIZE, &env_memcache_size);
 		if ((err = am_cuda_memhandle_cache_init(env_memcache_size.e_uint) != PSM2_OK))
@@ -2611,6 +2646,8 @@ static psm2_error_t amsh_fini(ptl_t *ptl_gen, int force, uint64_t timeout_ns)
 				num_disc++;
 		}
 		psmi_epid_itor_fini(&itor);
+		if (! num_disc)
+			goto poll;
 
 		mask =
 		    (int *)psmi_calloc(ptl->ep, UNDEFINED, num_disc,
@@ -2652,18 +2689,23 @@ static psm2_error_t amsh_fini(ptl_t *ptl_gen, int force, uint64_t timeout_ns)
 		psmi_free(epaddr_array);
 	}
 
+poll:
 	if (ptl->connect_incoming > 0 || ptl->connect_outgoing > 0) {
+		_HFI_CONNDBG("CCC polling disconnect from=%d,to=%d to=%"PRIu64" phase %d\n",
+			  ptl->connect_incoming, ptl->connect_outgoing, timeout_ns, ptl->connect_phase);
 		while (ptl->connect_incoming > 0 || ptl->connect_outgoing > 0) {
 			if (!psmi_cycles_left(t_start, timeout_ns)) {
 				err = PSM2_TIMEOUT;
-				_HFI_VDBG("CCC timed out with from=%d,to=%d\n",
+				_HFI_CONNDBG("CCC timed out with from=%d,to=%d\n",
 					  ptl->connect_incoming, ptl->connect_outgoing);
 				break;
 			}
 			psmi_poll_internal(ptl->ep, 1);
 		}
+		_HFI_CONNDBG("CCC done polling disconnect from=%d,to=%d\n",
+			  ptl->connect_incoming, ptl->connect_outgoing);
 	} else
-		_HFI_VDBG("CCC complete disconnect from=%d,to=%d\n",
+		_HFI_CONNDBG("CCC complete disconnect from=%d,to=%d\n",
 			  ptl->connect_incoming, ptl->connect_outgoing);
 
 	if ((err_seg = psmi_shm_detach(ptl_gen))) {
diff --git a/prov/psm3/psm3/ptl_am/ptl.c b/prov/psm3/psm3/ptl_am/ptl.c
index 2e42c1b..f3ee1c3 100644
--- a/prov/psm3/psm3/ptl_am/ptl.c
+++ b/prov/psm3/psm3/ptl_am/ptl.c
@@ -62,6 +62,11 @@
 #include "am_cuda_memhandle_cache.h"
 #endif
 
+/* not reported yet, so just track in a global so can pass a pointer to
+ * psmi_mq_handle_envelope and psmi_mq_handle_rts
+ */
+static struct ptl_strategy_stats strat_stats;
+
 /**
  * Callback function when a receive request is matched with the
  * tag obtained from the RTS packet.
@@ -96,8 +101,7 @@ ptl_handle_rtsmatch_request(psm2_mq_req_t req, int was_posted,
 		if (req->is_buf_gpu_mem) {
 			PSMI_CUDA_CALL(cuMemcpyDtoD, (CUdeviceptr)req->req_data.buf, cuda_ipc_dev_ptr,
 				       req->req_data.recv_msglen);
-			PSMI_CUDA_CALL(cuEventRecord, req->cuda_ipc_event, 0);
-			PSMI_CUDA_CALL(cuEventSynchronize, req->cuda_ipc_event);
+			PSMI_CUDA_CALL(cuStreamSynchronize, 0);
 		} else
 			PSMI_CUDA_CALL(cuMemcpyDtoH, req->req_data.buf, cuda_ipc_dev_ptr,
 				       req->req_data.recv_msglen);
@@ -129,8 +133,7 @@ ptl_handle_rtsmatch_request(psm2_mq_req_t req, int was_posted,
 			 * copies for msg sizes less than 64k. The event record
 			 * and synchronize calls are to guarentee completion.
 			 */
-			PSMI_CUDA_CALL(cuEventRecord, req->cuda_ipc_event, 0);
-			PSMI_CUDA_CALL(cuEventSynchronize, req->cuda_ipc_event);
+			PSMI_CUDA_CALL(cuStreamSynchronize, 0);
 			psmi_free(cuda_ipc_bounce_buf);
 		} else {
 			/* cma can be done in handler context or not. */
@@ -172,6 +175,11 @@ send_cts:
 		psmi_amsh_short_request((struct ptl *)ptl, epaddr, mq_handler_rtsmatch_hidx,
 					args, 5, NULL, 0, 0);
 
+	req->mq->stats.rx_user_num++;
+	req->mq->stats.rx_user_bytes += req->req_data.recv_msglen;
+	req->mq->stats.rx_shm_num++;
+	req->mq->stats.rx_shm_bytes += req->req_data.recv_msglen;
+
 	/* 0-byte completion or we used kassist */
 	if (pid || cma_succeed ||
 		req->req_data.recv_msglen == 0 || cuda_ipc_send_completion == 1) {
@@ -206,7 +214,7 @@ psmi_am_mq_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 	tag.tag[1] = args[1].u32w0;
 	tag.tag[2] = args[2].u32w1;
 	psmi_assert(toki != NULL);
-	_HFI_VDBG("mq=%p opcode=%d, len=%d, msglen=%d\n",
+	_HFI_VDBG("mq=%p opcode=%x, len=%d, msglen=%d\n",
 		  tok->mq, opcode, (int)len, msglen);
 
 	switch (opcode) {
@@ -214,12 +222,16 @@ psmi_am_mq_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 	case MQ_MSG_SHORT:
 	case MQ_MSG_EAGER:
 		rc = psmi_mq_handle_envelope(tok->mq, tok->tok.epaddr_incoming,
-					     &tag, msglen, 0, buf,
+					     &tag, &strat_stats, msglen, 0, buf,
 					     (uint32_t) len, 1, opcode, &req);
 
 		/* for eager matching */
 		req->ptl_req_ptr = (void *)tok->tok.epaddr_incoming;
 		req->msg_seqnum = 0;	/* using seqnum 0 */
+		req->mq->stats.rx_shm_num++;
+		// close enough, may not yet be matched,
+		//  don't know recv buf_len, so assume no truncation
+		req->mq->stats.rx_shm_bytes += msglen;
 		break;
 	default:{
 			void *sreq = (void *)(uintptr_t) args[3].u64w0;
@@ -227,7 +239,7 @@ psmi_am_mq_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 			psmi_assert(narg == 5);
 			psmi_assert_always(opcode == MQ_MSG_LONGRTS);
 			rc = psmi_mq_handle_rts(tok->mq, tok->tok.epaddr_incoming,
-						&tag, msglen, NULL, 0, 1,
+						&tag, &strat_stats, msglen, NULL, 0, 1,
 						ptl_handle_rtsmatch, &req);
 
 			req->rts_peer = tok->tok.epaddr_incoming;
@@ -265,7 +277,11 @@ psmi_am_mq_handler_data(void *toki, psm2_amarg_t *args, int narg, void *buf,
 	psm2_epaddr_t epaddr = (psm2_epaddr_t) tok->tok.epaddr_incoming;
 	psm2_mq_req_t req = mq_eager_match(tok->mq, epaddr, 0);	/* using seqnum 0 */
 	psmi_assert_always(req != NULL);
+#ifdef PSM_CUDA
+	psmi_mq_handle_data(tok->mq, req, args[2].u32w0, buf, len, 0, NULL);
+#else
 	psmi_mq_handle_data(tok->mq, req, args[2].u32w0, buf, len);
+#endif
 
 	return;
 }
@@ -289,6 +305,8 @@ psmi_am_mq_handler_rtsmatch(void *toki, psm2_amarg_t *args, int narg, void *buf,
 	 */
 	if (sreq->cuda_ipc_handle_attached) {
 		sreq->cuda_ipc_handle_attached = 0;
+		sreq->mq->stats.tx_shm_bytes += sreq->req_data.send_msglen;
+		sreq->mq->stats.tx_rndv_bytes += sreq->req_data.send_msglen;
 		psmi_mq_handle_rts_complete(sreq);
 		return;
 	}
@@ -334,6 +352,8 @@ no_kassist:
 					     1, sreq->req_data.buf, msglen, dest, 0);
 		}
 	}
+	sreq->mq->stats.tx_shm_bytes += sreq->req_data.send_msglen;
+	sreq->mq->stats.tx_rndv_bytes += sreq->req_data.send_msglen;
 	psmi_mq_handle_rts_complete(sreq);
 }
 
diff --git a/prov/psm3/psm3/ptl_ips/ips_config.h b/prov/psm3/psm3/ptl_ips/ips_config.h
index 06b3645..0017db3 100644
--- a/prov/psm3/psm3/ptl_ips/ips_config.h
+++ b/prov/psm3/psm3/ptl_ips/ips_config.h
@@ -98,6 +98,10 @@
 #define IPS_FAULTINJ_REG_MR	100	/* 1 every X reg_mr ENOMEM */
 #define IPS_FAULTINJ_NONPRI_REG_MR 50	/* 1 every X non-pri reg_mr ENOMEM */
 #define IPS_FAULTINJ_PRI_REG_MR	1000	/* 1 every X pri reg_mr ENOMEM */
+#ifdef PSM_CUDA
+#define IPS_FAULTINJ_GDRMMAP	100	/* 1 every X GPU pin and mmap ENOMEM */
+#define IPS_FAULTINJ_GPU_REG_MR	100	/* 1 every X GPU reg_mr */
+#endif
 
 #endif /* #ifdef PSM_FI */
 
diff --git a/prov/psm3/psm3/ptl_ips/ips_expected_proto.h b/prov/psm3/psm3/ptl_ips/ips_expected_proto.h
index c207067..d4dfb58 100644
--- a/prov/psm3/psm3/ptl_ips/ips_expected_proto.h
+++ b/prov/psm3/psm3/ptl_ips/ips_expected_proto.h
@@ -350,6 +350,7 @@ int ips_protoexp_process_err_chk_rdma_resp(struct ips_recvhdrq_event *rcv_ev);
 #endif
 
 
+
 PSMI_ALWAYS_INLINE(
 void ips_protoexp_unaligned_copy(uint8_t *dst, uint8_t *src, uint16_t len))
 {
diff --git a/prov/psm3/psm3/ptl_ips/ips_opp_path_rec.c b/prov/psm3/psm3/ptl_ips/ips_opp_path_rec.c
index 395326d..8616250 100644
--- a/prov/psm3/psm3/ptl_ips/ips_opp_path_rec.c
+++ b/prov/psm3/psm3/ptl_ips/ips_opp_path_rec.c
@@ -107,10 +107,6 @@ ips_opp_get_path_rec(ips_path_type_t type, struct ips_proto *proto,
 		    psmi_calloc(proto->ep, UNDEFINED, 1,
 				sizeof(ips_path_rec_t));
 		if (!elid.key || !path_rec) {
-			if (elid.key)
-				psmi_free(elid.key);
-			if (path_rec)
-				psmi_free(path_rec);
 			err = PSM2_NO_MEMORY;
 			goto fail;
 		}
@@ -121,8 +117,6 @@ ips_opp_get_path_rec(ips_path_type_t type, struct ips_proto *proto,
 							  &query,
 							  &opp_response);
 		if (opp_err) {
-			psmi_free(path_rec);
-			psmi_free(elid.key);
 			err = PSM2_EPID_PATH_RESOLUTION;
 			goto fail;
 		}
@@ -153,8 +147,6 @@ ips_opp_get_path_rec(ips_path_type_t type, struct ips_proto *proto,
 
 		/* Setup CCA parameters for path */
 		if (path_rec->pr_sl > PSMI_SL_MAX) {
-			psmi_free(path_rec);
-			psmi_free(elid.key);
 			err = PSM2_INTERNAL_ERR;
 			goto fail;
 		}
@@ -169,11 +161,8 @@ ips_opp_get_path_rec(ips_path_type_t type, struct ips_proto *proto,
 		if (proto->epinfo.ep_timeout_ack_max < timeout_ack_ms)
 			proto->epinfo.ep_timeout_ack_max = timeout_ack_ms;
 		err = ips_make_ah(proto->ep, path_rec);
-		if (err != PSM2_OK) {
-			psmi_free(elid.key);
-			psmi_free(path_rec);
-			return err;
-		}
+		if (err != PSM2_OK)
+			goto fail;
 
 		/* Add path record into cache */
 		strcpy(elid.key, eplid);
@@ -184,25 +173,30 @@ ips_opp_get_path_rec(ips_path_type_t type, struct ips_proto *proto,
 
 #ifdef _HFI_DEBUGGING
 	/* Dump path record stats */
-	_HFI_PRDBG("Path Record ServiceID: %" PRIx64 " %x -----> %x\n",
+	_HFI_CONNDBG("Path Record ServiceID: %" PRIx64 " %x -----> %x\n",
 		   (uint64_t) __be64_to_cpu(query.service_id),
 		   __be16_to_cpu(slid), __be16_to_cpu(dlid));
 	if (opp_response_set)
 	{
-		_HFI_PRDBG("MTU: %x, %x\n", (opp_response.mtu & 0x3f),
+		_HFI_CONNDBG("MTU: %x, %x\n", (opp_response.mtu & 0x3f),
 			   path_rec->pr_mtu);
-		_HFI_PRDBG("PKEY: 0x%04x\n", ntohs(opp_response.pkey));
-		_HFI_PRDBG("SL: 0x%04x\n", ntohs(opp_response.qos_class_sl));
-		_HFI_PRDBG("Rate: %x\n", (opp_response.rate & 0x3f));
+		_HFI_CONNDBG("PKEY: 0x%04x\n", ntohs(opp_response.pkey));
+		_HFI_CONNDBG("SL: 0x%04x\n", ntohs(opp_response.qos_class_sl));
+		_HFI_CONNDBG("Rate: %x\n", (opp_response.rate & 0x3f));
 	}
-	_HFI_PRDBG("Timeout Init.: 0x%" PRIx64 " Max: 0x%" PRIx64 "\n",
+	_HFI_CONNDBG("Timeout Init.: 0x%" PRIx64 " Max: 0x%" PRIx64 "\n",
 		   proto->epinfo.ep_timeout_ack,
 		   proto->epinfo.ep_timeout_ack_max);
 #endif
 	/* Return the IPS path record */
 	*ppath_rec = path_rec;
+	return err;
 
 fail:
+	if (elid.key)
+		psmi_free(elid.key);
+	if (path_rec)
+		psmi_free(path_rec);
 	return err;
 }
 
@@ -474,7 +468,7 @@ retry_low_path_res:
 
 fail:
 	if (err != PSM2_OK)
-		_HFI_PRDBG
+		_HFI_CONNDBG
 		    ("Unable to get path record for LID 0x%x <---> DLID 0x%x.\n",
 		     slid, dlid);
 	return err;
diff --git a/prov/psm3/psm3/ptl_ips/ips_path_rec.c b/prov/psm3/psm3/ptl_ips/ips_path_rec.c
index bac25e4..ac15304 100644
--- a/prov/psm3/psm3/ptl_ips/ips_path_rec.c
+++ b/prov/psm3/psm3/ptl_ips/ips_path_rec.c
@@ -100,11 +100,8 @@ ips_none_get_path_rec(struct ips_proto *proto,
 		    psmi_calloc(proto->ep, UNDEFINED, 1,
 				sizeof(ips_path_rec_t));
 		if (!elid.key || !path_rec) {
-			if (elid.key)
-				psmi_free(elid.key);
-			if (path_rec)
-				psmi_free(path_rec);
-			return PSM2_NO_MEMORY;
+			err = PSM2_NO_MEMORY;
+			goto fail;
 		}
 
 		/* Create path record */
@@ -119,16 +116,12 @@ ips_none_get_path_rec(struct ips_proto *proto,
 
 		/* Setup CCA parameters for path */
 		if (path_rec->pr_sl > PSMI_SL_MAX) {
-			psmi_free(elid.key);
-			psmi_free(path_rec);
-			return PSM2_INTERNAL_ERR;
+			err =  PSM2_INTERNAL_ERR;
+			goto fail;
 		}
 		err = ips_make_ah(proto->ep, path_rec);
-		if (err != PSM2_OK) {
-			psmi_free(elid.key);
-			psmi_free(path_rec);
-			return err;
-		}
+		if (err != PSM2_OK)
+			goto fail;
 
 		/* Add path record into cache */
 		strcpy(elid.key, eplid);
@@ -141,6 +134,13 @@ ips_none_get_path_rec(struct ips_proto *proto,
 	*ppath_rec = path_rec;
 
 	return err;
+
+fail:
+	if (elid.key)
+		psmi_free(elid.key);
+	if (path_rec)
+		psmi_free(path_rec);
+	return err;
 }
 
 // This works for UD address vectors as well as the ah_attr in an RC QP attrs
@@ -162,7 +162,7 @@ psm2_error_t ips_path_rec_to_ah_attr(psm2_ep_t ep,
 		ah_attr->src_path_bits = __be16_to_cpu(path_rec->pr_slid);
 		ah_attr->dlid = __be16_to_cpu(path_rec->pr_dlid);
 		ah_attr->is_global  = 0;
-		_HFI_UDDBG("creating AH with DLID %u\n", ah_attr->dlid);
+		_HFI_CONNDBG("creating AH with DLID %u\n", ah_attr->dlid);
 	} else {
 		ah_attr->src_path_bits = 0;
 		ah_attr->dlid = 1;	// not used on ethernet, make non-zero
@@ -175,9 +175,9 @@ psm2_error_t ips_path_rec_to_ah_attr(psm2_ep_t ep,
 		ah_attr->grh.sgid_index = ep->verbs_ep.lgid_index;
 		ah_attr->grh.hop_limit = 0xFF;
 		ah_attr->grh.traffic_class = 0;
-		if (_HFI_UDDBG_ON) {
+		if (_HFI_CONNDBG_ON) {
 			char buf[80];
-			_HFI_UDDBG("creating AH with DGID: %s\n",
+			_HFI_CONNDBG("creating AH with DGID: %s\n",
 				__psm2_dump_gid(&ah_attr->grh.dgid, buf, sizeof(buf)));
 		}
 	}
@@ -189,23 +189,23 @@ psm2_error_t ips_make_ah(psm2_ep_t ep, ips_path_rec_t *path_rec)
 	struct ibv_ah_attr ah_attr;
 
 	if (path_rec->ah) {
-		_HFI_UDDBG("make_ah called second time on given path_rec, skipping\n");
+		_HFI_CONNDBG("make_ah called second time on given path_rec, skipping\n");
 		return PSM2_OK;
 	}
 	if (PSM2_OK != ips_path_rec_to_ah_attr(ep, path_rec, &ah_attr)) {
-		_HFI_ERROR( "Unable to convert path_rec to AH\n");
+		_HFI_ERROR( "Unable to convert path_rec to AH for %s port %u\n", ep->dev_name, ep->portnum);
 		return PSM2_INTERNAL_ERR;
 	}
 	path_rec->ah = ibv_create_ah(ep->verbs_ep.pd, &ah_attr);
 	if (! path_rec->ah) {
 		int save_errno = errno;
-		_HFI_ERROR( "Unable to create AH: %s (%d)\n", strerror(save_errno), save_errno);
+		_HFI_ERROR( "Unable to create AH for %s: %s (%d)\n", ep->dev_name, strerror(save_errno), save_errno);
 		if (save_errno == ETIMEDOUT)
 			return PSM2_EPID_PATH_RESOLUTION;
 		else
 			return PSM2_INTERNAL_ERR;
 	}
-	_HFI_UDDBG("created AH %p\n", path_rec->ah);
+	_HFI_CONNDBG("created AH %p\n", path_rec->ah);
 	// PSM doesn't free path_rec structures on shutdown, so this will
 	// simply leak and be cleaned up by the kernel close when we shutdown
 	return PSM2_OK;
@@ -387,9 +387,9 @@ ips_none_path_rec(struct ips_proto *proto,
 
 fail:
 	if (err != PSM2_OK)
-		_HFI_PRDBG
-		    ("Unable to get path record for LID %x <---> DLID %x.\n",
-		     slid, dlid);
+		_HFI_CONNDBG
+		    ("Unable to get path record for %s port %u LID %x <---> DLID %x.\n",
+		     proto->ep->dev_name, proto->ep->portnum, slid, dlid);
 	return err;
 }
 
@@ -416,7 +416,7 @@ static psm2_error_t ips_none_path_rec_init(struct ips_proto *proto)
 
 		if (!psmi_getenv("PSM3_ERRCHK_TIMEOUT",
 				 "Errchk timeouts in mS <min:max:factor>",
-				 PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_STR,
+				 PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_STR,
 				 (union psmi_envvar_val)errchk_to, &env_to)) {
 			/* Not using default values, parse what we can */
 			errchk_to = env_to.e_str;
diff --git a/prov/psm3/psm3/ptl_ips/ips_path_rec.h b/prov/psm3/psm3/ptl_ips/ips_path_rec.h
index 684d7ca..fc0a433 100644
--- a/prov/psm3/psm3/ptl_ips/ips_path_rec.h
+++ b/prov/psm3/psm3/ptl_ips/ips_path_rec.h
@@ -186,6 +186,7 @@ typedef struct ips_path_rec {
 	psm2_rv_conn_t rv_conn;
 	uint8_t connecting;
 #endif
+
 } ips_path_rec_t;
 
 psm2_error_t ips_opp_init(struct ips_proto *proto);
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto.c b/prov/psm3/psm3/ptl_ips/ips_proto.c
index cf93b09..f001979 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto.c
@@ -86,29 +86,26 @@
 #define CTRL_MSG_DISCONNECT_REPLY_QUEUED	0x0100
 
 #ifdef PSM_CUDA
-uint32_t gpudirect_send_threshold;
-uint32_t gpudirect_recv_threshold;
+uint32_t gpudirect_send_limit;
+uint32_t gpudirect_recv_limit;
 #endif
 
 static void ctrlq_init(struct ips_ctrlq *ctrlq, struct ips_proto *proto);
+static psm2_error_t proto_sdma_init(struct ips_proto *proto,
+				   const psmi_context_t *context);
 
 #ifdef PSM_CUDA
 void psmi_cuda_hostbuf_alloc_func(int is_alloc, void *context, void *obj)
 {
-	struct ips_cuda_hostbuf *icb;
-	struct ips_cuda_hostbuf_mpool_cb_context *ctxt =
-		(struct ips_cuda_hostbuf_mpool_cb_context *) context;
-
-	icb = (struct ips_cuda_hostbuf *)obj;
+	struct ips_cuda_hostbuf *icb = (struct ips_cuda_hostbuf *)obj;
 	if (is_alloc) {
-		PSMI_CUDA_CALL(cuMemHostAlloc,
-			       (void **) &icb->host_buf,
-			       ctxt->bufsz,
-			       CU_MEMHOSTALLOC_PORTABLE);
-		PSMI_CUDA_CALL(cuEventCreate, &icb->copy_status, CU_EVENT_DEFAULT);
+		icb->host_buf = NULL;
+		icb->copy_status = NULL;
 	} else {
-		if (icb->host_buf) {
+		if (icb->host_buf != NULL) {
 			PSMI_CUDA_CALL(cuMemFreeHost, icb->host_buf);
+		}
+		if (icb->copy_status != NULL) {
 			PSMI_CUDA_CALL(cuEventDestroy, icb->copy_status);
 		}
 	}
@@ -116,6 +113,18 @@ void psmi_cuda_hostbuf_alloc_func(int is_alloc, void *context, void *obj)
 }
 #endif
 
+static uint64_t verbs_ep_send_num_free(void *context)
+{
+	struct psm2_verbs_ep *vep = &((struct ips_proto *)context)->ep->verbs_ep;
+	return vep->send_allocator.pool->send_num_free;
+}
+
+static uint64_t verbs_ep_send_rdma_outstanding(void *context)
+{
+	struct psm2_verbs_ep *vep = &((struct ips_proto *)context)->ep->verbs_ep;
+	return vep->send_rdma_outstanding;
+}
+
 psm2_error_t
 ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 	       int num_of_send_bufs, int num_of_send_desc, uint32_t imm_size,
@@ -185,31 +194,12 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 	if (err)
 		goto fail;
 
-	/* sdma queue size */
-	proto->sdma_queue_size = 16;	// hack until we ifdef rest of sdma
-	/* don't use the last slot */
-
-	if (proto->sdma_queue_size > 8) {
-		/* configure sdma_avail_counter */
-		proto->sdma_avail_counter = 8; // hack until we ifdef rest of sdma
-	} else {
-		err = PSM2_PARAM_ERR;
-		goto fail;
-	}
-
-
-	proto->sdma_fill_index = 0;
-	proto->sdma_done_index = 0;
-	proto->sdma_scb_queue = (struct ips_scb **)
-		psmi_calloc(proto->ep, UNDEFINED,
-		proto->sdma_queue_size, sizeof(struct ips_scb *));
-	if (proto->sdma_scb_queue == NULL) {
-		err = PSM2_NO_MEMORY;
-		goto fail;
-	}
 
 	proto->timeout_send = us_2_cycles(IPS_PROTO_SPIO_RETRY_US_DEFAULT);
 	proto->iovec_thresh_eager = proto->iovec_thresh_eager_blocking = ~0U;
+#ifdef PSM_CUDA
+	proto->iovec_gpu_thresh_eager = proto->iovec_gpu_thresh_eager_blocking = ~0U;
+#endif
 	proto->t_init = get_cycles();
 	proto->t_fini = 0;
 	proto->flags = env_cksum.e_uint ? IPS_PROTO_FLAG_CKSUM : 0;
@@ -289,11 +279,8 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 	/*
 	 * Initialize SDMA, otherwise, turn on all PIO.
 	 */
-	{
-		proto->flags |= IPS_PROTO_FLAG_SPIO;
-		proto->iovec_thresh_eager = proto->iovec_thresh_eager_blocking =
-		    ~0U;
-	}
+	// initialize sdma after PSM3_MR_CACHE_MODE
+	proto->flags |= IPS_PROTO_FLAG_SPIO;
 
 	/*
 	 * Setup the protocol wide short message ep flow.
@@ -351,6 +338,15 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 		struct psmi_stats_entry entries[] = {
 			PSMI_STATS_DECLU64("pio_busy_count",
 					   &proto->stats.pio_busy_cnt),
+			PSMI_STATS_DECLU64("pio_no_flow_credits",
+					   &proto->stats.pio_no_flow_credits),
+			PSMI_STATS_DECLU64("post_send_fail",
+					   &proto->stats.post_send_fail),
+			PSMI_STATS_DECL_FUNC("ud_sbuf_free",
+					   verbs_ep_send_num_free),
+			PSMI_STATS_DECL_FUNC("send_rdma_outstanding",
+					   verbs_ep_send_rdma_outstanding),
+
 			/* Throttling by kernel */
 			PSMI_STATS_DECLU64("writev_busy_cnt",
 					   &proto->stats.writev_busy_cnt),
@@ -369,6 +365,10 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 					   &proto->epaddr_stats.err_chk_rdma_send),
 			PSMI_STATS_DECLU64("err_chk_rdma_recv",
 					   &proto->epaddr_stats.err_chk_rdma_recv),
+			PSMI_STATS_DECLU64("err_chk_rdma_resp_send",
+					   &proto->epaddr_stats.err_chk_rdma_resp_send),
+			PSMI_STATS_DECLU64("err_chk_rdma_resp_recv",
+					   &proto->epaddr_stats.err_chk_rdma_resp_recv),
 #endif
 			PSMI_STATS_DECLU64("nak_send",
 					   &proto->epaddr_stats.nak_send),
@@ -390,23 +390,305 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 					   &proto->epaddr_stats.disconnect_rep_send),
 			PSMI_STATS_DECLU64("disconnect_rep_recv",
 					   &proto->epaddr_stats.disconnect_rep_recv),
-			PSMI_STATS_DECLU64("tids_grant_send",
-					   &proto->epaddr_stats.tids_grant_send),
-			PSMI_STATS_DECLU64("tids_grant_recv",
-					   &proto->epaddr_stats.tids_grant_recv),
+			PSMI_STATS_DECLU64("rts_send",
+					   &proto->epaddr_stats.rts_send),
+			PSMI_STATS_DECLU64("rts_recv",
+					   &proto->epaddr_stats.rts_recv),
+			PSMI_STATS_DECLU64("cts_long_data_send",
+					   &proto->epaddr_stats.cts_long_data_send),
+			PSMI_STATS_DECLU64("cts_long_data_recv",
+					   &proto->epaddr_stats.cts_long_data_recv),
+			PSMI_STATS_DECLU64("cts_rdma_send",
+					   &proto->epaddr_stats.cts_rdma_send),
+			PSMI_STATS_DECLU64("cts_rdma_recv",
+					   &proto->epaddr_stats.cts_rdma_recv),
 			PSMI_STATS_DECLU64("send_rexmit",
 					   &proto->epaddr_stats.send_rexmit),
 #ifdef RNDV_MOD
 			PSMI_STATS_DECLU64("rdma_rexmit",
 					   &proto->epaddr_stats.rdma_rexmit),
 #endif
+			PSMI_STATS_DECLU64("tiny_cpu_isend",
+					   &proto->strat_stats.tiny_cpu_isend),
+			PSMI_STATS_DECLU64("tiny_cpu_isend_bytes",
+					   &proto->strat_stats.tiny_cpu_isend_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("tiny_gdrcopy_isend",
+					   &proto->strat_stats.tiny_gdrcopy_isend),
+			PSMI_STATS_DECLU64("tiny_gdrcopy_isend_bytes",
+					   &proto->strat_stats.tiny_gdrcopy_isend_bytes),
+			PSMI_STATS_DECLU64("tiny_cuCopy_isend",
+					   &proto->strat_stats.tiny_cuCopy_isend),
+			PSMI_STATS_DECLU64("tiny_cuCopy_isend_bytes",
+					   &proto->strat_stats.tiny_cuCopy_isend_bytes),
+#endif
+			PSMI_STATS_DECLU64("tiny_cpu_send",
+					   &proto->strat_stats.tiny_cpu_send),
+			PSMI_STATS_DECLU64("tiny_cpu_send_bytes",
+					   &proto->strat_stats.tiny_cpu_send_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("tiny_gdrcopy_send",
+					   &proto->strat_stats.tiny_gdrcopy_send),
+			PSMI_STATS_DECLU64("tiny_gdrcopy_send_bytes",
+					   &proto->strat_stats.tiny_gdrcopy_send_bytes),
+			PSMI_STATS_DECLU64("tiny_cuCopy_send",
+					   &proto->strat_stats.tiny_cuCopy_send),
+			PSMI_STATS_DECLU64("tiny_cuCopy_send_bytes",
+					   &proto->strat_stats.tiny_cuCopy_send_bytes),
+#endif
+			PSMI_STATS_DECLU64("tiny_cpu_recv",
+					   &proto->strat_stats.tiny_cpu_recv),
+			PSMI_STATS_DECLU64("tiny_cpu_recv_bytes",
+					   &proto->strat_stats.tiny_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("tiny_sysbuf_recv",
+					   &proto->strat_stats.tiny_sysbuf_recv),
+			PSMI_STATS_DECLU64("tiny_sysbuf_recv_bytes",
+					   &proto->strat_stats.tiny_sysbuf_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("tiny_gdrcopy_recv",
+					   &proto->strat_stats.tiny_gdrcopy_recv),
+			PSMI_STATS_DECLU64("tiny_gdrcopy_recv_bytes",
+					   &proto->strat_stats.tiny_gdrcopy_recv_bytes),
+			PSMI_STATS_DECLU64("tiny_cuCopy_recv",
+					   &proto->strat_stats.tiny_cuCopy_recv),
+			PSMI_STATS_DECLU64("tiny_cuCopy_recv_bytes",
+					   &proto->strat_stats.tiny_cuCopy_recv_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("short_copy_cpu_isend",
+					   &proto->strat_stats.short_copy_cpu_isend),
+			PSMI_STATS_DECLU64("short_copy_cpu_isend_bytes",
+					   &proto->strat_stats.short_copy_cpu_isend_bytes),
+			PSMI_STATS_DECLU64("short_dma_cpu_isend",
+					   &proto->strat_stats.short_dma_cpu_isend),
+			PSMI_STATS_DECLU64("short_dma_cpu_isend_bytes",
+					   &proto->strat_stats.short_dma_cpu_isend_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("short_gdrcopy_isend",
+					   &proto->strat_stats.short_gdrcopy_isend),
+			PSMI_STATS_DECLU64("short_gdrcopy_isend_bytes",
+					   &proto->strat_stats.short_gdrcopy_isend_bytes),
+			PSMI_STATS_DECLU64("short_cuCopy_isend",
+					   &proto->strat_stats.short_cuCopy_isend),
+			PSMI_STATS_DECLU64("short_cuCopy_isend_bytes",
+					   &proto->strat_stats.short_cuCopy_isend_bytes),
+			PSMI_STATS_DECLU64("short_gdr_isend",
+					   &proto->strat_stats.short_gdr_isend),
+			PSMI_STATS_DECLU64("short_gdr_isend_bytes",
+					   &proto->strat_stats.short_gdr_isend_bytes),
+#endif
+			PSMI_STATS_DECLU64("short_copy_cpu_send",
+					   &proto->strat_stats.short_copy_cpu_send),
+			PSMI_STATS_DECLU64("short_copy_cpu_send_bytes",
+					   &proto->strat_stats.short_copy_cpu_send_bytes),
+			PSMI_STATS_DECLU64("short_dma_cpu_send",
+					   &proto->strat_stats.short_dma_cpu_send),
+			PSMI_STATS_DECLU64("short_dma_cpu_send_bytes",
+					   &proto->strat_stats.short_dma_cpu_send_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("short_gdrcopy_send",
+					   &proto->strat_stats.short_gdrcopy_send),
+			PSMI_STATS_DECLU64("short_gdrcopy_send_bytes",
+					   &proto->strat_stats.short_gdrcopy_send_bytes),
+			PSMI_STATS_DECLU64("short_cuCopy_send",
+					   &proto->strat_stats.short_cuCopy_send),
+			PSMI_STATS_DECLU64("short_cuCopy_send_bytes",
+					   &proto->strat_stats.short_cuCopy_send_bytes),
+			PSMI_STATS_DECLU64("short_gdr_send",
+					   &proto->strat_stats.short_gdr_send),
+			PSMI_STATS_DECLU64("short_gdr_send_bytes",
+					   &proto->strat_stats.short_gdr_send_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("short_cpu_recv",
+					   &proto->strat_stats.short_cpu_recv),
+			PSMI_STATS_DECLU64("short_cpu_recv_bytes",
+					   &proto->strat_stats.short_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("short_sysbuf_recv",
+					   &proto->strat_stats.short_sysbuf_recv),
+			PSMI_STATS_DECLU64("short_sysbuf_recv_bytes",
+					   &proto->strat_stats.short_sysbuf_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("short_gdrcopy_recv",
+					   &proto->strat_stats.short_gdrcopy_recv),
+			PSMI_STATS_DECLU64("short_gdrcopy_recv_bytes",
+					   &proto->strat_stats.short_gdrcopy_recv_bytes),
+			PSMI_STATS_DECLU64("short_cuCopy_recv",
+					   &proto->strat_stats.short_cuCopy_recv),
+			PSMI_STATS_DECLU64("short_cuCopy_recv_bytes",
+					   &proto->strat_stats.short_cuCopy_recv_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("eager_copy_cpu_isend",
+					   &proto->strat_stats.eager_copy_cpu_isend),
+			PSMI_STATS_DECLU64("eager_copy_cpu_isend_bytes",
+					   &proto->strat_stats.eager_copy_cpu_isend_bytes),
+			PSMI_STATS_DECLU64("eager_dma_cpu_isend",
+					   &proto->strat_stats.eager_dma_cpu_isend),
+			PSMI_STATS_DECLU64("eager_dma_cpu_isend_bytes",
+					   &proto->strat_stats.eager_dma_cpu_isend_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("eager_cuCopy_isend",
+					   &proto->strat_stats.eager_cuCopy_isend),
+			PSMI_STATS_DECLU64("eager_cuCopy_isend_bytes",
+					   &proto->strat_stats.eager_cuCopy_isend_bytes),
+			PSMI_STATS_DECLU64("eager_gdr_isend",
+					   &proto->strat_stats.eager_gdr_isend),
+			PSMI_STATS_DECLU64("eager_gdr_isend_bytes",
+					   &proto->strat_stats.eager_gdr_isend_bytes),
+#endif
+			PSMI_STATS_DECLU64("eager_copy_cpu_send",
+					   &proto->strat_stats.eager_copy_cpu_send),
+			PSMI_STATS_DECLU64("eager_copy_cpu_send_bytes",
+					   &proto->strat_stats.eager_copy_cpu_send_bytes),
+			PSMI_STATS_DECLU64("eager_dma_cpu_send",
+					   &proto->strat_stats.eager_dma_cpu_send),
+			PSMI_STATS_DECLU64("eager_dma_cpu_send_bytes",
+					   &proto->strat_stats.eager_dma_cpu_send_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("eager_cuCopy_send",
+					   &proto->strat_stats.eager_cuCopy_send),
+			PSMI_STATS_DECLU64("eager_cuCopy_send_bytes",
+					   &proto->strat_stats.eager_cuCopy_send_bytes),
+			PSMI_STATS_DECLU64("eager_gdr_send",
+					   &proto->strat_stats.eager_gdr_send),
+			PSMI_STATS_DECLU64("eager_gdr_send_bytes",
+					   &proto->strat_stats.eager_gdr_send_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("eager_cpu_recv",
+					   &proto->strat_stats.eager_cpu_recv),
+			PSMI_STATS_DECLU64("eager_cpu_recv_bytes",
+					   &proto->strat_stats.eager_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("eager_sysbuf_recv",
+					   &proto->strat_stats.eager_sysbuf_recv),
+			PSMI_STATS_DECLU64("eager_sysbuf_recv_bytes",
+					   &proto->strat_stats.eager_sysbuf_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("eager_gdrcopy_recv",
+					   &proto->strat_stats.eager_gdrcopy_recv),
+			PSMI_STATS_DECLU64("eager_gdrcopy_recv_bytes",
+					   &proto->strat_stats.eager_gdrcopy_recv_bytes),
+			PSMI_STATS_DECLU64("eager_cuCopy_recv",
+					   &proto->strat_stats.eager_cuCopy_recv),
+			PSMI_STATS_DECLU64("eager_cuCopy_recv_bytes",
+					   &proto->strat_stats.eager_cuCopy_recv_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("rndv_cpu_isend",
+					   &proto->strat_stats.rndv_cpu_isend),
+			PSMI_STATS_DECLU64("rndv_cpu_isend_bytes",
+					   &proto->strat_stats.rndv_cpu_isend_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_gpu_isend",
+					   &proto->strat_stats.rndv_gpu_isend),
+			PSMI_STATS_DECLU64("rndv_gpu_isend_bytes",
+					   &proto->strat_stats.rndv_gpu_isend_bytes),
+#endif
+			PSMI_STATS_DECLU64("rndv_cpu_send",
+					   &proto->strat_stats.rndv_cpu_send),
+			PSMI_STATS_DECLU64("rndv_cpu_send_bytes",
+					   &proto->strat_stats.rndv_cpu_send_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_gpu_send",
+					   &proto->strat_stats.rndv_gpu_send),
+			PSMI_STATS_DECLU64("rndv_gpu_send_bytes",
+					   &proto->strat_stats.rndv_gpu_send_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("rndv_rts_cpu_recv",
+					   &proto->strat_stats.rndv_rts_cpu_recv),
+			PSMI_STATS_DECLU64("rndv_rts_cpu_recv_bytes",
+					   &proto->strat_stats.rndv_rts_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("rndv_rts_sysbuf_recv",
+					   &proto->strat_stats.rndv_rts_sysbuf_recv),
+			PSMI_STATS_DECLU64("rndv_rts_sysbuf_recv_bytes",
+					   &proto->strat_stats.rndv_rts_sysbuf_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_rts_cuCopy_recv",
+					   &proto->strat_stats.rndv_rts_cuCopy_recv),
+			PSMI_STATS_DECLU64("rndv_rts_cuCopy_recv_bytes",
+					   &proto->strat_stats.rndv_rts_cuCopy_recv_bytes),
+#endif
+			PSMI_STATS_DECLU64("rndv_rts_copy_cpu_send",
+					   &proto->strat_stats.rndv_rts_copy_cpu_send),
+			PSMI_STATS_DECLU64("rndv_rts_copy_cpu_send_bytes",
+					   &proto->strat_stats.rndv_rts_copy_cpu_send_bytes),
+
+			PSMI_STATS_DECLU64("rndv_long_cpu_recv",
+					   &proto->strat_stats.rndv_long_cpu_recv),
+			PSMI_STATS_DECLU64("rndv_long_cpu_recv_bytes",
+					   &proto->strat_stats.rndv_long_cpu_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_long_cuCopy_recv",
+					   &proto->strat_stats.rndv_long_cuCopy_recv),
+			PSMI_STATS_DECLU64("rndv_long_cuCopy_recv_bytes",
+					   &proto->strat_stats.rndv_long_cuCopy_recv_bytes),
+			PSMI_STATS_DECLU64("rndv_long_gdr_recv",
+					   &proto->strat_stats.rndv_long_gdr_recv),
+			PSMI_STATS_DECLU64("rndv_long_gdr_recv_bytes",
+					   &proto->strat_stats.rndv_long_gdr_recv_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("rndv_long_copy_cpu_send",
+					   &proto->strat_stats.rndv_long_copy_cpu_send),
+			PSMI_STATS_DECLU64("rndv_long_copy_cpu_send_bytes",
+					   &proto->strat_stats.rndv_long_copy_cpu_send_bytes),
+			PSMI_STATS_DECLU64("rndv_long_dma_cpu_send",
+					   &proto->strat_stats.rndv_long_dma_cpu_send),
+			PSMI_STATS_DECLU64("rndv_long_dma_cpu_send_bytes",
+					   &proto->strat_stats.rndv_long_dma_cpu_send_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_long_cuCopy_send",
+					   &proto->strat_stats.rndv_long_cuCopy_send),
+			PSMI_STATS_DECLU64("rndv_long_cuCopy_send_bytes",
+					   &proto->strat_stats.rndv_long_cuCopy_send_bytes),
+			PSMI_STATS_DECLU64("rndv_long_gdrcopy_send",
+					   &proto->strat_stats.rndv_long_gdrcopy_send),
+			PSMI_STATS_DECLU64("rndv_long_gdrcopy_send_bytes",
+					   &proto->strat_stats.rndv_long_gdrcopy_send_bytes),
+			PSMI_STATS_DECLU64("rndv_long_gdr_send",
+					   &proto->strat_stats.rndv_long_gdr_send),
+			PSMI_STATS_DECLU64("rndv_long_gdr_send_bytes",
+					   &proto->strat_stats.rndv_long_gdr_send_bytes),
+#endif
+
+			PSMI_STATS_DECLU64("rndv_rdma_cpu_recv",
+					   &proto->strat_stats.rndv_rdma_cpu_recv),
+			PSMI_STATS_DECLU64("rndv_rdma_cpu_recv_bytes",
+					   &proto->strat_stats.rndv_rdma_cpu_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_rdma_gdr_recv",
+					   &proto->strat_stats.rndv_rdma_gdr_recv),
+			PSMI_STATS_DECLU64("rndv_rdma_gdr_recv_bytes",
+					   &proto->strat_stats.rndv_rdma_gdr_recv_bytes),
+			PSMI_STATS_DECLU64("rndv_rdma_hbuf_recv",
+					   &proto->strat_stats.rndv_rdma_hbuf_recv),
+			PSMI_STATS_DECLU64("rndv_rdma_hbuf_recv_bytes",
+					   &proto->strat_stats.rndv_rdma_hbuf_recv_bytes),
+#endif
+			PSMI_STATS_DECLU64("rndv_rdma_cpu_send",
+					   &proto->strat_stats.rndv_rdma_cpu_send),
+			PSMI_STATS_DECLU64("rndv_rdma_cpu_send_bytes",
+					   &proto->strat_stats.rndv_rdma_cpu_send_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_rdma_gdr_send",
+					   &proto->strat_stats.rndv_rdma_gdr_send),
+			PSMI_STATS_DECLU64("rndv_rdma_gdr_send_bytes",
+					   &proto->strat_stats.rndv_rdma_gdr_send_bytes),
+			PSMI_STATS_DECLU64("rndv_rdma_hbuf_send",
+					   &proto->strat_stats.rndv_rdma_hbuf_send),
+			PSMI_STATS_DECLU64("rndv_rdma_hbuf_send_bytes",
+					   &proto->strat_stats.rndv_rdma_hbuf_send_bytes),
+#endif
 		};
 
 		err =
 		    psmi_stats_register_type
 		    ("PSM_low-level_protocol_stats",
 		     PSMI_STATSTYPE_IPSPROTO, entries,
-		     PSMI_STATS_HOWMANY(entries), proto->ep->epid, proto);
+		     PSMI_STATS_HOWMANY(entries), proto->ep->epid, proto,
+		     proto->ep->dev_name);
 		if (err != PSM2_OK)
 			goto fail;
 	}
@@ -491,10 +773,7 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 	}
 	if (protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED) {
 #ifdef PSM_CUDA
-		if (PSMI_IS_CUDA_ENABLED) {
-			PSMI_CUDA_CALL(cuStreamCreate,
-				   &proto->cudastream_send, CU_STREAM_NON_BLOCKING);
-		}
+		proto->cudastream_send = NULL;
 #endif
 		if ((err = ips_protoexp_init(context, proto, protoexp_flags,
 					     num_of_send_bufs, num_of_send_desc,
@@ -504,49 +783,6 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 		proto->protoexp = NULL;
 	}
 
-	// we allocate MR cache here (as opposed to in protoexp) in case we later
-	// decide to implement RC send for medium messages and use it to register
-	// medium sized user eager buffers (SDMA-like)
-	if (protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED) {
-		union psmi_envvar_val env_mr_cache_size;
-		uint32_t default_cache_size;	// in entries
-		uint32_t cache_pri_entries;
-		uint64_t cache_pri_size;	// in bytes
-
-		// we can have at most HFI_TF_NFLOWS inbound RDMA and hfi_num_send_rdma
-		// outbound RDMA.  Each of which potentially needs an MR.
-		// so mr_cache_size should be >= HFI_TF_NFLOWS + ep->hfi_num_send_rdma
-		// but can survive if it's smaller as we will delay transfer til avail
-		cache_pri_entries =  HFI_TF_NFLOWS + proto->ep->hfi_num_send_rdma;
-		cache_pri_size  = (uint64_t)cache_pri_entries * proto->mq->hfi_base_window_rv;
-		if (proto->ep->mr_cache_mode == MR_CACHE_MODE_USER) {
-			// we attempt to cache, so can benefit from more than inflight
-			default_cache_size = cache_pri_entries * 16;
-		} else {
-			// we only reference count
-			// could benefit from some extra so we can preregister MRs for
-			// transfers we don't yet have resources for
-			default_cache_size = cache_pri_entries * 8;
-		}
-		/* Size of user space MR Cache
-		 */
-		psmi_getenv("PSM3_MR_CACHE_SIZE",
-				"user space MR table/cache size (num MRs)",
-				PSMI_ENVVAR_LEVEL_USER,
-				PSMI_ENVVAR_TYPE_UINT,
-				(union psmi_envvar_val)default_cache_size, &env_mr_cache_size);
-
-		proto->mr_cache = psm2_verbs_alloc_mr_cache(proto->ep,
-						env_mr_cache_size.e_uint, proto->ep->mr_cache_mode,
-						cache_pri_entries, cache_pri_size);
-		if (! proto->mr_cache) {
-			_HFI_ERROR( "Unable to allocate MR cache (%u entries)\n",
-					env_mr_cache_size.e_uint);
-			err = PSM2_NO_MEMORY;
-			goto fail;
-		}
-	}
-
 
 	/* Active Message interface. AM requests compete with MQ for eager
 	 * buffers, since request establish the amount of buffering in the
@@ -589,91 +825,49 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 	}
 #endif
 #ifdef PSM_CUDA
-	union psmi_envvar_val env_gpudirect_rdma;
-	psmi_getenv("PSM3_GPUDIRECT",
-				"Use GPUDirect RDMA support to allow the NIC to directly read"
-				" from the GPU for SDMA and write to the GPU for TID RDMA."
-				" Requires driver support.(default is disabled i.e. 0)",
-				PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
-				(union psmi_envvar_val)0, /* Disabled by default */
-				&env_gpudirect_rdma);
+	is_gpudirect_enabled = psmi_parse_gpudirect();
+	gpudirect_send_limit = psmi_parse_gpudirect_send_limit();
+	gpudirect_recv_limit = psmi_parse_gpudirect_recv_limit();
+
+	if (! is_gpudirect_enabled) {
+		gpudirect_send_limit = gpudirect_recv_limit = 0;
+	} else if (PSMI_IS_CUDA_DISABLED) {
+		// should not happen since we don't dynamically disable CUDA
+		_HFI_INFO("WARNING: Non-CUDA application, PSM3_GPUDIRECT option ignored\n");
+		is_gpudirect_enabled = 0;
+		gpudirect_send_limit = gpudirect_recv_limit = 0;
+	} else if (!device_support_gpudirect()) {
+		_HFI_INFO("WARNING: GPU device does not support GPU Direct, PSM3_GPUDIRECT option ignored\n");
+		is_gpudirect_enabled = 0;
+		gpudirect_send_limit = gpudirect_recv_limit = 0;
+	} else if (
+		PSMI_IS_DRIVER_GPUDIRECT_DISABLED) {
+		err = psmi_handle_error(PSMI_EP_NORETURN,
+				PSM2_INTERNAL_ERR,
+				"Unable to start run, PSM3_GPUDIRECT requires rv module with CUDA support.\n");
+	} else if (!(protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED)) {
+		// only GDR Copy and GPU Send DMA allowed
+		gpudirect_send_limit = gpudirect_recv_limit = 0;
+	} else {
+		if (gpudirect_send_limit)
+			proto->flags |= IPS_PROTO_FLAG_GPUDIRECT_RDMA_SEND;
+		if (gpudirect_recv_limit)
+			proto->flags |= IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV;
+	}
+	// from here forward can't use psmi_parse_gpudirect,
+	// must use is_gpudirect_enabled
+
 	/* The following cases need to be handled:
 	 * 1) GPU DIRECT is turned off but GDR COPY is turned on by the user or
 	 *    by default - Turn off GDR COPY
+	 * 2) GPU DIRECT is turned on but App, GPU or RV doesn't support it
+	 *    (tested above) - Turn off GDR COPY
 	 * 2) GPU DIRECT is on but GDR COPY is turned off by the user - Leave
 	 *.   this config as it is.
 	 */
-	if (!env_gpudirect_rdma.e_uint)
-		is_gdr_copy_enabled = 0;
-
-	/* Default Send threshold for Gpu-direct set to 30000 */
-	union psmi_envvar_val env_gpudirect_send_thresh;
-	psmi_getenv("PSM3_GPUDIRECT_SEND_THRESH",
-		    "GPUDirect feature on send side will be switched off if threshold value is exceeded.",
-		    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
-		    (union psmi_envvar_val)30000, &env_gpudirect_send_thresh);
-	gpudirect_send_threshold = env_gpudirect_send_thresh.e_uint;
-
-	union psmi_envvar_val env_gpudirect_recv_thresh;
-	psmi_getenv("PSM3_GPUDIRECT_RECV_THRESH",
-		    "GPUDirect feature on receive side will be switched off if threshold value is exceeded.",
-		    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
-		    (union psmi_envvar_val)UINT_MAX, &env_gpudirect_recv_thresh);
-	gpudirect_recv_threshold = env_gpudirect_recv_thresh.e_uint;
-
-	if (env_gpudirect_rdma.e_uint && device_support_gpudirect) {
-		if (PSMI_IS_CUDA_DISABLED ||
-			!(protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED) ||
-			PSMI_IS_DRIVER_GPUDIRECT_DISABLED)
-			err = psmi_handle_error(PSMI_EP_NORETURN,
-					PSM2_INTERNAL_ERR,
-					"Requires hfi1 driver with GPU-Direct feature enabled.\n");
-		proto->flags |= IPS_PROTO_FLAG_GPUDIRECT_RDMA_SEND;
-		proto->flags |= IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV;
-	} else {
-		/* The following environment variables are here for internal
-		 * experimentation and will not be documented for any customers.
-		 */
-		/* Use GPUDirect RDMA for SDMA send? */
-		union psmi_envvar_val env_gpudirect_rdma_send;
-		psmi_getenv("PSM3_GPUDIRECT_RDMA_SEND",
-					"Use GPUDirect RDMA support to allow the NIC to directly"
-					" read from the GPU for SDMA.  Requires driver"
-					" support.(default is disabled i.e. 0)",
-					PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
-					(union psmi_envvar_val)0, /* Disabled by default */
-					&env_gpudirect_rdma_send);
-
-		if (env_gpudirect_rdma_send.e_uint && device_support_gpudirect) {
-			if (PSMI_IS_CUDA_DISABLED
-				|| !(protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED)
-				)
-				err = psmi_handle_error(PSMI_EP_NORETURN,
-						PSM2_INTERNAL_ERR,
-						"Unable to start run as PSM would require cuda, sdma"
-						"and TID support\n");
-			proto->flags |= IPS_PROTO_FLAG_GPUDIRECT_RDMA_SEND;
-		}
-		/* Use GPUDirect RDMA for recv? */
-		union psmi_envvar_val env_gpudirect_rdma_recv;
-		psmi_getenv("PSM3_GPUDIRECT_RDMA_RECV",
-					"Use GPUDirect RDMA support to allow the NIC to directly"
-					" write into GPU.  Requires driver support.(default is"
-					" disabled i.e. 0)",
-					PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
-					(union psmi_envvar_val)0, /* Disabled by default */
-					&env_gpudirect_rdma_recv);
-
-		if (env_gpudirect_rdma_recv.e_uint && device_support_gpudirect) {
-			if (PSMI_IS_CUDA_DISABLED ||
-				!(protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED))
-					err = psmi_handle_error(PSMI_EP_NORETURN,
-							PSM2_INTERNAL_ERR,
-							"Unable to start run as PSM would require cuda,"
-							" sdma and TID support\n");
-			proto->flags |= IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV;
-		}
-	}
+	if (!is_gpudirect_enabled)
+		is_gdr_copy_enabled = gdr_copy_limit_send =
+			gdr_copy_limit_recv = 0;
 
 	if (PSMI_IS_CUDA_ENABLED &&
 		 (protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED)) {
@@ -724,13 +918,154 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 		union psmi_envvar_val env_prefetch_limit;
 
 		psmi_getenv("PSM3_CUDA_PREFETCH_LIMIT",
-			    "How many TID windows to prefetch at RTS time(default is 2)",
-			    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
+			    "How many RDMA windows to prefetch at RTS time(default is 2)",
+			    PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_UINT_FLAGS,
 			    (union psmi_envvar_val)CUDA_WINDOW_PREFETCH_DEFAULT,
 			    &env_prefetch_limit);
 		proto->cuda_prefetch_limit = env_prefetch_limit.e_uint;
 	}
 #endif
+
+	// we allocate MR cache here (as opposed to in protoexp) in case we later
+	// decide to implement RC send for medium messages and use it to register
+	// medium sized user eager buffers (SDMA-like)
+	// We also need to know GPU Direct Copy sizes for pri_size
+	// if RDMA=0 with PSM3_SDMA or PSM3_GPUDIRECT_SDMA can still
+	// allocate cache for just send DMA and perhaps gdrcopy
+	if ((protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED)
+		|| proto->ep->mr_cache_mode) {
+		union psmi_envvar_val env_mr_cache_size;
+		uint32_t default_cache_size;	// in entries
+		uint32_t cache_pri_entries;
+		uint64_t cache_pri_size;	// in bytes
+#ifdef PSM_CUDA
+		uint64_t cache_gpu_pri_size;	// in bytes
+		union psmi_envvar_val env_mr_cache_gpu_evict;
+#endif
+
+		// we can have at most HFI_TF_NFLOWS inbound RDMA and hfi_num_send_rdma
+		// outbound RDMA.  Each of which potentially needs an MR.
+		// so mr_cache_size should be >= HFI_TF_NFLOWS + ep->hfi_num_send_rdma
+		// but can survive if it's smaller as we will delay transfer til avail
+		if (protoexp_flags & IPS_PROTOEXP_FLAG_ENABLED) {
+			cache_pri_entries =  HFI_TF_NFLOWS + proto->ep->hfi_num_send_rdma;
+			cache_pri_size  = (uint64_t)cache_pri_entries * proto->mq->hfi_base_window_rv;
+			if (proto->ep->mr_cache_mode == MR_CACHE_MODE_USER) {
+				// we attempt to cache, so can benefit from more than inflight
+				default_cache_size = cache_pri_entries * 16;
+			} else {
+				// we only reference count
+				// could benefit from some extra so we can preregister MRs for
+				// transfers we don't yet have resources for
+				default_cache_size = cache_pri_entries * 8;
+			}
+		} else {
+			// just for non-priority send DMA
+			cache_pri_entries =  0;
+			cache_pri_size  = 0;
+			if (proto->ep->mr_cache_mode == MR_CACHE_MODE_USER) {
+				// we attempt to cache, so can benefit from more than inflight
+				default_cache_size = 128 * 16;
+			} else {
+				// we only reference count
+				default_cache_size = 128;
+			}
+		}
+		/* Size of user space MR Cache
+		 */
+		psmi_getenv("PSM3_MR_CACHE_SIZE",
+				"user space MR table/cache size (num MRs)",
+				PSMI_ENVVAR_LEVEL_USER,
+				PSMI_ENVVAR_TYPE_UINT,
+				(union psmi_envvar_val)default_cache_size, &env_mr_cache_size);
+
+#ifdef PSM_CUDA
+#ifndef ROUNDUP64P2
+#define ROUNDUP64P2(val, align)   \
+        (((uint64_t)(val) + (uint64_t)(align) - 1) & (~((uint64_t)(align)-1)))
+#endif
+
+		// cache_gpu_pri_size only used to confirm RV GPU cache size
+		// Without GPU Direct we will not register any GPU MRs
+		// if we have GPU Direct w/o RDMA, no priority pin/MRs except
+		// for GDRCopy
+		// since GdrCopy doesn't use psm2_mr_cache, no need to
+		// grow pri_entries to account for it
+		// Note cache_pri_size == 0 if rdmamode not enabled
+		cache_gpu_pri_size = 0;
+		if (PSMI_IS_CUDA_ENABLED && is_gpudirect_enabled) {
+			if (gpudirect_send_limit || gpudirect_recv_limit)
+				cache_gpu_pri_size = cache_pri_size;
+			if (gdr_copy_limit_send || gdr_copy_limit_recv) {
+				// min of one extra for GDRCopy
+				// largest recv with GDR copy is gdr_copy_limit_recv
+				// largest send with GDR copy is gdr_copy_limit_send
+				cache_gpu_pri_size +=
+					ROUNDUP64P2(max(proto->epinfo.ep_mtu,
+							max(gdr_copy_limit_recv,
+							gdr_copy_limit_send)),
+						PSMI_GPU_PAGESIZE);
+			}
+			psmi_getenv("PSM3_RV_GPU_CACHE_EVICT",
+				"Number of kilobytes to evict from GPU cache if can't pin memory for GPUDIRECT (0=just exact amount needed))",
+				PSMI_ENVVAR_LEVEL_USER,
+				PSMI_ENVVAR_TYPE_UINT,
+				(union psmi_envvar_val)0, &env_mr_cache_gpu_evict);
+			gpu_cache_evict = (uint64_t)env_mr_cache_gpu_evict.e_uint * 1024;
+		}
+
+#endif
+		proto->mr_cache = psm2_verbs_alloc_mr_cache(proto->ep,
+						env_mr_cache_size.e_uint, proto->ep->mr_cache_mode,
+						cache_pri_entries, cache_pri_size
+#ifdef PSM_CUDA
+						, cache_gpu_pri_size
+#endif
+						);
+		if (! proto->mr_cache) {
+			_HFI_ERROR( "Unable to allocate MR cache (%u entries)\n",
+					env_mr_cache_size.e_uint);
+			err = PSM2_NO_MEMORY;
+			goto fail;
+		}
+	}
+	// Send DMA only makes sense if we have a MR cache
+	if (proto->ep->mr_cache_mode) {
+		if ((err = proto_sdma_init(proto, context)))
+			goto fail;
+	} else {
+		if (psmi_parse_senddma())
+			_HFI_INFO("WARNING: Send DMA requires an MR Cache, disabling PSM3_SDMA\n");
+		proto->iovec_thresh_eager = proto->iovec_thresh_eager_blocking =
+		    ~0U;
+#ifdef PSM_CUDA
+		proto->iovec_gpu_thresh_eager = proto->iovec_gpu_thresh_eager_blocking =
+		    ~0U;
+#endif
+	}
+#ifdef PSM_CUDA
+	psmi_assert(proto->ep->mr_cache_mode || ! is_gdr_copy_enabled);
+#endif
+#ifdef PSM_CUDA
+	_HFI_DBG("Cuda %d GPU Direct support: driver %d GPU device %d\n",
+		is_cuda_enabled, is_driver_gpudirect_enabled, _device_support_gpudirect);
+	_HFI_DBG("GDR Copy: %d limit send=%u recv=%u cuda_rndv=%u GPU RDMA flags=0x%x limit send=%u recv=%u\n",
+		is_gdr_copy_enabled, gdr_copy_limit_send, gdr_copy_limit_recv,
+		cuda_thresh_rndv,
+		proto->flags & (IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV
+				|IPS_PROTO_FLAG_GPUDIRECT_RDMA_SEND),
+		gpudirect_send_limit, gpudirect_recv_limit);
+	_HFI_DBG("send dma thresh: %u %u GPU send DMA thresh %u %u\n",
+		proto->iovec_thresh_eager, proto->iovec_thresh_eager_blocking,
+		proto->iovec_gpu_thresh_eager,
+		proto->iovec_gpu_thresh_eager_blocking);
+#else
+	_HFI_DBG("send dma thresh: %u %u\n", proto->iovec_thresh_eager,
+		proto->iovec_thresh_eager_blocking);
+#endif
+	_HFI_DBG("rdma: %u MR cache %u\n", proto->ep->rdmamode,
+		proto->ep->mr_cache_mode);
+
 fail:
 	return err;
 }
@@ -922,13 +1257,73 @@ ips_proto_fini(struct ips_proto *proto, int force, uint64_t timeout_in)
 	psmi_mpool_destroy(proto->pend_sends_pool);
 	psmi_mpool_destroy(proto->timer_pool);
 
-	psmi_free(proto->sdma_scb_queue);
 
 fail:
 	proto->t_fini = proto->t_init = 0;
 	return err;
 }
 
+static
+psm2_error_t
+proto_sdma_init(struct ips_proto *proto, const psmi_context_t *context)
+{
+	union psmi_envvar_val env_sdma, env_hfiegr;
+	psm2_error_t err = PSM2_OK;
+
+	env_sdma.e_uint = psmi_parse_senddma();
+	if (!env_sdma.e_uint) {
+		proto->iovec_thresh_eager = proto->iovec_thresh_eager_blocking =
+		    ~0U;
+	} else if (! psm2_verbs_mr_cache_allows_user_mr(proto->mr_cache)) {
+		_HFI_INFO("WARNING: Cache does not allow user MRs, disabling PSM3_SDMA (check rv enable_user_mr)\n");
+		proto->iovec_thresh_eager = proto->iovec_thresh_eager_blocking =
+		    ~0U;
+	} else {
+		proto->iovec_thresh_eager = MQ_HFI_THRESH_EGR_SDMA_SQ;
+		proto->iovec_thresh_eager_blocking = MQ_HFI_THRESH_EGR_SDMA;
+
+		if (!psmi_getenv("PSM3_MQ_EAGER_SDMA_THRESH",
+				"UD copy-to-sdma eager switchover threshold",
+				PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
+				(union psmi_envvar_val) proto->iovec_thresh_eager,
+				&env_hfiegr)) {
+			proto->iovec_thresh_eager = proto->iovec_thresh_eager_blocking =
+				 env_hfiegr.e_uint;
+		}
+	}
+
+#ifdef PSM_CUDA
+	if (! is_gpudirect_enabled)
+		env_sdma.e_uint = 0;
+	else 
+		psmi_getenv("PSM3_GPUDIRECT_SDMA",
+		    "UD GPU send dma flags (0 disables send dma, 1 enables), default 1",
+		    PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT_FLAGS,
+		    (union psmi_envvar_val)1, &env_sdma);
+	if (!env_sdma.e_uint) {
+		proto->iovec_gpu_thresh_eager = proto->iovec_gpu_thresh_eager_blocking =
+		    ~0U;
+	} else if (! psm2_verbs_mr_cache_allows_user_mr(proto->mr_cache)) {
+		_HFI_INFO("WARNING: Cache does not allow user MRs, disabling PSM3_GPUDIRECT_SDMA (check rv enable_user_mr)\n");
+		proto->iovec_gpu_thresh_eager = proto->iovec_gpu_thresh_eager_blocking =
+		    ~0U;
+	} else {
+		proto->iovec_gpu_thresh_eager = MQ_HFI_THRESH_GPU_EGR_SDMA_SQ;
+		proto->iovec_gpu_thresh_eager_blocking = MQ_HFI_THRESH_GPU_EGR_SDMA;
+
+		if (!psmi_getenv("PSM3_GPU_MQ_EAGER_SDMA_THRESH",
+				"UD GPU copy-to-sdma eager switchover threshold",
+				PSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_UINT,
+				(union psmi_envvar_val) proto->iovec_gpu_thresh_eager,
+				&env_hfiegr)) {
+			proto->iovec_gpu_thresh_eager = proto->iovec_gpu_thresh_eager_blocking =
+				 env_hfiegr.e_uint;
+		}
+	}
+#endif
+
+	return err;
+}
 
 static
 void ctrlq_init(struct ips_ctrlq *ctrlq, struct ips_proto *proto)
@@ -1164,7 +1559,7 @@ ips_proto_send_ctrl_message(struct ips_flow *flow, uint8_t message_type,
 		ips_proto_epaddr_stats_set(proto, message_type);
 	}
 
-	_HFI_VDBG("transfer_frame of opcode=0x%x,remote_lid=%d,"
+	_HFI_VDBG("transfer_frame of opcode=%x,remote_lid=%d,"
 		  "src=%p,len=%d returns %d\n",
 		  (int)_get_proto_hfi_opcode(&ctrlscb->ips_lrh),
 		  __be16_to_cpu(ctrlscb->ips_lrh.lrh[1]), payload, paylen, err);
@@ -1331,7 +1726,7 @@ ips_proto_flow_flush_pio(struct ips_flow *flow, int *nflushed)
 
 	/* If out of flow credits re-schedule send timer */
 	if (!SLIST_EMPTY(scb_pend)) {
-		proto->stats.pio_busy_cnt++;
+		proto->stats.pio_no_flow_credits++;
 		psmi_timer_request(proto->timerq, flow->timer_send,
 				   get_cycles() + proto->timeout_send);
 	}
@@ -1342,180 +1737,12 @@ ips_proto_flow_flush_pio(struct ips_flow *flow, int *nflushed)
 	return err;
 }
 
-/*
- * Flush all packets currently marked as pending
- */
-static psm2_error_t scb_dma_send(struct ips_proto *proto, struct ips_flow *flow,
-				struct ips_scb_pendlist *slist, int *num_sent);
-
-/*
- * Flush all packets queued up on a flow via send DMA.
- *
- * Recoverable errors:
- * PSM2_OK: Able to flush entire pending queue for DMA.
- * PSM2_OK_NO_PROGRESS: Flushed at least 1 but not all pending packets for DMA.
- * PSM2_EP_NO_RESOURCES: No scb's available to handle unaligned packets
- *                      or writev returned a recoverable error (no mem for
- *                      descriptors, dma interrupted or no space left in dma
- *                      queue).
- *
- * Unrecoverable errors:
- * PSM2_EP_DEVICE_FAILURE: Unexpected error calling writev(), chip failure,
- *			  rxe/txe parity error.
- * PSM2_EP_NO_NETWORK: No network, no lid, ...
- */
-psm2_error_t
-ips_proto_flow_flush_dma(struct ips_flow *flow, int *nflushed)
-{
-	struct ips_proto *proto = ((psm2_epaddr_t) (flow->ipsaddr))->proto;
-	struct ips_scb_pendlist *scb_pend = &flow->scb_pend;
-	ips_scb_t *scb = NULL;
-	psm2_error_t err = PSM2_OK;
-	int nsent = 0;
-
-	psmi_assert(!SLIST_EMPTY(scb_pend));
-
-	/* Out of credits - ACKs/NAKs reclaim recredit or congested flow */
-	if_pf((flow->credits <= 0)
-		) {
-		if (nflushed)
-			*nflushed = 0;
-		return PSM2_EP_NO_RESOURCES;
-	}
-
-	// scb will descrbe header needed, which may be TID
-	err = scb_dma_send(proto, flow, scb_pend, &nsent);
-	if (err != PSM2_OK && err != PSM2_EP_NO_RESOURCES &&
-	    err != PSM2_OK_NO_PROGRESS)
-		goto fail;
-
-	if (nsent > 0) {
-		uint64_t t_cyc = get_cycles();
-		int i = 0;
-		/*
-		 * inflight counter proto->iovec_cntr_next_inflight should not drift
-		 * from completion counter proto->iovec_cntr_last_completed away too
-		 * far because we only have very small scb counter compared with
-		 * uint32_t counter value.
-		 */
-#ifdef PSM_DEBUG
-		flow->scb_num_pending -= nsent;
-#endif
-		SLIST_FOREACH(scb, scb_pend, next) {
-			if (++i > nsent)
-				break;
-
-			PSM2_LOG_PKT_STRM(PSM2_LOG_TX,&scb->ips_lrh,"PKT_STRM: (dma)");
 
-			scb->scb_flags &= ~IPS_SEND_FLAG_PENDING;
-			scb->ack_timeout =
-			    scb->nfrag * proto->epinfo.ep_timeout_ack;
-			scb->abs_timeout =
-			    scb->nfrag * proto->epinfo.ep_timeout_ack + t_cyc;
-
-			psmi_assert(proto->sdma_scb_queue
-					[proto->sdma_fill_index] == NULL);
-			proto->sdma_scb_queue[proto->sdma_fill_index] = scb;
-			scb->dma_complete = 0;
-
-			proto->sdma_avail_counter--;
-			proto->sdma_fill_index++;
-			if (proto->sdma_fill_index == proto->sdma_queue_size)
-				proto->sdma_fill_index = 0;
-
-			/* Flow credits can temporarily go to negative for
-			 * packets tracking purpose, because we have sdma
-			 * chunk processing which can't send exact number
-			 * of packets as the number of credits.
-			 */
-			flow->credits -= scb->nfrag;
-		}
-		SLIST_FIRST(scb_pend) = scb;
-	}
-
-	if (SLIST_FIRST(scb_pend) != NULL) {
-		psmi_assert(flow->scb_num_pending > 0);
-
-		switch (flow->protocol) {
-		case PSM_PROTOCOL_TIDFLOW:
-			// for UD we use RC QP instead of STL100's TIDFLOW HW
-			// UDP has no RDMA
-			psmi_assert_always(0);	// we don't allocate ips_flow for TID
-
-			break;
-		case PSM_PROTOCOL_GO_BACK_N:
-		default:
-			if (flow->credits > 0) {
-				/* Schedule send timer and increment writev_busy_cnt */
-				psmi_timer_request(proto->timerq,
-						   flow->timer_send,
-						   get_cycles() +
-						   (proto->timeout_send << 1));
-				proto->stats.writev_busy_cnt++;
-			} else {
-				/* Schedule ACK timer to reap flow credits */
-				psmi_timer_request(proto->timerq,
-						   flow->timer_ack,
-						   get_cycles() +
-						   (proto->epinfo.
-						    ep_timeout_ack >> 2));
-			}
-			break;
-		}
-	} else {
-		/* Schedule ack timer */
-		psmi_timer_cancel(proto->timerq, flow->timer_send);
-		psmi_timer_request(proto->timerq, flow->timer_ack,
-				   get_cycles() + proto->epinfo.ep_timeout_ack);
-	}
-
-	/* We overwrite error with its new meaning for flushing packets */
-	if (nsent > 0)
-		if (scb)
-			err = PSM2_OK_NO_PROGRESS;	/* partial flush */
-		else
-			err = PSM2_OK;	/* complete flush */
-	else
-		err = PSM2_EP_NO_RESOURCES;	/* no flush at all */
-
-fail:
-	if (nflushed)
-		*nflushed = nsent;
-
-	return err;
-}
 
 
 
 
 
-/*
- * Caller still expects num_sent to always be correctly set in case of an
- * error.
- *
- * Recoverable errors:
- * PSM2_OK: At least one packet was successfully queued up for DMA.
- * PSM2_EP_NO_RESOURCES: No scb's available to handle unaligned packets
- *                      or writev returned a recoverable error (no mem for
- *                      descriptors, dma interrupted or no space left in dma
- *                      queue).
- * PSM2_OK_NO_PROGRESS: Cable pulled.
- *
- * Unrecoverable errors:
- * PSM2_EP_DEVICE_FAILURE: Error calling hfi_sdma_inflight() or unexpected
- *                        error in calling writev(), or chip failure, rxe/txe
- *                        parity error.
- * PSM2_EP_NO_NETWORK: No network, no lid, ...
- */
-static
-psm2_error_t
-scb_dma_send(struct ips_proto *proto, struct ips_flow *flow,
-	     struct ips_scb_pendlist *slist, int *num_sent)
-{
-	psmi_assert_always(0);	// should not get here
-	return PSM2_INTERNAL_ERR;
-}
-
 
 psm2_error_t
 ips_proto_timer_ack_callback(struct psmi_timer *current_timer,
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto.h b/prov/psm3/psm3/ptl_ips/ips_proto.h
index 9a8270b..3a4b082 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto.h
+++ b/prov/psm3/psm3/ptl_ips/ips_proto.h
@@ -216,6 +216,8 @@ struct ips_protoexp;
 
 struct ips_proto_stats {
 	uint64_t pio_busy_cnt;
+	uint64_t pio_no_flow_credits;
+	uint64_t post_send_fail;
 	uint64_t writev_busy_cnt;
 	uint64_t scb_egr_unavail_cnt;
 	uint64_t unknown_packets;
@@ -232,6 +234,8 @@ struct ips_proto_epaddr_stats {
 #ifdef RNDV_MOD
 	uint64_t err_chk_rdma_send;
 	uint64_t err_chk_rdma_recv;
+	uint64_t err_chk_rdma_resp_send;
+	uint64_t err_chk_rdma_resp_recv;
 #endif
 	uint64_t nak_send;
 	uint64_t nak_recv;
@@ -243,8 +247,12 @@ struct ips_proto_epaddr_stats {
 	uint64_t disconnect_req_recv;
 	uint64_t disconnect_rep_send;
 	uint64_t disconnect_rep_recv;
-	uint64_t tids_grant_send;
-	uint64_t tids_grant_recv;
+	uint64_t rts_send;
+	uint64_t rts_recv;
+	uint64_t cts_long_data_send;
+	uint64_t cts_long_data_recv;
+	uint64_t cts_rdma_send;
+	uint64_t cts_rdma_recv;
 	uint64_t send_rexmit;
 #ifdef RNDV_MOD
 	uint64_t rdma_rexmit;
@@ -302,16 +310,15 @@ struct ips_proto {
 	struct ips_scbctrl scbc_egr;
 	struct ips_epinfo epinfo;
 
-	ips_scb_t **sdma_scb_queue;
-	uint16_t sdma_queue_size;
-	uint16_t sdma_fill_index;
-	uint16_t sdma_done_index;
-	uint16_t sdma_avail_counter;
 
 	uint64_t timeout_send;
 	uint32_t flags;
 	uint32_t iovec_thresh_eager;
 	uint32_t iovec_thresh_eager_blocking;
+#ifdef PSM_CUDA
+	uint32_t iovec_gpu_thresh_eager;
+	uint32_t iovec_gpu_thresh_eager_blocking;
+#endif
 	uint32_t psn_mask;
 	uint32_t scb_bufsize;
 	uint32_t multirail_thresh_load_balance;
@@ -321,6 +328,7 @@ struct ips_proto {
 	struct ips_ibta_compliance_fn ibta;
 	struct ips_proto_stats stats;
 	struct ips_proto_epaddr_stats epaddr_stats;
+	struct ptl_strategy_stats strat_stats;
 
 	struct ips_proto_am proto_am;
 
@@ -616,15 +624,10 @@ void MOCKABLE(ips_proto_flow_enqueue)(struct ips_flow *flow, ips_scb_t *scb);
 MOCK_DCL_EPILOGUE(ips_proto_flow_enqueue);
 
 psm2_error_t ips_proto_flow_flush_pio(struct ips_flow *flow, int *nflushed);
-psm2_error_t ips_proto_flow_flush_dma(struct ips_flow *flow, int *nflushed);
 
 /* Wrapper for enqueue + flush */
 psm2_error_t ips_proto_scb_pio_send(struct ips_flow *flow, ips_scb_t *scb);
 
-void ips_proto_scb_dma_enqueue(struct ips_proto *proto, ips_scb_t *scb);
-psm2_error_t ips_proto_scb_dma_flush(struct ips_proto *proto,
-				    ips_epaddr_t *ipsaddr, int *nflushed);
-psm2_error_t ips_proto_dma_wait_until(struct ips_proto *proto, ips_scb_t *scb);
 
 /*
  * Protocol receive processing
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_connect.c b/prov/psm3/psm3/ptl_ips/ips_proto_connect.c
index 4e7fa6a..fecb340 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_connect.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_connect.c
@@ -61,6 +61,7 @@
 #ifdef RNDV_MOD
 #include "psm_rndv_mod.h"
 #endif
+
 /*
  * define connection version. this is the basic version, optimized
  * version will be added later for scalability.
@@ -214,6 +215,7 @@ static int is_rv_connected(ips_epaddr_t *ipsaddr)
 #else // RNDV_MOD
 static inline int is_rv_connected(ips_epaddr_t *ipsaddr) { return 1; }
 #endif // RNDV_MOD
+	
 /**
  * Configure flows for an ipsaddr.
  *
@@ -707,11 +709,8 @@ MOCKABLE(ips_flow_init)(struct ips_flow *flow, struct ips_proto *proto,
 	psmi_assert_always(flow_index < EP_FLOW_LAST);
 
 	SLIST_NEXT(flow, next) = NULL;
-	if (transfer_type == PSM_TRANSFER_PIO) {
-		flow->flush = ips_proto_flow_flush_pio;
-	} else {
-		flow->flush = ips_proto_flow_flush_dma;
-	}
+	psmi_assert(transfer_type == PSM_TRANSFER_PIO);
+	flow->flush = ips_proto_flow_flush_pio;
 
 	flow->path =
 	    ips_select_path(proto, path_type, ipsaddr, ipsaddr->pathgrp);
@@ -725,7 +724,7 @@ MOCKABLE(ips_flow_init)(struct ips_flow *flow, struct ips_proto *proto,
 
 	/* min of local MTU and path MTU */
 	flow->frag_size = min(proto->epinfo.ep_mtu, flow->path->pr_mtu);
-	_HFI_VDBG("[ipsaddr=%p] UD flow->frag_size: %u = min("
+	_HFI_CONNDBG("[ipsaddr=%p] UD flow->frag_size: %u = min("
 		"proto->epinfo.ep_mtu(%u), flow->path->pr_mtu(%u))\n",
 		ipsaddr, flow->frag_size, proto->epinfo.ep_mtu,
 		flow->path->pr_mtu);
@@ -777,7 +776,6 @@ ips_alloc_epaddr(struct ips_proto *proto, int master, psm2_epid_t epid,
 	 */
 	if (master) {
 		struct ips_msgctl *msgctl;
-		_HFI_VDBG("ips_alloc_epaddr for EPID= 0x%"PRIx64"\n", epid);
 
 		/* Although an ips_msgtl is allocated here, it can be safely casted to
 		   both an ips_epaddr and a psm2_epaddr.  It is eventually freed as an
@@ -794,6 +792,8 @@ ips_alloc_epaddr(struct ips_proto *proto, int master, psm2_epid_t epid,
 		ipsaddr = &msgctl->master_epaddr;
 		epaddr = (psm2_epaddr_t) ipsaddr;
 
+		_HFI_CONNDBG("ips_alloc_epaddr %p for EPID= 0x%"PRIx64" %s\n",
+				epaddr, epid, hostname?hostname:"unknown");
 		ipsaddr->msgctl = msgctl;
 
 		/* initialize items in ips_msgctl_t */
@@ -830,7 +830,7 @@ ips_alloc_epaddr(struct ips_proto *proto, int master, psm2_epid_t epid,
 	/* Get path record for <service, slid, dlid> tuple */
 	lid = PSMI_EPID_GET_LID(epid);
 	ip_hi = PSMI_EPID_GET_LID(epid) >> 16;
-	_HFI_UDDBG("qpn=0x%x lid=0x%x ip_hi=0x%x\n", ipsaddr->remote_qpn, lid, ip_hi);
+	_HFI_CONNDBG("qpn=0x%x lid=0x%x ip_hi=0x%x\n", ipsaddr->remote_qpn, lid, ip_hi);
 
 	err = proto->ibta.get_path_rec(proto, proto->epinfo.ep_base_lid,
 				       __cpu_to_be16(lid),
@@ -963,8 +963,9 @@ void ips_free_epaddr(psm2_epaddr_t epaddr, struct ips_proto *proto)
 	ips_epaddr_t *ipsaddr = (ips_epaddr_t *) epaddr;
 	ips_flow_fini(ipsaddr, proto);
 
-	_HFI_VDBG("epaddr=%p,ipsaddr=%p,connidx_incoming=%d\n", epaddr, ipsaddr,
-		  ipsaddr->connidx_incoming);
+	_HFI_CONNDBG("epaddr=%p connidx_incoming=%d epid=0x%"PRIx64"\n",
+			epaddr, ipsaddr->connidx_incoming, epaddr->epid);
+	IPS_MCTXT_REMOVE(ipsaddr);
 #ifdef RNDV_MOD
 	_HFI_MMDBG("free_epaddr\n");
 	if (ipsaddr->rv_conn) {
@@ -1036,7 +1037,7 @@ ips_proto_process_connect(struct ips_proto *proto, uint8_t opcode,
 				     hdr->epid, psmi_epaddr_fmt_addr(hdr->epid));
 			} else if (ipsaddr->cstate_outgoing != CSTATE_OUTGOING_WAITING) {
 				/* possible dupe */
-				_HFI_VDBG("connect dupe, expected %d got %d\n",
+				_HFI_CONNDBG("connect dupe, expected %d got %d\n",
 					  CSTATE_OUTGOING_WAITING,
 					  ipsaddr->cstate_outgoing);
 			} else {
@@ -1081,7 +1082,7 @@ ips_proto_process_connect(struct ips_proto *proto, uint8_t opcode,
 			int epaddr_do_free = 0;
 			psmi_assert_always(paylen ==
 					   sizeof(struct ips_connect_hdr));
-			_HFI_VDBG("Got a disconnect from %s\n",
+			_HFI_CONNDBG("Got a disconnect from %s\n",
 				  psmi_epaddr_get_name(hdr->epid));
 			proto->num_disconnect_requests++;
 			proto->epaddr_stats.disconnect_req_recv++;
@@ -1127,7 +1128,7 @@ ips_proto_process_connect(struct ips_proto *proto, uint8_t opcode,
 					      PSM_PROTOCOL_GO_BACK_N,
 					      IPS_PATH_LOW_PRIORITY,
 					      EP_FLOW_GO_BACK_N_PIO);
-				_HFI_VDBG
+				_HFI_CONNDBG
 				    ("Disconnect on unknown epaddr, just echo request\n");
 			} else if (ipsaddr->cstate_incoming != CSTATE_NONE) {
 				ipsaddr->cstate_incoming = CSTATE_NONE;
@@ -1200,10 +1201,10 @@ ptl_handle_connect_req(struct ips_proto *proto, psm2_epaddr_t epaddr,
 	if (req->epid == proto->ep->epid) {
 		psmi_handle_error(PSMI_EP_NORETURN, PSM2_EPID_NETWORK_ERROR,
 				  "Network connectivity problem: Locally detected duplicate "
-				  "LIDs 0x%04x on hosts %s and %s. (Exiting)",
+				  "LIDs 0x%04x on hosts %s and %s (%s port %u). (Exiting)",
 				  (uint32_t) psm2_epid_nid(req->epid),
 				  psmi_epaddr_get_hostname(req->epid),
-				  psmi_gethostname());
+				  psmi_gethostname(), proto->ep->dev_name, proto->ep->portnum);
 		/* XXX no return */
 		abort();
 	} else if (epaddr == NULL) {	/* new ep connect before we call into connect */
@@ -1348,7 +1349,7 @@ ips_proto_connect(struct ips_proto *proto, int numep,
 
 	/* First pass: make sure array of errors is at least fully defined */
 	for (i = 0; i < numep; i++) {
-		_HFI_VDBG("epid-connect=%s connect to epid 0x%"PRIx64": %s\n",
+		_HFI_CONNDBG("epid-connect=%s connect to epid 0x%"PRIx64": %s\n",
 			  array_of_epid_mask[i] ? "YES" : " NO",
 			  array_of_epid[i],
 			  psmi_epaddr_fmt_addr(array_of_epid[i]));
@@ -1379,14 +1380,16 @@ ips_proto_connect(struct ips_proto *proto, int numep,
 			char buf2[INET_ADDRSTRLEN];
 			if (PSMI_EPID_VERSION == PSMI_EPID_V3)
 					psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-					  " Trying to connect to a node (subnet id - %"PRIx64") on a"
+					  " Trying to connect from %s port %u to a node (subnet id - %"PRIx64") on a"
 					  " different subnet - %"PRIx64"\n",
+					  proto->ep->dev_name, proto->ep->portnum,
 					  PSMI_GET_SUBNET_ID(proto->ep->gid_hi),
 					  (uint64_t)PSMI_EPID_GET_SUBNET_ID(array_of_epid[i]));
 			else // V4
 					psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-					  " Trying to connect to a node (subnet %s) on a"
+					  " Trying to connect from %s to a node (subnet %s) on a"
 					  " different subnet %s\n",
+					  proto->ep->dev_name,
 					  psmi_ipv4_ntop((uint32_t)PSMI_GET_SUBNET_ID(proto->ep->gid_hi), buf1, sizeof(buf1)),
 					  psmi_ipv4_ntop((uint32_t)PSMI_EPID_GET_SUBNET_ID(array_of_epid[i]), buf2, sizeof(buf2)));
 		}
@@ -1400,8 +1403,8 @@ ips_proto_connect(struct ips_proto *proto, int numep,
 						  NULL,
 						  (timeout_in / 1000000UL), &err);
 			if (epaddr == NULL) {
-				_HFI_ERROR("Unable to issue connect to %s: %s\n",
-						psmi_epaddr_get_name(array_of_epid[i]),
+				_HFI_ERROR("Unable to issue connect from %s to %s: %s\n",
+						proto->ep->dev_name, psmi_epaddr_get_name(array_of_epid[i]),
 						psm2_error_get_string(err));
 				goto fail;
 			}
@@ -1746,7 +1749,10 @@ ips_proto_disconnect(struct ips_proto *proto, int force, int numep,
 			psmi_assert_always(ipsaddr->cstate_outgoing ==
 					   CSTATE_ESTABLISHED);
 		}
-		_HFI_VDBG("disconnecting %p\n", array_of_epaddr[i]);
+		_HFI_CONNDBG("disconnecting %p force=%d EPID= 0x%"PRIx64" %s\n",
+				ipsaddr, force, ((psm2_epaddr_t)ipsaddr)->epid,
+
+				psmi_epaddr_get_hostname(((psm2_epaddr_t)ipsaddr)->epid));
 		array_of_errors[i] = PSM2_EPID_UNKNOWN;
 		numep_todisc++;
 	}
@@ -1866,7 +1872,7 @@ ips_proto_disconnect(struct ips_proto *proto, int force, int numep,
 					continue;
 				if (array_of_errors[i] == PSM2_EPID_UNKNOWN) {
 					array_of_errors[i] = PSM2_TIMEOUT;
-					_HFI_VDBG
+					_HFI_CONNDBG
 					    ("disc timeout on index %d, epaddr %s\n",
 					     i,
 					     psmi_epaddr_get_name
@@ -1904,7 +1910,7 @@ ips_proto_disconnect(struct ips_proto *proto, int force, int numep,
 			ipsaddr->cstate_outgoing = CSTATE_OUTGOING_DISCONNECTED;
 			array_of_errors[i] = PSM2_OK;
 		}
-		_HFI_VDBG("non-graceful close complete from %d peers\n", numep);
+		_HFI_CONNDBG("non-graceful close complete from %d peers\n", numep);
 	}
 
 	for (i = 0; i < numep; i++) {
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_expected.c b/prov/psm3/psm3/ptl_ips/ips_proto_expected.c
index 44db046..643a271 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_expected.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_expected.c
@@ -85,6 +85,7 @@ static void ips_protoexp_send_err_chk_rdma_resp(struct ips_flow *flow);
 static void ips_tid_reissue_rdma_write(struct ips_tid_send_desc *tidsendc);
 #endif
 
+
 static void ips_tid_scbavail_callback(struct ips_scbctrl *scbc, void *context);
 static void ips_tidflow_avail_callback(struct ips_tf *tfc, void *context);
 
@@ -234,6 +235,7 @@ MOCKABLE(ips_protoexp_init)(const psmi_context_t *context,
 #endif
 
 
+
 #ifdef PSM_CUDA
 	{
 		if (PSMI_IS_CUDA_ENABLED &&
@@ -283,9 +285,7 @@ MOCKABLE(ips_protoexp_init)(const psmi_context_t *context,
 				goto fail;
 			}
 
-			PSMI_CUDA_CALL(cuStreamCreate,
-				&protoexp->cudastream_recv,
-				CU_STREAM_NON_BLOCKING);
+			protoexp->cudastream_recv = NULL;
 			STAILQ_INIT(&protoexp->cudapend_getreqsq);
 		} else {
 			protoexp->cuda_hostbuf_pool_recv = NULL;
@@ -324,7 +324,9 @@ psm2_error_t ips_protoexp_fini(struct ips_protoexp *protoexp)
 		 !(protoexp->proto->flags & IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV)) {
 		psmi_mpool_destroy(protoexp->cuda_hostbuf_pool_small_recv);
 		psmi_mpool_destroy(protoexp->cuda_hostbuf_pool_recv);
-		PSMI_CUDA_CALL(cuStreamDestroy, protoexp->cudastream_recv);
+		if (protoexp->cudastream_recv != NULL) {
+			PSMI_CUDA_CALL(cuStreamDestroy, protoexp->cudastream_recv);
+		}
 	}
 #endif
 	psmi_mpool_destroy(protoexp->tid_getreq_pool);
@@ -369,7 +371,9 @@ void ips_tid_scbavail_callback(struct ips_scbctrl *scbc, void *context)
 
 void ips_tid_mravail_callback(struct ips_proto *proto)
 {
-	ips_tid_scbavail_callback(NULL, proto->protoexp);
+	// if we have Send DMA but not RDMA, no proto->protoexp
+	if (proto->protoexp)
+		ips_tid_scbavail_callback(NULL, proto->protoexp);
 }
 
 
@@ -459,13 +463,26 @@ ips_protoexp_tid_get_from_token(struct ips_protoexp *protoexp,
 	    !(protoexp->proto->flags & IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV)) ||
 	    ((req->is_buf_gpu_mem &&
 	     (protoexp->proto->flags & IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV) &&
-	     gpudirect_recv_threshold &&
-	     length > gpudirect_recv_threshold))) {
+	     (length > gpudirect_recv_limit
+		|| length & 0x03 || (uintptr_t)buf & 0x03
+ 		)))) {
 		getreq->cuda_hostbuf_used = 1;
 		getreq->tidgr_cuda_bytesdone = 0;
 		STAILQ_INIT(&getreq->pend_cudabuf);
-	} else
+		protoexp->proto->strat_stats.rndv_rdma_hbuf_recv++;
+		protoexp->proto->strat_stats.rndv_rdma_hbuf_recv_bytes += length;
+	} else {
 		getreq->cuda_hostbuf_used = 0;
+		if (req->is_buf_gpu_mem) {
+			protoexp->proto->strat_stats.rndv_rdma_gdr_recv++;
+			protoexp->proto->strat_stats.rndv_rdma_gdr_recv_bytes += length;
+		} else {
+#endif
+			protoexp->proto->strat_stats.rndv_rdma_cpu_recv++;
+			protoexp->proto->strat_stats.rndv_rdma_cpu_recv_bytes += length;
+#ifdef PSM_CUDA
+		}
+	}
 #endif
 
 	/* nbytes is the bytes each channel should transfer. */
@@ -587,6 +604,7 @@ ips_protoexp_send_tid_grant(struct ips_tid_recv_desc *tidrecvc)
 	PSM2_LOG_EPM(OPCODE_LONG_CTS,PSM2_LOG_TX, proto->ep->epid,
 		    flow->ipsaddr->epaddr.epid ,"tidrecvc->getreq->tidgr_sendtoken; %d",
 		    tidrecvc->getreq->tidgr_sendtoken);
+	proto->epaddr_stats.cts_rdma_send++;
 
 	ips_proto_flow_enqueue(flow, scb);
 	flow->flush(flow, NULL);
@@ -647,7 +665,9 @@ ips_protoexp_tidsendc_complete(struct ips_tid_send_desc *tidsendc)
 	}
 #endif
 	/* Check if we can complete the send request. */
-	if (req->send_msgoff == req->req_data.send_msglen) {
+	_HFI_MMDBG("ips_protoexp_tidsendc_complete off %u req len %u\n",
+		req->send_msgoff, req->req_data.send_msglen);
+	if (req->send_msgoff >= req->req_data.send_msglen) {
 		psmi_mq_handle_rts_complete(req);
 	}
 
@@ -710,7 +730,7 @@ ips_protoexp_rdma_write_completion_error(psm2_ep_t ep, uint64_t wr_id,
 	}
 	protoexp = tidsendc->protoexp;
 	_HFI_MMDBG("failed rv RDMA Write on %s to %s status: '%s' (%d)\n",
-			ep->verbs_ep.ib_devname,
+			ep->dev_name,
 			psmi_epaddr_get_name(tidsendc->ipsaddr->epaddr.epid),
 			ibv_wc_status_str(wc_status),(int)wc_status);
 
@@ -738,7 +758,7 @@ ips_protoexp_rdma_write_completion_error(psm2_ep_t ep, uint64_t wr_id,
 fail:
 	psmi_handle_error( PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
 			"failed rv RDMA Write on %s to %s status: '%s' (%d)\n",
-			ep->verbs_ep.ib_devname,
+			ep->dev_name,
 			psmi_epaddr_get_name(tidsendc->ipsaddr->epaddr.epid),
 			ibv_wc_status_str(wc_status),(int)wc_status);
 fail_ret:
@@ -771,7 +791,7 @@ static psm2_error_t ips_protoexp_send_err_chk_rdma(struct ips_tid_send_desc *tid
 			tidsendc->rv_sconn_index, &conn_count)) {
 		psmi_handle_error( PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
 			"send_err_chk_rdma: Connect unrecoverable on %s to %s\n",
-			proto->ep->verbs_ep.ib_devname,
+			proto->ep->dev_name,
 			psmi_epaddr_get_name(ipsaddr->epaddr.epid));
 		err = PSM2_TIMEOUT; /* force a resend reschedule */
 		goto done;
@@ -805,7 +825,7 @@ static psm2_error_t ips_protoexp_send_err_chk_rdma(struct ips_tid_send_desc *tid
 			ipsaddr->epaddr.epid,
 			"psmi_mpool_get_obj_index(tidsendc->mqreq): %d, tidsendc->rdescid. _desc_genc %d _desc_idx: %d, tidsendc->sdescid._desc_idx: %d",
 			psmi_mpool_get_obj_index(tidsendc->mqreq),
-			tidsendc->rdesc_id._dsc_genc,tidsendc->rdescid._desc_idx,
+			tidsendc->rdescid._desc_genc,tidsendc->rdescid._desc_idx,
 			tidsendc->sdescid._desc_idx);
 
 	ips_scb_opcode(scb) = OPCODE_ERR_CHK_RDMA;
@@ -881,14 +901,14 @@ int ips_protoexp_process_err_chk_rdma(struct ips_recvhdrq_event *rcv_ev)
 	PSM2_LOG_EPM(OPCODE_ERR_CHK_RDMA,PSM2_LOG_RX,ipsaddr->epaddr.epid,
 			proto->ep->epid,
 			"rdescid._desc_genc %d _desc_idx: %d, sdescid._desc_idx: %d",
-			rdesc_id._dsc_genc,rdescid._desc_idx, sdescid._desc_idx);
+			rdesc_id._desc_genc,rdesc_id._desc_idx, sdesc_id._desc_idx);
 
 	if (ipsaddr->rv_need_send_err_chk_rdma_resp) {
 		/* sender has >1 err chk rdma outstanding: protocol violation */
 		psmi_handle_error( PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
 			"process_err_chk_rdma: Protocol Violation: > 1 outstanding from remote node %s on %s\n",
 			psmi_epaddr_get_name(ipsaddr->epaddr.epid),
-			proto->ep->verbs_ep.ib_devname);
+			proto->ep->dev_name);
 		goto do_acks;
 	}
 
@@ -959,6 +979,7 @@ done:
 }
 #endif // RNDV_MOD
 
+
 #ifdef RNDV_MOD
 static
 void ips_protoexp_send_err_chk_rdma_resp(struct ips_flow *flow)
@@ -984,7 +1005,7 @@ void ips_protoexp_send_err_chk_rdma_resp(struct ips_flow *flow)
 			ipsaddr->rv_err_chk_rdma_resp_need_resend,
 			ipsaddr->rv_err_chk_rdma_resp_rdesc_id._desc_genc,
 			ipsaddr->rv_err_chk_rdma_resp_rdesc_id._desc_idx,
-			ipsaddr->rv_err_chk_rdma_resp_sdesc_id._desc_idx)
+			ipsaddr->rv_err_chk_rdma_resp_sdesc_id._desc_idx);
 
 	ips_scb_opcode(scb) = OPCODE_ERR_CHK_RDMA_RESP;
 	scb->ips_lrh.khdr.kdeth0 = 0;
@@ -1000,6 +1021,8 @@ void ips_protoexp_send_err_chk_rdma_resp(struct ips_flow *flow)
 	// The scb will own reliable transmission of resp, we can clear flag
 	ipsaddr->rv_need_send_err_chk_rdma_resp = 0;
 
+	proto->epaddr_stats.err_chk_rdma_resp_send++;
+
 	ips_proto_flow_enqueue(flow, scb);
 	flow->flush(flow, NULL);
 
@@ -1028,12 +1051,14 @@ int ips_protoexp_process_err_chk_rdma_resp(struct ips_recvhdrq_event *rcv_ev)
 
 	/* processing specific to err chk rdma resp packet */
 
+	protoexp->proto->epaddr_stats.err_chk_rdma_resp_recv++;
+
 	_HFI_MMDBG("received ERR_CHK_RDMA_RESP\n");
 	PSM2_LOG_EPM(OPCODE_ERR_CHK_RDMA,PSM2_LOG_RX,ipsaddr->epaddr.epid,
 			protoexp->proto->ep->epid,
 			"rdescid. _desc_genc %d _desc_idx: %d, sdescid._desc_idx: %d",
-			p_hdr->data[0]._dsc_genc,p_hdr->data[0]._desc_idx,
-			sdescid._desc_idx);
+			p_hdr->data[0]._desc_genc,p_hdr->data[0]._desc_idx,
+			sdesc_id._desc_idx);
 	/* Get the session send descriptor
 	 * a subset of get_tidflow in ips_proto_recv.c since we don't
 	 * have tidflow sequence numbers to check
@@ -1162,8 +1187,14 @@ int ips_protoexp_handle_immed_data(struct ips_proto *proto, uint64_t conn_ref,
 		// TBD - what to do?
 	}
 #endif
-	if (_HFI_PDBG_ON)
-		__psm2_dump_buf(tidrecvc->buffer, len);
+	if (_HFI_PDBG_ON) {
+#ifdef PSM_CUDA
+		if (tidrecvc->is_ptr_gpu_backed)
+			_HFI_PDBG_DUMP_GPU(tidrecvc->buffer, len);
+		else
+#endif
+			_HFI_PDBG_DUMP(tidrecvc->buffer, len);
+	}
 
 	/* Reset the swapped generation count as we received a valid packet */
 	tidrecvc->tidflow_nswap_gen = 0;
@@ -1221,7 +1252,6 @@ psmi_cuda_reclaim_hostbufs(struct ips_tid_get_request *getreq)
 	}
 	return PSM2_OK;
 }
-
 static
 struct ips_cuda_hostbuf* psmi_allocate_chb(uint32_t window_len)
 {
@@ -1257,12 +1287,17 @@ void psmi_cuda_run_prefetcher(struct ips_protoexp *protoexp,
 		window_len =
 			ips_cuda_next_window(tidsendc->ipsaddr->window_rv,
 					     offset, req->req_data.buf_len);
-		if (window_len <= CUDA_SMALLHOSTBUF_SZ)
+		unsigned bufsz;
+		if (window_len <= CUDA_SMALLHOSTBUF_SZ) {
 			chb = (struct ips_cuda_hostbuf *) psmi_mpool_get(
 				proto->cuda_hostbuf_pool_small_send);
-		if (chb == NULL)
+			bufsz = proto->cuda_hostbuf_small_send_cfg.bufsz;
+		}
+		if (chb == NULL) {
 			chb = (struct ips_cuda_hostbuf *) psmi_mpool_get(
 				proto->cuda_hostbuf_pool_send);
+			bufsz = proto->cuda_hostbuf_send_cfg.bufsz;
+		}
 		/* were any buffers available for the prefetcher? */
 		if (chb == NULL)
 			return;
@@ -1272,6 +1307,20 @@ void psmi_cuda_run_prefetcher(struct ips_protoexp *protoexp,
 		chb->req = req;
 		chb->gpu_buf = (CUdeviceptr) req->req_data.buf + offset;
 		chb->bytes_read = 0;
+
+		if (proto->cudastream_send == NULL) {
+			PSMI_CUDA_CALL(cuStreamCreate,
+				   &proto->cudastream_send, CU_STREAM_NON_BLOCKING);
+		}
+		if (chb->host_buf == NULL) {
+			PSMI_CUDA_CALL(cuMemHostAlloc,
+				       (void **) &chb->host_buf,
+				       bufsz,
+				       CU_MEMHOSTALLOC_PORTABLE);
+		}
+		if (chb->copy_status == NULL) {
+			PSMI_CUDA_CALL(cuEventCreate, &chb->copy_status, CU_EVENT_DEFAULT);
+		}
 		PSMI_CUDA_CALL(cuMemcpyDtoHAsync,
 			       chb->host_buf, chb->gpu_buf,
 			       window_len,
@@ -1306,12 +1355,17 @@ void psmi_attach_chb_to_tidsendc(struct ips_protoexp *protoexp,
 		window_len =
 			ips_cuda_next_window(tidsendc->ipsaddr->window_rv,
 					     offset, req->req_data.buf_len);
-		if (window_len <= CUDA_SMALLHOSTBUF_SZ)
+		unsigned bufsz;
+		if (window_len <= CUDA_SMALLHOSTBUF_SZ) {
 			chb = (struct ips_cuda_hostbuf *) psmi_mpool_get(
 				proto->cuda_hostbuf_pool_small_send);
-		if (chb == NULL)
+			bufsz = proto->cuda_hostbuf_small_send_cfg.bufsz;
+		}
+		if (chb == NULL) {
 			chb = (struct ips_cuda_hostbuf *) psmi_mpool_get(
 				proto->cuda_hostbuf_pool_send);
+			bufsz = proto->cuda_hostbuf_send_cfg.bufsz;
+		}
 
 		/* were any buffers available? If not force allocate */
 		if (chb == NULL) {
@@ -1325,6 +1379,19 @@ void psmi_attach_chb_to_tidsendc(struct ips_protoexp *protoexp,
 		chb->req = req;
 		chb->gpu_buf = (CUdeviceptr) req->req_data.buf + offset;
 		chb->bytes_read = 0;
+		if (proto->cudastream_send == NULL) {
+			PSMI_CUDA_CALL(cuStreamCreate,
+				   &proto->cudastream_send, CU_STREAM_NON_BLOCKING);
+		}
+		if (chb->host_buf == NULL) {
+			PSMI_CUDA_CALL(cuMemHostAlloc,
+				       (void **) &chb->host_buf,
+				       bufsz,
+				       CU_MEMHOSTALLOC_PORTABLE);
+		}
+		if (chb->copy_status == NULL) {
+			PSMI_CUDA_CALL(cuEventCreate, &chb->copy_status, CU_EVENT_DEFAULT);
+		}
 		PSMI_CUDA_CALL(cuMemcpyDtoHAsync,
 			       chb->host_buf, chb->gpu_buf,
 			       window_len,
@@ -1389,7 +1456,6 @@ void psmi_attach_chb_to_tidsendc(struct ips_protoexp *protoexp,
 	}
 }
 
-
 static
 psm2_chb_match_type_t psmi_find_match_in_prefeteched_chb(struct ips_cuda_hostbuf* chb,
 				       ips_tid_session_list *tid_list,
@@ -1542,8 +1608,17 @@ ips_tid_send_handle_tidreq(struct ips_protoexp *protoexp,
 							0,
 						    PSMI_CUDA_CONTINUE);
 		}
-	}
+		protoexp->proto->strat_stats.rndv_rdma_hbuf_send++;
+		protoexp->proto->strat_stats.rndv_rdma_hbuf_send_bytes += tid_list->tsess_length;
+	} else if (req->is_buf_gpu_mem) {
+		protoexp->proto->strat_stats.rndv_rdma_gdr_send++;
+		protoexp->proto->strat_stats.rndv_rdma_gdr_send_bytes += tid_list->tsess_length;
+	} else
 #endif // PSM_CUDA
+	{
+		protoexp->proto->strat_stats.rndv_rdma_cpu_send++;
+		protoexp->proto->strat_stats.rndv_rdma_cpu_send_bytes += tid_list->tsess_length;
+	}
 
 	tidsendc->is_complete = 0;
 	tidsendc->reserved = 0;
@@ -1553,6 +1628,7 @@ ips_tid_send_handle_tidreq(struct ips_protoexp *protoexp,
 	tidsendc->rv_conn_count = 0;
 #endif
 
+
 	_HFI_EXP
 	    ("alloc tidsend=%4d tidrecv=%4d srcoff=%6d length=%6d"
 		"\n",
@@ -1625,14 +1701,15 @@ psm2_error_t ips_tid_issue_rdma_write(struct ips_tid_send_desc *tidsendc)
 			// separate MR cache's per EP, so this confirms we have the same EP
 		tidsendc->mqreq->mr && tidsendc->mqreq->mr->cache == proto->mr_cache) {
 		// we can use the same MR as the whole mqreq
-		_HFI_MMDBG("CTS send chunk reference send: %p %u bytes via %p %"PRIu64"\n", tidsendc->buffer, tidsendc->length, tidsendc->mqreq->mr->addr, tidsendc->mqreq->mr->length);
+		_HFI_MMDBG("CTS send chunk reference send: %p %u bytes via %p %"PRIu64"\n",
+			tidsendc->buffer, tidsendc->length, tidsendc->mqreq->mr->addr, tidsendc->mqreq->mr->length);
 		tidsendc->mr = psm2_verbs_ref_mr(tidsendc->mqreq->mr);
 	} else {
 		// we need an MR for this chunk
 		_HFI_MMDBG("CTS send chunk register send: %p %u bytes\n", tidsendc->buffer , tidsendc->length);
 		tidsendc->mr = psm2_verbs_reg_mr(proto->mr_cache, 1,
                          proto->ep->verbs_ep.pd,
-                         tidsendc->buffer, tidsendc->length, 0
+                         tidsendc->buffer, tidsendc->length, IBV_ACCESS_RDMA
 #ifdef PSM_CUDA
 						| ((tidsendc->mqreq->is_buf_gpu_mem
 								 && !tidsendc->mqreq->cuda_hostbuf_used)
@@ -1645,11 +1722,13 @@ psm2_error_t ips_tid_issue_rdma_write(struct ips_tid_send_desc *tidsendc)
 
 	// if post_send fails below, we'll try again later
 	// completion handler decides how to handle any WQE/CQE errors
-	_HFI_MMDBG("tidsendc prior to post userbuf %p buffer %p length %u\n",
-			tidsendc->userbuf,  tidsendc->buffer, tidsendc->length);
+	_HFI_MMDBG("tidsendc prior to post userbuf %p buffer %p length %u err %d outstanding %u\n",
+			tidsendc->userbuf,  tidsendc->buffer, tidsendc->length,
+			err, protoexp->proto->ep->verbs_ep.send_rdma_outstanding);
 #ifdef RNDV_MOD
 	if (err == PSM2_OK) {
 		psmi_assert(IPS_PROTOEXP_FLAG_ENABLED & protoexp->proto->ep->rdmamode);
+
 		if (IPS_PROTOEXP_FLAG_KERNEL_QP(protoexp->proto->ep->rdmamode))
 			err = psm2_verbs_post_rv_rdma_write_immed(
 				protoexp->proto->ep,
@@ -1673,8 +1752,19 @@ psm2_error_t ips_tid_issue_rdma_write(struct ips_tid_send_desc *tidsendc)
 							 tidsendc->rdescid._desc_idx, 0),
 				(uintptr_t)tidsendc);
 	}
-	if (err == PSM2_OK)
+	if (err == PSM2_OK) {
+		if (_HFI_PDBG_ON) {
+#ifdef PSM_CUDA
+			if (tidsendc->mqreq->is_buf_gpu_mem && !tidsendc->mqreq->cuda_hostbuf_used)
+				_HFI_PDBG_DUMP_GPU(tidsendc->buffer, tidsendc->tid_list.tsess_length);
+			else
+#endif
+				_HFI_PDBG_DUMP(tidsendc->buffer, tidsendc->tid_list.tsess_length);
+		}
 		tidsendc->is_complete = 1;	// send queued
+	} else
+		_HFI_MMDBG("after posted IBV Write: err %d\n", err);
+
 #else // RNDV_MOD
 	if (err == PSM2_OK) {
 		psmi_assert(IPS_PROTOEXP_FLAG_ENABLED & protoexp->proto->ep->rdmamode);
@@ -1689,8 +1779,18 @@ psm2_error_t ips_tid_issue_rdma_write(struct ips_tid_send_desc *tidsendc)
 							 tidsendc->rdescid._desc_idx, 0),
 				(uintptr_t)tidsendc);
 	}
-	if (err == PSM2_OK)
+	if (err == PSM2_OK) {
+		if (_HFI_PDBG_ON) {
+#ifdef PSM_CUDA
+			if (tidsendc->mqreq->is_buf_gpu_mem && !tidsendc->mqreq->cuda_hostbuf_used)
+				_HFI_PDBG_DUMP_GPU(tidsendc->buffer, tidsendc->tid_list.tsess_length);
+			else
+#endif
+				_HFI_PDBG_DUMP(tidsendc->buffer, tidsendc->tid_list.tsess_length);
+		}
 		tidsendc->is_complete = 1;	// send queued
+	} else
+		_HFI_MMDBG("after posted IBV Write 2: err %d\n", err);
 #endif // RNDV_MOD
 	return err;
 }
@@ -1782,6 +1882,10 @@ psm2_error_t ips_tid_send_exp(struct ips_tid_send_desc *tidsendc)
 			}
 			psmi_cuda_run_prefetcher(protoexp, tidsendc);
 		}
+		/* Clean Up tidsendc ref's to split cuda hostbufs when no longer needed */
+		tidsendc->cuda_num_buf = 0;
+		tidsendc->cuda_hostbuf[0] = NULL;
+		tidsendc->cuda_hostbuf[1] = NULL;
 	}
 #endif
 	err = ips_tid_issue_rdma_write(tidsendc);
@@ -1935,14 +2039,19 @@ ips_tid_recv_alloc(struct ips_protoexp *protoexp,
 	/* 4. allocate a cuda bounce buffer, if required */
 	struct ips_cuda_hostbuf *chb = NULL;
 	if (getreq->cuda_hostbuf_used) {
-		if (nbytes_this <= CUDA_SMALLHOSTBUF_SZ)
+		unsigned bufsz;
+		if (nbytes_this <= CUDA_SMALLHOSTBUF_SZ) {
 			chb = (struct ips_cuda_hostbuf *)
 				psmi_mpool_get(
 					protoexp->cuda_hostbuf_pool_small_recv);
-		if (chb == NULL)
+			bufsz = protoexp->cuda_hostbuf_small_recv_cfg.bufsz;
+		}
+		if (chb == NULL) {
 			chb = (struct ips_cuda_hostbuf *)
 				psmi_mpool_get(
 					protoexp->cuda_hostbuf_pool_recv);
+			bufsz = protoexp->cuda_hostbuf_recv_cfg.bufsz;
+		}
 		if (chb == NULL) {
 			/* Unable to get a cudahostbuf for TID.
 			 * Release the resources we're holding and reschedule.*/
@@ -1956,6 +2065,12 @@ ips_tid_recv_alloc(struct ips_protoexp *protoexp,
 			return PSM2_EP_NO_RESOURCES;
 		}
 
+		if (chb->host_buf == NULL) {
+			PSMI_CUDA_CALL(cuMemHostAlloc,
+				       (void **) &chb->host_buf,
+				       bufsz,
+				       CU_MEMHOSTALLOC_PORTABLE);
+		}
 		tidrecvc->cuda_hostbuf = chb;
 		tidrecvc->buffer = chb->host_buf;
 		chb->size = 0;
@@ -1984,7 +2099,7 @@ ips_tid_recv_alloc(struct ips_protoexp *protoexp,
 		_HFI_MMDBG("CTS chunk register recv: %p %u bytes\n", tidrecvc->buffer, nbytes_this);
 		tidrecvc->mr = psm2_verbs_reg_mr(proto->mr_cache, 1,
                         proto->ep->verbs_ep.pd,
-                        tidrecvc->buffer, nbytes_this, IBV_ACCESS_REMOTE_WRITE
+                        tidrecvc->buffer, nbytes_this, IBV_ACCESS_RDMA|IBV_ACCESS_REMOTE_WRITE
 #ifdef PSM_CUDA
                			| (tidrecvc->is_ptr_gpu_backed?IBV_ACCESS_IS_GPU_ADDR:0)
 #endif
@@ -2082,11 +2197,9 @@ ips_tid_pendtids_timer_callback(struct psmi_timer *timer, uint64_t current)
 #endif
 
 #ifdef PSM_CUDA
-	if (!(((struct ips_protoexp *)timer->context)->proto->flags
-		& IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV) ||
-		((((struct ips_protoexp *)timer->context)->proto->flags &
-		   IPS_PROTO_FLAG_GPUDIRECT_RDMA_RECV) &&
-		   gpudirect_recv_threshold)) {
+	if (
+	    1	/* due to unaligned recv using hostbuf, must always do this */
+	) {
 		/* Before processing pending TID requests, first try to free up
 		 * any CUDA host buffers that are now idle. */
 		struct ips_tid_get_cudapend *cphead =
@@ -2310,13 +2423,21 @@ void psmi_cudamemcpy_tid_to_device(struct ips_tid_recv_desc *tidrecvc)
 
 	chb = tidrecvc->cuda_hostbuf;
 	chb->size += tidrecvc->recv_msglen;
-			;
+
+	if (protoexp->cudastream_recv == NULL) {
+		PSMI_CUDA_CALL(cuStreamCreate,
+			&protoexp->cudastream_recv,
+			CU_STREAM_NON_BLOCKING);
+	}
 
 	PSMI_CUDA_CALL(cuMemcpyHtoDAsync,
 		       chb->gpu_buf, chb->host_buf,
-		       tidrecvc->recv_msglen
-						,
+		       tidrecvc->recv_msglen,
 		       protoexp->cudastream_recv);
+
+	if (chb->copy_status == NULL) {
+		PSMI_CUDA_CALL(cuEventCreate, &chb->copy_status, CU_EVENT_DEFAULT);
+	}
 	PSMI_CUDA_CALL(cuEventRecord, chb->copy_status,
 		       protoexp->cudastream_recv);
 
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_help.h b/prov/psm3/psm3/ptl_ips/ips_proto_help.h
index a7b90fa..8a3ebee 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_help.h
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_help.h
@@ -252,7 +252,8 @@ void
 ips_scb_prepare_flow_inner(struct ips_proto *proto, struct ips_epaddr *ipsaddr,
 			   struct ips_flow *flow, ips_scb_t *scb))
 {
-	psmi_assert((scb->payload_size & 3) == 0);
+	// ips_ptl_mq_rndv can allow small odd sized payload in RTS
+	psmi_assert(scb->payload_size <= 3 || ! (scb->payload_size & 3));
 	ips_proto_hdr(proto, ipsaddr, flow, scb,
 		      ips_flow_gen_ackflags(scb, flow));
 
@@ -470,13 +471,13 @@ ips_proto_process_packet(const struct ips_recvhdrq_event *rcv_ev))
 	if_pf(PSMI_FAULTINJ_ENABLED_EP(rcv_ev->proto->ep)) {
 		PSMI_FAULTINJ_STATIC_DECL(fi_recv, "recvlost",
 					  "drop "
-					  "RC eager or any "
-					  "UD packet at recv",
+					  "RC eager or any UD "
+					  "packet at recv",
 					   1, IPS_FAULTINJ_RECVLOST);
-		if (psmi_faultinj_is_fault(fi_recv))
+		if_pf(PSMI_FAULTINJ_IS_FAULT(fi_recv, ""))
 			return IPS_RECVHDRQ_CONTINUE;
 	}
-#endif /* #ifdef PSM_FI */
+#endif // PSM_FI
 	/* see file ips_proto_header.h for details */
 	index = _get_proto_hfi_opcode(rcv_ev->p_hdr) - OPCODE_RESERVED;
 	if (index >= (OPCODE_FUTURE_FROM - OPCODE_RESERVED))
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_mq.c b/prov/psm3/psm3/ptl_ips/ips_proto_mq.c
index 1a71f31..15256d4 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_mq.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_mq.c
@@ -109,6 +109,19 @@ mq_alloc_pkts(struct ips_proto *proto, int npkts, int len, uint32_t flags))
 	}
 }
 
+static
+int ips_proto_scb_mr_complete(void *context, uint32_t nbytes)
+{
+	ips_scb_t *scb = (ips_scb_t *)context;
+	if (scb->mr) {
+		_HFI_MMDBG("SDMA complete, releasing MR: lkey: 0x%x\n", scb->mr->lkey);
+		psm2_verbs_release_mr(scb->mr);
+		scb->mr = NULL;
+		ips_tid_mravail_callback(scb->flow->ipsaddr->epaddr.proto);
+	}
+	return IPS_RECVHDRQ_CONTINUE;
+}
+
 // handle end to end completion of eager and LONG_DATA sends
 static
 int ips_proto_mq_eager_complete(void *reqp, uint32_t nbytes)
@@ -323,17 +336,21 @@ ips_ptl_mq_eager(struct ips_proto *proto, psm2_mq_req_t req,
 		    ("payload=%p, thislen=%d, frag_size=%d, nbytes_left=%d\n",
 		     (void *)buf, pktlen, flow->frag_size, nbytes_left);
 		ips_scb_buffer(scb) = (void *)buf;
+		if (req->mr) {
+			scb->mr = req->mr;
+			ips_scb_flags(scb) |= IPS_SEND_FLAG_SEND_MR;
+		}
 
 #ifdef PSM_CUDA
-		/* PSM would never send packets using eager protocol
-		 * if GPU Direct RDMA is turned off, which makes setting
-		 * these flags safe.
-		 */
 		if (req->is_buf_gpu_mem) {
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+			// flags will get handled in pio transfer_frame
+			// but use cuMemcpy instead of GDRCopy
+			if (!req->mr)
+				ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+			// TBD USER_BUF_GPU only useful for RTS
 			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
 		}
-#endif
+#endif // PSM_CUDA
 
 		buf += pktlen;
 		offset += pktlen;
@@ -366,8 +383,8 @@ ips_ptl_mq_eager(struct ips_proto *proto, psm2_mq_req_t req,
 		} else {
 			req->send_msgoff += pktlen;
 		}
-
 		ips_proto_flow_enqueue(flow, scb);
+
 		if (flow->transfer == PSM_TRANSFER_PIO) {
 			/* we need to flush the pio pending queue as quick as possible */
 			err = flow->flush(flow, NULL);
@@ -429,10 +446,11 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 #ifdef PSM_CUDA
 	    !req->is_buf_gpu_mem &&
 #endif
-	    !(len & 0x3)) {
+	    (len <= 3 || !(len & 0x3))) {
 		ips_scb_buffer(scb) = (void *)buf;
 		ips_scb_length(scb) = len;
 		req->send_msgoff = len;
+		req->mq->stats.tx_rndv_bytes += len;
 	} else {
 		ips_scb_length(scb) = 0;
 		req->send_msgoff = 0;
@@ -453,7 +471,7 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 	    (len > GPUDIRECT_THRESH_RV)) ||
 	    ((proto->flags & IPS_PROTO_FLAG_GPUDIRECT_RDMA_SEND)  &&
 	    req->is_buf_gpu_mem &&
-	    (len > gpudirect_send_threshold))) {
+	    (len > gpudirect_send_limit))) {
 		/* send from intermediate host buffer */
 		struct ips_cuda_hostbuf *chb;
 		uint32_t offset, window_len;
@@ -471,14 +489,19 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 				ips_cuda_next_window(ipsaddr->window_rv,
 						     offset, len);
 
-			if (window_len <= CUDA_SMALLHOSTBUF_SZ)
+			unsigned bufsz;
+			if (window_len <= CUDA_SMALLHOSTBUF_SZ) {
 				chb = (struct ips_cuda_hostbuf *)
 					psmi_mpool_get(
 					proto->cuda_hostbuf_pool_small_send);
-			if (chb == NULL)
+				bufsz = proto->cuda_hostbuf_small_send_cfg.bufsz;
+			}
+			if (chb == NULL) {
 				chb = (struct ips_cuda_hostbuf *)
 					psmi_mpool_get(
 					proto->cuda_hostbuf_pool_send);
+				bufsz = proto->cuda_hostbuf_send_cfg.bufsz;
+			}
 
 			/* any buffers available? */
 			if (chb == NULL)
@@ -492,6 +515,19 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 			chb->gpu_buf = (CUdeviceptr) buf + offset;
 			chb->bytes_read = 0;
 
+			if (proto->cudastream_send == NULL) {
+				PSMI_CUDA_CALL(cuStreamCreate,
+					   &proto->cudastream_send, CU_STREAM_NON_BLOCKING);
+			}
+			if (chb->host_buf == NULL) {
+				PSMI_CUDA_CALL(cuMemHostAlloc,
+					       (void **) &chb->host_buf,
+					       bufsz,
+					       CU_MEMHOSTALLOC_PORTABLE);
+			}
+			if (chb->copy_status == NULL) {
+				PSMI_CUDA_CALL(cuEventCreate, &chb->copy_status, CU_EVENT_DEFAULT);
+			}
 			PSMI_CUDA_CALL(cuMemcpyDtoHAsync,
 				       chb->host_buf, chb->gpu_buf,
 				       window_len,
@@ -512,6 +548,7 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 			  proto->protoexp,
 			  OPCODE_LONG_RTS,PSM2_LOG_TX,proto->ep->epid, req->rts_peer->epid,
 			    "scb->ips_lrh.hdr_data.u32w0: %d",scb->ips_lrh.hdr_data.u32w0);
+	proto->epaddr_stats.rts_send++;
 
 	_HFI_VDBG("sending with rndv %u\n", len);
 	/* If this is a fast path isend, then we cannot poll or
@@ -550,7 +587,7 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 #endif
 		) {
 		req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0, proto->ep->verbs_ep.pd,
-						 req->req_data.buf, req->req_data.send_msglen, 0
+						 req->req_data.buf, req->req_data.send_msglen, IBV_ACCESS_RDMA
 #ifdef PSM_CUDA
 						| (req->is_buf_gpu_mem?IBV_ACCESS_IS_GPU_ADDR:0)
 #endif
@@ -587,9 +624,8 @@ int psmi_cuda_is_buffer_gpu_mem(void *ubuf)
 static inline
 int psmi_cuda_is_needed_rendezvous(struct ips_proto *proto, uint32_t len)
 {
-	if (!(proto->flags & IPS_PROTO_FLAG_GPUDIRECT_RDMA_SEND) ||
-		!PSMI_IS_GDR_COPY_ENABLED ||
-		len < 1 || len > cuda_thresh_rndv){
+	if (
+		len > cuda_thresh_rndv){
 		return 1;
 	}
 
@@ -597,16 +633,6 @@ int psmi_cuda_is_needed_rendezvous(struct ips_proto *proto, uint32_t len)
 }
 #endif
 
-/* Find the correct flow (PIO/DMA) */
-static inline
-ips_epaddr_flow_t
-flow_select_type(struct ips_proto *proto, uint32_t len, int gpu_mem,
-		 uint32_t eager_thresh)
-{
-	// minor optimization, we don't use SDMA for UD or UDP yet, so just return a
-	// constant and compiler will optimize
-	return EP_FLOW_GO_BACK_N_PIO;
-}
 
 psm2_error_t ips_proto_msg_size_thresh_query (enum psm2_info_query_thresh_et qt,
 					      uint32_t *out, psm2_mq_t mq, psm2_epaddr_t epaddr)
@@ -652,21 +678,20 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 		   uint32_t len, void *context, psm2_mq_req_t *req_o)
 {
 	psm2_error_t err = PSM2_OK;
-	ips_epaddr_flow_t flow_type;
 	struct ips_proto *proto;
 	struct ips_flow *flow;
 	ips_epaddr_t *ipsaddr;
 	ips_scb_t *scb;
 	psm2_mq_req_t req;
 #if defined(PSM_CUDA)
-	int converted = 0;
+	int gpu_mem = 0;
 #endif // PSM_CUDA
 
 	req = psmi_mq_req_alloc(mq, MQE_TYPE_SEND);
 	if_pf(req == NULL)
 		return PSM2_NO_MEMORY;
 
-	_HFI_VDBG("(req=%p) ubuf=%p len=%u\n", req, ubuf, len);
+	_HFI_VDBG("(req=%p) ubuf=%p len=%u, flags_user=0x%x\n", req, ubuf, len, flags_user);
 
 	req->flags_user = flags_user;
 	req->flags_internal = flags_internal;
@@ -686,19 +711,16 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 	req->req_data.context = context;
 
 #ifdef PSM_CUDA
-	req->is_buf_gpu_mem = psmi_cuda_is_buffer_gpu_mem((void*)ubuf);
+	req->is_buf_gpu_mem = len && psmi_cuda_is_buffer_gpu_mem((void*)ubuf);
 	req->cuda_hostbuf_used = 0;
 	if (req->is_buf_gpu_mem) {
+		gpu_mem = 1;
 		psmi_cuda_set_attr_sync_memops(ubuf);
 		if (psmi_cuda_is_needed_rendezvous(proto, len))
 			goto do_rendezvous;
 	}
-#else
-	req->is_buf_gpu_mem = 0;
 #endif
-	flow_type = flow_select_type(proto, len, req->is_buf_gpu_mem,
-				     proto->iovec_thresh_eager);
-	flow = &ipsaddr->flows[flow_type];
+	flow = &ipsaddr->flows[EP_FLOW_GO_BACK_N_PIO];
 
 	if (flags_user & PSM2_MQ_FLAG_SENDSYNC) {
 		goto do_rendezvous;
@@ -714,26 +736,40 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 
 		const void *user_buffer = ubuf;
 #ifdef PSM_CUDA
-		if (req->is_buf_gpu_mem) {
+		if (!req->is_buf_gpu_mem) {
+			mq_copy_tiny_host_mem((uint32_t *) &scb->ips_lrh.hdr_data,
+							  (uint32_t *) user_buffer, len);
+			proto->strat_stats.tiny_cpu_isend++;
+			proto->strat_stats.tiny_cpu_isend_bytes += len;
+		} else {
+			// TBD USER_BUF_GPU only useful for RTS
+			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
 			/* The following functions PINS the GPU pages
 			 * and mmaps the pages into the process virtual
 			 * space. This allows PSM to issue a standard
 			 * memcpy to move data between HFI resources
 			 * and the GPU
 			 */
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
-			user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
-						(unsigned long)ubuf, len, 0, proto);
-			converted = 1;
-		}
-		mq_copy_tiny_host_mem((uint32_t *) &scb->ips_lrh.hdr_data,
+			if (len <= gdr_copy_limit_send &&
+				NULL != (user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
+						(unsigned long)ubuf, len, 0, proto->ep))) {
+				mq_copy_tiny_host_mem((uint32_t *) &scb->ips_lrh.hdr_data,
 							  (uint32_t *) user_buffer, len);
-		if (converted) {
-			gdr_unmap_gpu_host_addr(GDR_FD, user_buffer, len, proto);
+				proto->strat_stats.tiny_gdrcopy_isend++;
+				proto->strat_stats.tiny_gdrcopy_isend_bytes += len;
+			} else {
+				user_buffer = ubuf;
+#endif // PSM_CUDA
+				mq_copy_tiny((uint32_t *) &scb->ips_lrh.hdr_data,
+						 (uint32_t *) user_buffer, len);
+#ifdef PSM_CUDA
+				proto->strat_stats.tiny_cuCopy_isend++;
+				proto->strat_stats.tiny_cuCopy_isend_bytes += len;
+			}
 		}
 #else
-		mq_copy_tiny((uint32_t *) &scb->ips_lrh.hdr_data,
-					 (uint32_t *) user_buffer, len);
+		proto->strat_stats.tiny_cpu_isend++;
+		proto->strat_stats.tiny_cpu_isend_bytes += len;
 #endif
 
 		/* If this is a fast path isend, then we cannot allow
@@ -766,14 +802,68 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 		scb->ips_lrh.hdr_data.u32w1 = len;
 		ips_scb_copy_tag(scb->ips_lrh.tag, tag->tag);
 		const void * user_buffer = ubuf;
+		int used_send_dma = 0;
 #ifdef PSM_CUDA
-		if (req->is_buf_gpu_mem && len <= gdr_copy_threshold_send){
+		if (req->is_buf_gpu_mem) {
+			// TBD USER_BUF_GPU only useful for RTS
 			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
-			user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
-					(unsigned long)ubuf, len , 0, proto);
-			converted = 1;
+			if (len <= gdr_copy_limit_send &&
+				NULL != (user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
+					(unsigned long)ubuf, len , 0, proto->ep))) {
+				/* init req so ips_proto_mq_eager_complete can unmap */
+				req->req_data.buf = (uint8_t*)ubuf;
+				req->req_data.buf_len = len;
+				req->req_data.send_msglen = len;
+				proto->strat_stats.short_gdrcopy_isend++;
+				proto->strat_stats.short_gdrcopy_isend_bytes += len;
+			} else {
+				user_buffer = ubuf;
+				if (len > proto->iovec_gpu_thresh_eager) {
+					scb->mr = req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+							proto->ep->verbs_ep.pd,
+							(void*)user_buffer, len,
+							IBV_ACCESS_IS_GPU_ADDR);
+				}
+				if (req->mr) {
+					ips_scb_flags(scb) |= IPS_SEND_FLAG_SEND_MR;
+					req->rts_peer = (psm2_epaddr_t) ipsaddr;
+					ips_scb_cb(scb) = ips_proto_mq_eager_complete;
+					ips_scb_cb_param(scb) = req;
+					used_send_dma = 1;
+					proto->strat_stats.short_gdr_isend++;
+					proto->strat_stats.short_gdr_isend_bytes += len;
+				} else
+				{
+					ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+					// TBD for OPA flow_type could be DMA
+					proto->strat_stats.short_cuCopy_isend++;
+					proto->strat_stats.short_cuCopy_isend_bytes += len;
+				}
+			}
+		} else
+#endif // PSM_CUDA
+		{
+			if (len > proto->iovec_thresh_eager) {
+				scb->mr = req->mr = psm2_verbs_reg_mr(
+						proto->mr_cache, 0,
+						proto->ep->verbs_ep.pd,
+						(void*)user_buffer, len, 0);
+			}
+			if (req->mr) {
+				ips_scb_flags(scb) |= IPS_SEND_FLAG_SEND_MR;
+				req->rts_peer = (psm2_epaddr_t) ipsaddr;
+				ips_scb_cb(scb) = ips_proto_mq_eager_complete;
+				ips_scb_cb_param(scb) = req;
+				used_send_dma = 1;
+				proto->strat_stats.short_dma_cpu_isend++;
+				proto->strat_stats.short_dma_cpu_isend_bytes += len;
+			} else
+			{
+				// TBD for OPA flow_type could be DMA
+				proto->strat_stats.short_copy_cpu_isend++;
+				proto->strat_stats.short_copy_cpu_isend_bytes += len;
+			}
 		}
-#endif
 
 		ips_scb_buffer(scb) = (void *)user_buffer;
 
@@ -791,22 +881,12 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 			req->send_msgoff = 0;
 		}
 
-#if defined(PSM_CUDA)
-		if (converted) {
-			gdr_unmap_gpu_host_addr(GDR_FD, user_buffer, len, proto);
-		}
-#endif // PSM_CUDA
 		/*
 		 * Need ack for send side completion because we
-		 * send from user buffer.
+		 * send from user buffer.  ACK will trigger scb callback
 		 */
 		ips_scb_flags(scb) |= IPS_SEND_FLAG_ACKREQ;
-#ifdef PSM_CUDA
-		if (req->is_buf_gpu_mem && len > gdr_copy_threshold_send) {
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
-		}
-#endif
+
 		/* If this is a fast path isend, then we cannot allow
 		 * progressing of the mq from within the fast path
 		 * call otherwise messages will be lost. Therefore given fast path
@@ -822,8 +902,18 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 		 * 'scb' to be changed, when this scb is done, the
 		 * address is set to NULL when scb is put back to
 		 * scb pool. Even if the same scb is re-used, it
-		 * is not possible to set to this 'buf' address.
+		 * is not possible to set to this 'buf' address
+		 * because the app has not yet had a chance to start
+		 * another IO.  TBD - possible odd scenario if app
+		 * had this IO started in middle of a buffer which it also
+		 * had a multi-packet eager IO working on, then could see
+		 * same user_buffer from two IOs here.
 		 */
+		if (used_send_dma) {
+			// noop, callback already setup
+		} else
+		// TBD - could avoid this if/else code by always marking
+		// callback above, but may be less efficient for msgrate
 		if (ips_scb_buffer(scb) == (void *)user_buffer) {
 			/* continue to send from user buffer */
 			ips_scb_cb(scb) = ips_proto_mq_eager_complete;
@@ -840,6 +930,44 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 		     len, tag->tag[0], tag->tag[1], tag->tag[2], req);
 	} else if (len <= mq->hfi_thresh_rv) {
 		req->send_msgoff = 0;
+		req->rts_peer = (psm2_epaddr_t) ipsaddr;
+#ifdef PSM_CUDA
+		if (req->is_buf_gpu_mem) {
+			// TBD - no upper bound for send DMA here
+			// non-priority MR and will fallback if can't register
+			if (len > proto->iovec_gpu_thresh_eager) {
+				req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+                                        proto->ep->verbs_ep.pd,
+                                        (void*)ubuf, len, IBV_ACCESS_IS_GPU_ADDR);
+			}
+			if (req->mr) {
+				proto->strat_stats.eager_gdr_isend++;
+				proto->strat_stats.eager_gdr_isend_bytes += len;
+			} else
+			{
+				proto->strat_stats.eager_cuCopy_isend++;
+				proto->strat_stats.eager_cuCopy_isend_bytes += len;
+			}
+		} else
+#endif
+		{
+			// TBD - no upper bound for send DMA here
+			// non-priority MR and will fallback if can't register
+			if (len > proto->iovec_thresh_eager) {
+				req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+                                        	proto->ep->verbs_ep.pd,
+                                        	(void*)ubuf, len, 0);
+			}
+			if (req->mr) {
+				proto->strat_stats.eager_dma_cpu_isend++;
+				proto->strat_stats.eager_dma_cpu_isend_bytes += len;
+			} else
+			{
+				// TBD for OPA flow_type could be DMA
+				proto->strat_stats.eager_copy_cpu_isend++;
+				proto->strat_stats.eager_copy_cpu_isend_bytes += len;
+			}
+		}
 		err = ips_ptl_mq_eager(proto, req, flow, tag, ubuf, len);
 		if (err != PSM2_OK)
 			return err;
@@ -851,6 +979,22 @@ ips_proto_mq_isend(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags_user,
 		     len, tag->tag[0], tag->tag[1], tag->tag[2], req);
 	} else {		/* skip eager accounting below */
 do_rendezvous:
+#ifdef PSM_CUDA
+		if (gpu_mem) {
+			proto->strat_stats.rndv_gpu_isend++;
+			proto->strat_stats.rndv_gpu_isend_bytes += len;
+		} else {
+#endif
+			proto->strat_stats.rndv_cpu_isend++;
+			proto->strat_stats.rndv_cpu_isend_bytes += len;
+#ifdef PSM_CUDA
+		}
+#endif
+
+		mq->stats.tx_num++;
+		mq->stats.tx_rndv_num++;
+		// we count tx_rndv_bytes as we get CTS
+
 		err = ips_ptl_mq_rndv(proto, req, ipsaddr, ubuf, len);
 		*req_o = req;
 		return err;
@@ -860,6 +1004,15 @@ do_rendezvous:
 	mq->stats.tx_num++;
 	mq->stats.tx_eager_num++;
 	mq->stats.tx_eager_bytes += len;
+#ifdef PSM_CUDA
+	if (gpu_mem) {
+		mq->stats.tx_eager_gpu_num++;
+		mq->stats.tx_eager_gpu_bytes += len;
+	} else {
+		mq->stats.tx_eager_cpu_num++;
+		mq->stats.tx_eager_cpu_bytes += len;
+	}
+#endif
 
 	return err;
 }
@@ -869,17 +1022,15 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 		  psm2_mq_tag_t *tag, const void *ubuf, uint32_t len)
 {
 	psm2_error_t err = PSM2_OK;
-	ips_epaddr_flow_t flow_type;
 	struct ips_proto *proto;
 	struct ips_flow *flow;
 	ips_epaddr_t *ipsaddr;
 	ips_scb_t *scb;
+#if defined(PSM_OPA) || defined(PSM_CUDA)
 	int gpu_mem = 0;
-#if defined(PSM_CUDA)
-	int converted = 0;
-#endif // PSM_CUDA
+#endif
 
-	_HFI_VDBG("ubuf=%p len=%u\n", ubuf, len);
+	_HFI_VDBG("ubuf=%p len=%u flags=0x%x\n", ubuf, len, flags);
 
 	if (len >= mepaddr->proto->multirail_thresh_load_balance) {
 		ipsaddr = ((ips_epaddr_t *) mepaddr)->msgctl->ipsaddr_next;
@@ -894,16 +1045,14 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 	psmi_assert(proto->msgflowid < EP_FLOW_LAST);
 
 #ifdef PSM_CUDA
-	gpu_mem = psmi_cuda_is_buffer_gpu_mem((void*)ubuf);
+	gpu_mem = len && psmi_cuda_is_buffer_gpu_mem((void*)ubuf);
 	if (gpu_mem) {
 		psmi_cuda_set_attr_sync_memops(ubuf);
 		if (psmi_cuda_is_needed_rendezvous(proto, len))
 			goto do_rendezvous;
 	}
 #endif
-	flow_type = flow_select_type(proto, len, gpu_mem,
-				     proto->iovec_thresh_eager_blocking);
-	flow = &ipsaddr->flows[flow_type];
+	flow = &ipsaddr->flows[EP_FLOW_GO_BACK_N_PIO];
 
 	if (flags & PSM2_MQ_FLAG_SENDSYNC) {
 		goto do_rendezvous;
@@ -918,28 +1067,42 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 		ips_scb_copy_tag(scb->ips_lrh.tag, tag->tag);
 #ifdef PSM_CUDA
 		const void *user_buffer = ubuf;
-		if (gpu_mem){
+		if (!gpu_mem) {
+			mq_copy_tiny_host_mem((uint32_t *) &scb->ips_lrh.hdr_data,
+							  (uint32_t *) user_buffer, len);
+			proto->strat_stats.tiny_cpu_send++;
+			proto->strat_stats.tiny_cpu_send_bytes += len;
+		} else {
+			// TBD USER_BUF_GPU only useful for RTS
+			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
 			/* The following functions PINS the GPU pages
 			 * and mmaps the pages into the process virtual
 			 * space. This allows PSM to issue a standard
 			 * memcpy to move data between HFI resources
 			 * and the GPU
 			 */
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
-			user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
-						(unsigned long)ubuf, len, 0, proto);
-			converted = 1;
-		}
-		mq_copy_tiny_host_mem((uint32_t *) &scb->ips_lrh.hdr_data,
+			if (len <= gdr_copy_limit_send &&
+				NULL != (user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
+						(unsigned long)ubuf, len, 0, proto->ep))) {
+				mq_copy_tiny_host_mem((uint32_t *) &scb->ips_lrh.hdr_data,
 							  (uint32_t *) user_buffer, len);
-		if (converted) {
-			gdr_unmap_gpu_host_addr(GDR_FD, user_buffer, len, proto);
-		}
-#else // PSM_CUDA
-		mq_copy_tiny
-			((uint32_t *) &scb->ips_lrh.hdr_data,
-			     (uint32_t *) ubuf, len);
+				proto->strat_stats.tiny_gdrcopy_send++;
+				proto->strat_stats.tiny_gdrcopy_send_bytes += len;
+			} else {
+				user_buffer = ubuf;
 #endif // PSM_CUDA
+				mq_copy_tiny
+					((uint32_t *) &scb->ips_lrh.hdr_data,
+					     (uint32_t *) ubuf, len);
+#ifdef PSM_CUDA
+				proto->strat_stats.tiny_cuCopy_send++;
+				proto->strat_stats.tiny_cuCopy_send_bytes += len;
+			}
+		}
+#else
+		proto->strat_stats.tiny_cpu_send++;
+		proto->strat_stats.tiny_cpu_send_bytes += len;
+#endif
 		err = ips_mq_send_envelope(proto, flow, scb, PSMI_TRUE);
 		if (err != PSM2_OK)
 			return err;
@@ -961,14 +1124,61 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 
 		const void * user_buffer = ubuf;
 #ifdef PSM_CUDA
-		if (gpu_mem && len <= gdr_copy_threshold_send) {
+		int converted = 0;
+		if (gpu_mem) {
+			// TBD USER_BUF_GPU only useful for RTS
 			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
-			user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
-						(unsigned long)ubuf, len, 0, proto);
-			converted = 1;
+			/* will use PIO */
+			if (len <= gdr_copy_limit_send &&
+				NULL != (user_buffer = gdr_convert_gpu_to_host_addr(GDR_FD,
+						(unsigned long)ubuf, len, 0, proto->ep))) {
+				converted = 1;
+				proto->strat_stats.short_gdrcopy_send++;
+				proto->strat_stats.short_gdrcopy_send_bytes += len;
+			} else {
+				user_buffer = ubuf;
+				if (len > proto->iovec_gpu_thresh_eager_blocking) {
+					scb->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+						proto->ep->verbs_ep.pd,
+						(void*)user_buffer, len, IBV_ACCESS_IS_GPU_ADDR);
+				} else
+					scb->mr = NULL;
+				if (scb->mr) {
+					ips_scb_flags(scb) |= IPS_SEND_FLAG_SEND_MR;
+					ips_scb_cb(scb) = ips_proto_scb_mr_complete;
+					ips_scb_cb_param(scb) = scb;
+					proto->strat_stats.short_gdr_send++;
+					proto->strat_stats.short_gdr_send_bytes += len;
+				} else
+				{
+					ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+					// TBD for OPA flow_type could be DMA
+					proto->strat_stats.short_cuCopy_send++;
+					proto->strat_stats.short_cuCopy_send_bytes += len;
+				}
+			}
+		} else
+#endif // PSM_CUDA
+		{
+			if (len > proto->iovec_thresh_eager_blocking) {
+				scb->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+						proto->ep->verbs_ep.pd,
+						(void*)user_buffer, len, 0);
+			} else
+				scb->mr = NULL;
+			if (scb->mr) {
+				ips_scb_flags(scb) |= IPS_SEND_FLAG_SEND_MR;
+				ips_scb_cb(scb) = ips_proto_scb_mr_complete;
+				ips_scb_cb_param(scb) = scb;
+				proto->strat_stats.short_dma_cpu_send++;
+				proto->strat_stats.short_dma_cpu_send_bytes += len;
+			} else
+			{
+				// TBD for OPA flow_type could be DMA
+				proto->strat_stats.short_copy_cpu_send++;
+				proto->strat_stats.short_copy_cpu_send_bytes += len;
+			}
 		}
-#endif
-
 
 		ips_scb_buffer(scb) = (void *)user_buffer;
 		ips_scb_length(scb) = paylen;
@@ -979,23 +1189,12 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 				(uint32_t *)((uintptr_t)ubuf + paylen),
 				len - paylen);
 		}
-#if defined(PSM_CUDA)
-		if (converted) {
-			gdr_unmap_gpu_host_addr(GDR_FD, user_buffer, len, proto);
-		}
-#endif // PSM_CUDA
 
 		/*
 		 * Need ack for send side completion because we
-		 * send from user buffer.
+		 * send from user buffer. ACK will trigger scb callback
 		 */
 		ips_scb_flags(scb) |= IPS_SEND_FLAG_ACKREQ;
-#ifdef PSM_CUDA
-		if (gpu_mem && len > gdr_copy_threshold_send) {
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
-		}
-#endif
 		err = ips_mq_send_envelope(proto, flow, scb, PSMI_TRUE);
 		if (err != PSM2_OK)
 			return err;
@@ -1005,10 +1204,15 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 		 * 'scb' to be changed, when this scb is done, the
 		 * address is set to NULL when scb is put back to
 		 * scb pool. Even if the same scb is re-used, it
-		 * is not possible to set to this 'ubuf' address.
+		 * is not possible to set to this 'ubuf' address
+		 * because the app has not yet had a chance to start
+		 * another IO.  TBD - possible odd scenario if app
+		 * had this IO started in middle of a buffer which it also
+		 * had a multi-packet eager IO working on, then could see
+		 * same user_buffer from two IOs here.
 		 */
 		if (ips_scb_buffer(scb) == (void *)user_buffer) {
-			if (flow->transfer != PSM_TRANSFER_PIO ||
+			if ((ips_scb_flags(scb) & IPS_SEND_FLAG_SEND_MR) ||
 			    paylen > proto->scb_bufsize ||
 			    !ips_scbctrl_bufalloc(scb)) {
 				/* sdma transfer (can't change user buffer),
@@ -1021,14 +1225,26 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 					return err;
 				err = PSM2_OK;
 			} else {
+				/* PIO and now have a bounce buffer */
 				/* copy to bounce buffer */
 #ifdef PSM_CUDA
-				ips_shortcpy_host_mem
+				if (!gpu_mem || converted) {
+					// host address
+					ips_shortcpy_host_mem
+						(ips_scb_buffer(scb),
+					 	(void*)user_buffer, paylen);
+				} else {
+					// cuda address - undo flags so PIO
+					// doesn't cuMemcpy too
+					ips_scb_flags(scb) &= ~IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+					// TBD - could call cuMemcpy directly
+					ips_shortcpy(ips_scb_buffer(scb),
+					 	(void*)user_buffer, paylen);
+				}
 #else
-				ips_shortcpy
-#endif
-					(ips_scb_buffer(scb),
+				ips_shortcpy(ips_scb_buffer(scb),
 					 (void*)user_buffer, paylen);
+#endif
 			}
 		}
 		_HFI_VDBG("[shrt][%s->%s][b=%p][m=%d][t=%08x.%08x.%08x]\n",
@@ -1050,16 +1266,50 @@ ips_proto_mq_send(psm2_mq_t mq, psm2_epaddr_t mepaddr, uint32_t flags,
 		req->cuda_hostbuf_used = 0;
 		if (gpu_mem) {
 			req->is_buf_gpu_mem = 1;
-		} else
+			// TBD - no upper bound for send DMA here
+			// non-priority MR and will fallback if can't register
+			if (len > proto->iovec_gpu_thresh_eager_blocking) {
+				req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+                                        proto->ep->verbs_ep.pd,
+                                        (void*)ubuf, len, IBV_ACCESS_IS_GPU_ADDR);
+			}
+			if (req->mr) {
+				proto->strat_stats.eager_gdr_send++;
+				proto->strat_stats.eager_gdr_send_bytes += len;
+			} else
+			{
+				proto->strat_stats.eager_cuCopy_send++;
+				proto->strat_stats.eager_cuCopy_send_bytes += len;
+			}
+		} else {
 			req->is_buf_gpu_mem = 0;
-#endif
-
+#else
+		{
+#endif // PSM_CUDA
+			// TBD - no upper bound for send DMA here
+			// non-priority MR and will fallback if can't register
+			if (len > proto->iovec_thresh_eager_blocking) {
+				req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+                                        	proto->ep->verbs_ep.pd,
+                                        	(void*)ubuf, len, 0);
+			}
+			if (req->mr) {
+				proto->strat_stats.eager_dma_cpu_send++;
+				proto->strat_stats.eager_dma_cpu_send_bytes += len;
+			} else
+			{
+				// TBD for OPA flow_type could be DMA
+				proto->strat_stats.eager_copy_cpu_send++;
+				proto->strat_stats.eager_copy_cpu_send_bytes += len;
+			}
+		}
 		req->type |= MQE_TYPE_WAITING;
 		req->req_data.send_msglen = len;
 		req->req_data.tag = *tag;
 		req->send_msgoff = 0;
 		req->flags_user = flags;
 		req->flags_internal |= PSMI_REQ_FLAG_IS_INTERNAL;
+		req->rts_peer = (psm2_epaddr_t) ipsaddr;
 
 		err = ips_ptl_mq_eager(proto, req, flow, tag, ubuf, len);
 		if (err != PSM2_OK)
@@ -1088,9 +1338,20 @@ do_rendezvous:
 #ifdef PSM_CUDA
 		if (gpu_mem) {
 			req->is_buf_gpu_mem = 1;
-		} else
+			proto->strat_stats.rndv_gpu_send++;
+			proto->strat_stats.rndv_gpu_send_bytes += len;
+		} else {
 			req->is_buf_gpu_mem = 0;
 #endif
+			proto->strat_stats.rndv_cpu_send++;
+			proto->strat_stats.rndv_cpu_send_bytes += len;
+#ifdef PSM_CUDA
+		}
+#endif
+
+		mq->stats.tx_num++;
+		mq->stats.tx_rndv_num++;
+		// we count tx_rndv_bytes as we get CTS
 
 		err = ips_ptl_mq_rndv(proto, req, ipsaddr, ubuf, len);
 		if (err != PSM2_OK)
@@ -1102,6 +1363,15 @@ do_rendezvous:
 	mq->stats.tx_num++;
 	mq->stats.tx_eager_num++;
 	mq->stats.tx_eager_bytes += len;
+#ifdef PSM_CUDA
+	if (gpu_mem) {
+		mq->stats.tx_eager_gpu_num++;
+		mq->stats.tx_eager_gpu_bytes += len;
+	} else {
+		mq->stats.tx_eager_cpu_num++;
+		mq->stats.tx_eager_cpu_bytes += len;
+	}
+#endif
 
 	return err;
 }
@@ -1120,14 +1390,20 @@ ips_proto_mq_rts_match_callback(psm2_mq_req_t req, int was_posted)
 	 */
 	PSM2_LOG_MSG("entering");
 	_HFI_MMDBG("rts_match_callback\n");
+	// while matching RTS we set both recv and send msglen to min of the two
+	psmi_assert(req->req_data.recv_msglen == req->req_data.send_msglen);
+	req->mq->stats.rx_user_num++;
+	req->mq->stats.rx_user_bytes += req->req_data.recv_msglen;
 #ifdef PSM_CUDA
 	/* Cases where we do not use TIDs:
+	 * 0) Received full message as payload to RTS, CTS is just an ack
 	 * 1) Recv on a host buffer, Send on a gpu buffer and len is less than 3 bytes
 	 * 2) Recv on a host buffer, Send on a host buffer and len is less than hfi_thresh_rv
 	 * 3) Recv on gpu buf and len is less than 3 bytes
 	 * 4) Expected protocol not initialized.
 	 */
-	if ((!req->is_buf_gpu_mem && ((req->is_sendbuf_gpu_mem &&
+	if (req->recv_msgoff >= req->req_data.recv_msglen
+	    || (!req->is_buf_gpu_mem && ((req->is_sendbuf_gpu_mem &&
 	     req->req_data.recv_msglen <= GPUDIRECT_THRESH_RV)||
 	    (!req->is_sendbuf_gpu_mem &&
 	     req->req_data.recv_msglen <= proto->mq->hfi_thresh_rv))) ||
@@ -1135,7 +1411,7 @@ ips_proto_mq_rts_match_callback(psm2_mq_req_t req, int was_posted)
 		! ips_epaddr_connected((ips_epaddr_t *) epaddr) ||
 	    proto->protoexp == NULL) {	/* no expected tid recieve */
 #else // PSM_CUDA
-	if (
+	if (req->recv_msgoff >= req->req_data.recv_msglen ||
 		! ips_epaddr_connected((ips_epaddr_t *) epaddr) ||
 	    req->req_data.recv_msglen <= proto->mq->hfi_thresh_rv || /* less rv theshold */
 	    proto->protoexp == NULL) {  /* no expected tid recieve */
@@ -1145,7 +1421,35 @@ ips_proto_mq_rts_match_callback(psm2_mq_req_t req, int was_posted)
 
 		/* there is no order requirement, try to push CTS request
 		 * directly, if fails, then queue it for later try. */
-		_HFI_VDBG("pushing CTS\n");
+		_HFI_VDBG("pushing CTS recv off %u len %u"
+#ifdef PSM_CUDA
+			" rGPU %u sGPU %u"
+#endif
+			" rv thresh %u"
+			" conn %u"
+			" epaddr %p RDMA %u\n",
+			req->recv_msgoff, req->req_data.recv_msglen,
+#ifdef PSM_CUDA
+			req->is_buf_gpu_mem, req->is_sendbuf_gpu_mem,
+#endif
+			proto->mq->hfi_thresh_rv,
+			ips_epaddr_connected((ips_epaddr_t *) epaddr),
+			epaddr, proto->protoexp != NULL);
+
+		if (req->recv_msgoff < req->req_data.recv_msglen) {
+			// RTS did not have the message as payload
+#ifdef PSM_CUDA
+			if (req->is_buf_gpu_mem) {
+				proto->strat_stats.rndv_long_gpu_recv++;
+				proto->strat_stats.rndv_long_gpu_recv_bytes += req->req_data.recv_msglen;
+			} else {
+#endif
+				proto->strat_stats.rndv_long_cpu_recv++;
+				proto->strat_stats.rndv_long_cpu_recv_bytes += req->req_data.recv_msglen;
+#ifdef PSM_CUDA
+			}
+#endif
+		}
 		if (ips_proto_mq_push_cts_req(proto, req) != PSM2_OK) {
 			struct ips_pend_sends *pends = &proto->pend_sends;
 			struct ips_pend_sreq *sreq =
@@ -1191,7 +1495,7 @@ ips_proto_mq_rts_match_callback(psm2_mq_req_t req, int was_posted)
 			req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
 					proto->ep->verbs_ep.pd,
 					req->req_data.buf, req->req_data.send_msglen,
-					IBV_ACCESS_REMOTE_WRITE);
+					IBV_ACCESS_RDMA|IBV_ACCESS_REMOTE_WRITE);
 			if (! req->mr) {
 				// ips_protoexp_tid_get_from_token will try to get MR again
 				// and will retry via ips_tid_pendtids_timer_callback.  So we
@@ -1244,6 +1548,7 @@ ips_proto_mq_push_cts_req(struct ips_proto *proto, psm2_mq_req_t req)
 	PSM2_LOG_EPM(OPCODE_LONG_CTS,PSM2_LOG_TX, proto->ep->epid,
 		    flow->ipsaddr->epaddr.epid ,"req->rts_reqidx_peer: %d",
 		    req->rts_reqidx_peer);
+	proto->epaddr_stats.cts_long_data_send++;
 
 	ips_proto_flow_enqueue(flow, scb);
 	flow->flush(flow, NULL);
@@ -1258,6 +1563,7 @@ ips_proto_mq_push_cts_req(struct ips_proto *proto, psm2_mq_req_t req)
 }
 
 // rendezvous using LONG DATA "eager push" instead of TID
+// If we run out of resources (scbs), this is called again to continue
 psm2_error_t
 ips_proto_mq_push_rts_data(struct ips_proto *proto, psm2_mq_req_t req)
 {
@@ -1268,8 +1574,12 @@ ips_proto_mq_push_rts_data(struct ips_proto *proto, psm2_mq_req_t req)
 	uint32_t nbytes_sent = 0;
 	uint32_t nbytes_this, chunk_size;
 	uint16_t frag_size, unaligned_bytes;
+#ifdef PSM_CUDA
+	int converted = 0;
+#endif
 	struct ips_flow *flow;
 	ips_scb_t *scb;
+	int dostats = !req->recv_msgoff; // if continuing, don't update stats
 
 	psmi_assert(nbytes_left > 0);
 
@@ -1278,6 +1588,64 @@ ips_proto_mq_push_rts_data(struct ips_proto *proto, psm2_mq_req_t req)
 		/* use PIO transfer */
 		flow = &ipsaddr->flows[EP_FLOW_GO_BACK_N_PIO];
 		chunk_size = frag_size = flow->frag_size;
+#ifdef PSM_CUDA
+		if (req->is_buf_gpu_mem) {
+			// rare, but when RV connection not available, we
+			// can select LONG DATA for a GPU send buffer.  Normally
+			// won't happen for GPU send >3 unless RDMA disabled
+			// or RV not connected
+			// TBD - no upper bound for send DMA here
+			// non-priority MR and will fallback if can't register
+			if (!req->mr && req->req_data.send_msglen > proto->iovec_gpu_thresh_eager) {
+				req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+					proto->ep->verbs_ep.pd,
+					req->req_data.buf, req->req_data.send_msglen, 
+					IBV_ACCESS_IS_GPU_ADDR);
+			}
+			if (req->mr) {
+				proto->strat_stats.rndv_long_gdr_send += dostats;
+				proto->strat_stats.rndv_long_gdr_send_bytes += dostats*req->req_data.send_msglen;
+			} else
+#ifdef PSM_CUDA
+				// for GPU send buffer <= 3, receiver can select
+				// LONG DATA and we can use GDRCopy
+				// must repin per attempt
+			if (req->req_data.send_msglen <= gdr_copy_limit_send &&
+				0 != (buf =  (uintptr_t)gdr_convert_gpu_to_host_addr(GDR_FD,
+					(unsigned long)req->req_data.buf,
+					req->req_data.send_msglen, 0, proto->ep))) {
+				converted = 1;
+				proto->strat_stats.rndv_long_gdrcopy_send += dostats;
+				proto->strat_stats.rndv_long_gdrcopy_send_bytes += dostats*req->req_data.send_msglen;
+			} else {
+				buf = (uintptr_t) req->req_data.buf + req->recv_msgoff;
+#else
+			{
+#endif
+				proto->strat_stats.rndv_long_cuCopy_send += dostats;
+				proto->strat_stats.rndv_long_cuCopy_send_bytes += dostats*req->req_data.send_msglen;
+			}
+		} else {
+#endif
+			// TBD - no upper bound for send DMA here
+			// non-priority MR and will fallback if can't register
+			if (!req->mr && req->req_data.send_msglen > proto->iovec_thresh_eager) {
+				req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
+					proto->ep->verbs_ep.pd,
+					req->req_data.buf,
+					req->req_data.send_msglen, 0);
+			}
+			if (req->mr) {
+				proto->strat_stats.rndv_long_dma_cpu_send += dostats;
+				proto->strat_stats.rndv_long_dma_cpu_send_bytes += dostats*req->req_data.send_msglen;
+			} else
+			{
+				proto->strat_stats.rndv_long_copy_cpu_send += dostats;
+				proto->strat_stats.rndv_long_copy_cpu_send_bytes += dostats*req->req_data.send_msglen;
+			}
+#ifdef PSM_CUDA
+		}
+#endif
 	}
 
 	do {
@@ -1311,6 +1679,14 @@ ips_proto_mq_push_rts_data(struct ips_proto *proto, psm2_mq_req_t req)
 		/* attached unaligned bytes into packet header */
 		unaligned_bytes = nbytes_left & 0x3;
 		if (unaligned_bytes) {
+#ifdef PSM_CUDA
+			if (!req->is_buf_gpu_mem
+			    || converted
+			    )
+				mq_copy_tiny_host_mem((uint32_t *)&scb->ips_lrh.mdata,
+					(uint32_t *)buf, unaligned_bytes);
+			else
+#endif
 			mq_copy_tiny((uint32_t *)&scb->ips_lrh.mdata,
 				(uint32_t *)buf, unaligned_bytes);
 
@@ -1327,11 +1703,17 @@ ips_proto_mq_push_rts_data(struct ips_proto *proto, psm2_mq_req_t req)
 		}
 		scb->ips_lrh.data[1].u32w0 = req->recv_msgoff;
 		ips_scb_buffer(scb) = (void *)buf;
+		if (req->mr) {
+			scb->mr = req->mr;
+			ips_scb_flags(scb) |= IPS_SEND_FLAG_SEND_MR;
+		}
 #ifdef PSM_CUDA
 		// SDMA identifies GPU buffers itself. But PIO path needs flags
 		if (req->is_buf_gpu_mem
 		) {
-			ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+			if (!req->mr && !converted)
+				ips_scb_flags(scb) |= IPS_SEND_FLAG_PAYLOAD_BUF_GPU;
+			// TBD USER_BUF_GPU only useful for RTS
 			ips_scb_flags(scb) |= IPS_SEND_FLAG_USER_BUF_GPU;
 		}
 #endif
@@ -1370,7 +1752,6 @@ ips_proto_mq_push_rts_data(struct ips_proto *proto, psm2_mq_req_t req)
 		} else {
 			req->send_msgoff += nbytes_this;
 		}
-
 		ips_proto_flow_enqueue(flow, scb);
 		if (flow->transfer == PSM_TRANSFER_PIO) {
 			/* we need to flush the pio pending queue as quick as possible */
@@ -1421,14 +1802,19 @@ ips_proto_mq_handle_cts(struct ips_recvhdrq_event *rcv_ev)
 		PSM2_LOG_EPM(OPCODE_LONG_CTS,PSM2_LOG_RX,rcv_ev->ipsaddr->epaddr.epid,
 			    mq->ep->epid,"p_hdr->data[1].u32w0 %d",
 			    p_hdr->data[1].u32w0);
-		proto->epaddr_stats.tids_grant_recv++;
+		proto->epaddr_stats.cts_rdma_recv++;
 
+#ifdef PSM_CUDA
+		psmi_assert(p_hdr->data[1].u32w1 > min(cuda_thresh_rndv, mq->hfi_thresh_rv));	// msglen
+#else
 		psmi_assert(p_hdr->data[1].u32w1 > mq->hfi_thresh_rv);	// msglen
+#endif
 		psmi_assert(proto->protoexp != NULL);
 
 		/* ptl_req_ptr will be set to each tidsendc */
 		if (req->ptl_req_ptr == NULL) {
 			req->req_data.send_msglen = p_hdr->data[1].u32w1;
+			req->mq->stats.tx_rndv_bytes += req->req_data.send_msglen;
 		}
 		psmi_assert(req->req_data.send_msglen == p_hdr->data[1].u32w1);
 
@@ -1441,7 +1827,7 @@ ips_proto_mq_handle_cts(struct ips_recvhdrq_event *rcv_ev)
 			// or we failed to register memory previously.
 			req->mr = psm2_verbs_reg_mr(proto->mr_cache, 0,
 							proto->ep->verbs_ep.pd,
-							req->req_data.buf, req->req_data.send_msglen, 0
+							req->req_data.buf, req->req_data.send_msglen, IBV_ACCESS_RDMA
 #ifdef PSM_CUDA
 								| (req->is_buf_gpu_mem?IBV_ACCESS_IS_GPU_ADDR:0)
 #endif
@@ -1465,25 +1851,57 @@ ips_proto_mq_handle_cts(struct ips_recvhdrq_event *rcv_ev)
 		}
 	} else {
 		// we will use LONG DATA push
+		PSM2_LOG_EPM(OPCODE_LONG_CTS,PSM2_LOG_RX,rcv_ev->ipsaddr->epaddr.epid,
+			    mq->ep->epid, "long data");
+		proto->epaddr_stats.cts_long_data_recv++;
 		req->rts_reqidx_peer = p_hdr->data[0].u32w0; /* eager receive only */
 		req->req_data.send_msglen = p_hdr->data[1].u32w1;
 
 		if (req->send_msgoff >= req->req_data.send_msglen) {
+// TBD - should cleanup from pin as needed
 			/* already sent enough bytes, may truncate so using >= */
+			/* RTS payload is only used for CPU memory */
+			proto->strat_stats.rndv_rts_copy_cpu_send++;
+			proto->strat_stats.rndv_rts_copy_cpu_send_bytes += req->req_data.send_msglen;
 			ips_proto_mq_rv_complete(req);
-		} else if (ips_proto_mq_push_rts_data(proto, req) != PSM2_OK) {
-			/* there is no order requirement, tried to push RTS data
-			 * directly and not done, so queue it for later try. */
-			struct ips_pend_sreq *sreq =
-				psmi_mpool_get(proto->pend_sends_pool);
-			psmi_assert(sreq != NULL);
+		} else {
+			req->mq->stats.tx_rndv_bytes += (req->req_data.send_msglen - req->send_msgoff);
+#ifdef RNDV_MOD
+			// If we have an MR due to incorrect prediction of RDMA
+			// release it if can't be used for send DMA or don't
+			// want send DMA.  push_rts_data will attempt to use
+			// for send DMA if req->mr != NULL.
+			if (req->mr &&
+				(!psm2_verbs_user_space_mr(req->mr)
+#ifdef PSM_CUDA
+				|| (req->is_buf_gpu_mem && req->req_data.send_msglen <= proto->iovec_gpu_thresh_eager)
+				|| (!req->is_buf_gpu_mem && req->req_data.send_msglen <= proto->iovec_thresh_eager)
+#else
+				|| (req->req_data.send_msglen <= proto->iovec_thresh_eager)
+#endif
+				)) {
 
-			sreq->type = IPS_PENDSEND_EAGER_DATA;
-			sreq->req = req;
-			STAILQ_INSERT_TAIL(&proto->pend_sends.pendq, sreq, next);
-			/* Make sure it's processed by timer */
-			psmi_timer_request(proto->timerq, &proto->pend_sends.timer,
-					   PSMI_TIMER_PRIO_1);
+				_HFI_MMDBG("Using LONG_DATA, releasing RV RDMA MR: %p rkey: 0x%x\n", req->mr, req->mr->rkey);
+				psm2_verbs_release_mr(req->mr);
+				req->mr = NULL;
+				ips_tid_mravail_callback(req->rts_peer->proto);
+			}
+#endif
+
+			if (ips_proto_mq_push_rts_data(proto, req) != PSM2_OK) {
+				/* there is no order requirement, tried to push RTS data
+				 * directly and not done, so queue it for later try. */
+				struct ips_pend_sreq *sreq =
+					psmi_mpool_get(proto->pend_sends_pool);
+				psmi_assert(sreq != NULL);
+
+				sreq->type = IPS_PENDSEND_EAGER_DATA;
+				sreq->req = req;
+				STAILQ_INSERT_TAIL(&proto->pend_sends.pendq, sreq, next);
+				/* Make sure it's processed by timer */
+				psmi_timer_request(proto->timerq, &proto->pend_sends.timer,
+						   PSMI_TIMER_PRIO_1);
+			}
 		}
 	}
 
@@ -1554,6 +1972,7 @@ ips_proto_mq_handle_rts(struct ips_recvhdrq_event *rcv_ev)
 				    (psm2_epaddr_t) &ipsaddr->msgctl->
 				    master_epaddr,
 				    (psm2_mq_tag_t *) p_hdr->tag,
+				    &rcv_ev->proto->strat_stats,
 				    p_hdr->data[1].u32w1, payload, paylen,
 				    msgorder, ips_proto_mq_rts_match_callback,
 				    &req);
@@ -1571,6 +1990,8 @@ ips_proto_mq_handle_rts(struct ips_recvhdrq_event *rcv_ev)
 		return IPS_RECVHDRQ_REVISIT;
 	}
 
+	rcv_ev->proto->epaddr_stats.rts_recv++;
+
 	req->rts_peer = (psm2_epaddr_t) ipsaddr;
 	req->rts_reqidx_peer = p_hdr->data[1].u32w0;
 	if (req->req_data.send_msglen > mq->hfi_thresh_rv)
@@ -1659,14 +2080,16 @@ ips_proto_mq_handle_tiny(struct ips_recvhdrq_event *rcv_ev)
 	 */
 	psmi_assert(msgorder != IPS_MSG_ORDER_PAST);
 
-	_HFI_VDBG("tag=%08x.%08x.%08x opcode=%d, msglen=%d\n",
+	_HFI_VDBG("tag=%08x.%08x.%08x opcode=%x, msglen=%d\n",
 		  p_hdr->tag[0], p_hdr->tag[1], p_hdr->tag[2],
-		  OPCODE_TINY, p_hdr->hdr_data.u32w1);
+		  OPCODE_TINY, (p_hdr->khdr.kdeth0 >> HFI_KHDR_TINYLEN_SHIFT)
+				& HFI_KHDR_TINYLEN_MASK);
 
 	/* store in req below too! */
 	int rc = psmi_mq_handle_envelope(mq,
 				(psm2_epaddr_t) &ipsaddr->msgctl->master_epaddr,
-				(psm2_mq_tag_t *) p_hdr->tag, paylen, 0,
+				(psm2_mq_tag_t *) p_hdr->tag,
+				&rcv_ev->proto->strat_stats,  paylen, 0,
 				payload, paylen, msgorder, OPCODE_TINY, &req);
 	if (unlikely(rc == MQ_RET_UNEXP_NO_RESOURCES)) {
 		uint32_t psn_mask = ((psm2_epaddr_t)ipsaddr)->proto->psn_mask;
@@ -1745,7 +2168,7 @@ ips_proto_mq_handle_short(struct ips_recvhdrq_event *rcv_ev)
 	 */
 	psmi_assert(msgorder != IPS_MSG_ORDER_PAST);
 
-	_HFI_VDBG("tag=%08x.%08x.%08x opcode=%d, msglen=%d\n",
+	_HFI_VDBG("tag=%08x.%08x.%08x opcode=%x, msglen=%d\n",
 		  p_hdr->tag[0], p_hdr->tag[1], p_hdr->tag[2],
 		  OPCODE_SHORT, p_hdr->hdr_data.u32w1);
 
@@ -1753,6 +2176,7 @@ ips_proto_mq_handle_short(struct ips_recvhdrq_event *rcv_ev)
 	int rc = psmi_mq_handle_envelope(mq,
 				(psm2_epaddr_t) &ipsaddr->msgctl->master_epaddr,
 				(psm2_mq_tag_t *) p_hdr->tag,
+				&rcv_ev->proto->strat_stats,
 				p_hdr->hdr_data.u32w1, p_hdr->hdr_data.u32w0,
 				payload, paylen, msgorder, OPCODE_SHORT, &req);
 	if (unlikely(rc == MQ_RET_UNEXP_NO_RESOURCES)) {
@@ -1807,9 +2231,6 @@ ips_proto_mq_handle_eager(struct ips_recvhdrq_event *rcv_ev)
 	char *payload;
 	uint32_t paylen;
 	psm2_mq_req_t req;
-#if defined(PSM_CUDA)
-	int converted = 0;
-#endif // PSM_CUDA
 
 	/*
 	 * if PSN does not match, drop the packet.
@@ -1840,21 +2261,39 @@ ips_proto_mq_handle_eager(struct ips_recvhdrq_event *rcv_ev)
 		 * error is caught below.
 		 */
 		if (req) {
+			//u32w0 is offset - only cnt recv msgs on 1st pkt in msg
 #ifdef PSM_CUDA
-			if (PSMI_USE_GDR_COPY(req, req->req_data.send_msglen)) {
-				req->req_data.buf = gdr_convert_gpu_to_host_addr(GDR_FD,
-							(unsigned long)req->user_gpu_buffer,
-							req->req_data.send_msglen, 1, rcv_ev->proto);
-				converted = 1;
+			int use_gdrcopy = 0;
+			if (!req->is_buf_gpu_mem) {
+				if (req->state == MQ_STATE_UNEXP) {
+					if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_sysbuf_recv++;
+					rcv_ev->proto->strat_stats.eager_sysbuf_recv_bytes += paylen;
+				} else {
+					if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_cpu_recv++;
+					rcv_ev->proto->strat_stats.eager_cpu_recv_bytes += paylen;
+				}
+			} else if (PSMI_USE_GDR_COPY_RECV(paylen)) {
+				use_gdrcopy = 1;
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_gdrcopy_recv++;
+				rcv_ev->proto->strat_stats.eager_gdrcopy_recv_bytes += paylen;
+			} else {
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_cuCopy_recv++;
+				rcv_ev->proto->strat_stats.eager_cuCopy_recv_bytes += paylen;
 			}
-#endif
 			psmi_mq_handle_data(mq, req,
-				p_hdr->data[1].u32w0, payload, paylen);
-#if defined(PSM_CUDA)
-			if (converted) {
-				gdr_unmap_gpu_host_addr(GDR_FD, req->req_data.buf,
-                                    req->req_data.send_msglen, rcv_ev->proto);
+				p_hdr->data[1].u32w0, payload, paylen,
+				use_gdrcopy,
+				rcv_ev->proto->ep);
+#else
+			if (req->state == MQ_STATE_UNEXP) {
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_sysbuf_recv++;
+				rcv_ev->proto->strat_stats.eager_sysbuf_recv_bytes += paylen;
+			} else {
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_cpu_recv++;
+				rcv_ev->proto->strat_stats.eager_cpu_recv_bytes += paylen;
 			}
+			psmi_mq_handle_data(mq, req,
+				p_hdr->data[1].u32w0, payload, paylen);
 #endif // PSM_CUDA
 
 			if (msgorder == IPS_MSG_ORDER_FUTURE_RECV)
@@ -1888,7 +2327,7 @@ ips_proto_mq_handle_eager(struct ips_recvhdrq_event *rcv_ev)
 	 */
 	psmi_assert(msgorder != IPS_MSG_ORDER_PAST);
 
-	_HFI_VDBG("tag=%08x.%08x.%08x opcode=%d, msglen=%d\n",
+	_HFI_VDBG("tag=%08x.%08x.%08x opcode=%x, msglen=%d\n",
 		p_hdr->tag[0], p_hdr->tag[1], p_hdr->tag[2],
 		OPCODE_EAGER, p_hdr->hdr_data.u32w1);
 
@@ -1896,6 +2335,7 @@ ips_proto_mq_handle_eager(struct ips_recvhdrq_event *rcv_ev)
 	int rc = psmi_mq_handle_envelope(mq,
 				(psm2_epaddr_t) &ipsaddr->msgctl->master_epaddr,
 				(psm2_mq_tag_t *) p_hdr->tag,
+				&rcv_ev->proto->strat_stats,
 				p_hdr->hdr_data.u32w1, p_hdr->hdr_data.u32w0,
 				payload, paylen, msgorder, OPCODE_EAGER, &req);
 	if (unlikely(rc == MQ_RET_UNEXP_NO_RESOURCES)) {
@@ -1963,6 +2403,7 @@ ips_proto_mq_handle_outoforder_queue(psm2_mq_t mq, ips_msgctl_t *msgctl)
 	return;
 }
 
+// LONG_DATA packet handler
 int
 ips_proto_mq_handle_data(struct ips_recvhdrq_event *rcv_ev)
 {
@@ -1972,6 +2413,12 @@ ips_proto_mq_handle_data(struct ips_recvhdrq_event *rcv_ev)
 	uint32_t paylen;
 	psm2_mq_req_t req;
 	struct ips_flow *flow;
+#if defined(PSM_CUDA)
+	int use_gdrcopy = 0;
+	struct ips_proto *proto = rcv_ev->proto;
+#endif // PSM_CUDA
+	psmi_copy_tiny_fn_t psmi_copy_tiny_fn = mq_copy_tiny;
+
 
 	/*
 	 * if PSN does not match, drop the packet.
@@ -1981,8 +2428,44 @@ ips_proto_mq_handle_data(struct ips_recvhdrq_event *rcv_ev)
 
 	req = psmi_mpool_find_obj_by_index(mq->rreq_pool, p_hdr->data[0].u32w0);
 	psmi_assert(req != NULL);
+	// while matching RTS we set both recv and send msglen to min of the two
+	psmi_assert(req->req_data.recv_msglen == req->req_data.send_msglen);
 	psmi_assert(p_hdr->data[1].u32w1 == req->req_data.send_msglen);
 
+	payload = ips_recvhdrq_event_payload(rcv_ev);
+	paylen = ips_recvhdrq_event_paylen(rcv_ev);
+	psmi_assert(paylen == 0 || payload);
+
+#ifdef PSM_CUDA
+	// cpu stats already tracked when sent CTS
+	if (req->is_buf_gpu_mem) {
+		req->req_data.buf = req->user_gpu_buffer;
+		// 1st packet with any unaligned data we handle here
+		if (p_hdr->data[1].u32w0 < 4) {
+			void *buf;
+			if (PSMI_USE_GDR_COPY_RECV(paylen + p_hdr->data[1].u32w0) &&
+				NULL != (buf = gdr_convert_gpu_to_host_addr(GDR_FD,
+						(unsigned long)req->user_gpu_buffer,
+						paylen + p_hdr->data[1].u32w0, 1, proto->ep))) {
+				req->req_data.buf = buf;
+				psmi_copy_tiny_fn = mq_copy_tiny_host_mem;
+				proto->strat_stats.rndv_long_gdr_recv++;
+				proto->strat_stats.rndv_long_gdr_recv_bytes += paylen;
+			} else {
+				proto->strat_stats.rndv_long_cuCopy_recv++;
+				proto->strat_stats.rndv_long_cuCopy_recv_bytes += paylen;
+			}
+		} else if (PSMI_USE_GDR_COPY_RECV(paylen)) {
+			// let mq_handle_data do the conversion
+			use_gdrcopy = 1;
+			//proto->strat_stats.rndv_long_gdr_recv++;
+			proto->strat_stats.rndv_long_gdr_recv_bytes += paylen;
+		} else {
+			if (p_hdr->data[1].u32w0 < 4) proto->strat_stats.rndv_long_cuCopy_recv++;
+			proto->strat_stats.rndv_long_cuCopy_recv_bytes += paylen;
+		}
+	}
+#endif
 	/*
 	 * if a packet has very small offset, it must have unaligned data
 	 * attached in the packet header, and this must be the first packet
@@ -1990,17 +2473,18 @@ ips_proto_mq_handle_data(struct ips_recvhdrq_event *rcv_ev)
 	 */
 	if (p_hdr->data[1].u32w0 < 4 && p_hdr->data[1].u32w0 > 0) {
 		psmi_assert(p_hdr->data[1].u32w0 == (req->req_data.send_msglen&0x3));
-		mq_copy_tiny((uint32_t *)req->req_data.buf,
+		psmi_copy_tiny_fn((uint32_t *)req->req_data.buf,
 				(uint32_t *)&p_hdr->mdata,
 				p_hdr->data[1].u32w0);
 		req->send_msgoff += p_hdr->data[1].u32w0;
 	}
 
-	payload = ips_recvhdrq_event_payload(rcv_ev);
-	paylen = ips_recvhdrq_event_paylen(rcv_ev);
-	psmi_assert(paylen == 0 || payload);
-
-	psmi_mq_handle_data(mq, req, p_hdr->data[1].u32w0, payload, paylen);
+	psmi_mq_handle_data(mq, req, p_hdr->data[1].u32w0, payload, paylen
+#ifdef PSM_CUDA
+				, use_gdrcopy, rcv_ev->proto->ep);
+#else
+				);
+#endif
 
 	flow = &rcv_ev->ipsaddr->flows[ips_proto_flowid(p_hdr)];
 	if ((__be32_to_cpu(p_hdr->bth[2]) & IPS_SEND_FLAG_ACKREQ) ||
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_params.h b/prov/psm3/psm3/ptl_ips/ips_proto_params.h
index 0ad8b6d..ffdfd22 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_params.h
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_params.h
@@ -169,6 +169,9 @@
 #define IPS_SEND_FLAG_PAYLOAD_BUF_GPU   0x0800
 #endif
 
+#define IPS_SEND_FLAG_SEND_MR          0x1000
+
+
 /* 0x10000000, interrupt when done */
 #define IPS_SEND_FLAG_INTR		(1<<HFI_KHDR_INTR_SHIFT)
 
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_recv.c b/prov/psm3/psm3/ptl_ips/ips_proto_recv.c
index 69a25a4..e5a5a94 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_recv.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_recv.c
@@ -747,7 +747,7 @@ ips_proto_connect_disconnect(struct ips_recvhdrq_event *rcv_ev)
 					paylen);
 	if (err != PSM2_OK)
 		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-			"Process connect/disconnect error: %d, opcode %d\n",
+			"Process connect/disconnect error: %d, opcode %x\n",
 			err, _get_proto_hfi_opcode(rcv_ev->p_hdr));
 
 	return IPS_RECVHDRQ_CONTINUE;
@@ -805,7 +805,7 @@ int ips_proto_process_unknown(const struct ips_recvhdrq_event *rcv_ev)
 		ips_proto_show_header(p_hdr, "invalid connidx");
 
 	psmi_handle_error(PSMI_EP_LOGEVENT, PSM2_EPID_NETWORK_ERROR,
-			 "Received message(s) opcode=0x%x from an unknown process", opcode);
+			 "Received message(s) opcode=%x from an unknown process", opcode);
 
 	return 0;		/* Always skip this packet unless the above call was a noreturn
 				 * call */
diff --git a/prov/psm3/psm3/ptl_ips/ips_recvhdrq.c b/prov/psm3/psm3/ptl_ips/ips_recvhdrq.c
index 9e97e76..b71a15d 100644
--- a/prov/psm3/psm3/ptl_ips/ips_recvhdrq.c
+++ b/prov/psm3/psm3/ptl_ips/ips_recvhdrq.c
@@ -164,6 +164,7 @@ process_pending_acks(struct ips_recvhdrq *recvq))
 	}
 }
 
+
 #ifdef RNDV_MOD
 // check for and process RV RDMA sends and RDMA recv
 psm2_error_t check_rv_completion(psm2_ep_t ep, struct ips_proto *proto)
@@ -179,6 +180,7 @@ psm2_error_t check_rv_completion(psm2_ep_t ep, struct ips_proto *proto)
 		switch (ev.event_type) {
 			case RV_WC_RDMA_WRITE:
 				ep->verbs_ep.send_rdma_outstanding--;
+				_HFI_MMDBG("got RV RDMA Write SQ CQE status %u outstanding %u\n", ev.wc.status, ep->verbs_ep.send_rdma_outstanding);
 				if_pf (ev.wc.status || ev.wc.wr_id == 0) {
 					if (PSM2_OK != ips_protoexp_rdma_write_completion_error(
 								ep, ev.wc.wr_id, ev.wc.status))
@@ -193,8 +195,8 @@ psm2_error_t check_rv_completion(psm2_ep_t ep, struct ips_proto *proto)
 					if (ep->rv_reconnect_timeout)
 						break;	/* let sender handle errors */
 					psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-							"failed rv recv RDMA '%s' (%d) on epid 0x%lx\n",
-							ibv_wc_status_str(ev.wc.status), (int)ev.wc.status, ep->epid);
+							"failed rv recv RDMA '%s' (%d) on %s port %u epid 0x%lx\n",
+							ibv_wc_status_str(ev.wc.status), (int)ev.wc.status, ep->dev_name, ep->portnum, ep->epid);
 					return PSM2_INTERNAL_ERR;
 				}
 				_HFI_MMDBG("got RV RDMA Write Immediate RQ CQE %u bytes\n",
@@ -205,9 +207,9 @@ psm2_error_t check_rv_completion(psm2_ep_t ep, struct ips_proto *proto)
 				break;
 			default:
 				psmi_handle_error( PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-					"unexpected rv event %d status '%s' (%d) on epid 0x%lx\n",
+					"unexpected rv event %d status '%s' (%d) on %s port %u epid 0x%lx\n",
 					ev.event_type, ibv_wc_status_str(ev.wc.status),
-					(int)ev.wc.status, ep->epid);
+					(int)ev.wc.status, ep->dev_name, ep->portnum, ep->epid);
 				break;
 			}
 	}
@@ -260,8 +262,8 @@ psm2_error_t ips_recvhdrq_progress(struct ips_recvhdrq *recvq)
 				if (errno == EAGAIN || errno == EWOULDBLOCK
 				    || errno == EBUSY || errno = EINTR)
 					break;
-				_HFI_ERROR("failed ibv_poll_cq '%s' (%d) on epid 0x%lx\n",
-					strerror(errno), errno, ep->epid);
+				_HFI_ERROR("failed ibv_poll_cq '%s' (%d) on %s port %u epid 0x%lx\n",
+					strerror(errno), errno, ep->dev_name, ep->portnum, ep->epid);
 				goto fail;
 			}
 			ep->verbs_ep.recv_wc_count = err;
@@ -293,8 +295,8 @@ psm2_error_t ips_recvhdrq_progress(struct ips_recvhdrq *recvq)
 			if (errno == EAGAIN || errno == EWOULDBLOCK
 			    || errno == EBUSY || errno == EINTR)
 				break;
-			_HFI_ERROR("failed ibv_poll_cq '%s' (%d) on epid 0x%lx\n",
-				strerror(errno), errno, ep->epid);
+			_HFI_ERROR("failed ibv_poll_cq '%s' (%d) on %s port %u epid 0x%lx\n",
+				strerror(errno), errno, ep->dev_name, ep->portnum, ep->epid);
 			goto fail;
 		} else {
 #endif	// VERBS_RECV_CQE_BATCH > 1
@@ -302,8 +304,8 @@ psm2_error_t ips_recvhdrq_progress(struct ips_recvhdrq *recvq)
 			buf = (rbuf_t)WC(wr_id);
 			if_pf (WC(status)) {
 				if (WC(status) != IBV_WC_WR_FLUSH_ERR)
-					_HFI_ERROR("failed recv '%s' (%d) on epid 0x%lx QP %u\n",
-						ibv_wc_status_str(WC(status)), (int)WC(status), ep->epid, WC(qp_num));
+					_HFI_ERROR("failed recv '%s' (%d) on %s port %u epid 0x%lx QP %u\n",
+						ibv_wc_status_str(WC(status)), (int)WC(status), ep->dev_name, ep->portnum, ep->epid, WC(qp_num));
 				goto fail;
 			}
 			switch (WC(opcode)) {
@@ -318,8 +320,8 @@ psm2_error_t ips_recvhdrq_progress(struct ips_recvhdrq *recvq)
 				goto repost;
 				break;
 			default:
-				_HFI_ERROR("unexpected recv opcode %d on epid 0x%lx QP %u\n",
-					WC(opcode), ep->epid, WC(qp_num));
+				_HFI_ERROR("unexpected recv opcode %d on %s port %u epid 0x%lx QP %u\n",
+					WC(opcode), ep->dev_name, ep->portnum, ep->epid, WC(qp_num));
 				goto repost;
 				break;
 			case IBV_WC_RECV:
@@ -332,9 +334,9 @@ psm2_error_t ips_recvhdrq_progress(struct ips_recvhdrq *recvq)
 				// 		slid - remote SLID
 				// 		probably have GRH at start of buffer with remote GID
 				if_pf (_HFI_PDBG_ON)
-					__psm2_dump_buf(rbuf_to_buffer(buf), WC(byte_len));
+					_HFI_PDBG_DUMP(rbuf_to_buffer(buf), WC(byte_len));
 				if_pf (WC(byte_len) < rbuf_addition(buf)+sizeof(struct ips_message_header)) {
-					_HFI_ERROR( "unexpected small recv: %u\n", WC(byte_len));
+					_HFI_ERROR( "unexpected small recv: %u on %s port %u\n", WC(byte_len), ep->dev_name, ep->portnum);
 					goto repost;
 				}
 				rcv_ev.payload_size = WC(byte_len) - rbuf_addition(buf) - sizeof(struct ips_message_header);
@@ -376,7 +378,7 @@ repost:
 		// buffer processing is done, we can requeue it on QP
 		if_pf (PSM2_OK != __psm2_ep_verbs_post_recv(
 									buf))
-			_HFI_ERROR( "unable to post recv\n"); // leak the buffer
+			_HFI_ERROR( "unable to post recv on %s port %u\n", ep->dev_name, ep->portnum); // leak the buffer
 
 		// if we can't process this now (such as an RTS we revisited and
 		// ended up queueing on unexpected queue) we're told
diff --git a/prov/psm3/psm3/ptl_ips/ips_scb.c b/prov/psm3/psm3/ptl_ips/ips_scb.c
index 83517ac..4ba1dfa 100644
--- a/prov/psm3/psm3/ptl_ips/ips_scb.c
+++ b/prov/psm3/psm3/ptl_ips/ips_scb.c
@@ -281,6 +281,7 @@ ips_scb_t *MOCKABLE(ips_scbctrl_alloc)(struct ips_scbctrl *scbc, int scbnum, int
 #ifdef PSM_CUDA
 		scb->mq_req = NULL;
 #endif
+		scb->mr = NULL;
 
 		scbc->scb_num_cur--;
 		if (scbc->scb_num_cur < (scbc->scb_num >> 1))
@@ -305,6 +306,7 @@ void ips_scbctrl_free(ips_scb_t *scb)
 
 	ips_scb_buffer(scb) = NULL;
 	scb->tidsendc = NULL;
+	scb->mr = NULL;
 	scb->payload_size = 0;
 	scbc->scb_num_cur++;
 	if (SLIST_EMPTY(&scbc->scb_free)) {
@@ -338,6 +340,7 @@ ips_scb_t *MOCKABLE(ips_scbctrl_alloc_tiny)(struct ips_scbctrl *scbc)
 #ifdef PSM_CUDA
 	scb->mq_req = NULL;
 #endif
+	scb->mr = NULL;
 
 	scbc->scb_num_cur--;
 	if (scbc->scb_num_cur < (scbc->scb_num >> 1))
diff --git a/prov/psm3/psm3/ptl_ips/ips_scb.h b/prov/psm3/psm3/ptl_ips/ips_scb.h
index 8d1eb49..8446f73 100644
--- a/prov/psm3/psm3/ptl_ips/ips_scb.h
+++ b/prov/psm3/psm3/ptl_ips/ips_scb.h
@@ -155,8 +155,7 @@ struct ips_scb {
 	uint16_t tidctrl;
 	uint16_t frag_size;	/* max packet size in sequence */
 	uint16_t opcode;
-	uint16_t tsess_length;
-	uint32_t *tsess;
+	psm2_verbs_mr_t mr;
 	struct ips_flow *flow;
 	struct ips_tid_send_desc *tidsendc;
 
diff --git a/prov/psm3/psm3/ptl_ips/ips_tidcache.h b/prov/psm3/psm3/ptl_ips/ips_tidcache.h
index 20d45bf..6d31284 100644
--- a/prov/psm3/psm3/ptl_ips/ips_tidcache.h
+++ b/prov/psm3/psm3/ptl_ips/ips_tidcache.h
@@ -115,7 +115,7 @@ typedef struct {
 #define RBTREE_MI_PL  rbtree_tidcache_mapitem_pl_t
 #define RBTREE_MAP_PL rbtree_tidcache_map_pl_t
 
-#include "rbtree.h"
+#include "psm3_rbtree.h"
 
 /*
  * Macro definition for easy programming.
diff --git a/prov/psm3/psm3/ptl_ips/ips_tidflow.c b/prov/psm3/psm3/ptl_ips/ips_tidflow.c
index 9d671fd..61d97ad 100644
--- a/prov/psm3/psm3/ptl_ips/ips_tidflow.c
+++ b/prov/psm3/psm3/ptl_ips/ips_tidflow.c
@@ -148,7 +148,8 @@ psm2_error_t ips_tf_init(struct ips_protoexp *protoexp,
 					PSMI_STATSTYPE_TIDS,
 					entries,
 					PSMI_STATS_HOWMANY(entries),
-					protoexp->proto->ep->epid, tfc);
+					protoexp->proto->ep->epid, tfc,
+					protoexp->proto->ep->dev_name);
 #else
 	return PSM2_OK;
 #endif
diff --git a/prov/psm3/psm3/ptl_ips/ptl_rcvthread.c b/prov/psm3/psm3/ptl_ips/ptl_rcvthread.c
index 175b895..7d1d998 100644
--- a/prov/psm3/psm3/ptl_ips/ptl_rcvthread.c
+++ b/prov/psm3/psm3/ptl_ips/ptl_rcvthread.c
@@ -96,7 +96,7 @@ struct ptl_rcvthread {
          * stored to provide hints during a cuda failure
          * due to a null cuda context.
          */
-	CUcontext ctxt;
+	CUcontext cu_ctxt;
 #endif
 
 /*
@@ -124,7 +124,7 @@ psm2_error_t ips_ptl_rcvthread_init(ptl_t *ptl_gen, struct ips_recvhdrq *recvq)
 
 #ifdef PSM_CUDA
 	if (PSMI_IS_CUDA_ENABLED)
-		PSMI_CUDA_CALL(cuCtxGetCurrent, &ctxt);
+		PSMI_CUDA_CALL(cuCtxGetCurrent, &cu_ctxt);
 #endif
 
 	if (psmi_hal_has_sw_status(PSM_HAL_PSMI_RUNTIME_RTS_RX_THREAD) &&
@@ -151,11 +151,10 @@ psm2_error_t ips_ptl_rcvthread_init(ptl_t *ptl_gen, struct ips_recvhdrq *recvq)
 						strerror(errno));
 			goto fail;
 		}
-
+		if ((err = rcvthread_initstats(ptl_gen)))
+			goto fail;
 	}
 
-	if ((err = rcvthread_initstats(ptl_gen)))
-		goto fail;
 
 fail:
 	return err;
@@ -330,7 +329,7 @@ static void process_async_event(psm2_ep_t ep)
 
 	if (ibv_get_async_event(ep->verbs_ep.context, &async_event)) {
 		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-			"Receive thread ibv_get_async_event() error: %s", strerror(errno));
+			"Receive thread ibv_get_async_event() error on %s port %u: %s", ep->dev_name, ep->portnum, strerror(errno));
 	}
 	/* Ack the event */
 	ibv_ack_async_event(&async_event);
@@ -361,7 +360,7 @@ static void process_async_event(psm2_ep_t ep)
 	}
 	if (errstr)
 		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-			  "Fatal %s Async Event: %s", errstr,
+			  "Fatal %s Async Event on %s port %u: %s", errstr, ep->dev_name, ep->portnum,
 				ibv_event_type_str(async_event.event_type));
 }
 
@@ -373,8 +372,8 @@ static void rearm_cq_event(psm2_ep_t ep)
 	_HFI_VDBG("rcvthread got solicited event\n");
 	if (ibv_get_cq_event(ep->verbs_ep.recv_comp_channel, &ev_cq, &ev_ctx)) {
 		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-			  "Receive thread ibv_get_cq_event() error: %s",
-			  strerror(errno));
+			  "Receive thread ibv_get_cq_event() error on %s port %u: %s",
+			  ep->dev_name, ep->portnum, strerror(errno));
 	}
 
 	/* Ack the event */
@@ -388,8 +387,8 @@ static void rearm_cq_event(psm2_ep_t ep)
 	// are one-shots, that seems like overkill
 	if (ibv_req_notify_cq(ep->verbs_ep.recv_cq, 1)) {
 		psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-			  "Receive thread ibv_req_notify_cq() error: %s",
-			  strerror(errno));
+			  "Receive thread ibv_req_notify_cq() error on %s port %u: %s",
+			  ep->dev_name, ep->portnum, strerror(errno));
 	}
 }
 
@@ -409,7 +408,7 @@ static void poll_async_events(psm2_ep_t ep)
 		if (IPS_PROTOEXP_FLAG_KERNEL_QP(ep->rdmamode)
 		    && __psm2_rv_cq_overflowed(ep->verbs_ep.rv))
 			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
-				  "RV event ring overflow");
+				  "RV event ring overflow for %s port %u", ep->dev_name, ep->portnum);
 #endif
 		pfd[num_ep].fd = ep->verbs_ep.context->async_fd;
 		pfd[num_ep].events = POLLIN;
@@ -458,8 +457,8 @@ void *ips_ptl_pollintr(void *rcvthreadc)
 	psm2_error_t err;
 
 #ifdef PSM_CUDA
-	if (PSMI_IS_CUDA_ENABLED && ctxt != NULL)
-		PSMI_CUDA_CALL(cuCtxSetCurrent, ctxt);
+	if (PSMI_IS_CUDA_ENABLED && cu_ctxt != NULL)
+		PSMI_CUDA_CALL(cuCtxSetCurrent, cu_ctxt);
 #endif
 
 	PSM2_LOG_MSG("entering");
@@ -630,8 +629,10 @@ static psm2_error_t rcvthread_initstats(ptl_t *ptl_gen)
 		}
 	}
 
+	// one rcvThread per process, so omit id (ptl->ep->epid) and
+	// info (ptl->ep->dev_name)
 	return psmi_stats_register_type("RcvThread_statistics",
 					PSMI_STATSTYPE_RCVTHREAD,
 					entries,
-					PSMI_STATS_HOWMANY(entries), ptl->ep->epid, rcvc);
+					PSMI_STATS_HOWMANY(entries), 0, rcvc, NULL);
 }
diff --git a/prov/psm3/psm3/ptl_self/ptl.c b/prov/psm3/psm3/ptl_self/ptl.c
index 040e6d4..6d2fc2d 100644
--- a/prov/psm3/psm3/ptl_self/ptl.c
+++ b/prov/psm3/psm3/ptl_self/ptl.c
@@ -68,6 +68,11 @@ struct ptl_self {
 	ptl_ctl_t *ctl;
 } __attribute__((aligned(16)));
 
+/* not reported yet, so just track in a global so can pass a pointer to
+ * psmi_mq_handle_envelope and psmi_mq_handle_rts
+ */
+static struct ptl_strategy_stats strat_stats;
+
 static
 psm2_error_t
 ptl_handle_rtsmatch(psm2_mq_req_t recv_req, int was_posted)
@@ -79,12 +84,14 @@ ptl_handle_rtsmatch(psm2_mq_req_t recv_req, int was_posted)
 			       recv_req->req_data.recv_msglen);
 	}
 
+	recv_req->mq->stats.rx_user_num++;
+	recv_req->mq->stats.rx_user_bytes += recv_req->req_data.recv_msglen;
 	psmi_mq_handle_rts_complete(recv_req);
 
+	send_req->mq->stats.tx_rndv_bytes += send_req->req_data.send_msglen;
 	/* If the send is already marked complete, that's because it was internally
 	 * buffered. */
 	if (send_req->state == MQ_STATE_COMPLETE) {
-		psmi_mq_stats_rts_account(send_req);
 		if (send_req->req_data.buf != NULL && send_req->req_data.send_msglen > 0)
 			psmi_mq_sysbuf_free(send_req->mq, send_req->req_data.buf);
 		/* req was left "live" even though the sender was told that the
@@ -143,14 +150,17 @@ self_mq_isend(psm2_mq_t mq, psm2_epaddr_t epaddr, uint32_t flags_user,
 	    return PSM2_NO_MEMORY;
 
 #ifdef PSM_CUDA
-	if (PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(ubuf)) {
+	if (len && PSMI_IS_CUDA_ENABLED && PSMI_IS_CUDA_MEM(ubuf)) {
 		psmi_cuda_set_attr_sync_memops(ubuf);
 		send_req->is_buf_gpu_mem = 1;
 	} else
 		send_req->is_buf_gpu_mem = 0;
 #endif
 
-	rc = psmi_mq_handle_rts(mq, epaddr, tag,
+	mq->stats.tx_num++;
+	mq->stats.tx_rndv_num++;
+
+	rc = psmi_mq_handle_rts(mq, epaddr, tag, &strat_stats,
 				len, NULL, 0, 1,
 				ptl_handle_rtsmatch, &recv_req);
 	send_req->req_data.tag = *tag;
@@ -294,6 +304,7 @@ self_connect(ptl_t *ptl_gen,
 			continue;
 
 		if (array_of_epid[i] == ptl->epid) {
+			_HFI_CONNDBG("connect self\n");
 			array_of_epaddr[i] = ptl->epaddr;
 			array_of_epaddr[i]->ptlctl = ptl->ctl;
 			array_of_epaddr[i]->epid = ptl->epid;
@@ -328,6 +339,7 @@ self_disconnect(ptl_t *ptl_gen, int force, int numep,
 			continue;
 
 		if (array_of_epaddr[i] == ptl->epaddr) {
+			_HFI_CONNDBG("disconnect self\n");
 			psmi_epid_remove(ptl->ep, ptl->epid);
 			array_of_errors[i] = PSM2_OK;
 		}
diff --git a/prov/psm3/src/.gitignore b/prov/psm3/src/.gitignore
index 89260b1..9a799b2 100644
--- a/prov/psm3/src/.gitignore
+++ b/prov/psm3/src/.gitignore
@@ -1 +1,2 @@
 psm3_revision.c
+psm3_src_chksum.h
diff --git a/prov/psm3/src/psm3_revision.c.in b/prov/psm3/src/psm3_revision.c.in
index 74082cd..3229fa9 100644
--- a/prov/psm3/src/psm3_revision.c.in
+++ b/prov/psm3/src/psm3_revision.c.in
@@ -1,17 +1,20 @@
+#include "psmx3.h"
+#include "psm3_src_chksum.h"
+
 #ifndef PSMX3_IFS_VERSION
-#define PSMX3_IFS_VERSION	"@IFS_VERSION@"
+#define PSMX3_IFS_VERSION	"@PSM3_IFS_VERSION@"
 #endif
 
 #ifndef PSMX3_BUILD_TIMESTAMP
-#define PSMX3_BUILD_TIMESTAMP	"@BUILD_TIMESTAMP@"
+#define PSMX3_BUILD_TIMESTAMP	"@PSM3_BUILD_TIMESTAMP@"
 #endif
 
 #ifndef PSMX3_SRC_CHECKSUM
-#define PSMX3_SRC_CHECKSUM	"@SRC_CHECKSUM@"
+#define PSMX3_SRC_CHECKSUM	"@PSM3_SRC_CHECKSUM@"
 #endif
 
 #ifndef PSMX3_GIT_CHECKSUM
-#define PSMX3_GIT_CHECKSUM	"@GIT_HASH@"
+#define PSMX3_GIT_CHECKSUM	"@PSM3_GIT_HASH@"
 #endif
 
 char psmi_hfi_IFS_version[] = PSMX3_IFS_VERSION;
@@ -19,3 +22,18 @@ char psmi_hfi_build_timestamp[] = PSMX3_BUILD_TIMESTAMP;
 char psmi_hfi_sources_checksum[] = PSMX3_SRC_CHECKSUM;
 char psmi_hfi_git_checksum[] = PSMX3_GIT_CHECKSUM;
 
+#define PSM3_PROV_VER_MAJOR @PSM3_PROV_VER_MAJOR@
+#define PSM3_PROV_VER_MINOR @PSM3_PROV_VER_MINOR@
+#define PSM3_PROV_VER_MAINT @PSM3_PROV_VER_MAINT@
+#define PSM3_PROV_VER_PATCH @PSM3_PROV_VER_PATCH@
+
+/* Leave last digit open for special use */
+#define PSM3_PROV_VER(major, minor, maint, patch) \
+	( ( ( ((major) * 100) + (minor)) << 16)	| ( ( ((maint) * 1000) + ((patch) * 10)) & 0xFFFF) )
+
+
+static uint32_t psm3_provider_version =
+	PSM3_PROV_VER(PSM3_PROV_VER_MAJOR, PSM3_PROV_VER_MINOR, PSM3_PROV_VER_MAINT, PSM3_PROV_VER_PATCH);
+uint32_t get_psm3_provider_version() {
+	return psm3_provider_version;
+}
diff --git a/prov/psm3/src/psmx3.h b/prov/psm3/src/psmx3.h
index 95fb475..af04942 100644
--- a/prov/psm3/src/psmx3.h
+++ b/prov/psm3/src/psmx3.h
@@ -85,7 +85,6 @@ extern "C" {
 
 extern struct fi_provider psmx3_prov;
 
-
 #define PSMX3_OP_FLAGS	(FI_INJECT | FI_MULTI_RECV | FI_COMPLETION | \
 			 FI_TRIGGER | FI_INJECT_COMPLETE | \
 			 FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)
@@ -938,7 +937,6 @@ void	psmx3_alter_prov_info(uint32_t api_version, const struct fi_info *hints,
 			      struct fi_info *info);
 
 void	psmx3_init_tag_layout(struct fi_info *info);
-int	psmx3_get_round_robin_unit(int idx);
 
 int	psmx3_fabric(struct fi_fabric_attr *attr,
 		     struct fid_fabric **fabric, void *context);
diff --git a/prov/psm3/src/psmx3_attr.c b/prov/psm3/src/psmx3_attr.c
index 30c673e..f1d9a31 100644
--- a/prov/psm3/src/psmx3_attr.c
+++ b/prov/psm3/src/psmx3_attr.c
@@ -115,7 +115,6 @@ static struct fi_domain_attr psmx3_domain_attr = {
 
 static struct fi_fabric_attr psmx3_fabric_attr = {
 	.name			= PSMX3_FABRIC_NAME,
-	.prov_version		= OFI_VERSION_DEF_PROV,
 };
 
 static struct fi_info psmx3_prov_info = {
@@ -266,6 +265,33 @@ fail:
 #define psmx3_dupinfo fi_dupinfo
 #endif /* HAVE_PSM3_DL */
 
+#ifdef PSM_CUDA
+/* mimic parsing functionality of psmi_getenv */
+static long get_psm3_env(const char *var, int default_value) {
+	char *ep;
+	long val;
+	char *e = getenv(var);
+
+	if (!e || !*e)
+		return default_value; /* no value supplied */
+
+	val = strtol(e, &ep, 10);
+	if (!ep ||  *ep) { /* parse error - didn't consume all */
+		val = strtol(e, &ep, 16); /* try hex */
+		if (!ep ||  *ep)
+			return default_value;
+	}
+	return val;
+}
+#endif
+static uint64_t psmx3_check_fi_hmem_cap(void) {
+#ifdef PSM_CUDA
+	if (get_psm3_env("PSM3_CUDA", 0) || get_psm3_env("PSM3_GPUDIRECT", 0))
+		return FI_HMEM;
+#endif
+	return 0;
+}
+
 /*
  * Possible provider variations:
  *
@@ -300,6 +326,7 @@ int psmx3_init_prov_info(const struct fi_info *hints, struct fi_info **info)
 	int addr_format2 = FI_ADDR_STR;
 	int ep_type = FI_EP_RDM;
 	int ep_type2 = FI_EP_DGRAM;
+	uint64_t extra_caps = 0;
 
 	if (!hints)
 		goto alloc_info;
@@ -351,13 +378,22 @@ int psmx3_init_prov_info(const struct fi_info *hints, struct fi_info **info)
 			return -FI_ENODATA;
 	}
 
-	if ((hints->caps & PSMX3_CAPS) != hints->caps) {
+	/* Check if CUDA is enable */
+	extra_caps |= psmx3_check_fi_hmem_cap();
+
+	prov_info->caps |= extra_caps;
+	prov_info->tx_attr->caps |= extra_caps;
+	prov_info->rx_attr->caps |= extra_caps;
+	prov_info->domain_attr->caps |= extra_caps;
+
+	if ((hints->caps & prov_info->caps) != hints->caps) {
 		FI_INFO(&psmx3_prov, FI_LOG_CORE, "caps not supported\n");
 		FI_INFO_CHECK(&psmx3_prov, prov_info, hints, caps, FI_TYPE_CAPS);
 		return -FI_ENODATA;
 	}
 
 alloc_info:
+	psmx3_prov_info.fabric_attr->prov_version = get_psm3_provider_version();
 	info_out = NULL;
 	if (!hints || !(hints->caps & (FI_TAGGED | FI_MSG))) {
 		info_new = psmx3_dupinfo(&psmx3_prov_info);
@@ -365,11 +401,11 @@ alloc_info:
 			/* rma only, 64 bit CQ data */
 			info_new->addr_format = addr_format;
 			info_new->ep_attr->type = ep_type;
-			info_new->caps = PSMX3_RMA_CAPS;
+			info_new->caps = PSMX3_RMA_CAPS | extra_caps;
 			info_new->mode = 0;
-			info_new->tx_attr->caps = PSMX3_RMA_TX_CAPS;
+			info_new->tx_attr->caps = PSMX3_RMA_TX_CAPS | extra_caps;
 			info_new->tx_attr->mode = 0;
-			info_new->rx_attr->caps = PSMX3_RMA_RX_CAPS;
+			info_new->rx_attr->caps = PSMX3_RMA_RX_CAPS | extra_caps;
 			info_new->rx_attr->mode = 0;
 			info_new->domain_attr->cq_data_size = 8;
 			info_out = info_new;
diff --git a/prov/psm3/src/psmx3_fabric.c b/prov/psm3/src/psmx3_fabric.c
index 951aedc..77a17ab 100644
--- a/prov/psm3/src/psmx3_fabric.c
+++ b/prov/psm3/src/psmx3_fabric.c
@@ -78,7 +78,6 @@ static struct fi_ops_fabric psmx3_fabric_ops = {
 
 static struct fi_fabric_attr psmx3_fabric_attr = {
 	.name = PSMX3_FABRIC_NAME,
-	.prov_version = OFI_VERSION_DEF_PROV,
 };
 
 int psmx3_fabric(struct fi_fabric_attr *attr,
@@ -117,6 +116,7 @@ int psmx3_fabric(struct fi_fabric_attr *attr,
 		ofi_ns_start_server(&fabric_priv->name_server);
 	}
 
+	psmx3_fabric_attr.prov_version = get_psm3_provider_version();
 	ret = ofi_fabric_init(&psmx3_prov, &psmx3_fabric_attr, attr,
 			     &fabric_priv->util_fabric, context);
 	if (ret) {
diff --git a/prov/psm3/src/psmx3_init.c b/prov/psm3/src/psmx3_init.c
index b6f6154..aad27d2 100644
--- a/prov/psm3/src/psmx3_init.c
+++ b/prov/psm3/src/psmx3_init.c
@@ -51,7 +51,7 @@ struct psmx3_env psmx3_env = {
 	.conn_timeout	= 10,
 	.prog_interval	= -1,
 	.prog_affinity	= NULL,
-	.multi_ep	= 0,
+	.multi_ep	= 1,
 	.inject_size	= 64,
 	.lock_level	= 2,
 	.lazy_conn	= 0,
@@ -222,7 +222,7 @@ static int psmx3_check_multi_ep_cap(void)
 	uint64_t caps = PSM2_MULTI_EP_CAP;
 	char *s = getenv("PSM3_MULTI_EP");
 
-	if (psm2_get_capability_mask(caps) == caps && psmx3_get_yes_no(s, 0))
+	if (psm2_get_capability_mask(caps) == caps && psmx3_get_yes_no(s, 1))
 		psmx3_env.multi_ep = 1;
 	else
 		psmx3_env.multi_ep = 0;
@@ -420,13 +420,6 @@ static int psmx3_update_hfi_info(void)
 	return 0;
 }
 
-int psmx3_get_round_robin_unit(int idx)
-{
-	return psmx3_hfi_info.num_active_units ?
-			psmx3_hfi_info.active_units[idx % psmx3_hfi_info.num_active_units] :
-			-1;
-}
-
 static void psmx3_update_hfi_nic_info(struct fi_info *info)
 {
         char *path;
@@ -669,7 +662,6 @@ static void psmx3_fini(void)
 
 struct fi_provider psmx3_prov = {
 	.name = PSMX3_PROV_NAME,
-	.version = OFI_VERSION_DEF_PROV,
 	.fi_version = OFI_VERSION_LATEST,
 	.getinfo = psmx3_getinfo,
 	.fabric = psmx3_fabric,
@@ -678,9 +670,14 @@ struct fi_provider psmx3_prov = {
 
 PROVIDER_INI
 {
-	FI_INFO(&psmx3_prov, FI_LOG_CORE, "build options: HAVE_PSM3_SRC=%d, "
-			"PSMX3_USE_REQ_CONTEXT=%d\n", HAVE_PSM3_SRC,
-			PSMX3_USE_REQ_CONTEXT);
+	psmx3_prov.version = get_psm3_provider_version();
+
+	FI_INFO(&psmx3_prov, FI_LOG_CORE, "build options: VERSION=%u.%u=%u.%u.%u.%u, "
+			"HAVE_PSM3_SRC=%d, PSM3_CUDA=%d\n",
+			(psmx3_prov.version >> 16), (psmx3_prov.version & 0xFFFF),
+			(psmx3_prov.version >> 16) / 100, (psmx3_prov.version >> 16) % 100,
+			(psmx3_prov.version & 0xFFFF) / 1000, ((psmx3_prov.version & 0xFFFF) % 1000) / 10,
+			HAVE_PSM3_SRC, PSM3_CUDA);
 
 	fi_param_define(&psmx3_prov, "name_server", FI_PARAM_BOOL,
 			"Whether to turn on the name server or not "
diff --git a/prov/psm3/src/psmx3_trx_ctxt.c b/prov/psm3/src/psmx3_trx_ctxt.c
index 971e0dd..83dcee0 100644
--- a/prov/psm3/src/psmx3_trx_ctxt.c
+++ b/prov/psm3/src/psmx3_trx_ctxt.c
@@ -302,13 +302,6 @@ struct psmx3_trx_ctxt *psmx3_trx_ctxt_alloc(struct psmx3_fid_domain *domain,
 	FI_INFO(&psmx3_prov, FI_LOG_CORE,
 		"ep_open_opts: unit=%d port=%u\n", opts.unit, opts.port);
 
-	if (opts.unit < 0 && sep_ctxt_idx >= 0) {
-		should_retry = 1;
-		opts.unit = psmx3_get_round_robin_unit(sep_ctxt_idx);
-		FI_INFO(&psmx3_prov, FI_LOG_CORE,
-			"sep %d: ep_open_opts: unit=%d\n", sep_ctxt_idx, opts.unit);
-	}
-
 	err = psm2_ep_open(uuid, &opts,
 			   &trx_ctxt->psm2_ep, &trx_ctxt->psm2_epid);
 	if (err != PSM2_OK) {
diff --git a/prov/psm3/src/version.h b/prov/psm3/src/version.h
index 82f5313..ca094cd 100644
--- a/prov/psm3/src/version.h
+++ b/prov/psm3/src/version.h
@@ -169,5 +169,7 @@ struct psmx3_context {
 
 #endif /* !PSMX3_USE_REQ_CONTEXT */
 
+uint32_t get_psm3_provider_version();
+
 #endif
 
diff --git a/prov/rxd/src/rxd_av.c b/prov/rxd/src/rxd_av.c
index fb8e3b7..8d30a98 100644
--- a/prov/rxd/src/rxd_av.c
+++ b/prov/rxd/src/rxd_av.c
@@ -294,42 +294,21 @@ static int rxd_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr, size_t count
 			uint64_t flags)
 {
 	int ret = 0;
-	size_t i, addrlen;
+	size_t i;
 	fi_addr_t rxd_addr;
-	fi_addr_t dg_addr;
 	struct rxd_av *av;
-	uint8_t addr[RXD_NAME_LENGTH];
 
 	av = container_of(av_fid, struct rxd_av, util_av.av_fid);
 	fastlock_acquire(&av->util_av.lock);
 	for (i = 0; i < count; i++) {
-
-		addrlen = RXD_NAME_LENGTH;
 		rxd_addr = (intptr_t)ofi_idx_lookup(&av->fi_addr_idx,
 						    RXD_IDX_OFFSET(fi_addr[i]));
 		if (!rxd_addr)
 			goto err;
 
-		dg_addr = (intptr_t)ofi_idx_lookup(&av->rxdaddr_dg_idx, rxd_addr);
-
-		ret = fi_av_lookup(av->dg_av, dg_addr, addr, &addrlen);
-		if (ret)
-			goto err;
-
-		ret = ofi_rbmap_find_delete(&av->rbmap, (void *) addr);
-		if (ret)
-			goto err;
-
-		ret = fi_av_remove(av->dg_av, &dg_addr, 1, flags);
-
-		if (ret)
-			goto err;
-
 		ofi_idx_remove_ordered(&(av->fi_addr_idx),
 				       RXD_IDX_OFFSET(fi_addr[i]));
-		ofi_idx_remove_ordered(&(av->rxdaddr_dg_idx), rxd_addr);
 		ofi_idm_clear(&(av->rxdaddr_fi_idm), rxd_addr);
-		av->dg_av_used--;
 	}
 
 err:
@@ -375,16 +354,33 @@ static struct fi_ops_av rxd_av_ops = {
 static int rxd_av_close(struct fid *fid)
 {
 	struct rxd_av *av;
+	struct ofi_rbnode *node;
+	fi_addr_t dg_addr, rxd_addr;
 	int ret;
 
-
 	av = container_of(fid, struct rxd_av, util_av.av_fid);
-	ret = fi_close(&av->dg_av->fid);
+
+	ret = ofi_av_close(&av->util_av);
 	if (ret)
 		return ret;
 
+	while ((node = ofi_rbmap_get_root(&av->rbmap))) {
+		rxd_addr = (fi_addr_t) node->data;
+		dg_addr = (intptr_t)ofi_idx_lookup(&av->rxdaddr_dg_idx,
+						   rxd_addr);
+
+		ret = fi_av_remove(av->dg_av, &dg_addr, 1, 0);
+		if (ret)
+			FI_WARN(&rxd_prov, FI_LOG_AV,
+				"failed to remove dg addr: %d (%s)\n",
+				-ret, fi_strerror(-ret));
+
+		ofi_idx_remove_ordered(&(av->rxdaddr_dg_idx), rxd_addr);
+		ofi_rbmap_delete(&av->rbmap, node);
+	}
 	ofi_rbmap_cleanup(&av->rbmap);
-	ret = ofi_av_close(&av->util_av);
+
+	ret = fi_close(&av->dg_av->fid);
 	if (ret)
 		return ret;
 
diff --git a/prov/rxd/src/rxd_ep.c b/prov/rxd/src/rxd_ep.c
index 548e264..ca06cfa 100644
--- a/prov/rxd/src/rxd_ep.c
+++ b/prov/rxd/src/rxd_ep.c
@@ -613,19 +613,19 @@ static void rxd_close_peer(struct rxd_ep *ep, struct rxd_peer *peer)
 		peer->unacked_cnt--;
 	}
 
-	while(!dlist_empty(&peer->tx_list)) {
+	while (!dlist_empty(&peer->tx_list)) {
 		dlist_pop_front(&peer->tx_list, struct rxd_x_entry,
 				x_entry, entry);
 		rxd_tx_entry_free(ep, x_entry);
 	}
 
-	while(!dlist_empty(&peer->rx_list)) {
+	while (!dlist_empty(&peer->rx_list)) {
 		dlist_pop_front(&peer->rx_list, struct rxd_x_entry,
 				x_entry, entry);
 		rxd_rx_entry_free(ep, x_entry);
 	}
 
-	while(!dlist_empty(&peer->rma_rx_list)) {
+	while (!dlist_empty(&peer->rma_rx_list)) {
 		dlist_pop_front(&peer->rma_rx_list, struct rxd_x_entry,
 				x_entry, entry);
 		rxd_tx_entry_free(ep, x_entry);
@@ -670,6 +670,9 @@ static int rxd_ep_close(struct fid *fid)
 
 	dlist_foreach_container(&ep->active_peers, struct rxd_peer, peer, entry)
 		rxd_close_peer(ep, peer);
+	dlist_foreach_container(&ep->rts_sent_list, struct rxd_peer, peer, entry)
+		rxd_close_peer(ep, peer);
+	ofi_idm_reset(&(ep->peers_idm), free);
 
 	ret = fi_close(&ep->dg_ep->fid);
 	if (ret)
@@ -696,7 +699,6 @@ static int rxd_ep_close(struct fid *fid)
 		ofi_buf_free(pkt_entry);
 	}
 
-	ofi_idm_reset(&(ep->peers_idm), free);
 	rxd_ep_free_res(ep);
 	ofi_endpoint_close(&ep->util_ep);
 	free(ep);
diff --git a/prov/rxm/src/rxm.h b/prov/rxm/src/rxm.h
index 8a20882..440d451 100644
--- a/prov/rxm/src/rxm.h
+++ b/prov/rxm/src/rxm.h
@@ -64,8 +64,39 @@
 #define RXM_OP_VERSION		3
 #define RXM_CTRL_VERSION	4
 
-extern size_t rxm_eager_limit;
+enum {
+	RXM_REJECT_UNSPEC,
+	RXM_REJECT_ECONNREFUSED,
+	RXM_REJECT_EALREADY,
+};
+
+union rxm_cm_data {
+	struct _connect {
+		uint8_t version;
+		uint8_t endianness;
+		uint8_t ctrl_version;
+		uint8_t op_version;
+		uint16_t port;
+		uint8_t padding[2];
+		uint32_t eager_limit;
+		uint32_t rx_size; /* used? */
+		uint64_t client_conn_id;
+	} connect;
+
+	struct _accept {
+		uint64_t server_conn_id;
+		uint32_t rx_size; /* used? */
+	} accept;
+
+	struct _reject {
+		uint8_t version;
+		uint8_t reason;
+	} reject;
+};
+
+
 extern size_t rxm_buffer_size;
+extern size_t rxm_packet_size;
 
 #define RXM_SAR_TX_ERROR	UINT64_MAX
 #define RXM_SAR_RX_INIT		UINT64_MAX
@@ -125,6 +156,13 @@ extern struct util_prov rxm_util_prov;
 extern struct fi_ops_rma rxm_ops_rma;
 extern struct fi_ops_atomic rxm_ops_atomic;
 
+enum {
+	RXM_MSG_RXTX_SIZE = 128,
+	RXM_MSG_SRX_SIZE = 4096,
+	RXM_RX_SIZE = 65536,
+	RXM_TX_SIZE = 16384,
+};
+
 extern size_t rxm_msg_tx_size;
 extern size_t rxm_msg_rx_size;
 extern size_t rxm_cm_progress_interval;
@@ -134,141 +172,63 @@ extern int rxm_use_write_rndv;
 extern enum fi_wait_obj def_wait_obj, def_tcp_wait_obj;
 
 struct rxm_ep;
+struct rxm_av;
 
 
-/*
- * Connection Map
- */
-
-#define RXM_CMAP_IDX_BITS OFI_IDX_INDEX_BITS
-
-enum rxm_cmap_signal {
-	RXM_CMAP_UNSPEC,
-	RXM_CMAP_FREE,
-	RXM_CMAP_EXIT,
+enum rxm_cm_state {
+	RXM_CM_IDLE,
+	RXM_CM_CONNECTING,
+	RXM_CM_ACCEPTING,
+	RXM_CM_CONNECTED,
 };
 
-#define RXM_CM_STATES(FUNC)		\
-	FUNC(RXM_CMAP_IDLE),		\
-	FUNC(RXM_CMAP_CONNREQ_SENT),	\
-	FUNC(RXM_CMAP_CONNREQ_RECV),	\
-	FUNC(RXM_CMAP_CONNECTED),	\
-	FUNC(RXM_CMAP_SHUTDOWN),	\
-
-enum rxm_cmap_state {
-	RXM_CM_STATES(OFI_ENUM_VAL)
+enum {
+	RXM_CONN_INDEXED = BIT(0),
 };
 
-extern char *rxm_cm_state_str[];
-
-#define RXM_CM_UPDATE_STATE(handle, new_state)				\
-	do {								\
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "[CM] handle: "	\
-		       "%p %s -> %s\n",	handle,				\
-		       rxm_cm_state_str[handle->state],			\
-		       rxm_cm_state_str[new_state]);			\
-		handle->state = new_state;				\
-	} while (0)
-
-struct rxm_cmap_handle {
-	struct rxm_cmap *cmap;
-	enum rxm_cmap_state state;
-	/* Unique identifier for a connection. Can be exchanged with a peer
-	 * during connection setup and can later be used in a message header
-	 * to identify the source of the message (Used for FI_SOURCE, RNDV
-	 * protocol, etc.) */
-	uint64_t key;
-	uint64_t remote_key;
+/* There will be at most 1 peer address per AV entry.  There
+ * may be addresses that have not been inserted into the local
+ * AV, and have no matching entry.  This can occur if we are
+ * only receiving data from the remote rxm ep.
+ */
+struct rxm_peer_addr {
+	struct rxm_av *av;
 	fi_addr_t fi_addr;
-	struct rxm_cmap_peer *peer;
-};
-
-struct rxm_cmap_peer {
-	struct rxm_cmap_handle *handle;
-	struct dlist_entry entry;
-	uint8_t addr[];
+	struct ofi_rbnode *node;
+	int index;
+	int refcnt;
+	union ofi_sock_ip addr;
 };
 
-struct rxm_cmap_attr {
-	void 				*name;
-};
-
-struct rxm_cmap {
-	struct rxm_ep		*ep;
-	struct util_av		*av;
-
-	/* cmap handles that correspond to addresses in AV */
-	struct rxm_cmap_handle **handles_av;
-	size_t			num_allocated;
+struct rxm_peer_addr *rxm_get_peer(struct rxm_av *av, const void *addr);
+void rxm_put_peer(struct rxm_peer_addr *peer);
 
-	/* Store all cmap handles (inclusive of handles_av) in an indexer.
-	 * This allows reverse lookup of the handle using the index. */
-	struct indexer		handles_idx;
-
-	struct ofi_key_idx	key_idx;
-
-	struct dlist_entry	peer_list;
-	struct rxm_cmap_attr	attr;
-	pthread_t		cm_thread;
-	ofi_fastlock_acquire_t	acquire;
-	ofi_fastlock_release_t	release;
-	fastlock_t		lock;
-};
-
-enum rxm_cmap_reject_reason {
-	RXM_CMAP_REJECT_UNSPEC,
-	RXM_CMAP_REJECT_GENUINE,
-	RXM_CMAP_REJECT_SIMULT_CONN,
-};
-
-union rxm_cm_data {
-	struct _connect {
-		uint8_t version;
-		uint8_t endianness;
-		uint8_t ctrl_version;
-		uint8_t op_version;
-		uint16_t port;
-		uint8_t padding[2];
-		uint32_t eager_size;
-		uint32_t rx_size;
-		uint64_t client_conn_id;
-	} connect;
+/* Each local rxm ep will have at most 1 connection to a single
+ * remote rxm ep.  A local rxm ep may not be connected to all
+ * remote rxm ep's.
+ */
+struct rxm_conn {
+	enum rxm_cm_state state;
+	struct rxm_peer_addr *peer;
+	struct fid_ep *msg_ep;
+	struct rxm_ep *ep;
 
-	struct _accept {
-		uint64_t server_conn_id;
-		uint32_t rx_size;
-	} accept;
+	/* Prior versions of libfabric did not guarantee that all connections
+	 * from the same peer would have the same conn_id.  For compatibility
+	 * we need to store the remote_index per connection, rather than with
+	 * the peer_addr.
+	 */
+	int remote_index;
+	uint8_t flags;
 
-	struct _reject {
-		uint8_t version;
-		uint8_t reason;
-	} reject;
+	struct dlist_entry deferred_entry;
+	struct dlist_entry deferred_tx_queue;
+	struct dlist_entry deferred_sar_msgs;
+	struct dlist_entry deferred_sar_segments;
+	struct dlist_entry loopback_entry;
 };
 
-int rxm_cmap_alloc_handle(struct rxm_cmap *cmap, fi_addr_t fi_addr,
-			  enum rxm_cmap_state state,
-			  struct rxm_cmap_handle **handle);
-struct rxm_cmap_handle *rxm_cmap_key2handle(struct rxm_cmap *cmap, uint64_t key);
-int rxm_cmap_update(struct rxm_cmap *cmap, const void *addr, fi_addr_t fi_addr);
-
-void rxm_cmap_process_reject(struct rxm_cmap *cmap,
-			     struct rxm_cmap_handle *handle,
-			     enum rxm_cmap_reject_reason cm_reject_reason);
-void rxm_cmap_process_shutdown(struct rxm_cmap *cmap,
-			       struct rxm_cmap_handle *handle);
-int rxm_cmap_connect(struct rxm_ep *rxm_ep, fi_addr_t fi_addr,
-		     struct rxm_cmap_handle *handle);
-void rxm_cmap_free(struct rxm_cmap *cmap);
-int rxm_cmap_alloc(struct rxm_ep *rxm_ep, struct rxm_cmap_attr *attr);
-int rxm_cmap_remove(struct rxm_cmap *cmap, int index);
-int rxm_msg_eq_progress(struct rxm_ep *rxm_ep);
-
-static inline struct rxm_cmap_handle *
-rxm_cmap_acquire_handle(struct rxm_cmap *cmap, fi_addr_t fi_addr)
-{
-	assert(fi_addr < cmap->num_allocated);
-	return cmap->handles_av[fi_addr];
-}
+void rxm_freeall_conns(struct rxm_ep *ep);
 
 struct rxm_fabric {
 	struct util_fabric util_fabric;
@@ -287,8 +247,33 @@ struct rxm_domain {
 	fastlock_t amo_bufpool_lock;
 };
 
+/* All peer addresses, whether they've been inserted into the AV
+ * or an endpoint has an active connection to it, are stored in
+ * the addr_map.  Peers are allocated from a buffer pool and
+ * assigned a local index using the pool.  All rxm endpoints
+ * maintain a connection array which is aligned with the peer_pool.
+ *
+ * We technically only need to store the index of each peer in
+ * the AV itself.  The 'util_av' could basically be replaced by
+ * an ofi_index_map.  However, too much of the existing code
+ * relies on the util_av existing and storing the AV addresses.
+ *
+ * A future cleanup would be to remove using the util_av and have the
+ * rxm_av implementation be independent.
+ */
+ struct rxm_av {
+	struct util_av util_av;
+	struct ofi_rbmap addr_map;
+	struct ofi_bufpool *peer_pool;
+	struct ofi_bufpool *conn_pool;
+};
+
 int rxm_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
-		struct fid_av **av, void *context);
+		struct fid_av **fid_av, void *context);
+size_t rxm_av_max_peers(struct rxm_av *av);
+void rxm_ref_peer(struct rxm_peer_addr *peer);
+struct rxm_conn *rxm_av_alloc_conn(struct rxm_av *av);
+void rxm_av_free_conn(struct rxm_conn *conn);
 
 struct rxm_mr {
 	struct fid_mr mr_fid;
@@ -361,14 +346,14 @@ struct rxm_atomic_resp_hdr {
 	FUNC(RXM_RNDV_WRITE_DATA_WAIT),	\
 	FUNC(RXM_RNDV_WRITE_DONE_WAIT),	\
 	FUNC(RXM_RNDV_READ),		\
-	FUNC(RXM_RNDV_WRITE),		\
+	FUNC(RXM_RNDV_WRITE), /* not used */ \
 	FUNC(RXM_RNDV_READ_DONE_SENT),	\
 	FUNC(RXM_RNDV_READ_DONE_RECVD),	\
 	FUNC(RXM_RNDV_WRITE_DATA_SENT),	\
-	FUNC(RXM_RNDV_WRITE_DATA_RECVD),\
+	FUNC(RXM_RNDV_WRITE_DATA_RECVD), /* not used */ \
 	FUNC(RXM_RNDV_WRITE_DONE_SENT),	\
 	FUNC(RXM_RNDV_WRITE_DONE_RECVD),\
-	FUNC(RXM_RNDV_FINISH),		\
+	FUNC(RXM_RNDV_FINISH), /* not needed */	\
 	FUNC(RXM_ATOMIC_RESP_WAIT),	\
 	FUNC(RXM_ATOMIC_RESP_SENT)
 
@@ -438,24 +423,6 @@ struct rxm_iov {
 	uint8_t count;
 };
 
-enum rxm_buf_pool_type {
-	RXM_BUF_POOL_RX		= 0,
-	RXM_BUF_POOL_START	= RXM_BUF_POOL_RX,
-	RXM_BUF_POOL_TX,
-	RXM_BUF_POOL_TX_START	= RXM_BUF_POOL_TX,
-	RXM_BUF_POOL_TX_INJECT,
-	RXM_BUF_POOL_TX_RNDV_RD_DONE,
-	RXM_BUF_POOL_TX_RNDV_WR_DONE,
-	RXM_BUF_POOL_TX_RNDV_REQ,
-	RXM_BUF_POOL_TX_RNDV_WR_DATA,
-	RXM_BUF_POOL_TX_ATOMIC,
-	RXM_BUF_POOL_TX_CREDIT,
-	RXM_BUF_POOL_TX_SAR,
-	RXM_BUF_POOL_TX_END	= RXM_BUF_POOL_TX_SAR,
-	RXM_BUF_POOL_RMA,
-	RXM_BUF_POOL_MAX,
-};
-
 struct rxm_buf {
 	/* Must stay at top */
 	struct fi_context fi_context;
@@ -487,48 +454,27 @@ struct rxm_rx_buf {
 	size_t rndv_rma_index;
 	struct fid_mr *mr[RXM_IOV_LIMIT];
 
+	/* Only differs from pkt.data for unexpected messages */
+	void *data;
 	/* Must stay at bottom */
 	struct rxm_pkt pkt;
 };
 
-struct rxm_tx_base_buf {
-	/* Must stay at top */
-	struct rxm_buf hdr;
-
-	/* Must stay at bottom */
-	struct rxm_pkt pkt;
-};
-
-struct rxm_tx_eager_buf {
+struct rxm_tx_buf {
 	/* Must stay at top */
 	struct rxm_buf hdr;
 
+	OFI_DBG_VAR(bool, user_tx)
 	void *app_context;
 	uint64_t flags;
 
-	/* Must stay at bottom */
-	struct rxm_pkt pkt;
-};
-
-struct rxm_tx_sar_buf {
-	/* Must stay at top */
-	struct rxm_buf hdr;
-
-	void *app_context;
-	uint64_t flags;
-
-	/* Must stay at bottom */
-	struct rxm_pkt pkt;
-};
-
-struct rxm_tx_rndv_buf {
-	/* Must stay at top */
-	struct rxm_buf hdr;
-
-	void *app_context;
-	uint64_t flags;
-	struct fid_mr *mr[RXM_IOV_LIMIT];
-	uint8_t count;
+	union {
+		struct {
+			struct fid_mr *mr[RXM_IOV_LIMIT];
+			uint8_t count;
+		} rma;
+		struct rxm_iov atomic_result;
+	};
 
 	struct {
 		struct iovec iov[RXM_IOV_LIMIT];
@@ -536,7 +482,7 @@ struct rxm_tx_rndv_buf {
 		struct rxm_conn *conn;
 		size_t rndv_rma_index;
 		size_t rndv_rma_count;
-		struct rxm_tx_base_buf *done_buf;
+		struct rxm_tx_buf *done_buf;
 		struct rxm_rndv_hdr remote_hdr;
 	} write_rndv;
 
@@ -544,32 +490,9 @@ struct rxm_tx_rndv_buf {
 	struct rxm_pkt pkt;
 };
 
-struct rxm_rma_buf {
-	/* Must stay at top */
-	struct rxm_buf hdr;
-
-	void *app_context;
-	uint64_t flags;
-
-	struct {
-		struct fid_mr *mr[RXM_IOV_LIMIT];
-		uint8_t count;
-	} mr;
-	/* Must stay at bottom */
-	struct rxm_pkt pkt;
-};
-
-struct rxm_tx_atomic_buf {
-	/* Must stay at top */
-	struct rxm_buf hdr;
-
-	void *app_context;
-	uint64_t flags;
-	struct rxm_iov result_iov;
-
-	/* Must stay at bottom */
-	struct rxm_pkt pkt;
-};
+/* Used for application transmits, provides credit check */
+struct rxm_tx_buf *rxm_get_tx_buf(struct rxm_ep *ep);
+void rxm_free_rx_buf(struct rxm_ep *ep, struct rxm_tx_buf *buf);
 
 enum rxm_deferred_tx_entry_type {
 	RXM_DEFERRED_TX_RNDV_ACK,
@@ -593,7 +516,7 @@ struct rxm_deferred_tx_entry {
 			size_t pkt_size;
 		} rndv_ack;
 		struct {
-			struct rxm_tx_rndv_buf *tx_buf;
+			struct rxm_tx_buf *tx_buf;
 		} rndv_done;
 		struct {
 			struct rxm_rx_buf *rx_buf;
@@ -601,12 +524,12 @@ struct rxm_deferred_tx_entry {
 			struct rxm_iov rxm_iov;
 		} rndv_read;
 		struct {
-			struct rxm_tx_rndv_buf *tx_buf;
+			struct rxm_tx_buf *tx_buf;
 			struct fi_rma_iov rma_iov;
 			struct rxm_iov rxm_iov;
 		} rndv_write;
 		struct {
-			struct rxm_tx_sar_buf *cur_seg_tx_buf;
+			struct rxm_tx_buf *cur_seg_tx_buf;
 			struct {
 				struct iovec iov[RXM_IOV_LIMIT];
 				uint8_t count;
@@ -626,11 +549,11 @@ struct rxm_deferred_tx_entry {
 			uint64_t device;
 		} sar_seg;
 		struct {
-			struct rxm_tx_atomic_buf *tx_buf;
+			struct rxm_tx_buf *tx_buf;
 			ssize_t len;
 		} atomic_resp;
 		struct {
-			struct rxm_tx_base_buf *tx_buf;
+			struct rxm_tx_buf *tx_buf;
 		} credit_msg;
 	};
 };
@@ -657,7 +580,7 @@ struct rxm_recv_entry {
 	/* Used for Rendezvous protocol */
 	struct {
 		/* This is used to send RNDV ACK */
-		struct rxm_tx_base_buf *tx_buf;
+		struct rxm_tx_buf *tx_buf;
 	} rndv;
 };
 OFI_DECLARE_FREESTACK(struct rxm_recv_entry, rxm_recv_fs);
@@ -679,33 +602,12 @@ struct rxm_recv_queue {
 	dlist_func_t		*match_unexp;
 };
 
-struct rxm_buf_pool {
-	enum rxm_buf_pool_type type;
-	struct ofi_bufpool *pool;
-	struct rxm_ep *rxm_ep;
-};
-
-struct rxm_msg_eq_entry {
-	ssize_t			rd;
-	uint32_t		event;
-	/* Used for connection refusal */
-	void			*context;
-	struct fi_eq_err_entry	err_entry;
-	/* must stay at the bottom */
-	struct fi_eq_cm_entry	cm_entry;
-};
-
-#define RXM_MSG_EQ_ENTRY_SZ (sizeof(struct rxm_msg_eq_entry) + \
-			     sizeof(union rxm_cm_data))
-#define RXM_CM_ENTRY_SZ (sizeof(struct fi_eq_cm_entry) + \
-			 sizeof(union rxm_cm_data))
-
-ssize_t rxm_get_dyn_rbuf(struct fi_cq_data_entry *entry, struct iovec *iov,
+ssize_t rxm_get_dyn_rbuf(struct ofi_cq_rbuf_entry *entry, struct iovec *iov,
 			 size_t *count);
 
 struct rxm_eager_ops {
 	void (*comp_tx)(struct rxm_ep *rxm_ep,
-			struct rxm_tx_eager_buf *tx_eager_buf);
+			struct rxm_tx_buf *tx_eager_buf);
 	void (*handle_rx)(struct rxm_rx_buf *rx_buf);
 };
 
@@ -726,14 +628,19 @@ struct rxm_ep {
 	struct util_ep 		util_ep;
 	struct fi_info 		*rxm_info;
 	struct fi_info 		*msg_info;
-	struct rxm_cmap		*cmap;
+
+	struct index_map	conn_idx_map;
+	struct dlist_entry	loopback_list;
+	union ofi_sock_ip	addr;
+
+	pthread_t		cm_thread;
 	struct fid_pep 		*msg_pep;
 	struct fid_eq 		*msg_eq;
+	struct fid_ep 		*srx_ctx;
+
 	struct fid_cq 		*msg_cq;
 	uint64_t		msg_cq_last_poll;
-	struct fid_ep 		*srx_ctx;
 	size_t 			comp_per_progress;
-	ofi_atomic32_t		atomic_tx_credits;
 	int			cq_eq_fairness;
 
 	bool			msg_mr_local;
@@ -745,12 +652,16 @@ struct rxm_ep {
 	size_t			buffered_min;
 	size_t			buffered_limit;
 	size_t			inject_limit;
+
+	size_t			eager_limit;
 	size_t			sar_limit;
+	size_t			tx_credit;
 
-	struct rxm_buf_pool	*buf_pools;
+	struct ofi_bufpool	*rx_pool;
+	struct ofi_bufpool	*tx_pool;
+	struct rxm_pkt		*inject_pkt;
 
-	struct dlist_entry	repost_ready_list;
-	struct dlist_entry	deferred_tx_conn_queue;
+	struct dlist_entry	deferred_queue;
 	struct dlist_entry	rndv_wait_list;
 
 	struct rxm_recv_queue	recv_queue;
@@ -761,25 +672,10 @@ struct rxm_ep {
 	struct rxm_rndv_ops	*rndv_ops;
 };
 
-struct rxm_conn {
-	/* This should stay at the top */
-	struct rxm_cmap_handle handle;
-
-	struct fid_ep *msg_ep;
+int rxm_start_listen(struct rxm_ep *ep);
+void rxm_stop_listen(struct rxm_ep *ep);
+void rxm_conn_progress(struct rxm_ep *ep);
 
-	/* This is used only in non-FI_THREAD_SAFE case */
-	struct rxm_pkt *inject_pkt;
-	struct rxm_pkt *inject_data_pkt;
-	struct rxm_pkt *tinject_pkt;
-	struct rxm_pkt *tinject_data_pkt;
-
-	struct dlist_entry deferred_conn_entry;
-	struct dlist_entry deferred_tx_queue;
-	struct dlist_entry sar_rx_msg_list;
-	struct dlist_entry sar_deferred_rx_msg_list;
-
-	uint32_t rndv_tx_credits;
-};
 
 extern struct fi_provider rxm_prov;
 extern struct fi_fabric_attr rxm_fabric_attr;
@@ -804,7 +700,6 @@ ssize_t rxm_handle_rx_buf(struct rxm_rx_buf *rx_buf);
 int rxm_endpoint(struct fid_domain *domain, struct fi_info *info,
 			  struct fid_ep **ep, void *context);
 
-int rxm_conn_cmap_alloc(struct rxm_ep *rxm_ep);
 void rxm_cq_write_error(struct util_cq *cq, struct util_cntr *cntr,
 			void *op_context, int err);
 void rxm_cq_write_error_all(struct rxm_ep *rxm_ep, int err);
@@ -817,9 +712,9 @@ void rxm_ep_do_progress(struct util_ep *util_ep);
 void rxm_handle_eager(struct rxm_rx_buf *rx_buf);
 void rxm_handle_coll_eager(struct rxm_rx_buf *rx_buf);
 void rxm_finish_eager_send(struct rxm_ep *rxm_ep,
-			   struct rxm_tx_eager_buf *tx_eager_buf);
+			   struct rxm_tx_buf *tx_eager_buf);
 void rxm_finish_coll_eager_send(struct rxm_ep *rxm_ep,
-				struct rxm_tx_eager_buf *tx_eager_buf);
+				struct rxm_tx_buf *tx_eager_buf);
 
 int rxm_prepost_recv(struct rxm_ep *rxm_ep, struct fid_ep *rx_ep);
 
@@ -835,13 +730,13 @@ void rxm_rndv_hdr_init(struct rxm_ep *rxm_ep, void *buf,
 
 static inline size_t rxm_ep_max_atomic_size(struct fi_info *info)
 {
-	assert(rxm_eager_limit >= sizeof(struct rxm_atomic_hdr));
-	return rxm_eager_limit - sizeof(struct rxm_atomic_hdr);
+	assert(rxm_buffer_size >= sizeof(struct rxm_atomic_hdr));
+	return rxm_buffer_size - sizeof(struct rxm_atomic_hdr);
 }
 
 static inline ssize_t
 rxm_atomic_send_respmsg(struct rxm_ep *rxm_ep, struct rxm_conn *conn,
-			struct rxm_tx_atomic_buf *resp_buf, ssize_t len)
+			struct rxm_tx_buf *resp_buf, ssize_t len)
 {
 	struct iovec iov = {
 		.iov_base = (void *) &resp_buf->pkt,
@@ -857,17 +752,6 @@ rxm_atomic_send_respmsg(struct rxm_ep *rxm_ep, struct rxm_conn *conn,
 	return fi_sendmsg(conn->msg_ep, &msg, FI_COMPLETION);
 }
 
-static inline int rxm_needs_atomic_progress(const struct fi_info *info)
-{
-	return (info->caps & FI_ATOMIC) && info->domain_attr &&
-			info->domain_attr->data_progress == FI_PROGRESS_AUTO;
-}
-
-static inline struct rxm_conn *rxm_key2conn(struct rxm_ep *rxm_ep, uint64_t key)
-{
-	return (struct rxm_conn *)rxm_cmap_key2handle(rxm_ep->cmap, key);
-}
-
 void rxm_ep_progress_deferred_queue(struct rxm_ep *rxm_ep,
 				    struct rxm_conn *rxm_conn);
 
@@ -876,29 +760,32 @@ rxm_ep_alloc_deferred_tx_entry(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 			       enum rxm_deferred_tx_entry_type type);
 
 static inline void
-rxm_ep_enqueue_deferred_tx_queue(struct rxm_deferred_tx_entry *tx_entry)
+rxm_queue_deferred_tx(struct rxm_deferred_tx_entry *tx_entry,
+		      enum ofi_list_end list_end)
 {
-	if (dlist_empty(&tx_entry->rxm_conn->deferred_tx_queue))
-		dlist_insert_tail(&tx_entry->rxm_conn->deferred_conn_entry,
-				  &tx_entry->rxm_ep->deferred_tx_conn_queue);
-	dlist_insert_tail(&tx_entry->entry, &tx_entry->rxm_conn->deferred_tx_queue);
+	struct rxm_conn *conn = tx_entry->rxm_conn;
+
+	if (dlist_empty(&conn->deferred_tx_queue))
+		dlist_insert_tail(&conn->deferred_entry,
+				  &conn->ep->deferred_queue);
+	if (list_end == OFI_LIST_HEAD) {
+		dlist_insert_head(&tx_entry->entry,
+				  &conn->deferred_tx_queue);
+	} else  {
+		dlist_insert_tail(&tx_entry->entry,
+				  &conn->deferred_tx_queue);
+	}
 }
 
 static inline void
-rxm_ep_enqueue_deferred_tx_queue_priority(struct rxm_deferred_tx_entry *tx_entry)
+rxm_dequeue_deferred_tx(struct rxm_deferred_tx_entry *tx_entry)
 {
-	if (dlist_empty(&tx_entry->rxm_conn->deferred_tx_queue))
-		dlist_insert_head(&tx_entry->rxm_conn->deferred_conn_entry,
-				  &tx_entry->rxm_ep->deferred_tx_conn_queue);
-	dlist_insert_head(&tx_entry->entry, &tx_entry->rxm_conn->deferred_tx_queue);
-}
+	struct rxm_conn *conn = tx_entry->rxm_conn;
 
-static inline void
-rxm_ep_dequeue_deferred_tx_queue(struct rxm_deferred_tx_entry *tx_entry)
-{
-	dlist_remove_init(&tx_entry->entry);
-	if (dlist_empty(&tx_entry->rxm_conn->deferred_tx_queue))
-		dlist_remove(&tx_entry->rxm_conn->deferred_conn_entry);
+	assert(!dlist_empty(&conn->deferred_tx_queue));
+	dlist_remove(&tx_entry->entry);
+	if (dlist_empty(&conn->deferred_tx_queue))
+		dlist_remove_init(&conn->deferred_entry);
 }
 
 int rxm_conn_process_eq_events(struct rxm_ep *rxm_ep);
@@ -959,7 +846,7 @@ rxm_ep_format_tx_buf_pkt(struct rxm_conn *rxm_conn, size_t len, uint8_t op,
 			 uint64_t data, uint64_t tag, uint64_t flags,
 			 struct rxm_pkt *pkt)
 {
-	pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
+	pkt->ctrl_hdr.conn_id = rxm_conn->remote_index;
 	pkt->hdr.size = len;
 	pkt->hdr.op = op;
 	pkt->hdr.tag = tag;
@@ -967,27 +854,19 @@ rxm_ep_format_tx_buf_pkt(struct rxm_conn *rxm_conn, size_t len, uint8_t op,
 	pkt->hdr.data = data;
 }
 
-static inline void *
-rxm_tx_buf_alloc(struct rxm_ep *rxm_ep, enum rxm_buf_pool_type type)
-{
-	assert((type == RXM_BUF_POOL_TX) ||
-	       (type == RXM_BUF_POOL_TX_INJECT) ||
-	       (type == RXM_BUF_POOL_TX_RNDV_RD_DONE) ||
-	       (type == RXM_BUF_POOL_TX_RNDV_WR_DATA) ||
-	       (type == RXM_BUF_POOL_TX_RNDV_WR_DONE) ||
-	       (type == RXM_BUF_POOL_TX_RNDV_REQ) ||
-	       (type == RXM_BUF_POOL_TX_ATOMIC) ||
-	       (type == RXM_BUF_POOL_TX_CREDIT) ||
-	       (type == RXM_BUF_POOL_TX_SAR));
-	return ofi_buf_alloc(rxm_ep->buf_pools[type].pool);
-}
+int rxm_post_recv(struct rxm_rx_buf *rx_buf);
 
 static inline void
 rxm_rx_buf_free(struct rxm_rx_buf *rx_buf)
 {
-	if (rx_buf->repost) {
-		dlist_insert_tail(&rx_buf->repost_entry,
-				  &rx_buf->ep->repost_ready_list);
+	if (rx_buf->data != rx_buf->pkt.data) {
+		free(rx_buf->data);
+		rx_buf->data = &rx_buf->pkt.data;
+	}
+
+	/* Discard rx buffer if its msg_ep was closed */
+	if (rx_buf->repost && (rx_buf->ep->srx_ctx || rx_buf->conn->msg_ep)) {
+		rxm_post_recv(rx_buf);
 	} else {
 		ofi_buf_free(rx_buf);
 	}
@@ -1010,7 +889,7 @@ rxm_cq_write_recv_comp(struct rxm_rx_buf *rx_buf, void *context, uint64_t flags,
 		rxm_cq_write_src(rx_buf->ep->util_ep.rx_cq, context,
 				 flags, len, buf, rx_buf->pkt.hdr.data,
 				 rx_buf->pkt.hdr.tag,
-				 rx_buf->conn->handle.fi_addr);
+				 rx_buf->conn->peer->fi_addr);
 	else
 		rxm_cq_write(rx_buf->ep->util_ep.rx_cq, context,
 			     flags, len, buf, rx_buf->pkt.hdr.data,
diff --git a/prov/rxm/src/rxm_atomic.c b/prov/rxm/src/rxm_atomic.c
index 113acdb..caef29c 100644
--- a/prov/rxm/src/rxm_atomic.c
+++ b/prov/rxm/src/rxm_atomic.c
@@ -38,14 +38,14 @@
 
 static void
 rxm_ep_format_atomic_pkt_hdr(struct rxm_conn *rxm_conn,
-		 struct rxm_tx_atomic_buf *tx_buf, size_t data_len,
+		 struct rxm_tx_buf *tx_buf, size_t data_len,
 		 uint32_t pkt_op, enum fi_datatype datatype,
 		 uint8_t atomic_op, uint64_t flags, uint64_t data,
 		 const struct fi_rma_ioc *rma_ioc, size_t rma_ioc_count)
 {
 	struct rxm_atomic_hdr *atomic_hdr;
 
-	atomic_hdr = (struct rxm_atomic_hdr *)tx_buf->pkt.data;
+	atomic_hdr = (struct rxm_atomic_hdr *) tx_buf->pkt.data;
 	rxm_ep_format_tx_buf_pkt(rxm_conn, data_len, pkt_op, data, 0,
 				 flags, &tx_buf->pkt);
 	tx_buf->pkt.ctrl_hdr.type = rxm_ctrl_atomic;
@@ -61,7 +61,7 @@ rxm_ep_format_atomic_pkt_hdr(struct rxm_conn *rxm_conn,
 
 static inline int
 rxm_ep_send_atomic_req(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		       struct rxm_tx_atomic_buf *tx_buf, uint64_t len)
+		       struct rxm_tx_buf *tx_buf, uint64_t len)
 {
 	int ret;
 
@@ -94,7 +94,7 @@ rxm_ep_atomic_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 		struct fi_ioc *resultv, void **result_desc,
 		size_t result_iov_count, uint32_t op, uint64_t flags)
 {
-	struct rxm_tx_atomic_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 	struct rxm_atomic_hdr *atomic_hdr;
 	struct iovec buf_iov[RXM_IOV_LIMIT];
 	struct iovec cmp_iov[RXM_IOV_LIMIT];
@@ -105,7 +105,7 @@ rxm_ep_atomic_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 	size_t datatype_sz = ofi_datatype_size(msg->datatype);
 	size_t buf_len = 0;
 	size_t cmp_len = 0;
-	size_t tot_len;
+	size_t data_len, tot_len;
 	ssize_t ret;
 	int i;
 
@@ -141,29 +141,20 @@ rxm_ep_atomic_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 							  &cmp_device);
 	}
 
-	tot_len = buf_len + cmp_len + sizeof(struct rxm_atomic_hdr) +
-			sizeof(struct rxm_pkt);
+	data_len = buf_len + cmp_len + sizeof(struct rxm_atomic_hdr);
+	tot_len = data_len + sizeof(struct rxm_pkt);
 
-	if (tot_len > rxm_buffer_size) {
+	if (tot_len > rxm_packet_size) {
 		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
 			"atomic data too large %zu\n", tot_len);
 		return -FI_EINVAL;
 	}
 
-	if (ofi_atomic_dec32(&rxm_ep->atomic_tx_credits) < 0) {
-		ret = -FI_EAGAIN;
-		goto restore_credit;
-	}
-
-	tx_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_ATOMIC);
-	if (OFI_UNLIKELY(!tx_buf)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-			"Ran out of buffers from Atomic buffer pool\n");
-		ret = -FI_EAGAIN;
-		goto restore_credit;
-	}
+	tx_buf = rxm_get_tx_buf(rxm_ep);
+	if (!tx_buf)
+		return -FI_EAGAIN;
 
-	rxm_ep_format_atomic_pkt_hdr(rxm_conn, tx_buf, tot_len, op,
+	rxm_ep_format_atomic_pkt_hdr(rxm_conn, tx_buf, data_len, op,
 				msg->datatype, msg->op, flags, msg->data,
 				msg->rma_iov, msg->rma_iov_count);
 	tx_buf->pkt.ctrl_hdr.msg_id = ofi_buf_index(tx_buf);
@@ -182,24 +173,21 @@ rxm_ep_atomic_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 		assert(ret == cmp_len);
 	}
 
-	tx_buf->result_iov.count = result_iov_count;
+	tx_buf->atomic_result.count = result_iov_count;
 	if (resultv) {
-		ofi_ioc_to_iov(resultv, tx_buf->result_iov.iov,
+		ofi_ioc_to_iov(resultv, tx_buf->atomic_result.iov,
 			       result_iov_count, datatype_sz);
 
 		if (result_desc) {
 			for (i = 0; i < result_iov_count; i++)
-				tx_buf->result_iov.desc[i] = result_desc[i];
+				tx_buf->atomic_result.desc[i] = result_desc[i];
 		}
 	}
 
 	ret = rxm_ep_send_atomic_req(rxm_ep, rxm_conn, tx_buf, tot_len);
-	if (OFI_LIKELY(!ret))
-		return ret;
+	if (ret)
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 
-	ofi_buf_free(tx_buf);
-restore_credit:
-	ofi_atomic_inc32(&rxm_ep->atomic_tx_credits);
 	return ret;
 }
 
diff --git a/prov/rxm/src/rxm_attr.c b/prov/rxm/src/rxm_attr.c
index cb620d4..2078dfa 100644
--- a/prov/rxm/src/rxm_attr.c
+++ b/prov/rxm/src/rxm_attr.c
@@ -53,7 +53,7 @@ struct fi_tx_attr rxm_tx_attr = {
 	.op_flags = RXM_PASSTHRU_TX_OP_FLAGS | RXM_TX_OP_FLAGS,
 	.msg_order = ~0x0ULL,
 	.comp_order = FI_ORDER_NONE,
-	.size = 65536,
+	.size = RXM_TX_SIZE,
 	.iov_limit = RXM_IOV_LIMIT,
 	.rma_iov_limit = RXM_IOV_LIMIT,
 };
@@ -63,7 +63,7 @@ struct fi_rx_attr rxm_rx_attr = {
 	.op_flags = RXM_PASSTHRU_RX_OP_FLAGS | RXM_RX_OP_FLAGS,
 	.msg_order = ~0x0ULL,
 	.comp_order = FI_ORDER_NONE,
-	.size = 65536,
+	.size = RXM_RX_SIZE,
 	.iov_limit= RXM_IOV_LIMIT,
 };
 
@@ -72,7 +72,7 @@ struct fi_tx_attr rxm_tx_attr_coll = {
 	.op_flags = RXM_PASSTHRU_TX_OP_FLAGS | RXM_TX_OP_FLAGS,
 	.msg_order = ~0x0ULL,
 	.comp_order = FI_ORDER_NONE,
-	.size = 65536,
+	.size = RXM_TX_SIZE,
 	.iov_limit = RXM_IOV_LIMIT,
 	.rma_iov_limit = RXM_IOV_LIMIT,
 };
@@ -82,7 +82,7 @@ struct fi_rx_attr rxm_rx_attr_coll = {
 	.op_flags = RXM_PASSTHRU_RX_OP_FLAGS | RXM_RX_OP_FLAGS,
 	.msg_order = ~0x0ULL,
 	.comp_order = FI_ORDER_NONE,
-	.size = 65536,
+	.size = RXM_RX_SIZE,
 	.iov_limit= RXM_IOV_LIMIT,
 };
 
diff --git a/prov/rxm/src/rxm_av.c b/prov/rxm/src/rxm_av.c
index b278dcb..375b41a 100644
--- a/prov/rxm/src/rxm_av.c
+++ b/prov/rxm/src/rxm_av.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018 Intel Corporation. All rights reserved.
+ * Copyright (c) 2018-2021 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -34,142 +34,253 @@
 
 #include "rxm.h"
 
-static int rxm_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
-			 size_t count, uint64_t flags)
+
+size_t rxm_av_max_peers(struct rxm_av *av)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
-	struct rxm_ep *rxm_ep;
-	int i, ret = 0;
-
-	fastlock_acquire(&av->ep_list_lock);
-	/* This should be before ofi_ip_av_remove as we need to know
-	 * fi_addr -> addr mapping when moving handle to peer list. */
-	dlist_foreach_container(&av->ep_list, struct rxm_ep,
-				rxm_ep, util_ep.av_entry) {
-		ofi_ep_lock_acquire(&rxm_ep->util_ep);
-		for (i = 0; i < count; i++) {
-			ret = rxm_cmap_remove(rxm_ep->cmap, *fi_addr + i);
-			if (ret)
-				FI_WARN(&rxm_prov, FI_LOG_AV,
-					"cmap remove failed for fi_addr: %"
-					PRIu64 "\n", *fi_addr + i);
-		}
-		ofi_ep_lock_release(&rxm_ep->util_ep);
+	size_t cnt;
+
+	fastlock_acquire(&av->util_av.lock);
+	cnt = av->peer_pool->entry_cnt;
+	fastlock_release(&av->util_av.lock);
+	return cnt;
+}
+
+struct rxm_conn *rxm_av_alloc_conn(struct rxm_av *av)
+{
+	struct rxm_conn *conn;
+	fastlock_acquire(&av->util_av.lock);
+	conn = ofi_buf_alloc(av->conn_pool);
+	fastlock_release(&av->util_av.lock);
+	return conn;
+}
+
+void rxm_av_free_conn(struct rxm_conn *conn)
+{
+	struct rxm_av *av;
+	av = container_of(conn->ep->util_ep.av, struct rxm_av, util_av);
+	fastlock_acquire(&av->util_av.lock);
+	ofi_buf_free(conn);
+	fastlock_release(&av->util_av.lock);
+}
+
+static int rxm_addr_compare(struct ofi_rbmap *map, void *key, void *data)
+{
+	return memcmp(&((struct rxm_peer_addr *) data)->addr, key,
+		container_of(map, struct rxm_av, addr_map)->util_av.addrlen);
+}
+
+static struct rxm_peer_addr *
+rxm_alloc_peer(struct rxm_av *av, const void *addr)
+{
+	struct rxm_peer_addr *peer;
+
+	assert(fastlock_held(&av->util_av.lock));
+	peer = ofi_ibuf_alloc(av->peer_pool);
+	if (!peer)
+		return NULL;
+
+	peer->av = av;
+	peer->index = (int) ofi_buf_index(peer);
+	peer->fi_addr = FI_ADDR_NOTAVAIL;
+	peer->refcnt = 1;
+	memcpy(&peer->addr, addr, av->util_av.addrlen);
+
+	if (ofi_rbmap_insert(&av->addr_map, &peer->addr, peer, &peer->node)) {
+		ofi_ibuf_free(peer);
+		peer = NULL;
 	}
-	fastlock_release(&av->ep_list_lock);
 
-	return ofi_ip_av_remove(av_fid, fi_addr, count, flags);
+	return peer;
+}
+
+static void rxm_free_peer(struct rxm_peer_addr *peer)
+{
+	assert(fastlock_held(&peer->av->util_av.lock));
+	assert(!peer->refcnt);
+	ofi_rbmap_delete(&peer->av->addr_map, peer->node);
+	ofi_ibuf_free(peer);
+}
+
+struct rxm_peer_addr *
+rxm_get_peer(struct rxm_av *av, const void *addr)
+{
+	struct rxm_peer_addr *peer;
+	struct ofi_rbnode *node;
+
+	fastlock_acquire(&av->util_av.lock);
+	node = ofi_rbmap_find(&av->addr_map, (void *) addr);
+	if (node) {
+		peer = node->data;
+		peer->refcnt++;
+	} else {
+		peer = rxm_alloc_peer(av, addr);
+	}
+
+	fastlock_release(&av->util_av.lock);
+	return peer;
+}
+
+void rxm_put_peer(struct rxm_peer_addr *peer)
+{
+	struct rxm_av *av;
+
+	av = peer->av;
+	fastlock_acquire(&av->util_av.lock);
+	if (--peer->refcnt == 0)
+		rxm_free_peer(peer);
+	fastlock_release(&av->util_av.lock);
+}
+
+void rxm_ref_peer(struct rxm_peer_addr *peer)
+{
+	fastlock_acquire(&peer->av->util_av.lock);
+	peer->refcnt++;
+	fastlock_release(&peer->av->util_av.lock);
+}
+
+static void
+rxm_set_av_context(struct rxm_av *av, fi_addr_t fi_addr,
+		   struct rxm_peer_addr *peer)
+{
+	struct rxm_peer_addr **peer_ctx;
+
+	peer_ctx = ofi_av_addr_context(&av->util_av, fi_addr);
+	*peer_ctx = peer;
+}
+
+static void
+rxm_put_peer_addr(struct rxm_av *av, fi_addr_t fi_addr)
+{
+	struct rxm_peer_addr **peer;
+
+	fastlock_acquire(&av->util_av.lock);
+	peer = ofi_av_addr_context(&av->util_av, fi_addr);
+	if (--(*peer)->refcnt == 0)
+		rxm_free_peer(*peer);
+
+	rxm_set_av_context(av, fi_addr, NULL);
+	fastlock_release(&av->util_av.lock);
 }
 
-/* TODO: Determine if it's cleaner to insert an address into the cmap only
- * when we need to send to that address, rather than inserting the address
- * into the cmap when adding it to the AV.
- */
 static int
-rxm_av_insert_cmap(struct fid_av *av_fid, const void *addr, size_t count,
-		   fi_addr_t *fi_addr, uint64_t flags)
+rxm_av_add_peers(struct rxm_av *av, const void *addr, size_t count,
+		 fi_addr_t *fi_addr)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
-	struct rxm_ep *rxm_ep;
-	fi_addr_t fi_addr_tmp;
-	size_t i;
-	int ret = 0;
+	struct rxm_peer_addr *peer;
 	const void *cur_addr;
+	fi_addr_t cur_fi_addr;
+	size_t i;
+
+	for (i = 0; i < count; i++) {
+		cur_addr = ((char *) addr + i * av->util_av.addrlen);
+		peer = rxm_get_peer(av, cur_addr);
+		if (!peer)
+			goto err;
+
+		peer->fi_addr = fi_addr ? fi_addr[i] :
+				ofi_av_lookup_fi_addr(&av->util_av, cur_addr);
+
+		/* lookup can fail if prior AV insertion failed */
+		if (peer->fi_addr != FI_ADDR_NOTAVAIL)
+			rxm_set_av_context(av, peer->fi_addr, peer);
+	}
+	return 0;
 
-	fastlock_acquire(&av->ep_list_lock);
-	dlist_foreach_container(&av->ep_list, struct rxm_ep,
-				rxm_ep, util_ep.av_entry) {
-		ofi_ep_lock_acquire(&rxm_ep->util_ep);
-		for (i = 0; i < count; i++) {
-			if (!rxm_ep->cmap)
-				break;
-
-			cur_addr = (const void *) ((char *) addr + i * av->addrlen);
-			fi_addr_tmp = (fi_addr ? fi_addr[i] :
-				       ofi_av_lookup_fi_addr_unsafe(av, cur_addr));
-			if (fi_addr_tmp == FI_ADDR_NOTAVAIL)
-				continue;
-
-			ret = rxm_cmap_update(rxm_ep->cmap, cur_addr, fi_addr_tmp);
-			if (OFI_UNLIKELY(ret)) {
-				FI_WARN(&rxm_prov, FI_LOG_AV,
-					"cmap update failed for fi_addr: %"
-					PRIu64 "\n", fi_addr_tmp);
-				break;
-			}
+err:
+	while (i--) {
+		if (fi_addr) {
+			cur_fi_addr = fi_addr[i];
+		} else {
+			cur_addr = ((char *) addr + i * av->util_av.addrlen);
+			cur_fi_addr = ofi_av_lookup_fi_addr(&av->util_av,
+							    cur_addr);
 		}
-		ofi_ep_lock_release(&rxm_ep->util_ep);
+		if (cur_fi_addr != FI_ADDR_NOTAVAIL)
+			rxm_put_peer_addr(av, cur_fi_addr);
 	}
-	fastlock_release(&av->ep_list_lock);
-	return ret;
+	return -FI_ENOMEM;
+}
+
+static int rxm_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
+			 size_t count, uint64_t flags)
+{
+	struct rxm_av *av;
+	size_t i;
+
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid);
+	for (i = 0; i < count; i++)
+		rxm_put_peer_addr(av, fi_addr[i]);
+
+	return ofi_ip_av_remove(av_fid, fi_addr, count, flags);
 }
 
 static int rxm_av_insert(struct fid_av *av_fid, const void *addr, size_t count,
 			 fi_addr_t *fi_addr, uint64_t flags, void *context)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
-	int ret, retv;
+	struct rxm_av *av;
+	int ret;
 
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid.fid);
 	ret = ofi_ip_av_insert(av_fid, addr, count, fi_addr, flags, context);
 	if (ret < 0)
 		return ret;
 
-	if (!av->eq && !ret)
-		return ret;
+	if (!av->util_av.eq)
+		count = ret;
 
-	retv = rxm_av_insert_cmap(av_fid, addr, count, fi_addr, flags);
-	if (retv) {
-		ret = rxm_av_remove(av_fid, fi_addr, count, flags);
-		if (ret)
-			FI_WARN(&rxm_prov, FI_LOG_AV, "Failed to remove addr "
-				"from AV during error handling\n");
-		return retv;
+	ret = rxm_av_add_peers(av, addr, count, fi_addr);
+	if (ret) {
+		/* If insert was async, ofi_ip_av_insert() will have written
+		 * an event to the EQ with the number of insertions.  For
+		 * correctness we need to delay writing the event to the EQ
+		 * until all processing has completed.  This should be done
+		 * when separating the rxm av from the util av.  For now,
+		 * assume synchronous operation (most common case) and fail
+		 * the insert.  This could leave a bogus entry on the EQ.
+		 * But the app should detect that insert failed and is likely
+		 * to abort.
+		 */
+		rxm_av_remove(av_fid, fi_addr, count, flags);
+		return ret;
 	}
-	return ret;
+
+	return av->util_av.eq ? 0 : count;
 }
 
 static int rxm_av_insertsym(struct fid_av *av_fid, const char *node,
 			    size_t nodecnt, const char *service, size_t svccnt,
 			    fi_addr_t *fi_addr, uint64_t flags, void *context)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
+	struct rxm_av *av;
 	void *addr;
-	size_t addrlen, count = nodecnt * svccnt;
-	int ret, retv;
+	size_t addrlen, count;
+	int ret;
 
-	ret = ofi_verify_av_insert(av, flags, context);
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid.fid);
+	ret = ofi_verify_av_insert(&av->util_av, flags, context);
 	if (ret)
 		return ret;
 
-	ret = ofi_ip_av_sym_getaddr(av, node, nodecnt, service,
+	ret = ofi_ip_av_sym_getaddr(&av->util_av, node, nodecnt, service,
 				    svccnt, &addr, &addrlen);
 	if (ret <= 0)
 		return ret;
 
-	assert(ret == count);
-
-	ret = ofi_ip_av_insertv(av, addr, addrlen, count, fi_addr, flags,
+	count = ret;
+	ret = ofi_ip_av_insertv(&av->util_av, addr, addrlen, count, fi_addr, flags,
 				context);
-	if (!av->eq && ret < count) {
+	if (ret > 0 && ret < count)
 		count = ret;
-	}
 
-	/* If the AV is bound to an EQ, we can't determine which entries were
-	 * added successfully to the AV until we process the insertion events
-	 * later when reading the EQ.  Add all addresses to the cmap
-	 * optimistically.
-	 */
-	retv = rxm_av_insert_cmap(av_fid, addr, count, fi_addr, flags);
-	if (retv) {
-		ret = rxm_av_remove(av_fid, fi_addr, count, flags);
-		if (ret)
-			FI_WARN(&rxm_prov, FI_LOG_AV, "Failed to remove addr "
-				"from AV during error handling\n");
-		ret = retv;
+	ret = rxm_av_add_peers(av, addr, count, fi_addr);
+	if (ret) {
+		/* See comment in rxm_av_insert. */
+		rxm_av_remove(av_fid, fi_addr, count, flags);
+		return ret;
 	}
 
 	free(addr);
-	return ret;
+	return av->util_av.eq ? 0 : count;
 }
 
 int rxm_av_insertsvc(struct fid_av *av, const char *node, const char *service,
@@ -190,6 +301,30 @@ int rxm_av_lookup(struct fid_av *av_fid, fi_addr_t fi_addr,
 	return ofi_ip_av_lookup(av_fid, fi_addr, addr, addrlen);
 }
 
+static int rxm_av_close(struct fid *av_fid)
+{
+	struct rxm_av *av;
+	int ret;
+
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid.fid);
+	ret = ofi_av_close(&av->util_av);
+	if (ret)
+		return ret;
+
+	ofi_rbmap_cleanup(&av->addr_map);
+	ofi_bufpool_destroy(av->conn_pool);
+	ofi_bufpool_destroy(av->peer_pool);
+	free(av);
+	return 0;
+}
+
+static struct fi_ops rxm_av_fi_ops = {
+	.size = sizeof(struct fi_ops),
+	.close = rxm_av_close,
+	.bind = ofi_av_bind,
+	.control = fi_no_control,
+	.ops_open = fi_no_ops_open,
+};
 
 static struct fi_ops_av rxm_av_ops = {
 	.size = sizeof(struct fi_ops_av),
@@ -203,15 +338,54 @@ static struct fi_ops_av rxm_av_ops = {
 };
 
 int rxm_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
-		struct fid_av **av, void *context)
+		struct fid_av **fid_av, void *context)
 {
+	struct rxm_domain *domain;
+	struct util_av_attr util_attr;
+	struct rxm_av *av;
 	int ret;
 
-	ret = ofi_ip_av_create(domain_fid, attr, av, context);
+	av = calloc(1, sizeof(*av));
+	if (!av)
+		return -FI_ENOMEM;
+
+	ret = ofi_bufpool_create(&av->peer_pool, sizeof(struct rxm_peer_addr),
+				 0, 0, 0, OFI_BUFPOOL_INDEXED |
+				 OFI_BUFPOOL_NO_TRACK);
 	if (ret)
-		return ret;
+		goto free;
+
+	ret = ofi_bufpool_create(&av->conn_pool, sizeof(struct rxm_conn),
+				 0, 0, 0, 0);
+	if (ret)
+		goto destroy1;
+
+	ofi_rbmap_init(&av->addr_map, rxm_addr_compare);
+	domain = container_of(domain_fid, struct rxm_domain,
+			      util_domain.domain_fid);
 
-	(*av)->ops = &rxm_av_ops;
+	util_attr.context_len = sizeof(struct rxm_peer_addr *);
+	util_attr.flags = 0;
+	util_attr.addrlen = ofi_sizeof_addr_format(domain->util_domain.
+						   addr_format);
+	if (attr->type == FI_AV_UNSPEC)
+		attr->type = FI_AV_TABLE;
+
+	ret = ofi_av_init(&domain->util_domain, attr, &util_attr,
+			  &av->util_av, context);
+	if (ret)
+		goto destroy2;
+
+	av->util_av.av_fid.fid.ops = &rxm_av_fi_ops;
+	av->util_av.av_fid.ops = &rxm_av_ops;
+	*fid_av = &av->util_av.av_fid;
 	return 0;
-}
 
+destroy2:
+	ofi_bufpool_destroy(av->conn_pool);
+destroy1:
+	ofi_bufpool_destroy(av->peer_pool);
+free:
+	free(av);
+	return ret;
+}
diff --git a/prov/rxm/src/rxm_conn.c b/prov/rxm/src/rxm_conn.c
index 7250d6f..d9cae06 100644
--- a/prov/rxm/src/rxm_conn.c
+++ b/prov/rxm/src/rxm_conn.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2016 Intel Corporation, Inc.  All rights reserved.
+ * Copyright (c) 2016-2021 Intel Corporation, Inc.  All rights reserved.
  * Copyright (c) 2019 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -39,1326 +39,720 @@
 #include <ofi_util.h>
 #include "rxm.h"
 
-static struct rxm_cmap_handle *rxm_conn_alloc(struct rxm_cmap *cmap);
-static int rxm_conn_connect(struct rxm_ep *ep,
-			    struct rxm_cmap_handle *handle, const void *addr);
-static int rxm_conn_signal(struct rxm_ep *ep, void *context,
-			   enum rxm_cmap_signal signal);
-static void rxm_conn_av_updated_handler(struct rxm_cmap_handle *handle);
-static void *rxm_conn_progress(void *arg);
-static void *rxm_conn_atomic_progress(void *arg);
-static int rxm_conn_handle_event(struct rxm_ep *rxm_ep,
-				 struct rxm_msg_eq_entry *entry);
 
+static void *rxm_cm_progress(void *arg);
+static void *rxm_cm_atomic_progress(void *arg);
+static void rxm_flush_msg_cq(struct rxm_ep *rxm_ep);
 
-/*
- * Connection map
- */
 
-char *rxm_cm_state_str[] = {
-	RXM_CM_STATES(OFI_STR)
+/* castable to fi_eq_cm_entry - we can't use fi_eq_cm_entry directly
+ * here because of a compiler error with a 0-sized array
+ */
+struct rxm_eq_cm_entry {
+	fid_t fid;
+	struct fi_info *info;
+	union rxm_cm_data data;
 };
 
-static inline ssize_t rxm_eq_readerr(struct rxm_ep *rxm_ep,
-				     struct rxm_msg_eq_entry *entry)
+
+static void rxm_close_conn(struct rxm_conn *conn)
 {
-	ssize_t ret;
+	struct rxm_deferred_tx_entry *tx_entry;
+	struct rxm_recv_entry *rx_entry;
+	struct rxm_rx_buf *buf;
 
-	/* reset previous err data info */
-	entry->err_entry.err_data_size = 0;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "closing conn %p\n", conn);
 
-	ret = fi_eq_readerr(rxm_ep->msg_eq, &entry->err_entry, 0);
-	if (ret != sizeof(entry->err_entry)) {
-		if (ret != -FI_EAGAIN)
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"unable to fi_eq_readerr: %zd\n", ret);
-		return ret < 0 ? ret : -FI_EINVAL;
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+	/* All deferred transfers are internally generated */
+	while (!dlist_empty(&conn->deferred_tx_queue)) {
+		tx_entry = container_of(conn->deferred_tx_queue.next,
+				     struct rxm_deferred_tx_entry, entry);
+		rxm_dequeue_deferred_tx(tx_entry);
+		free(tx_entry);
 	}
 
-	if (entry->err_entry.err == ECONNREFUSED) {
-		entry->context = entry->err_entry.fid->context;
-		return -FI_ECONNREFUSED;
+	while (!dlist_empty(&conn->deferred_sar_segments)) {
+		buf = container_of(conn->deferred_sar_segments.next,
+				   struct rxm_rx_buf, unexp_msg.entry);
+		dlist_remove(&buf->unexp_msg.entry);
+		rxm_rx_buf_free(buf);
 	}
 
-	OFI_EQ_STRERROR(&rxm_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
-			rxm_ep->msg_eq, &entry->err_entry);
-	return -entry->err_entry.err;
-}
-
-static ssize_t rxm_eq_read(struct rxm_ep *ep, size_t len,
-			   struct rxm_msg_eq_entry *entry)
-{
-	ssize_t ret;
-
-	ret = fi_eq_read(ep->msg_eq, &entry->event, &entry->cm_entry, len, 0);
-	if (ret == -FI_EAVAIL)
-		ret = rxm_eq_readerr(ep, entry);
-
-	return ret;
-}
-
-static void rxm_cmap_set_key(struct rxm_cmap_handle *handle)
-{
-	handle->key = ofi_idx2key(&handle->cmap->key_idx,
-		ofi_idx_insert(&handle->cmap->handles_idx, handle));
-}
-
-static void rxm_cmap_clear_key(struct rxm_cmap_handle *handle)
-{
-	int index = ofi_key2idx(&handle->cmap->key_idx, handle->key);
-
-	if (!ofi_idx_is_valid(&handle->cmap->handles_idx, index))
-		FI_WARN(handle->cmap->av->prov, FI_LOG_AV, "Invalid key!\n");
-	else
-		ofi_idx_remove(&handle->cmap->handles_idx, index);
-}
-
-struct rxm_cmap_handle *rxm_cmap_key2handle(struct rxm_cmap *cmap, uint64_t key)
-{
-	struct rxm_cmap_handle *handle;
-
-	if (!(handle = ofi_idx_lookup(&cmap->handles_idx,
-				      ofi_key2idx(&cmap->key_idx, key)))) {
-		FI_WARN(cmap->av->prov, FI_LOG_AV, "Invalid key!\n");
-	} else {
-		if (handle->key != key) {
-			FI_WARN(cmap->av->prov, FI_LOG_AV,
-				"handle->key not matching given key\n");
-			handle = NULL;
-		}
+	while (!dlist_empty(&conn->deferred_sar_msgs)) {
+		rx_entry = container_of(conn->deferred_sar_msgs.next,
+					struct rxm_recv_entry, sar.entry);
+		dlist_remove(&rx_entry->entry);
+		rxm_recv_entry_release(rx_entry);
 	}
-	return handle;
-}
-
-static void rxm_cmap_init_handle(struct rxm_cmap_handle *handle,
-				  struct rxm_cmap *cmap,
-				  enum rxm_cmap_state state,
-				  fi_addr_t fi_addr,
-				  struct rxm_cmap_peer *peer)
-{
-	handle->cmap = cmap;
-	RXM_CM_UPDATE_STATE(handle, state);
-	rxm_cmap_set_key(handle);
-	handle->fi_addr = fi_addr;
-	handle->peer = peer;
+	fi_close(&conn->msg_ep->fid);
+	rxm_flush_msg_cq(conn->ep);
+	dlist_remove_init(&conn->loopback_entry);
+	conn->msg_ep = NULL;
+	conn->state = RXM_CM_IDLE;
 }
 
-static int rxm_cmap_match_peer(struct dlist_entry *entry, const void *addr)
+static int rxm_open_conn(struct rxm_conn *conn, struct fi_info *msg_info)
 {
-	struct rxm_cmap_peer *peer;
-
-	peer = container_of(entry, struct rxm_cmap_peer, entry);
-	return !memcmp(peer->addr, addr, peer->handle->cmap->av->addrlen);
-}
-
-static int rxm_cmap_del_handle(struct rxm_cmap_handle *handle)
-{
-	struct rxm_cmap *cmap = handle->cmap;
+	struct rxm_domain *domain;
+	struct rxm_ep *ep;
+	struct fid_ep *msg_ep;
 	int ret;
 
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-	       "marking connection handle: %p for deletion\n", handle);
-	rxm_cmap_clear_key(handle);
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "open msg ep %p\n", conn);
 
-	RXM_CM_UPDATE_STATE(handle, RXM_CMAP_SHUTDOWN);
-
-	/* Signal CM thread to delete the handle. This is required
-	 * so that the CM thread handles any pending events for this
-	 * ep correctly. Handle would be freed finally after processing the
-	 * events */
-	ret = rxm_conn_signal(cmap->ep, handle, RXM_CMAP_FREE);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+	ep = conn->ep;
+	domain = container_of(ep->util_ep.domain, struct rxm_domain,
+			      util_domain);
+	ret = fi_endpoint(domain->msg_domain, msg_info, &msg_ep, conn);
 	if (ret) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Unable to signal CM thread\n");
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to create msg_ep: %d\n", ret);
 		return ret;
 	}
-	return 0;
-}
-
-ssize_t rxm_get_conn(struct rxm_ep *rxm_ep, fi_addr_t addr,
-		     struct rxm_conn **rxm_conn)
-{
-	struct rxm_cmap_handle *handle;
-	ssize_t ret;
-
-	assert(rxm_ep->util_ep.tx_cq);
-	handle = rxm_cmap_acquire_handle(rxm_ep->cmap, addr);
-	if (!handle) {
-		ret = rxm_cmap_alloc_handle(rxm_ep->cmap, addr,
-					    RXM_CMAP_IDLE, &handle);
-		if (ret)
-			return ret;
-	}
-
-	*rxm_conn = container_of(handle, struct rxm_conn, handle);
 
-	if (handle->state != RXM_CMAP_CONNECTED) {
-		ret = rxm_cmap_connect(rxm_ep, addr, handle);
-		if (ret)
-			return ret;
-	}
-
-	if (!dlist_empty(&(*rxm_conn)->deferred_tx_queue)) {
-		rxm_ep_do_progress(&rxm_ep->util_ep);
-		if (!dlist_empty(&(*rxm_conn)->deferred_tx_queue))
-			return -FI_EAGAIN;
+	ret = fi_ep_bind(msg_ep, &ep->msg_eq->fid, 0);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to bind msg EP to EQ: %d\n", ret);
+		goto err;
 	}
-	return 0;
-}
 
-static inline int
-rxm_cmap_check_and_realloc_handles_table(struct rxm_cmap *cmap,
-					 fi_addr_t fi_addr)
-{
-	void *new_handles;
-	size_t grow_size;
-
-	if (OFI_LIKELY(fi_addr < cmap->num_allocated))
-		return 0;
-
-	grow_size = MAX(ofi_av_size(cmap->av), fi_addr - cmap->num_allocated + 1);
-
-	new_handles = realloc(cmap->handles_av,
-			      (grow_size + cmap->num_allocated) *
-			      sizeof(*cmap->handles_av));
-	if (OFI_LIKELY(!new_handles))
-		return -FI_ENOMEM;
-
-	cmap->handles_av = new_handles;
-	memset(&cmap->handles_av[cmap->num_allocated], 0,
-	       sizeof(*cmap->handles_av) * grow_size);
-	cmap->num_allocated += grow_size;
-	return 0;
-}
-
-static struct rxm_pkt *
-rxm_conn_inject_pkt_alloc(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-			  uint8_t op, uint64_t flags)
-{
-	struct rxm_pkt *inject_pkt;
-	int ret = ofi_memalign((void **) &inject_pkt, 16,
-			       rxm_ep->inject_limit + sizeof(*inject_pkt));
-
-	if (ret)
-		return NULL;
-
-	memset(inject_pkt, 0, rxm_ep->inject_limit + sizeof(*inject_pkt));
-	inject_pkt->ctrl_hdr.version = RXM_CTRL_VERSION;
-	inject_pkt->ctrl_hdr.type = rxm_ctrl_eager;
-	inject_pkt->hdr.version = OFI_OP_VERSION;
-	inject_pkt->hdr.op = op;
-	inject_pkt->hdr.flags = flags;
-
-	return inject_pkt;
-}
-
-static void rxm_conn_res_free(struct rxm_conn *rxm_conn)
-{
-	ofi_freealign(rxm_conn->inject_pkt);
-	rxm_conn->inject_pkt = NULL;
-	ofi_freealign(rxm_conn->inject_data_pkt);
-	rxm_conn->inject_data_pkt = NULL;
-	ofi_freealign(rxm_conn->tinject_pkt);
-	rxm_conn->tinject_pkt = NULL;
-	ofi_freealign(rxm_conn->tinject_data_pkt);
-	rxm_conn->tinject_data_pkt = NULL;
-}
-
-static int rxm_conn_res_alloc(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn)
-{
-	dlist_init(&rxm_conn->deferred_conn_entry);
-	dlist_init(&rxm_conn->deferred_tx_queue);
-	dlist_init(&rxm_conn->sar_rx_msg_list);
-	dlist_init(&rxm_conn->sar_deferred_rx_msg_list);
-
-	if (rxm_ep->util_ep.domain->threading != FI_THREAD_SAFE) {
-		rxm_conn->inject_pkt =
-			rxm_conn_inject_pkt_alloc(rxm_ep, rxm_conn,
-						  ofi_op_msg, 0);
-		rxm_conn->inject_data_pkt =
-			rxm_conn_inject_pkt_alloc(rxm_ep, rxm_conn,
-						  ofi_op_msg, FI_REMOTE_CQ_DATA);
-		rxm_conn->tinject_pkt =
-			rxm_conn_inject_pkt_alloc(rxm_ep, rxm_conn,
-						  ofi_op_tagged, 0);
-		rxm_conn->tinject_data_pkt =
-			rxm_conn_inject_pkt_alloc(rxm_ep, rxm_conn,
-						  ofi_op_tagged, FI_REMOTE_CQ_DATA);
-
-		if (!rxm_conn->inject_pkt || !rxm_conn->inject_data_pkt ||
-		    !rxm_conn->tinject_pkt || !rxm_conn->tinject_data_pkt) {
-			rxm_conn_res_free(rxm_conn);
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to allocate "
-				"inject pkt for connection\n");
-			return -FI_ENOMEM;
+	if (ep->srx_ctx) {
+		ret = fi_ep_bind(msg_ep, &ep->srx_ctx->fid, 0);
+		if (ret) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to bind msg "
+				"EP to shared RX ctx: %d\n", ret);
+			goto err;
 		}
 	}
-	return 0;
-}
 
-static void rxm_conn_close(struct rxm_cmap_handle *handle)
-{
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-	struct rxm_conn *rxm_conn_tmp;
-	struct rxm_deferred_tx_entry *def_tx_entry;
-	struct dlist_entry *conn_entry_tmp;
-
-	dlist_foreach_container_safe(&handle->cmap->ep->deferred_tx_conn_queue,
-				     struct rxm_conn, rxm_conn_tmp,
-				     deferred_conn_entry, conn_entry_tmp)
-	{
-		if (rxm_conn_tmp->handle.key != handle->key)
-			continue;
-
-		while (!dlist_empty(&rxm_conn_tmp->deferred_tx_queue)) {
-			def_tx_entry =
-				container_of(rxm_conn_tmp->deferred_tx_queue.next,
-					     struct rxm_deferred_tx_entry, entry);
-			FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-			       "cancelled deferred message\n");
-			rxm_ep_dequeue_deferred_tx_queue(def_tx_entry);
-			free(def_tx_entry);
-		}
+	ret = fi_ep_bind(msg_ep, &ep->msg_cq->fid, FI_TRANSMIT | FI_RECV);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"unable to bind msg_ep to msg_cq: %d\n", ret);
+		goto err;
 	}
 
-	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "closing msg ep\n");
-	if (!rxm_conn->msg_ep)
-		return;
-
-	if (fi_close(&rxm_conn->msg_ep->fid))
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to close msg_ep\n");
-
-	rxm_conn->msg_ep = NULL;
-}
-
-static void rxm_conn_free(struct rxm_cmap_handle *handle)
-{
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-
-	rxm_conn_close(handle);
-	rxm_conn_res_free(rxm_conn);
-	free(rxm_conn);
-}
-
-int rxm_cmap_alloc_handle(struct rxm_cmap *cmap, fi_addr_t fi_addr,
-			  enum rxm_cmap_state state,
-			  struct rxm_cmap_handle **handle)
-{
-	int ret;
-
-	*handle = rxm_conn_alloc(cmap);
-	if (!*handle)
-		return -FI_ENOMEM;
-
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-	       "Allocated handle: %p for fi_addr: %" PRIu64 "\n",
-	       *handle, fi_addr);
-
-	ret = rxm_cmap_check_and_realloc_handles_table(cmap, fi_addr);
+	ret = fi_enable(msg_ep);
 	if (ret) {
-		rxm_conn_free(*handle);
-		return ret;
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to enable msg_ep: %d\n", ret);
+		goto err;
 	}
 
-	rxm_cmap_init_handle(*handle, cmap, state, fi_addr, NULL);
-	cmap->handles_av[fi_addr] = *handle;
-	return 0;
-}
-
-static int rxm_cmap_alloc_handle_peer(struct rxm_cmap *cmap, void *addr,
-				       enum rxm_cmap_state state,
-				       struct rxm_cmap_handle **handle)
-{
-	struct rxm_cmap_peer *peer;
-
-	peer = calloc(1, sizeof(*peer) + cmap->av->addrlen);
-	if (!peer)
-		return -FI_ENOMEM;
-
-	*handle = rxm_conn_alloc(cmap);
-	if (!*handle) {
-		free(peer);
-		return -FI_ENOMEM;
+	ret = domain->flow_ctrl_ops->enable(msg_ep);
+	if (!ret) {
+		domain->flow_ctrl_ops->set_threshold(msg_ep,
+					ep->msg_info->rx_attr->size / 2);
 	}
 
-	ofi_straddr_dbg(cmap->av->prov, FI_LOG_AV,
-			"Allocated handle for addr", addr);
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "handle: %p\n", *handle);
+	if (!ep->srx_ctx) {
+		ret = rxm_prepost_recv(ep, msg_ep);
+		if (ret)
+			goto err;
+	}
 
-	rxm_cmap_init_handle(*handle, cmap, state, FI_ADDR_NOTAVAIL, peer);
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Adding handle to peer list\n");
-	peer->handle = *handle;
-	memcpy(peer->addr, addr, cmap->av->addrlen);
-	dlist_insert_tail(&peer->entry, &cmap->peer_list);
+	conn->msg_ep = msg_ep;
 	return 0;
+err:
+	fi_close(&msg_ep->fid);
+	return ret;
 }
 
-static struct rxm_cmap_handle *
-rxm_cmap_get_handle_peer(struct rxm_cmap *cmap, const void *addr)
+/* We send passive endpoint's port to the server as connection request
+ * would be from a different one.
+ */
+static int rxm_init_connect_data(struct rxm_conn *conn,
+				 union rxm_cm_data *cm_data)
 {
-	struct rxm_cmap_peer *peer;
-	struct dlist_entry *entry;
-
-	entry = dlist_find_first_match(&cmap->peer_list, rxm_cmap_match_peer,
-				       addr);
-	if (!entry)
-		return NULL;
-
-	ofi_straddr_dbg(cmap->av->prov, FI_LOG_AV,
-			"handle found in peer list for addr", addr);
-	peer = container_of(entry, struct rxm_cmap_peer, entry);
-	return peer->handle;
-}
+	size_t cm_data_size = 0;
+	size_t opt_size = sizeof(cm_data_size);
+	int ret;
 
-int rxm_cmap_remove(struct rxm_cmap *cmap, int index)
-{
-	struct rxm_cmap_handle *handle;
-	int ret = -FI_ENOENT;
+	memset(cm_data, 0, sizeof(*cm_data));
+	cm_data->connect.version = RXM_CM_DATA_VERSION;
+	cm_data->connect.ctrl_version = RXM_CTRL_VERSION;
+	cm_data->connect.op_version = RXM_OP_VERSION;
+	cm_data->connect.endianness = ofi_detect_endianness();
+	cm_data->connect.eager_limit = conn->ep->eager_limit;
+	cm_data->connect.rx_size = conn->ep->msg_info->rx_attr->size;
 
-	handle = cmap->handles_av[index];
-	if (!handle) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cmap entry not found\n");
+	ret = fi_getopt(&conn->ep->msg_pep->fid, FI_OPT_ENDPOINT,
+			FI_OPT_CM_DATA_SIZE, &cm_data_size, &opt_size);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "fi_getopt failed\n");
 		return ret;
 	}
 
-	handle->peer = calloc(1, sizeof(*handle->peer) + cmap->av->addrlen);
-	if (!handle->peer) {
-		ret = -FI_ENOMEM;
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to allocate memory "
-			"for moving handle to peer list, deleting it instead\n");
-		rxm_cmap_del_handle(handle);
-		return ret;
+	if (cm_data_size < sizeof(*cm_data)) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data too small\n");
+		return -FI_EOTHER;
 	}
-	handle->fi_addr = FI_ADDR_NOTAVAIL;
-	cmap->handles_av[index] = NULL;
-	handle->peer->handle = handle;
-	memcpy(handle->peer->addr, ofi_av_get_addr(cmap->av, index),
-	       cmap->av->addrlen);
-	dlist_insert_tail(&handle->peer->entry, &cmap->peer_list);
+
+	cm_data->connect.port = ofi_addr_get_port(&conn->ep->addr.sa);
+	cm_data->connect.client_conn_id = conn->peer->index;
 	return 0;
 }
 
-static int rxm_cmap_move_handle(struct rxm_cmap_handle *handle,
-				fi_addr_t fi_addr)
+static int rxm_send_connect(struct rxm_conn *conn)
 {
+	union rxm_cm_data cm_data;
+	struct fi_info *info;
 	int ret;
 
-	dlist_remove(&handle->peer->entry);
-	free(handle->peer);
-	handle->peer = NULL;
-	handle->fi_addr = fi_addr;
-	ret = rxm_cmap_check_and_realloc_handles_table(handle->cmap, fi_addr);
-	if (OFI_UNLIKELY(ret))
-		return ret;
-	handle->cmap->handles_av[fi_addr] = handle;
-	return 0;
-}
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connecting %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
 
-int rxm_cmap_update(struct rxm_cmap *cmap, const void *addr, fi_addr_t fi_addr)
-{
-	struct rxm_cmap_handle *handle;
-	int ret;
+	info = conn->ep->msg_info;
+	info->dest_addrlen = conn->ep->msg_info->src_addrlen;
 
-	/* Check whether we have already allocated a handle for this `fi_addr`. */
-	/* We rely on the fact that `ofi_ip_av_insert`/`ofi_av_insert_addr` returns
-	 * the same `fi_addr` for the equal addresses */
-	if (fi_addr < cmap->num_allocated) {
-		handle = rxm_cmap_acquire_handle(cmap, fi_addr);
-		if (handle)
-			return 0;
-	}
+	free(info->dest_addr);
+	info->dest_addr = mem_dup(&conn->peer->addr, info->dest_addrlen);
+	if (!info->dest_addr)
+		return -FI_ENOMEM;
 
-	handle = rxm_cmap_get_handle_peer(cmap, addr);
-	if (!handle) {
-		ret = rxm_cmap_alloc_handle(cmap, fi_addr,
-					    RXM_CMAP_IDLE, &handle);
-		return ret;
-	}
-	ret = rxm_cmap_move_handle(handle, fi_addr);
+	ret = rxm_open_conn(conn, info);
 	if (ret)
 		return ret;
 
-	rxm_conn_av_updated_handler(handle);
-	return 0;
-}
-
-void rxm_cmap_process_shutdown(struct rxm_cmap *cmap,
-			       struct rxm_cmap_handle *handle)
-{
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-		"Processing shutdown for handle: %p\n", handle);
-	if (handle->state > RXM_CMAP_SHUTDOWN) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Invalid handle on shutdown event\n");
-	} else if (handle->state != RXM_CMAP_SHUTDOWN) {
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Got remote shutdown\n");
-		rxm_cmap_del_handle(handle);
-	} else {
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Got local shutdown\n");
-	}
-}
-
-void rxm_cmap_process_connect(struct rxm_cmap *cmap,
-			      struct rxm_cmap_handle *handle,
-			      union rxm_cm_data *cm_data)
-{
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-	       "processing FI_CONNECTED event for handle: %p\n", handle);
-	if (cm_data) {
-		assert(handle->state == RXM_CMAP_CONNREQ_SENT);
-		handle->remote_key = cm_data->accept.server_conn_id;
-		rxm_conn->rndv_tx_credits = cm_data->accept.rx_size;
-	} else {
-		assert(handle->state == RXM_CMAP_CONNREQ_RECV);
-	}
-	RXM_CM_UPDATE_STATE(handle, RXM_CMAP_CONNECTED);
-
-	/* Set the remote key to the inject packets */
-	if (cmap->ep->util_ep.domain->threading != FI_THREAD_SAFE) {
-		rxm_conn->inject_pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
-		rxm_conn->inject_data_pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
-		rxm_conn->tinject_pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
-		rxm_conn->tinject_data_pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
-	}
-}
-
-void rxm_cmap_process_reject(struct rxm_cmap *cmap,
-			     struct rxm_cmap_handle *handle,
-			     enum rxm_cmap_reject_reason reject_reason)
-{
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-		"Processing reject for handle: %p\n", handle);
-	switch (handle->state) {
-	case RXM_CMAP_CONNREQ_RECV:
-	case RXM_CMAP_CONNECTED:
-		/* Handle is being re-used for incoming connection request */
-		break;
-	case RXM_CMAP_CONNREQ_SENT:
-		if (reject_reason == RXM_CMAP_REJECT_GENUINE) {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			       "Deleting connection handle\n");
-			rxm_cmap_del_handle(handle);
-		} else {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			       "Connection handle is being re-used. Close the connection\n");
-			rxm_conn_close(handle);
-		}
-		break;
-	case RXM_CMAP_SHUTDOWN:
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Connection handle already being deleted\n");
-		break;
-	default:
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL, "Invalid cmap state: "
-			"%d when receiving connection reject\n", handle->state);
-		assert(0);
-	}
-}
+	ret = rxm_init_connect_data(conn, &cm_data);
+	if (ret)
+		goto err;
 
-int rxm_cmap_process_connreq(struct rxm_cmap *cmap, void *addr,
-			     struct rxm_cmap_handle **handle_ret,
-			     uint8_t *reject_reason)
-{
-	struct rxm_cmap_handle *handle;
-	int ret = 0, cmp;
-	fi_addr_t fi_addr = ofi_ip_av_get_fi_addr(cmap->av, addr);
-
-	ofi_straddr_dbg(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Processing connreq from remote pep", addr);
-
-	if (fi_addr == FI_ADDR_NOTAVAIL)
-		handle = rxm_cmap_get_handle_peer(cmap, addr);
-	else
-		handle = rxm_cmap_acquire_handle(cmap, fi_addr);
-
-	if (!handle) {
-		if (fi_addr == FI_ADDR_NOTAVAIL)
-			ret = rxm_cmap_alloc_handle_peer(cmap, addr,
-							 RXM_CMAP_CONNREQ_RECV,
-							 &handle);
-		else
-			ret = rxm_cmap_alloc_handle(cmap, fi_addr,
-						    RXM_CMAP_CONNREQ_RECV,
-						    &handle);
-		if (ret)
-			goto unlock;
+	ret = fi_connect(conn->msg_ep, info->dest_addr, &cm_data,
+			 sizeof(cm_data));
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to connect msg_ep\n");
+		goto err;
 	}
+	conn->state = RXM_CM_CONNECTING;
+	return 0;
 
-	switch (handle->state) {
-	case RXM_CMAP_CONNECTED:
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Connection already present.\n");
-		ret = -FI_EALREADY;
-		break;
-	case RXM_CMAP_CONNREQ_SENT:
-		ofi_straddr_dbg(cmap->av->prov, FI_LOG_EP_CTRL, "local_name",
-				cmap->attr.name);
-		ofi_straddr_dbg(cmap->av->prov, FI_LOG_EP_CTRL, "remote_name",
-				addr);
-
-		cmp = ofi_addr_cmp(cmap->av->prov, addr, cmap->attr.name);
-
-		if (cmp < 0) {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-				"Remote name lower than local name.\n");
-			*reject_reason = RXM_CMAP_REJECT_SIMULT_CONN;
-			ret = -FI_EALREADY;
-			break;
-		} else if (cmp > 0) {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-				"Re-using handle: %p to accept remote "
-				"connection\n", handle);
-			*reject_reason = RXM_CMAP_REJECT_GENUINE;
-			rxm_conn_close(handle);
-		} else {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-				"Endpoint connects to itself\n");
-			ret = rxm_cmap_alloc_handle_peer(cmap, addr,
-							  RXM_CMAP_CONNREQ_RECV,
-							  &handle);
-			if (ret)
-				goto unlock;
-
-			assert(fi_addr != FI_ADDR_NOTAVAIL);
-			handle->fi_addr = fi_addr;
-		}
-		/* Fall through */
-	case RXM_CMAP_IDLE:
-		RXM_CM_UPDATE_STATE(handle, RXM_CMAP_CONNREQ_RECV);
-		/* Fall through */
-	case RXM_CMAP_CONNREQ_RECV:
-		*handle_ret = handle;
-		break;
-	case RXM_CMAP_SHUTDOWN:
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL, "handle :%p marked for "
-			"deletion / shutdown, reject connection\n", handle);
-		*reject_reason = RXM_CMAP_REJECT_GENUINE;
-		ret = -FI_EOPBADSTATE;
-		break;
-	default:
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-		       "invalid handle state: %d\n", handle->state);
-		assert(0);
-		ret = -FI_EOPBADSTATE;
-	}
-unlock:
+err:
+	fi_close(&conn->msg_ep->fid);
+	conn->msg_ep = NULL;
 	return ret;
 }
 
-int rxm_msg_eq_progress(struct rxm_ep *rxm_ep)
+static int rxm_connect(struct rxm_conn *conn)
 {
-	struct rxm_msg_eq_entry *entry;
 	int ret;
 
-	entry = alloca(RXM_MSG_EQ_ENTRY_SZ);
-	if (!entry) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to allocate memory!\n");
-		return -FI_ENOMEM;
-	}
-
-	while (1) {
-		entry->rd = rxm_eq_read(rxm_ep, RXM_MSG_EQ_ENTRY_SZ, entry);
-		if (entry->rd < 0 && entry->rd != -FI_ECONNREFUSED) {
-			ret = (int) entry->rd;
-			break;
-		}
-		ret = rxm_conn_handle_event(rxm_ep, entry);
-		if (ret) {
-			FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-			       "invalid connection handle event: %d\n", ret);
-			break;
-		}
-	}
-	return ret;
-}
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
 
-int rxm_cmap_connect(struct rxm_ep *rxm_ep, fi_addr_t fi_addr,
-		     struct rxm_cmap_handle *handle)
-{
-	int ret = FI_SUCCESS;
-
-	switch (handle->state) {
-	case RXM_CMAP_IDLE:
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "initiating MSG_EP connect "
-		       "for fi_addr: %" PRIu64 "\n", fi_addr);
-		ret = rxm_conn_connect(rxm_ep, handle,
-				       ofi_av_get_addr(rxm_ep->cmap->av, fi_addr));
-		if (ret) {
-			if (ret == -FI_ECONNREFUSED)
-				return -FI_EAGAIN;
-
-			rxm_cmap_del_handle(handle);
-		} else {
-			RXM_CM_UPDATE_STATE(handle, RXM_CMAP_CONNREQ_SENT);
-			ret = -FI_EAGAIN;
-		}
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		ret = rxm_send_connect(conn);
+		if (ret)
+			return ret;
 		break;
-	case RXM_CMAP_CONNREQ_SENT:
-	case RXM_CMAP_CONNREQ_RECV:
-	case RXM_CMAP_SHUTDOWN:
-		ret = -FI_EAGAIN;
+	case RXM_CM_CONNECTING:
+	case RXM_CM_ACCEPTING:
 		break;
+	case RXM_CM_CONNECTED:
+		return 0;
 	default:
-		FI_WARN(rxm_ep->cmap->av->prov, FI_LOG_EP_CTRL,
-			"Invalid cmap handle state\n");
 		assert(0);
-		ret = -FI_EOPBADSTATE;
-	}
-	if (ret == -FI_EAGAIN)
-		rxm_msg_eq_progress(rxm_ep);
-
-	return ret;
-}
-
-static int rxm_cmap_cm_thread_close(struct rxm_cmap *cmap)
-{
-	int ret;
-
-	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "stopping CM thread\n");
-	if (!cmap->cm_thread)
-		return 0;
-
-	ofi_ep_lock_acquire(&cmap->ep->util_ep);
-	cmap->ep->do_progress = false;
-	ofi_ep_lock_release(&cmap->ep->util_ep);
-	ret = rxm_conn_signal(cmap->ep, NULL, RXM_CMAP_EXIT);
-	if (ret) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Unable to signal CM thread\n");
-		return ret;
-	}
-	ret = pthread_join(cmap->cm_thread, NULL);
-	if (ret) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Unable to join CM thread\n");
-		return ret;
-	}
-	return 0;
-}
-
-void rxm_cmap_free(struct rxm_cmap *cmap)
-{
-	struct rxm_cmap_peer *peer;
-	struct dlist_entry *entry;
-	size_t i;
-
-	FI_INFO(cmap->av->prov, FI_LOG_EP_CTRL, "Closing cmap\n");
-	rxm_cmap_cm_thread_close(cmap);
-
-	for (i = 0; i < cmap->num_allocated; i++) {
-		if (cmap->handles_av[i]) {
-			rxm_cmap_clear_key(cmap->handles_av[i]);
-			rxm_conn_free(cmap->handles_av[i]);
-		}
-	}
-
-	while (!dlist_empty(&cmap->peer_list)) {
-		entry = cmap->peer_list.next;
-		peer = container_of(entry, struct rxm_cmap_peer, entry);
-		dlist_remove(&peer->entry);
-		rxm_cmap_clear_key(peer->handle);
-		rxm_conn_free(peer->handle);
-		free(peer);
-	}
-
-	free(cmap->handles_av);
-	free(cmap->attr.name);
-	ofi_idx_reset(&cmap->handles_idx);
-	free(cmap);
-}
-
-static int
-rxm_cmap_update_addr(struct util_av *av, void *addr,
-		     fi_addr_t fi_addr, void *arg)
-{
-	return rxm_cmap_update((struct rxm_cmap *)arg, addr, fi_addr);
-}
-
-int rxm_cmap_bind_to_av(struct rxm_cmap *cmap, struct util_av *av)
-{
-	cmap->av = av;
-	return ofi_av_elements_iter(av, rxm_cmap_update_addr, (void *)cmap);
-}
-
-int rxm_cmap_alloc(struct rxm_ep *rxm_ep, struct rxm_cmap_attr *attr)
-{
-	struct rxm_cmap *cmap;
-	struct util_ep *ep = &rxm_ep->util_ep;
-	int ret;
-
-	cmap = calloc(1, sizeof *cmap);
-	if (!cmap)
-		return -FI_ENOMEM;
-
-	cmap->ep = rxm_ep;
-	cmap->av = ep->av;
-
-	cmap->handles_av = calloc(ofi_av_size(ep->av), sizeof(*cmap->handles_av));
-	if (!cmap->handles_av) {
-		ret = -FI_ENOMEM;
-		goto err1;
-	}
-	cmap->num_allocated = ofi_av_size(ep->av);
-
-	cmap->attr = *attr;
-	cmap->attr.name = mem_dup(attr->name, ep->av->addrlen);
-	if (!cmap->attr.name) {
-		ret = -FI_ENOMEM;
-		goto err2;
-	}
-
-	memset(&cmap->handles_idx, 0, sizeof(cmap->handles_idx));
-	ofi_key_idx_init(&cmap->key_idx, RXM_CMAP_IDX_BITS);
-
-	dlist_init(&cmap->peer_list);
-
-	rxm_ep->cmap = cmap;
-
-	if (ep->domain->data_progress == FI_PROGRESS_AUTO || force_auto_progress) {
-
-		assert(ep->domain->threading == FI_THREAD_SAFE);
-		rxm_ep->do_progress = true;
-		if (pthread_create(&cmap->cm_thread, 0,
-				   rxm_ep->rxm_info->caps & FI_ATOMIC ?
-				   rxm_conn_atomic_progress :
-				   rxm_conn_progress, ep)) {
-			FI_WARN(ep->av->prov, FI_LOG_EP_CTRL,
-				"unable to create cmap thread\n");
-			ret = -ofi_syserr();
-			goto err3;
-		}
+		conn->state = RXM_CM_IDLE;
+		break;
 	}
 
-	assert(ep->av);
-	ret = rxm_cmap_bind_to_av(cmap, ep->av);
-	if (ret)
-		goto err4;
-
-	return FI_SUCCESS;
-err4:
-	rxm_cmap_cm_thread_close(cmap);
-err3:
-	rxm_ep->cmap = NULL;
-	free(cmap->attr.name);
-err2:
-	free(cmap->handles_av);
-err1:
-	free(cmap);
-	return ret;
+	return -FI_EAGAIN;
 }
 
-static int rxm_msg_ep_open(struct rxm_ep *rxm_ep, struct fi_info *msg_info,
-			   struct rxm_conn *rxm_conn, void *context)
+static void rxm_free_conn(struct rxm_conn *conn)
 {
-	struct rxm_domain *rxm_domain;
-	struct fid_ep *msg_ep;
-	int ret;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "free conn %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
 
-	rxm_domain = container_of(rxm_ep->util_ep.domain, struct rxm_domain,
-			util_domain);
+	if (conn->flags & RXM_CONN_INDEXED)
+		ofi_idm_clear(&conn->ep->conn_idx_map, conn->peer->index);
 
-	ret = fi_endpoint(rxm_domain->msg_domain, msg_info, &msg_ep, context);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to create msg_ep: %d\n", ret);
-		return ret;
-	}
-
-	ret = fi_ep_bind(msg_ep, &rxm_ep->msg_eq->fid, 0);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to bind msg EP to EQ: %d\n", ret);
-		goto err;
-	}
-
-	if (rxm_ep->srx_ctx) {
-		ret = fi_ep_bind(msg_ep, &rxm_ep->srx_ctx->fid, 0);
-		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to bind msg "
-				"EP to shared RX ctx: %d\n", ret);
-			goto err;
-		}
-	}
-
-	// TODO add other completion flags
-	ret = fi_ep_bind(msg_ep, &rxm_ep->msg_cq->fid, FI_TRANSMIT | FI_RECV);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"unable to bind msg_ep to msg_cq: %d\n", ret);
-		goto err;
-	}
-
-	ret = fi_enable(msg_ep);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to enable msg_ep: %d\n", ret);
-		goto err;
-	}
-
-	ret = rxm_domain->flow_ctrl_ops->enable(msg_ep);
-	if (!ret) {
-		rxm_domain->flow_ctrl_ops->set_threshold(
-			msg_ep, rxm_ep->msg_info->rx_attr->size / 2);
-	}
-
-	if (!rxm_ep->srx_ctx) {
-		ret = rxm_prepost_recv(rxm_ep, msg_ep);
-		if (ret)
-			goto err;
-	}
-
-	rxm_conn->msg_ep = msg_ep;
-	return 0;
-err:
-	fi_close(&msg_ep->fid);
-	return ret;
+	rxm_put_peer(conn->peer);
+	rxm_av_free_conn(conn);
 }
 
-static int rxm_conn_reprocess_directed_recvs(struct rxm_recv_queue *recv_queue)
+void rxm_freeall_conns(struct rxm_ep *ep)
 {
-	struct rxm_rx_buf *rx_buf;
-	struct dlist_entry *entry, *tmp_entry;
-	struct rxm_recv_match_attr match_attr;
-	struct fi_cq_err_entry err_entry = {0};
-	int ret, count = 0;
-
-	dlist_foreach_container_safe(&recv_queue->unexp_msg_list,
-				     struct rxm_rx_buf, rx_buf,
-				     unexp_msg.entry, tmp_entry) {
-		if (rx_buf->unexp_msg.addr == rx_buf->conn->handle.fi_addr)
-			continue;
-
-		assert(rx_buf->unexp_msg.addr == FI_ADDR_NOTAVAIL);
+	struct rxm_conn *conn;
+	struct dlist_entry *tmp;
+	struct rxm_av *av;
+	int i, cnt;
 
-		rx_buf->unexp_msg.addr = rx_buf->conn->handle.fi_addr;
-		match_attr.addr = rx_buf->unexp_msg.addr;
-		match_attr.tag = rx_buf->unexp_msg.tag;
+	av = container_of(ep->util_ep.av, struct rxm_av, util_av);
+	ofi_ep_lock_acquire(&ep->util_ep);
 
-		entry = dlist_remove_first_match(&recv_queue->recv_list,
-						 recv_queue->match_recv,
-						 &match_attr);
-		if (!entry)
+	/* We can't have more connections than the current number of
+	 * possible peers.
+	 */
+	cnt = (int) rxm_av_max_peers(av);
+	for (i = 0; i < cnt; i++) {
+		conn = ofi_idm_lookup(&ep->conn_idx_map, i);
+		if (!conn)
 			continue;
 
-		dlist_remove(&rx_buf->unexp_msg.entry);
-		rx_buf->recv_entry = container_of(entry, struct rxm_recv_entry,
-						  entry);
+		if (conn->state != RXM_CM_IDLE)
+			rxm_close_conn(conn);
+		rxm_free_conn(conn);
+	}
 
-		ret = rxm_handle_rx_buf(rx_buf);
-		if (ret) {
-			err_entry.op_context = rx_buf;
-			err_entry.flags = rx_buf->recv_entry->comp_flags;
-			err_entry.len = rx_buf->pkt.hdr.size;
-			err_entry.data = rx_buf->pkt.hdr.data;
-			err_entry.tag = rx_buf->pkt.hdr.tag;
-			err_entry.err = ret;
-			err_entry.prov_errno = ret;
-			ofi_cq_write_error(recv_queue->rxm_ep->util_ep.rx_cq,
-					   &err_entry);
-			if (rx_buf->ep->util_ep.flags & OFI_CNTR_ENABLED)
-				rxm_cntr_incerr(rx_buf->ep->util_ep.rx_cntr);
-
-			rxm_rx_buf_free(rx_buf);
-
-			if (!(rx_buf->recv_entry->flags & FI_MULTI_RECV))
-				rxm_recv_entry_release(rx_buf->recv_entry);
-		}
-		count++;
+	dlist_foreach_container_safe(&ep->loopback_list, struct rxm_conn,
+				     conn, loopback_entry, tmp) {
+		rxm_close_conn(conn);
+		rxm_free_conn(conn);
 	}
-	return count;
+
+	ofi_ep_lock_release(&ep->util_ep);
 }
 
-static void
-rxm_conn_av_updated_handler(struct rxm_cmap_handle *handle)
+static struct rxm_conn *
+rxm_alloc_conn(struct rxm_ep *ep, struct rxm_peer_addr *peer)
 {
-	struct rxm_ep *ep = handle->cmap->ep;
-	int count = 0;
+	struct rxm_conn *conn;
+	struct rxm_av *av;
+
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	av = container_of(ep->util_ep.av, struct rxm_av, util_av);
+	conn = rxm_av_alloc_conn(av);
+	if (!conn)
+		return NULL;
 
-	if (ep->rxm_info->caps & FI_DIRECTED_RECV) {
-		count += rxm_conn_reprocess_directed_recvs(&ep->recv_queue);
-		count += rxm_conn_reprocess_directed_recvs(&ep->trecv_queue);
+	conn->ep = ep;
+	conn->state = RXM_CM_IDLE;
+	conn->remote_index = -1;
+	conn->flags = 0;
+	dlist_init(&conn->deferred_entry);
+	dlist_init(&conn->deferred_tx_queue);
+	dlist_init(&conn->deferred_sar_msgs);
+	dlist_init(&conn->deferred_sar_segments);
+	dlist_init(&conn->loopback_entry);
 
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-		       "Reprocessed directed recvs - %d\n", count);
-	}
+	conn->peer = peer;
+	rxm_ref_peer(peer);
+
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "allocated conn %p\n", conn);
+	return conn;
 }
 
-static struct rxm_cmap_handle *rxm_conn_alloc(struct rxm_cmap *cmap)
+static struct rxm_conn *
+rxm_add_conn(struct rxm_ep *ep, struct rxm_peer_addr *peer)
 {
-	struct rxm_conn *rxm_conn;
+	struct rxm_conn *conn;
+
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	conn = ofi_idm_lookup(&ep->conn_idx_map, peer->index);
+	if (conn)
+		return conn;
 
-	rxm_conn = calloc(1, sizeof(*rxm_conn));
-	if (!rxm_conn)
+	conn = rxm_alloc_conn(ep, peer);
+	if (!conn)
 		return NULL;
 
-	if (rxm_conn_res_alloc(cmap->ep, rxm_conn)) {
-		free(rxm_conn);
+	if (ofi_idm_set(&ep->conn_idx_map, peer->index, conn) < 0) {
+		rxm_free_conn(conn);
 		return NULL;
 	}
 
-	return &rxm_conn->handle;
+	conn->flags |= RXM_CONN_INDEXED;
+	return conn;
 }
 
-static inline int
-rxm_conn_verify_cm_data(union rxm_cm_data *remote_cm_data,
-			union rxm_cm_data *local_cm_data)
+/* The returned conn is only valid if the function returns success. */
+ssize_t rxm_get_conn(struct rxm_ep *ep, fi_addr_t addr, struct rxm_conn **conn)
 {
-	/* This should stay at top as it helps to avoid endian conversion
-	 * for other fields in rxm_cm_data */
-	if (remote_cm_data->connect.version != local_cm_data->connect.version) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data version mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.version,
-			remote_cm_data->connect.version);
-		goto err;
-	}
-	if (remote_cm_data->connect.endianness != local_cm_data->connect.endianness) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data endianness mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.endianness,
-			remote_cm_data->connect.endianness);
-		goto err;
-	}
-	if (remote_cm_data->connect.ctrl_version != local_cm_data->connect.ctrl_version) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data ctrl_version mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.ctrl_version,
-			remote_cm_data->connect.ctrl_version);
-		goto err;
-	}
-	if (remote_cm_data->connect.op_version != local_cm_data->connect.op_version) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data op_version mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.op_version,
-			remote_cm_data->connect.op_version);
-		goto err;
-	}
-	if (remote_cm_data->connect.eager_size != local_cm_data->connect.eager_size) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data eager_size mismatch "
-			"(local: %" PRIu32 ", remote:  %" PRIu32 ")\n",
-			local_cm_data->connect.eager_size,
-			remote_cm_data->connect.eager_size);
-		goto err;
+	struct rxm_peer_addr **peer;
+	ssize_t ret;
+
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	peer = ofi_av_addr_context(ep->util_ep.av, addr);
+	*conn = rxm_add_conn(ep, *peer);
+	if (!*conn)
+		return -FI_ENOMEM;
+
+	if ((*conn)->state == RXM_CM_CONNECTED) {
+		if (!dlist_empty(&(*conn)->deferred_tx_queue)) {
+			rxm_ep_do_progress(&ep->util_ep);
+			if (!dlist_empty(&(*conn)->deferred_tx_queue))
+				return -FI_EAGAIN;
+		}
+		return 0;
 	}
-	return FI_SUCCESS;
-err:
-	return -FI_EINVAL;
+
+	ret = rxm_connect(*conn);
+
+	/* If the progress function encounters an error trying to establish
+	 * the connection, it may free the connection object.  This resets
+	 * the connection process to restart from the beginning.
+	 */
+	if (ret == -FI_EAGAIN)
+		rxm_conn_progress(ep);
+	return ret;
 }
 
-static size_t rxm_conn_get_rx_size(struct rxm_ep *rxm_ep,
-				   struct fi_info *msg_info)
+void rxm_process_connect(struct rxm_eq_cm_entry *cm_entry)
 {
-	if (msg_info->ep_attr->rx_ctx_cnt == FI_SHARED_CONTEXT)
-		return MAX(MIN(16, msg_info->rx_attr->size),
-			   (msg_info->rx_attr->size /
-			    ofi_av_size(rxm_ep->util_ep.av)));
-	else
-		return msg_info->rx_attr->size;
+	struct rxm_conn *conn;
+
+	conn = cm_entry->fid->context;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
+	       "processing connected for handle: %p\n", conn);
+
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+	if (conn->state == RXM_CM_CONNECTING)
+		conn->remote_index = cm_entry->data.accept.server_conn_id;
+
+	conn->state = RXM_CM_CONNECTED;
 }
 
-static int
-rxm_msg_process_connreq(struct rxm_ep *rxm_ep, struct fi_info *msg_info,
-			union rxm_cm_data *remote_cm_data)
+/* For simultaneous connection requests, if the peer won the coin
+ * flip (reject EALREADY), our connection request is discarded.
+ */
+static void
+rxm_process_reject(struct rxm_conn *conn, struct fi_eq_err_entry *entry)
 {
-	struct rxm_conn *rxm_conn;
-	union rxm_cm_data cm_data = {
-		.connect = {
-			.version = RXM_CM_DATA_VERSION,
-			.endianness = ofi_detect_endianness(),
-			.ctrl_version = RXM_CTRL_VERSION,
-			.op_version = RXM_OP_VERSION,
-			.eager_size = rxm_eager_limit,
-		},
-	};
-	union rxm_cm_data reject_cm_data = {
-		.reject = {
-			.version = RXM_CM_DATA_VERSION,
-			.reason = RXM_CMAP_REJECT_GENUINE,
+	union rxm_cm_data *cm_data;
+	uint8_t reason;
+
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
+	       "Processing reject for handle: %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+
+	if (entry->err_data_size >= sizeof(cm_data->reject)) {
+		cm_data = entry->err_data;
+		if (cm_data->reject.version != RXM_CM_DATA_VERSION) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "invalid reject version\n");
+			reason = RXM_REJECT_ECONNREFUSED;
+		} else {
+			reason = cm_data->reject.reason;
 		}
-	};
-	struct rxm_cmap_handle *handle;
-	struct sockaddr_storage remote_pep_addr;
-	int ret;
+	} else {
+		reason = RXM_REJECT_ECONNREFUSED;
+	}
 
-	assert(sizeof(uint32_t) == sizeof(cm_data.accept.rx_size));
-	assert(msg_info->rx_attr->size <= (uint32_t)-1);
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		/* Unlikely, but can occur if our request was rejected, and
+		 * there was a failure trying to accept the peer's.
+		 */
+		break;
+	case RXM_CM_CONNECTING:
+		rxm_close_conn(conn);
+		if (reason != RXM_REJECT_EALREADY)
+			rxm_free_conn(conn);
+		break;
+	case RXM_CM_ACCEPTING:
+	case RXM_CM_CONNECTED:
+		/* Our request was rejected, but we accepted the peer's. */
+		break;
+	default:
+		assert(0);
+		break;
+	}
+}
 
-	if (rxm_conn_verify_cm_data(remote_cm_data, &cm_data)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"CM data mismatch was detected\n");
-		ret = -FI_EINVAL;
-		goto err1;
+static int
+rxm_verify_connreq(struct rxm_ep *ep, union rxm_cm_data *cm_data)
+{
+	if (cm_data->connect.version != RXM_CM_DATA_VERSION) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm version mismatch");
+		return -FI_EINVAL;
 	}
 
-	memcpy(&remote_pep_addr, msg_info->dest_addr, msg_info->dest_addrlen);
-	ofi_addr_set_port((struct sockaddr *)&remote_pep_addr,
-			  remote_cm_data->connect.port);
+	if (cm_data->connect.endianness != ofi_detect_endianness()) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "endianness mismatch");
+		return -FI_EINVAL;
+	}
 
-	ret = rxm_cmap_process_connreq(rxm_ep->cmap, &remote_pep_addr,
-				       &handle, &reject_cm_data.reject.reason);
-	if (ret)
-		goto err1;
+	if (cm_data->connect.ctrl_version != RXM_CTRL_VERSION) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm ctrl_version mismatch");
+		return -FI_EINVAL;
+	}
 
-	rxm_conn = container_of(handle, struct rxm_conn, handle);
+	if (cm_data->connect.op_version != RXM_OP_VERSION) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm op_version mismatch");
+		return -FI_EINVAL;
+	}
 
-	rxm_conn->handle.remote_key = remote_cm_data->connect.client_conn_id;
-	rxm_conn->rndv_tx_credits = remote_cm_data->connect.rx_size;
-	assert(rxm_conn->rndv_tx_credits);
+	if (cm_data->connect.eager_limit != ep->eager_limit) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "eager_limit mismatch");
+		return -FI_EINVAL;
+	}
 
-	ret = rxm_msg_ep_open(rxm_ep, msg_info, rxm_conn, handle);
-	if (ret)
-		goto err2;
+	return FI_SUCCESS;
+}
 
-	cm_data.accept.server_conn_id = rxm_conn->handle.key;
-	cm_data.accept.rx_size = rxm_conn_get_rx_size(rxm_ep, msg_info);
+static void
+rxm_reject_connreq(struct rxm_ep *ep, struct rxm_eq_cm_entry *cm_entry,
+		   uint8_t reason)
+{
+	union rxm_cm_data cm_data;
 
-	ret = fi_accept(rxm_conn->msg_ep, &cm_data.accept.server_conn_id,
-			sizeof(cm_data.accept));
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to accept incoming connection\n");
-		goto err2;
-	}
+	cm_data.reject.version = RXM_CM_DATA_VERSION;
+	cm_data.reject.reason = reason;
 
-	return ret;
-err2:
-	rxm_cmap_del_handle(&rxm_conn->handle);
-err1:
-	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-	       "rejecting incoming connection request (reject reason: %d)\n",
-	       (enum rxm_cmap_reject_reason)reject_cm_data.reject.reason);
-	fi_reject(rxm_ep->msg_pep, msg_info->handle,
-		  &reject_cm_data.reject, sizeof(reject_cm_data.reject));
-	return ret;
+	fi_reject(ep->msg_pep, cm_entry->info->handle,
+		  &cm_data.reject, sizeof(cm_data.reject));
 }
 
-static void rxm_flush_msg_cq(struct rxm_ep *rxm_ep)
+static int
+rxm_accept_connreq(struct rxm_conn *conn, struct rxm_eq_cm_entry *cm_entry)
 {
-	struct fi_cq_data_entry comp;
-	int ret;
-	do {
-		ret = fi_cq_read(rxm_ep->msg_cq, &comp, 1);
-		if (ret > 0) {
-			ret = rxm_handle_comp(rxm_ep, &comp);
-			if (OFI_UNLIKELY(ret)) {
-				rxm_cq_write_error_all(rxm_ep, ret);
-			} else {
-				ret = 1;
-			}
-		} else if (ret == -FI_EAVAIL) {
-			rxm_handle_comp_error(rxm_ep);
-			ret = 1;
-		} else if (ret < 0 && ret != -FI_EAGAIN) {
-			rxm_cq_write_error_all(rxm_ep, ret);
-		}
-	} while (ret > 0);
+	union rxm_cm_data cm_data;
+
+	cm_data.accept.server_conn_id = conn->peer->index;
+	cm_data.accept.rx_size = cm_entry->info->rx_attr->size;
+
+	return fi_accept(conn->msg_ep, &cm_data.accept, sizeof(cm_data.accept));
 }
 
-static int rxm_conn_handle_notify(struct fi_eq_entry *eq_entry)
+static void
+rxm_process_connreq(struct rxm_ep *ep, struct rxm_eq_cm_entry *cm_entry)
 {
-	struct rxm_cmap *cmap;
-	struct rxm_cmap_handle *handle;
+	union ofi_sock_ip peer_addr;
+	struct rxm_peer_addr *peer;
+	struct rxm_conn *conn;
+	struct rxm_av *av;
+	ssize_t ret;
+	int cmp;
 
-	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "notify event %" PRIu64 "\n",
-		eq_entry->data);
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	if (rxm_verify_connreq(ep, &cm_entry->data))
+		goto reject;
 
-	if ((enum rxm_cmap_signal) eq_entry->data != RXM_CMAP_FREE)
-		return -FI_EOTHER;
+	memcpy(&peer_addr, cm_entry->info->dest_addr,
+	       cm_entry->info->dest_addrlen);
+	ofi_addr_set_port(&peer_addr.sa, cm_entry->data.connect.port);
 
-	handle = eq_entry->context;
-	assert(handle->state == RXM_CMAP_SHUTDOWN);
-	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "freeing handle: %p\n", handle);
-	cmap = handle->cmap;
+	av = container_of(ep->util_ep.av, struct rxm_av, util_av);
+	peer = rxm_get_peer(av, &peer_addr);
+	if (!peer)
+		goto reject;
 
-	rxm_conn_close(handle);
+	conn = rxm_add_conn(ep, peer);
+	if (!conn)
+		goto remove;
 
-	// after closing the connection, we need to flush any dangling references to the
-	// handle from msg_cq entries that have not been cleaned up yet, otherwise we
-	// could run into problems during CQ cleanup.  these entries will be errored so
-	// keep reading through EAVAIL.
-	rxm_flush_msg_cq(cmap->ep);
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connreq for %p\n", conn);
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		break;
+	case RXM_CM_CONNECTING:
+		/* simultaneous connections */
+		cmp = ofi_addr_cmp(&rxm_prov, &peer_addr.sa, &ep->addr.sa);
+		if (cmp < 0) {
+			/* let our request finish */
+			rxm_reject_connreq(ep, cm_entry,
+					   RXM_REJECT_ECONNREFUSED);
+			goto put;
+		} else if (cmp > 0) {
+			/* accept peer's request */
+			rxm_close_conn(conn);
+		} else {
+			/* connecting to ourself, create loopback conn */
+			conn = rxm_alloc_conn(ep, peer);
+			if (!conn)
+				goto remove;
 
-	if (handle->peer) {
-		dlist_remove(&handle->peer->entry);
-		free(handle->peer);
-		handle->peer = NULL;
-	} else {
-		cmap->handles_av[handle->fi_addr] = NULL;
+			dlist_insert_tail(&conn->loopback_entry, &ep->loopback_list);
+			break;
+		}
+		break;
+	case RXM_CM_ACCEPTING:
+	case RXM_CM_CONNECTED:
+		goto put;
+	default:
+		assert(0);
+		break;
 	}
-	rxm_conn_free(handle);
-	return 0;
+
+	conn->remote_index = cm_entry->data.connect.client_conn_id;
+	ret = rxm_open_conn(conn, cm_entry->info);
+	if (ret)
+		goto free;
+
+	ret = rxm_accept_connreq(conn, cm_entry);
+	if (ret)
+		goto close;
+
+	conn->state = RXM_CM_ACCEPTING;
+put:
+	rxm_put_peer(peer);
+	fi_freeinfo(cm_entry->info);
+	return;
+
+close:
+	rxm_close_conn(conn);
+free:
+	rxm_free_conn(conn);
+remove:
+	rxm_put_peer(peer);
+reject:
+	rxm_reject_connreq(ep, cm_entry, RXM_REJECT_ECONNREFUSED);
+	fi_freeinfo(cm_entry->info);
 }
 
-static void rxm_conn_wake_up_wait_obj(struct rxm_ep *rxm_ep)
+void rxm_process_shutdown(struct rxm_conn *conn)
 {
-	if (rxm_ep->util_ep.tx_cq && rxm_ep->util_ep.tx_cq->wait)
-		util_cq_signal(rxm_ep->util_ep.tx_cq);
-	if (rxm_ep->util_ep.tx_cntr && rxm_ep->util_ep.tx_cntr->wait)
-		util_cntr_signal(rxm_ep->util_ep.tx_cntr);
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "shutdown conn %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		break;
+	case RXM_CM_CONNECTING:
+	case RXM_CM_ACCEPTING:
+	case RXM_CM_CONNECTED:
+		rxm_close_conn(conn);
+		rxm_free_conn(conn);
+		break;
+	default:
+		break;
+	}
 }
 
-static int
-rxm_conn_handle_reject(struct rxm_ep *rxm_ep, struct rxm_msg_eq_entry *entry)
+static void rxm_handle_error(struct rxm_ep *ep)
 {
-	union rxm_cm_data *cm_data = entry->err_entry.err_data;
-
-	if (!cm_data || entry->err_entry.err_data_size != sizeof(cm_data->reject)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-			"no reject error data (cm_data) was found "
-			"(data length expected: %zu found: %zu)\n",
-			sizeof(cm_data->reject),
-			entry->err_entry.err_data_size);
-		return -FI_EOTHER;
-	}
+	struct fi_eq_err_entry entry = {0};
+	ssize_t ret;
 
-	if (cm_data->reject.version != RXM_CM_DATA_VERSION) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-			"cm data version mismatch (local: %" PRIu8
-			", remote:  %" PRIu8 ")\n",
-			(uint8_t) RXM_CM_DATA_VERSION,
-			cm_data->reject.version);
-		return -FI_EOTHER;
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	ret = fi_eq_readerr(ep->msg_eq, &entry, 0);
+	if (ret != sizeof(entry)) {
+		if (ret != -FI_EAGAIN)
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"unable to fi_eq_readerr: %zd\n", ret);
+		return;
 	}
 
-	if (cm_data->reject.reason == RXM_CMAP_REJECT_GENUINE) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		       "remote peer didn't accept the connection\n");
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		       "(reason: RXM_CMAP_REJECT_GENUINE)\n");
-		OFI_EQ_STRERROR(&rxm_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
-				rxm_ep->msg_eq, &entry->err_entry);
-	} else if (cm_data->reject.reason == RXM_CMAP_REJECT_SIMULT_CONN) {
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		       "(reason: RXM_CMAP_REJECT_SIMULT_CONN)\n");
+	if (entry.err == ECONNREFUSED) {
+		rxm_process_reject(entry.fid->context, &entry);
 	} else {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		        "received unknown reject reason: %d\n",
-			cm_data->reject.reason);
+		OFI_EQ_STRERROR(&rxm_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
+				ep->msg_eq, &entry);
 	}
-	rxm_cmap_process_reject(rxm_ep->cmap, entry->context,
-				cm_data->reject.reason);
-	return 0;
 }
 
-static int
-rxm_conn_handle_event(struct rxm_ep *rxm_ep, struct rxm_msg_eq_entry *entry)
+static void
+rxm_handle_event(struct rxm_ep *ep, uint32_t event,
+		 struct rxm_eq_cm_entry *cm_entry, size_t len)
 {
-	if (entry->rd == -FI_ECONNREFUSED)
-		return rxm_conn_handle_reject(rxm_ep, entry);
-
-	switch (entry->event) {
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	switch (event) {
 	case FI_NOTIFY:
-		return rxm_conn_handle_notify((struct fi_eq_entry *)
-					      &entry->cm_entry);
+		break;
 	case FI_CONNREQ:
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "Got new connection\n");
-		if ((size_t)entry->rd != RXM_CM_ENTRY_SZ) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"Received a connection request with no CM data. "
-				"Is sender running FI_PROTO_RXM?\n");
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Received CM entry "
-				"size (%zd) not matching expected (%zu)\n",
-				entry->rd, RXM_CM_ENTRY_SZ);
-			return -FI_EOTHER;
-		}
-		rxm_msg_process_connreq(rxm_ep, entry->cm_entry.info,
-					(union rxm_cm_data *) entry->cm_entry.data);
-		fi_freeinfo(entry->cm_entry.info);
+		rxm_process_connreq(ep, cm_entry);
 		break;
 	case FI_CONNECTED:
-		assert(entry->cm_entry.fid->context);
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-		       "connection successful\n");
-		rxm_cmap_process_connect(rxm_ep->cmap,
-			entry->cm_entry.fid->context,
-			entry->rd - sizeof(entry->cm_entry) > 0 ?
-			(union rxm_cm_data *) entry->cm_entry.data : NULL);
-		rxm_conn_wake_up_wait_obj(rxm_ep);
+		rxm_process_connect(cm_entry);
 		break;
 	case FI_SHUTDOWN:
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-		       "Received connection shutdown\n");
-		rxm_cmap_process_shutdown(rxm_ep->cmap,
-					  entry->cm_entry.fid->context);
+		rxm_process_shutdown(cm_entry->fid->context);
 		break;
 	default:
 		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unknown event: %u\n", entry->event);
-		return -FI_EOTHER;
+			"Unknown event: %u\n", event);
+		break;
 	}
-	return 0;
 }
 
-static ssize_t rxm_eq_sread(struct rxm_ep *rxm_ep, size_t len,
-			    struct rxm_msg_eq_entry *entry)
+void rxm_conn_progress(struct rxm_ep *ep)
 {
-	ssize_t rd;
-	int once = 1;
+	struct rxm_eq_cm_entry cm_entry;
+	uint32_t event;
+	int ret;
 
+	assert(ofi_ep_lock_held(&ep->util_ep));
 	do {
-		/* TODO convert this to poll + fi_eq_read so that we can grab
-		 * rxm_ep lock before reading the EQ. This is needed to avoid
-		 * processing events / error entries from closed MSG EPs. This
-		 * can be done only for non-Windows OSes as Windows doesn't
-		 * have poll for a generic file descriptor. */
-		rd = fi_eq_sread(rxm_ep->msg_eq, &entry->event, &entry->cm_entry,
-				 len, -1, 0);
-		if (rd >= 0)
-			return rd;
-		if (rd == -FI_EINTR && once) {
-			FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "Ignoring EINTR\n");
-			once = 0;
+		ret = fi_eq_read(ep->msg_eq, &event, &cm_entry,
+				 sizeof(cm_entry), 0);
+		if (ret > 0) {
+			rxm_handle_event(ep, event, &cm_entry, ret);
+		} else if (ret == -FI_EAVAIL) {
+			rxm_handle_error(ep);
+			ret = 1;
 		}
-	} while (rd == -FI_EINTR);
+	} while (ret > 0);
+}
 
-	if (rd != -FI_EAVAIL) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to fi_eq_sread: %s (%zd)\n",
-			fi_strerror(-rd), -rd);
-		return rd;
+void rxm_stop_listen(struct rxm_ep *ep)
+{
+	struct fi_eq_entry entry = {0};
+	int ret;
+
+	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "stopping CM thread\n");
+	if (!ep->cm_thread)
+		return;
+
+	ofi_ep_lock_acquire(&ep->util_ep);
+	ep->do_progress = false;
+	ofi_ep_lock_release(&ep->util_ep);
+
+	ret = fi_eq_write(ep->msg_eq, FI_NOTIFY, &entry, sizeof(entry), 0);
+	if (ret != sizeof(entry)) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to signal\n");
+		return;
 	}
 
-	ofi_ep_lock_acquire(&rxm_ep->util_ep);
-	rd = rxm_eq_readerr(rxm_ep, entry);
-	ofi_ep_lock_release(&rxm_ep->util_ep);
-	return rd;
+	ret = pthread_join(ep->cm_thread, NULL);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"Unable to join CM thread\n");
+	}
 }
 
-static inline int rxm_conn_eq_event(struct rxm_ep *rxm_ep,
-				    struct rxm_msg_eq_entry *entry)
+static void rxm_flush_msg_cq(struct rxm_ep *ep)
 {
+	struct fi_cq_data_entry comp;
 	int ret;
 
-	ofi_ep_lock_acquire(&rxm_ep->util_ep);
-	ret = rxm_conn_handle_event(rxm_ep, entry) ? -1 : 0;
-	ofi_ep_lock_release(&rxm_ep->util_ep);
-
-	return ret;
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	do {
+		ret = fi_cq_read(ep->msg_cq, &comp, 1);
+		if (ret > 0) {
+			ret = rxm_handle_comp(ep, &comp);
+			if (ret) {
+				rxm_cq_write_error_all(ep, ret);
+			} else {
+				ret = 1;
+			}
+		} else if (ret == -FI_EAVAIL) {
+			rxm_handle_comp_error(ep);
+			ret = 1;
+		} else if (ret < 0 && ret != -FI_EAGAIN) {
+			rxm_cq_write_error_all(ep, ret);
+		}
+	} while (ret > 0);
 }
 
-static void *rxm_conn_progress(void *arg)
+static void *rxm_cm_progress(void *arg)
 {
 	struct rxm_ep *ep = container_of(arg, struct rxm_ep, util_ep);
-	struct rxm_msg_eq_entry *entry;
-
-	entry = alloca(RXM_MSG_EQ_ENTRY_SZ);
-	if (!entry)
-		return NULL;
+	struct rxm_eq_cm_entry cm_entry;
+	uint32_t event;
+	ssize_t ret;
 
 	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "Starting auto-progress thread\n");
 
 	ofi_ep_lock_acquire(&ep->util_ep);
 	while (ep->do_progress) {
 		ofi_ep_lock_release(&ep->util_ep);
-		memset(entry, 0, RXM_MSG_EQ_ENTRY_SZ);
-		entry->rd = rxm_eq_sread(ep, RXM_CM_ENTRY_SZ, entry);
-		if (entry->rd >= 0 || entry->rd == -FI_ECONNREFUSED)
-			rxm_conn_eq_event(ep, entry);
+
+		ret = fi_eq_sread(ep->msg_eq, &event, &cm_entry,
+				  sizeof(cm_entry), -1, 0);
 
 		ofi_ep_lock_acquire(&ep->util_ep);
+		if (ret > 0) {
+			rxm_handle_event(ep, event, &cm_entry, ret);
+		} else if (ret == -FI_EAVAIL) {
+			rxm_handle_error(ep);
+		} else {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"Fatal error reading from msg eq");
+			break;
+		}
 	}
 	ofi_ep_lock_release(&ep->util_ep);
 
@@ -1366,27 +760,9 @@ static void *rxm_conn_progress(void *arg)
 	return NULL;
 }
 
-static inline int
-rxm_conn_auto_progress_eq(struct rxm_ep *rxm_ep, struct rxm_msg_eq_entry *entry)
-{
-	memset(entry, 0, RXM_MSG_EQ_ENTRY_SZ);
-
-	ofi_ep_lock_acquire(&rxm_ep->util_ep);
-	entry->rd = rxm_eq_read(rxm_ep, RXM_CM_ENTRY_SZ, entry);
-	ofi_ep_lock_release(&rxm_ep->util_ep);
-
-	if (!entry->rd || entry->rd == -FI_EAGAIN)
-		return FI_SUCCESS;
-	if (entry->rd < 0 && entry->rd != -FI_ECONNREFUSED)
-		return entry->rd;
-
-	return rxm_conn_eq_event(rxm_ep, entry);
-}
-
-static void *rxm_conn_atomic_progress(void *arg)
+static void *rxm_cm_atomic_progress(void *arg)
 {
 	struct rxm_ep *ep = container_of(arg, struct rxm_ep, util_ep);
-	struct rxm_msg_eq_entry *entry;
 	struct rxm_fabric *fabric;
 	struct fid *fids[2] = {
 		&ep->msg_eq->fid,
@@ -1398,13 +774,8 @@ static void *rxm_conn_atomic_progress(void *arg)
 	};
 	int ret;
 
-	entry = alloca(RXM_MSG_EQ_ENTRY_SZ);
-	if (!entry)
-		return NULL;
-
 	fabric = container_of(ep->util_ep.domain->fabric,
 			      struct rxm_fabric, util_fabric);
-
 	ret = fi_control(&ep->msg_eq->fid, FI_GETWAIT, &fds[0].fd);
 	if (ret) {
 		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
@@ -1426,163 +797,72 @@ static void *rxm_conn_atomic_progress(void *arg)
 		ret = fi_trywait(fabric->msg_fabric, fids, 2);
 
 		if (!ret) {
-			fds[0].revents = 0;
-			fds[1].revents = 0;
-
 			ret = poll(fds, 2, -1);
-			if (ret == -1 && errno != EINTR) {
+			if (ret == -1) {
 				FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-					"Select error %s, closing CM thread\n",
-					strerror(errno));
-				goto out;
+					"Select error %s\n", strerror(errno));
 			}
 		}
-		rxm_conn_auto_progress_eq(ep, entry);
 		ep->util_ep.progress(&ep->util_ep);
 		ofi_ep_lock_acquire(&ep->util_ep);
+		rxm_conn_progress(ep);
 	}
 	ofi_ep_lock_release(&ep->util_ep);
 
-out:
 	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "Stopping auto progress thread\n");
 	return NULL;
 }
 
-static int rxm_prepare_cm_data(struct fid_pep *pep, struct rxm_cmap_handle *handle,
-		union rxm_cm_data *cm_data)
+int rxm_start_listen(struct rxm_ep *ep)
 {
-	struct sockaddr_storage name;
-	size_t cm_data_size = 0;
-	size_t name_size = sizeof(name);
-	size_t opt_size = sizeof(cm_data_size);
+	size_t addr_len;
 	int ret;
 
-	ret = fi_getopt(&pep->fid, FI_OPT_ENDPOINT, FI_OPT_CM_DATA_SIZE,
-			&cm_data_size, &opt_size);
+	ret = fi_listen(ep->msg_pep);
 	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "fi_getopt failed\n");
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to set msg PEP to listen state\n");
 		return ret;
 	}
 
-	if (cm_data_size < sizeof(*cm_data)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "MSG EP CM data size too small\n");
-		return -FI_EOTHER;
-	}
-
-	ret = fi_getname(&pep->fid, &name, &name_size);
+	addr_len = sizeof(ep->addr);
+	ret = fi_getname(&ep->msg_pep->fid, &ep->addr, &addr_len);
 	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to get msg pep name\n");
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"Unable to get msg pep name\n");
 		return ret;
 	}
 
-	cm_data->connect.port = ofi_addr_get_port((struct sockaddr *)&name);
-	cm_data->connect.client_conn_id = handle->key;
-	return 0;
-}
-
-static int
-rxm_conn_connect(struct rxm_ep *ep, struct rxm_cmap_handle *handle,
-		 const void *addr)
-{
-	int ret;
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-	union rxm_cm_data cm_data = {
-		.connect = {
-			.version = RXM_CM_DATA_VERSION,
-			.ctrl_version = RXM_CTRL_VERSION,
-			.op_version = RXM_OP_VERSION,
-			.endianness = ofi_detect_endianness(),
-			.eager_size = rxm_eager_limit,
-		},
-	};
-
-	assert(sizeof(uint32_t) == sizeof(cm_data.connect.eager_size));
-	assert(sizeof(uint32_t) == sizeof(cm_data.connect.rx_size));
-	assert(ep->msg_info->rx_attr->size <= (uint32_t) -1);
-
-	free(ep->msg_info->dest_addr);
-	ep->msg_info->dest_addrlen = ep->msg_info->src_addrlen;
-
-	ep->msg_info->dest_addr = mem_dup(addr, ep->msg_info->dest_addrlen);
-	if (!ep->msg_info->dest_addr) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "mem_dup failed, len %zu\n",
-			ep->msg_info->dest_addrlen);
-		return -FI_ENOMEM;
-	}
-
-	ret = rxm_msg_ep_open(ep, ep->msg_info, rxm_conn, &rxm_conn->handle);
-	if (ret)
-		return ret;
-
-	/* We have to send passive endpoint's address to the server since the
-	 * address from which connection request would be sent would have a
-	 * different port. */
-	ret = rxm_prepare_cm_data(ep->msg_pep, &rxm_conn->handle, &cm_data);
-	if (ret)
-		goto err;
-
-	cm_data.connect.rx_size = rxm_conn_get_rx_size(ep, ep->msg_info);
-
-	ret = fi_connect(rxm_conn->msg_ep, ep->msg_info->dest_addr,
-			 &cm_data, sizeof(cm_data));
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to connect msg_ep\n");
-		goto err;
+	/* Update src_addr that will be used for active endpoints.
+	 * Zero out the port to avoid address conflicts, as we will
+	 * create multiple msg ep's for a single rdm ep.
+	 */
+	if (ep->msg_info->src_addr) {
+		free(ep->msg_info->src_addr);
+		ep->msg_info->src_addr = NULL;
+		ep->msg_info->src_addrlen = 0;
 	}
-	return 0;
 
-err:
-	fi_close(&rxm_conn->msg_ep->fid);
-	rxm_conn->msg_ep = NULL;
-	return ret;
-}
+	ep->msg_info->src_addr = mem_dup(&ep->addr, addr_len);
+	if (!ep->msg_info->src_addr)
+		return -FI_ENOMEM;
 
-static int rxm_conn_signal(struct rxm_ep *ep, void *context,
-			   enum rxm_cmap_signal signal)
-{
-	struct fi_eq_entry entry = {0};
-	ssize_t rd;
+	ep->msg_info->src_addrlen = addr_len;
+	ofi_addr_set_port(ep->msg_info->src_addr, 0);
 
-	entry.context = context;
-	entry.data = (uint64_t) signal;
+	if (ep->util_ep.domain->data_progress == FI_PROGRESS_AUTO ||
+	    force_auto_progress) {
 
-	rd = fi_eq_write(ep->msg_eq, FI_NOTIFY, &entry, sizeof(entry), 0);
-	if (rd != sizeof(entry)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to signal\n");
-		return (int)rd;
+		assert(ep->util_ep.domain->threading == FI_THREAD_SAFE);
+		ep->do_progress = true;
+		if (pthread_create(&ep->cm_thread, 0,
+				   ep->rxm_info->caps & FI_ATOMIC ?
+				   rxm_cm_atomic_progress :
+				   rxm_cm_progress, ep)) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"unable to create cm thread\n");
+			return -ofi_syserr();
+		}
 	}
 	return 0;
 }
-
-int rxm_conn_cmap_alloc(struct rxm_ep *rxm_ep)
-{
-	struct rxm_cmap_attr attr;
-	int ret;
-	size_t len = rxm_ep->util_ep.av->addrlen;
-	void *name = calloc(1, len);
-	if (!name) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to allocate memory for EP name\n");
-		return -FI_ENOMEM;
-	}
-
-	/* Passive endpoint should already have fi_setname or fi_listen
-	 * called on it for this to work */
-	ret = fi_getname(&rxm_ep->msg_pep->fid, name, &len);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to fi_getname on msg_ep\n");
-		goto fn;
-	}
-	ofi_straddr_dbg(&rxm_prov, FI_LOG_EP_CTRL, "local_name", name);
-
-	attr.name		= name;
-
-	ret = rxm_cmap_alloc(rxm_ep, &attr);
-	if (ret)
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to allocate CMAP\n");
-fn:
-	free(name);
-	return ret;
-}
diff --git a/prov/rxm/src/rxm_cq.c b/prov/rxm/src/rxm_cq.c
index fb02f7c..7968ed6 100644
--- a/prov/rxm/src/rxm_cq.c
+++ b/prov/rxm/src/rxm_cq.c
@@ -64,41 +64,42 @@ rxm_cq_strerror(struct fid_cq *cq_fid, int prov_errno,
 }
 
 static struct rxm_rx_buf *
-rxm_rx_buf_alloc(struct rxm_ep *rxm_ep, struct fid_ep *rx_ep, bool repost)
+rxm_rx_buf_alloc(struct rxm_ep *rxm_ep, struct fid_ep *rx_ep)
 {
 	struct rxm_rx_buf *rx_buf;
 
-	rx_buf = ofi_buf_alloc(rxm_ep->buf_pools[RXM_BUF_POOL_RX].pool);
+	rx_buf = ofi_buf_alloc(rxm_ep->rx_pool);
 	if (!rx_buf)
 		return NULL;
 
 	assert(rx_buf->ep == rxm_ep);
 	rx_buf->hdr.state = RXM_RX;
 	rx_buf->rx_ep = rx_ep;
-	rx_buf->repost = repost;
+	rx_buf->repost = true;
 
-	if (!rxm_ep->srx_ctx) {
-		rx_buf->conn = container_of(rx_ep->fid.context,
-					    struct rxm_conn, handle);
-	}
+	if (!rxm_ep->srx_ctx)
+		rx_buf->conn = rx_ep->fid.context;
 
 	return rx_buf;
 }
 
-static void rxm_repost_new_rx(struct rxm_rx_buf *rx_buf)
+/* Processing on the current rx buffer is expected to be slow.
+ * Post a new buffer to take its place, and mark the current
+ * buffer to return to the free pool when finished.
+ */
+static void rxm_replace_rx_buf(struct rxm_rx_buf *rx_buf)
 {
 	struct rxm_rx_buf *new_rx_buf;
+	int ret;
 
-	if (!rx_buf->repost)
-		return;
-
-	new_rx_buf = rxm_rx_buf_alloc(rx_buf->ep, rx_buf->rx_ep, true);
+	new_rx_buf = rxm_rx_buf_alloc(rx_buf->ep, rx_buf->rx_ep);
 	if (!new_rx_buf)
 		return;
 
 	rx_buf->repost = false;
-	dlist_insert_tail(&new_rx_buf->repost_entry,
-			  &new_rx_buf->ep->repost_ready_list);
+	ret = rxm_post_recv(new_rx_buf);
+	if (ret)
+		ofi_buf_free(new_rx_buf);
 }
 
 static void rxm_finish_buf_recv(struct rxm_rx_buf *rx_buf)
@@ -109,9 +110,8 @@ static void rxm_finish_buf_recv(struct rxm_rx_buf *rx_buf)
 	if ((rx_buf->pkt.ctrl_hdr.type == rxm_ctrl_seg) &&
 	    rxm_sar_get_seg_type(&rx_buf->pkt.ctrl_hdr) != RXM_SAR_SEG_FIRST) {
 		dlist_insert_tail(&rx_buf->unexp_msg.entry,
-				  &rx_buf->conn->sar_deferred_rx_msg_list);
-		/* repost a new buffer since SAR takes some time to complete */
-		rxm_repost_new_rx(rx_buf);
+				  &rx_buf->conn->deferred_sar_segments);
+		rxm_replace_rx_buf(rx_buf);
 	}
 
 	flags = (rx_buf->pkt.hdr.flags | ofi_rx_flags[rx_buf->pkt.hdr.op]);
@@ -192,7 +192,7 @@ rxm_cq_write_tx_comp(struct rxm_ep *rxm_ep, uint64_t comp_flags,
 	}
 }
 
-static void rxm_finish_rma(struct rxm_ep *rxm_ep, struct rxm_rma_buf *rma_buf,
+static void rxm_finish_rma(struct rxm_ep *rxm_ep, struct rxm_tx_buf *rma_buf,
 			  uint64_t comp_flags)
 {
 	assert(((comp_flags & FI_WRITE) && !(comp_flags & FI_READ)) ||
@@ -208,13 +208,13 @@ static void rxm_finish_rma(struct rxm_ep *rxm_ep, struct rxm_rma_buf *rma_buf,
 
 	if (!(rma_buf->flags & FI_INJECT) && !rxm_ep->rdm_mr_local &&
 	    rxm_ep->msg_mr_local) {
-		rxm_msg_mr_closev(rma_buf->mr.mr, rma_buf->mr.count);
+		rxm_msg_mr_closev(rma_buf->rma.mr, rma_buf->rma.count);
 	}
 
-	ofi_buf_free(rma_buf);
+	rxm_free_rx_buf(rxm_ep, rma_buf);
 }
 
-void rxm_finish_eager_send(struct rxm_ep *rxm_ep, struct rxm_tx_eager_buf *tx_buf)
+void rxm_finish_eager_send(struct rxm_ep *rxm_ep, struct rxm_tx_buf *tx_buf)
 {
 	assert(ofi_tx_cq_flags(tx_buf->pkt.hdr.op) & FI_SEND);
 
@@ -224,23 +224,22 @@ void rxm_finish_eager_send(struct rxm_ep *rxm_ep, struct rxm_tx_eager_buf *tx_bu
 }
 
 static bool rxm_complete_sar(struct rxm_ep *rxm_ep,
-			     struct rxm_tx_sar_buf *tx_buf)
+			     struct rxm_tx_buf *tx_buf)
 {
-	struct rxm_tx_sar_buf *first_tx_buf;
+	struct rxm_tx_buf *first_tx_buf;
 
 	assert(ofi_tx_cq_flags(tx_buf->pkt.hdr.op) & FI_SEND);
 	switch (rxm_sar_get_seg_type(&tx_buf->pkt.ctrl_hdr)) {
 	case RXM_SAR_SEG_FIRST:
 		break;
 	case RXM_SAR_SEG_MIDDLE:
-		ofi_buf_free(tx_buf);
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 		break;
 	case RXM_SAR_SEG_LAST:
-		first_tx_buf = ofi_bufpool_get_ibuf(rxm_ep->
-					buf_pools[RXM_BUF_POOL_TX_SAR].pool,
-					tx_buf->pkt.ctrl_hdr.msg_id);
-		ofi_buf_free(first_tx_buf);
-		ofi_buf_free(tx_buf);
+		first_tx_buf = ofi_bufpool_get_ibuf(rxm_ep->tx_pool,
+						tx_buf->pkt.ctrl_hdr.msg_id);
+		rxm_free_rx_buf(rxm_ep, first_tx_buf);
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 		return true;
 	}
 
@@ -248,7 +247,7 @@ static bool rxm_complete_sar(struct rxm_ep *rxm_ep,
 }
 
 static void rxm_handle_sar_comp(struct rxm_ep *rxm_ep,
-				struct rxm_tx_sar_buf *tx_buf)
+				struct rxm_tx_buf *tx_buf)
 {
 	void *app_context;
 	uint64_t comp_flags, tx_flags;
@@ -281,13 +280,13 @@ static void rxm_rndv_rx_finish(struct rxm_rx_buf *rx_buf)
 }
 
 static void rxm_rndv_tx_finish(struct rxm_ep *rxm_ep,
-			       struct rxm_tx_rndv_buf *tx_buf)
+			       struct rxm_tx_buf *tx_buf)
 {
 	assert(ofi_tx_cq_flags(tx_buf->pkt.hdr.op) & FI_SEND);
 
 	RXM_UPDATE_STATE(FI_LOG_CQ, tx_buf, RXM_RNDV_FINISH);
 	if (!rxm_ep->rdm_mr_local)
-		rxm_msg_mr_closev(tx_buf->mr, tx_buf->count);
+		rxm_msg_mr_closev(tx_buf->rma.mr, tx_buf->rma.count);
 
 	rxm_cq_write_tx_comp(rxm_ep, ofi_tx_cq_flags(tx_buf->pkt.hdr.op),
 			     tx_buf->app_context, tx_buf->flags);
@@ -298,18 +297,18 @@ static void rxm_rndv_tx_finish(struct rxm_ep *rxm_ep,
 		tx_buf->write_rndv.done_buf = NULL;
 	}
 	ofi_ep_tx_cntr_inc(&rxm_ep->util_ep);
-	ofi_buf_free(tx_buf);
+	rxm_free_rx_buf(rxm_ep, tx_buf);
 }
 
 static void rxm_rndv_handle_rd_done(struct rxm_ep *rxm_ep,
 				    struct rxm_rx_buf *rx_buf)
 {
-	struct rxm_tx_rndv_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 
 	FI_DBG(&rxm_prov, FI_LOG_CQ, "Got ACK for msg_id: 0x%" PRIx64 "\n",
 	       rx_buf->pkt.ctrl_hdr.msg_id);
 
-	tx_buf = ofi_bufpool_get_ibuf(rxm_ep->buf_pools[RXM_BUF_POOL_TX_RNDV_REQ].pool,
+	tx_buf = ofi_bufpool_get_ibuf(rxm_ep->tx_pool,
 				      rx_buf->pkt.ctrl_hdr.msg_id);
 	assert(tx_buf->pkt.ctrl_hdr.msg_id == rx_buf->pkt.ctrl_hdr.msg_id);
 
@@ -410,15 +409,15 @@ static void rxm_process_seg_data(struct rxm_rx_buf *rx_buf, int *done)
 	} else {
 		if (rx_buf->recv_entry->sar.msg_id == RXM_SAR_RX_INIT) {
 			if (!rx_buf->conn) {
-				rx_buf->conn = rxm_key2conn(rx_buf->ep,
-							    rx_buf->pkt.ctrl_hdr.conn_id);
+				rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+						(int) rx_buf->pkt.ctrl_hdr.conn_id);
 			}
 
 			rx_buf->recv_entry->sar.conn = rx_buf->conn;
 			rx_buf->recv_entry->sar.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 
 			dlist_insert_tail(&rx_buf->recv_entry->sar.entry,
-					  &rx_buf->conn->sar_rx_msg_list);
+					  &rx_buf->conn->deferred_sar_msgs);
 		}
 
 		/* The RX buffer can be reposted for further re-use */
@@ -445,7 +444,7 @@ static void rxm_handle_seg_data(struct rxm_rx_buf *rx_buf)
 	conn = rx_buf->conn;
 	msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 
-	dlist_foreach_container_safe(&conn->sar_deferred_rx_msg_list,
+	dlist_foreach_container_safe(&conn->deferred_sar_segments,
 				     struct rxm_rx_buf, rx_buf,
 				     unexp_msg.entry, entry) {
 		if (!rxm_rx_buf_match_msg_id(&rx_buf->unexp_msg.entry, &msg_id))
@@ -494,7 +493,7 @@ static ssize_t rxm_rndv_xfer(struct rxm_ep *rxm_ep, struct fid_ep *msg_ep,
 
 				if (ret)
 					break;
-				rxm_ep_enqueue_deferred_tx_queue(def_tx_entry);
+				rxm_queue_deferred_tx(def_tx_entry, OFI_LIST_TAIL);
 				continue;
 			}
 			break;
@@ -507,21 +506,21 @@ static ssize_t rxm_rndv_xfer(struct rxm_ep *rxm_ep, struct fid_ep *msg_ep,
 ssize_t rxm_rndv_read(struct rxm_rx_buf *rx_buf)
 {
 	ssize_t ret;
-	size_t total_len =
-		MIN(rx_buf->recv_entry->total_len, rx_buf->pkt.hdr.size);
+	size_t total_len;
 
+	total_len = MIN(rx_buf->recv_entry->total_len, rx_buf->pkt.hdr.size);
 	RXM_UPDATE_STATE(FI_LOG_CQ, rx_buf, RXM_RNDV_READ);
 
-	ret = rxm_rndv_xfer(rx_buf->ep, rx_buf->conn->msg_ep, rx_buf->remote_rndv_hdr,
+	ret = rxm_rndv_xfer(rx_buf->ep, rx_buf->conn->msg_ep,
+			    rx_buf->remote_rndv_hdr,
 			    rx_buf->recv_entry->rxm_iov.iov,
 			    rx_buf->recv_entry->rxm_iov.desc,
 			    rx_buf->recv_entry->rxm_iov.count, total_len,
 			    rx_buf);
-
-	if (ret)
+	if (ret) {
 		rxm_cq_write_error(rx_buf->ep->util_ep.rx_cq,
-				   rx_buf->ep->util_ep.rx_cntr,
-				   rx_buf, ret);
+				   rx_buf->ep->util_ep.rx_cntr, rx_buf, ret);
+	}
 	return ret;
 }
 
@@ -529,13 +528,12 @@ static ssize_t rxm_rndv_handle_wr_data(struct rxm_rx_buf *rx_buf)
 {
 	int i;
 	ssize_t ret;
-	struct rxm_tx_rndv_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 	size_t total_len, rma_len = 0;
 	struct rxm_rndv_hdr *rx_hdr = (struct rxm_rndv_hdr *) rx_buf->pkt.data;
 
-	tx_buf = ofi_bufpool_get_ibuf(
-		rx_buf->ep->buf_pools[RXM_BUF_POOL_TX_RNDV_REQ].pool,
-		rx_buf->pkt.ctrl_hdr.msg_id);
+	tx_buf = ofi_bufpool_get_ibuf(rx_buf->ep->tx_pool,
+				      rx_buf->pkt.ctrl_hdr.msg_id);
 	total_len = tx_buf->pkt.hdr.size;
 
 	tx_buf->write_rndv.remote_hdr.count = rx_hdr->count;
@@ -551,11 +549,16 @@ static ssize_t rxm_rndv_handle_wr_data(struct rxm_rx_buf *rx_buf)
 		}
 	}
 
+	/* BUG: This is forcing a state change without knowing what state
+	 * we're currently in.  This loses whether we processed the completion
+	 * for the original send request.  Valid states here are
+	 * RXM_RNDV_TX or RXM_RNDV_WRITE_DATA_WAIT.
+	 */
 	RXM_UPDATE_STATE(FI_LOG_CQ, tx_buf, RXM_RNDV_WRITE);
 
 	ret = rxm_rndv_xfer(rx_buf->ep, tx_buf->write_rndv.conn->msg_ep, rx_hdr,
 			    tx_buf->write_rndv.iov, tx_buf->write_rndv.desc,
-			    tx_buf->count, total_len, tx_buf);
+			    tx_buf->rma.count, total_len, tx_buf);
 
 	if (ret)
 		rxm_cq_write_error(rx_buf->ep->util_ep.rx_cq,
@@ -570,15 +573,12 @@ static ssize_t rxm_handle_rndv(struct rxm_rx_buf *rx_buf)
 	int ret = 0, i;
 	size_t total_recv_len;
 
-	/* En-queue new rx buf to be posted ASAP so that we don't block any
-	 * incoming messages. RNDV processing can take a while.
-	 */
-	rxm_repost_new_rx(rx_buf);
+	rxm_replace_rx_buf(rx_buf);
 
 	if (!rx_buf->conn) {
 		assert(rx_buf->ep->srx_ctx);
-		rx_buf->conn = rxm_key2conn(rx_buf->ep,
-					    rx_buf->pkt.ctrl_hdr.conn_id);
+		rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					  (int) rx_buf->pkt.ctrl_hdr.conn_id);
 		if (!rx_buf->conn)
 			return -FI_EOTHER;
 	}
@@ -636,7 +636,7 @@ void rxm_handle_eager(struct rxm_rx_buf *rx_buf)
 	done_len = ofi_copy_to_hmem_iov(iface, device,
 					rx_buf->recv_entry->rxm_iov.iov,
 					rx_buf->recv_entry->rxm_iov.count, 0,
-					rx_buf->pkt.data, rx_buf->pkt.hdr.size);
+					rx_buf->data, rx_buf->pkt.hdr.size);
 	assert(done_len == rx_buf->pkt.hdr.size);
 
 	rxm_finish_recv(rx_buf, done_len);
@@ -655,14 +655,14 @@ void rxm_handle_coll_eager(struct rxm_rx_buf *rx_buf)
 	done_len = ofi_copy_to_hmem_iov(iface, device,
 					rx_buf->recv_entry->rxm_iov.iov,
 					rx_buf->recv_entry->rxm_iov.count, 0,
-					rx_buf->pkt.data, rx_buf->pkt.hdr.size);
+					rx_buf->data, rx_buf->pkt.hdr.size);
 	assert(done_len == rx_buf->pkt.hdr.size);
 
 	if (rx_buf->pkt.hdr.tag & OFI_COLL_TAG_FLAG) {
 		ofi_coll_handle_xfer_comp(rx_buf->pkt.hdr.tag,
 				rx_buf->recv_entry->context);
-		rxm_rx_buf_free(rx_buf);
 		rxm_recv_entry_release(rx_buf->recv_entry);
+		rxm_rx_buf_free(rx_buf);
 	} else {
 		rxm_finish_recv(rx_buf, done_len);
 	}
@@ -756,11 +756,7 @@ rxm_match_rx_buf(struct rxm_rx_buf *rx_buf,
 
 	dlist_insert_tail(&rx_buf->unexp_msg.entry,
 			  &recv_queue->unexp_msg_list);
-
-	/* post a new buffer since we don't know when the unexpected buffer
-	 * will be consumed
-	 */
-	rxm_repost_new_rx(rx_buf);
+	rxm_replace_rx_buf(rx_buf);
 	return 0;
 }
 
@@ -772,11 +768,11 @@ static ssize_t rxm_handle_recv_comp(struct rxm_rx_buf *rx_buf)
 
 	if (rx_buf->ep->rxm_info->caps & (FI_SOURCE | FI_DIRECTED_RECV)) {
 		if (rx_buf->ep->srx_ctx)
-			rx_buf->conn = rxm_key2conn(rx_buf->ep, rx_buf->
-						    pkt.ctrl_hdr.conn_id);
+			rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					(int) rx_buf->pkt.ctrl_hdr.conn_id);
 		if (!rx_buf->conn)
 			return -FI_EOTHER;
-		match_attr.addr = rx_buf->conn->handle.fi_addr;
+		match_attr.addr = rx_buf->conn->peer->fi_addr;
 	}
 
 	if (rx_buf->ep->rxm_info->mode & FI_BUFFERED_RECV) {
@@ -814,15 +810,15 @@ static ssize_t rxm_sar_handle_segment(struct rxm_rx_buf *rx_buf)
 {
 	struct dlist_entry *sar_entry;
 
-	rx_buf->conn = rxm_key2conn(rx_buf->ep,
-				    rx_buf->pkt.ctrl_hdr.conn_id);
+	rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+				  (int) rx_buf->pkt.ctrl_hdr.conn_id);
 	if (!rx_buf->conn)
 		return -FI_EOTHER;
 
 	FI_DBG(&rxm_prov, FI_LOG_CQ,
 	       "Got incoming recv with msg_id: 0x%" PRIx64 " for conn - %p\n",
 	       rx_buf->pkt.ctrl_hdr.msg_id, rx_buf->conn);
-	sar_entry = dlist_find_first_match(&rx_buf->conn->sar_rx_msg_list,
+	sar_entry = dlist_find_first_match(&rx_buf->conn->deferred_sar_msgs,
 					   rxm_sar_match_msg_id,
 					   &rx_buf->pkt.ctrl_hdr.msg_id);
 	if (!sar_entry)
@@ -837,21 +833,21 @@ static ssize_t rxm_sar_handle_segment(struct rxm_rx_buf *rx_buf)
 static void rxm_rndv_send_rd_done(struct rxm_rx_buf *rx_buf)
 {
 	struct rxm_deferred_tx_entry *def_entry;
-	struct rxm_tx_base_buf *buf;
+	struct rxm_tx_buf *buf;
 	ssize_t ret;
 
 	assert(rx_buf->conn);
 	assert(rx_buf->hdr.state == RXM_RNDV_READ);
-	buf = rxm_tx_buf_alloc(rx_buf->ep, RXM_BUF_POOL_TX_RNDV_RD_DONE);
+	buf = ofi_buf_alloc(rx_buf->ep->tx_pool);
 	if (!buf) {
 		ret = -FI_ENOMEM;
 		goto err;
 	}
 
 	rx_buf->recv_entry->rndv.tx_buf = buf;
-	assert(buf->pkt.ctrl_hdr.type == rxm_ctrl_rndv_rd_done);
 
-	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->handle.remote_key;
+	buf->pkt.ctrl_hdr.type = rxm_ctrl_rndv_rd_done;
+	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->remote_index;
 	buf->pkt.ctrl_hdr.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 
 	ret = fi_send(rx_buf->conn->msg_ep, &buf->pkt, sizeof(buf->pkt),
@@ -864,7 +860,7 @@ static void rxm_rndv_send_rd_done(struct rxm_rx_buf *rx_buf)
 			if (def_entry) {
 				def_entry->rndv_ack.rx_buf = rx_buf;
 				def_entry->rndv_ack.pkt_size = sizeof(rx_buf->pkt);
-				rxm_ep_enqueue_deferred_tx_queue(def_entry);
+				rxm_queue_deferred_tx(def_entry, OFI_LIST_TAIL);
 				return;
 			}
 		}
@@ -889,22 +885,22 @@ err:
 }
 
 static void
-rxm_rndv_send_wr_done(struct rxm_ep *rxm_ep, struct rxm_tx_rndv_buf *tx_buf)
+rxm_rndv_send_wr_done(struct rxm_ep *rxm_ep, struct rxm_tx_buf *tx_buf)
 {
 	struct rxm_deferred_tx_entry *def_entry;
-	struct rxm_tx_base_buf *buf;
+	struct rxm_tx_buf *buf;
 	ssize_t ret;
 
 	assert(tx_buf->hdr.state == RXM_RNDV_WRITE);
-	buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_RNDV_WR_DONE);
+	buf = ofi_buf_alloc(rxm_ep->tx_pool);
 	if (!buf) {
 		ret = -FI_ENOMEM;
 		goto err;
 	}
 
 	tx_buf->write_rndv.done_buf = buf;
-	assert(buf->pkt.ctrl_hdr.type == rxm_ctrl_rndv_wr_done);
 
+	buf->pkt.ctrl_hdr.type = rxm_ctrl_rndv_wr_done;
 	buf->pkt.ctrl_hdr.conn_id = tx_buf->pkt.ctrl_hdr.conn_id;
 	buf->pkt.ctrl_hdr.msg_id = tx_buf->pkt.ctrl_hdr.msg_id;
 
@@ -917,7 +913,7 @@ rxm_rndv_send_wr_done(struct rxm_ep *rxm_ep, struct rxm_tx_rndv_buf *tx_buf)
 						RXM_DEFERRED_TX_RNDV_DONE);
 			if (def_entry) {
 				def_entry->rndv_done.tx_buf = tx_buf;
-				rxm_ep_enqueue_deferred_tx_queue(def_entry);
+				rxm_queue_deferred_tx(def_entry, OFI_LIST_TAIL);
 				return;
 			}
 		}
@@ -944,21 +940,21 @@ err:
 ssize_t rxm_rndv_send_wr_data(struct rxm_rx_buf *rx_buf)
 {
 	struct rxm_deferred_tx_entry *def_entry;
-	struct rxm_tx_base_buf *buf;
+	struct rxm_tx_buf *buf;
 	ssize_t ret;
 
 	assert(rx_buf->conn);
 
-	buf = rxm_tx_buf_alloc(rx_buf->ep, RXM_BUF_POOL_TX_RNDV_WR_DATA);
+	buf = ofi_buf_alloc(rx_buf->ep->tx_pool);
 	if (!buf) {
 		ret = -FI_ENOMEM;
 		goto err;
 	}
 
-	assert(buf->pkt.ctrl_hdr.type == rxm_ctrl_rndv_wr_data);
 	rx_buf->recv_entry->rndv.tx_buf = buf;
 
-	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->handle.remote_key;
+	buf->pkt.ctrl_hdr.type = rxm_ctrl_rndv_wr_data;
+	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->remote_index;
 	buf->pkt.ctrl_hdr.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 	rxm_rndv_hdr_init(rx_buf->ep, buf->pkt.data,
 			  rx_buf->recv_entry->rxm_iov.iov,
@@ -976,7 +972,7 @@ ssize_t rxm_rndv_send_wr_data(struct rxm_rx_buf *rx_buf)
 				def_entry->rndv_ack.pkt_size =
 						sizeof(buf->pkt) +
 						sizeof(struct rxm_rndv_hdr);
-				rxm_ep_enqueue_deferred_tx_queue(def_entry);
+				rxm_queue_deferred_tx(def_entry, OFI_LIST_TAIL);
 				return 0;
 			}
 		}
@@ -1010,7 +1006,7 @@ static void rxm_handle_remote_write(struct rxm_ep *rxm_ep,
 }
 
 static void rxm_format_atomic_resp_pkt_hdr(struct rxm_conn *rxm_conn,
-					   struct rxm_tx_atomic_buf *tx_buf,
+					   struct rxm_tx_buf *tx_buf,
 					   size_t data_len, uint32_t pkt_op,
 					   enum fi_datatype datatype,
 					   uint8_t atomic_op)
@@ -1026,36 +1022,36 @@ static void rxm_format_atomic_resp_pkt_hdr(struct rxm_conn *rxm_conn,
 
 static ssize_t rxm_atomic_send_resp(struct rxm_ep *rxm_ep,
 				    struct rxm_rx_buf *rx_buf,
-				    struct rxm_tx_atomic_buf *resp_buf,
+				    struct rxm_tx_buf *resp_buf,
 				    ssize_t result_len, uint32_t status)
 {
 	struct rxm_deferred_tx_entry *def_tx_entry;
 	struct rxm_atomic_resp_hdr *atomic_hdr;
 	ssize_t ret;
-	ssize_t resp_len;
+	size_t data_len, tot_len;
 
-	resp_len = result_len + sizeof(struct rxm_atomic_resp_hdr) +
-		   sizeof(struct rxm_pkt);
+	data_len = result_len + sizeof(struct rxm_atomic_resp_hdr);
+	tot_len = data_len + sizeof(struct rxm_pkt);
 
 	resp_buf->hdr.state = RXM_ATOMIC_RESP_SENT;
-	rxm_format_atomic_resp_pkt_hdr(rx_buf->conn, resp_buf, resp_len,
+	rxm_format_atomic_resp_pkt_hdr(rx_buf->conn, resp_buf, data_len,
 				       rx_buf->pkt.hdr.op,
 				       rx_buf->pkt.hdr.atomic.datatype,
 				       rx_buf->pkt.hdr.atomic.op);
-	resp_buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->handle.remote_key;
+	resp_buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->remote_index;
 	resp_buf->pkt.ctrl_hdr.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 	atomic_hdr = (struct rxm_atomic_resp_hdr *) resp_buf->pkt.data;
 	atomic_hdr->status = htonl(status);
 	atomic_hdr->result_len = htonl(result_len);
 
-	if (resp_len < rxm_ep->inject_limit) {
+	if (tot_len < rxm_ep->inject_limit) {
 		ret = fi_inject(rx_buf->conn->msg_ep, &resp_buf->pkt,
-				resp_len, 0);
+				tot_len, 0);
 		if (!ret)
 			ofi_buf_free(resp_buf);
 	} else {
 		ret = rxm_atomic_send_respmsg(rxm_ep, rx_buf->conn, resp_buf,
-					      resp_len);
+					      tot_len);
 	}
 	if (ret) {
 		FI_WARN(&rxm_prov, FI_LOG_CQ,
@@ -1072,8 +1068,8 @@ static ssize_t rxm_atomic_send_resp(struct rxm_ep *rxm_ep,
 			}
 
 			def_tx_entry->atomic_resp.tx_buf = resp_buf;
-			def_tx_entry->atomic_resp.len = resp_len;
-			rxm_ep_enqueue_deferred_tx_queue(def_tx_entry);
+			def_tx_entry->atomic_resp.len = tot_len;
+			rxm_queue_deferred_tx(def_tx_entry, OFI_LIST_TAIL);
 			ret = 0;
 		}
 	}
@@ -1114,7 +1110,7 @@ static int rxm_do_device_mem_atomic(struct rxm_mr *dev_mr, uint8_t op,
 				    enum fi_op amo_op, size_t amo_op_size)
 {
 	struct rxm_domain *dom = dev_mr->domain;
-	void *bounce_buf;
+	void *tx_buf;
 	ssize_t ret __attribute__((unused));
 	struct iovec iov = {
 		.iov_base = dev_dst,
@@ -1122,28 +1118,28 @@ static int rxm_do_device_mem_atomic(struct rxm_mr *dev_mr, uint8_t op,
 	};
 
 	fastlock_acquire(&dom->amo_bufpool_lock);
-	bounce_buf = ofi_buf_alloc(dom->amo_bufpool);
+	tx_buf = ofi_buf_alloc(dom->amo_bufpool);
 	fastlock_release(&dom->amo_bufpool_lock);
 
-	if (!bounce_buf)
+	if (!tx_buf)
 		return -FI_ENOMEM;
 
 	fastlock_acquire(&dev_mr->amo_lock);
-	ret = ofi_copy_from_hmem_iov(bounce_buf, amo_op_size, dev_mr->iface, 0,
+	ret = ofi_copy_from_hmem_iov(tx_buf, amo_op_size, dev_mr->iface, 0,
 				    &iov, 1, 0);
 	assert(ret == amo_op_size);
 
-	rxm_do_atomic(op, bounce_buf, src, cmp, res, amo_count, datatype,
+	rxm_do_atomic(op, tx_buf, src, cmp, res, amo_count, datatype,
 		      amo_op);
 
-	ret = ofi_copy_to_hmem_iov(dev_mr->iface, 0, &iov, 1, 0, bounce_buf,
+	ret = ofi_copy_to_hmem_iov(dev_mr->iface, 0, &iov, 1, 0, tx_buf,
 				   amo_op_size);
 	assert(ret == amo_op_size);
 
 	fastlock_release(&dev_mr->amo_lock);
 
 	fastlock_acquire(&dom->amo_bufpool_lock);
-	ofi_buf_free(bounce_buf);
+	ofi_buf_free(tx_buf);
 	fastlock_release(&dom->amo_bufpool_lock);
 
 	return FI_SUCCESS;
@@ -1162,7 +1158,7 @@ static ssize_t rxm_handle_atomic_req(struct rxm_ep *rxm_ep,
 	uint64_t offset;
 	int i;
 	ssize_t ret = 0;
-	struct rxm_tx_atomic_buf *resp_buf;
+	struct rxm_tx_buf *resp_buf;
 	struct rxm_atomic_resp_hdr *resp_hdr;
 	struct rxm_domain *domain = container_of(rxm_ep->util_ep.domain,
 					 struct rxm_domain, util_domain);
@@ -1174,20 +1170,19 @@ static ssize_t rxm_handle_atomic_req(struct rxm_ep *rxm_ep,
 	       op == ofi_op_atomic_compare);
 
 	if (rx_buf->ep->srx_ctx)
-		rx_buf->conn = rxm_key2conn(rx_buf->ep,
-					    rx_buf->pkt.ctrl_hdr.conn_id);
+		rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					  (int) rx_buf->pkt.ctrl_hdr.conn_id);
 	if (!rx_buf->conn)
 		return -FI_EOTHER;
 
-	resp_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_ATOMIC);
+	resp_buf = ofi_buf_alloc(rxm_ep->tx_pool);
 	if (!resp_buf) {
 		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-			"Unable to allocate from Atomic buffer pool\n");
-		/* TODO: Should this be -FI_ENOMEM - how does it get
-		 * processed again */
-		return -FI_EAGAIN;
+			"Unable to allocate for atomic response\n");
+		return -FI_ENOMEM;
 	}
 
+	resp_buf->pkt.ctrl_hdr.type = rxm_ctrl_atomic;
 	for (i = 0; i < rx_buf->pkt.hdr.atomic.ioc_count; i++) {
 		ret = ofi_mr_verify(&domain->util_domain.mr_map,
 				    req_hdr->rma_ioc[i].count * datatype_sz,
@@ -1250,7 +1245,7 @@ static ssize_t rxm_handle_atomic_req(struct rxm_ep *rxm_ep,
 static ssize_t rxm_handle_atomic_resp(struct rxm_ep *rxm_ep,
 				      struct rxm_rx_buf *rx_buf)
 {
-	struct rxm_tx_atomic_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 	struct rxm_atomic_resp_hdr *resp_hdr;
 	struct util_cntr *cntr = NULL;
 	uint64_t len;
@@ -1260,14 +1255,14 @@ static ssize_t rxm_handle_atomic_resp(struct rxm_ep *rxm_ep,
 	uint64_t device;
 
 	resp_hdr = (struct rxm_atomic_resp_hdr *) rx_buf->pkt.data;
-	tx_buf = ofi_bufpool_get_ibuf(rxm_ep->buf_pools[RXM_BUF_POOL_TX_ATOMIC].pool,
+	tx_buf = ofi_bufpool_get_ibuf(rxm_ep->tx_pool,
 				      rx_buf->pkt.ctrl_hdr.msg_id);
 	FI_DBG(&rxm_prov, FI_LOG_CQ, "received atomic response: op: %" PRIu8
 	       " msg_id: 0x%" PRIx64 "\n", rx_buf->pkt.hdr.op,
 	       rx_buf->pkt.ctrl_hdr.msg_id);
 
-	iface = rxm_mr_desc_to_hmem_iface_dev(tx_buf->result_iov.desc,
-					      tx_buf->result_iov.count,
+	iface = rxm_mr_desc_to_hmem_iface_dev(tx_buf->atomic_result.desc,
+					      tx_buf->atomic_result.count,
 					      &device);
 
 	assert(!(rx_buf->comp_flags & ~(FI_RECV | FI_REMOTE_CQ_DATA)));
@@ -1280,16 +1275,16 @@ static ssize_t rxm_handle_atomic_resp(struct rxm_ep *rxm_ep,
 		goto write_err;
 	}
 
-	len = ofi_total_iov_len(tx_buf->result_iov.iov,
-				tx_buf->result_iov.count);
+	len = ofi_total_iov_len(tx_buf->atomic_result.iov,
+				tx_buf->atomic_result.count);
 	if (ntohl(resp_hdr->result_len) != len) {
 		ret = -FI_EIO;
 		FI_WARN(&rxm_prov, FI_LOG_CQ, "result size mismatch\n");
 		goto write_err;
 	}
 
-	copy_len = ofi_copy_to_hmem_iov(iface, device, tx_buf->result_iov.iov,
-				   tx_buf->result_iov.count, 0, resp_hdr->data,
+	copy_len = ofi_copy_to_hmem_iov(iface, device, tx_buf->atomic_result.iov,
+				   tx_buf->atomic_result.count, 0, resp_hdr->data,
 				   len);
 	if (copy_len != len) {
 		ret = -FI_EIO;
@@ -1312,10 +1307,7 @@ static ssize_t rxm_handle_atomic_resp(struct rxm_ep *rxm_ep,
 	}
 free:
 	rxm_rx_buf_free(rx_buf);
-	ofi_buf_free(tx_buf);
-	ofi_atomic_inc32(&rxm_ep->atomic_tx_credits);
-	assert(ofi_atomic_get32(&rxm_ep->atomic_tx_credits) <=
-	       rxm_ep->rxm_info->tx_attr->size);
+	rxm_free_rx_buf(rxm_ep, tx_buf);
 	return ret;
 
 write_err:
@@ -1348,7 +1340,7 @@ static ssize_t rxm_handle_credit(struct rxm_ep *rxm_ep, struct rxm_rx_buf *rx_bu
 }
 
 void rxm_finish_coll_eager_send(struct rxm_ep *rxm_ep,
-			       struct rxm_tx_eager_buf *tx_eager_buf)
+			        struct rxm_tx_buf *tx_eager_buf)
 {
 	if (tx_eager_buf->pkt.hdr.tag & OFI_COLL_TAG_FLAG) {
 		ofi_coll_handle_xfer_comp(tx_eager_buf->pkt.hdr.tag,
@@ -1356,17 +1348,12 @@ void rxm_finish_coll_eager_send(struct rxm_ep *rxm_ep,
 	} else {
 		rxm_finish_eager_send(rxm_ep, tx_eager_buf);
 	}
-};
+}
 
 ssize_t rxm_handle_comp(struct rxm_ep *rxm_ep, struct fi_cq_data_entry *comp)
 {
 	struct rxm_rx_buf *rx_buf;
-	struct rxm_tx_base_buf *tx_buf;
-	struct rxm_tx_sar_buf *tx_sar_buf;
-	struct rxm_tx_eager_buf *tx_eager_buf;
-	struct rxm_tx_rndv_buf *tx_rndv_buf;
-	struct rxm_tx_atomic_buf *tx_atomic_buf;
-	struct rxm_rma_buf *rma_buf;
+	struct rxm_tx_buf *tx_buf;
 
 	/* Remote write events may not consume a posted recv so op context
 	 * and hence state would be NULL */
@@ -1377,23 +1364,21 @@ ssize_t rxm_handle_comp(struct rxm_ep *rxm_ep, struct fi_cq_data_entry *comp)
 
 	switch (RXM_GET_PROTO_STATE(comp->op_context)) {
 	case RXM_TX:
-		tx_eager_buf = comp->op_context;
-		rxm_ep->eager_ops->comp_tx(rxm_ep, tx_eager_buf);
-		ofi_buf_free(tx_eager_buf);
+	case RXM_INJECT_TX:
+		tx_buf = comp->op_context;
+		rxm_ep->eager_ops->comp_tx(rxm_ep, tx_buf);
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 		return 0;
 	case RXM_CREDIT_TX:
 		tx_buf = comp->op_context;
 		assert(comp->flags & FI_SEND);
 		ofi_buf_free(tx_buf);
 		return 0;
-	case RXM_INJECT_TX:
-		assert(0);
-		return 0;
 	case RXM_RMA:
-		rma_buf = comp->op_context;
+		tx_buf = comp->op_context;
 		assert((comp->flags & (FI_WRITE | FI_RMA)) ||
 		       (comp->flags & (FI_READ | FI_RMA)));
-		rxm_finish_rma(rxm_ep, rma_buf, comp->flags);
+		rxm_finish_rma(rxm_ep, tx_buf, comp->flags);
 		return 0;
 	case RXM_RX:
 		rx_buf = comp->op_context;
@@ -1426,18 +1411,18 @@ ssize_t rxm_handle_comp(struct rxm_ep *rxm_ep, struct fi_cq_data_entry *comp)
 			return -FI_EINVAL;
 		}
 	case RXM_SAR_TX:
-		tx_sar_buf = comp->op_context;
+		tx_buf = comp->op_context;
 		assert(comp->flags & FI_SEND);
-		rxm_handle_sar_comp(rxm_ep, tx_sar_buf);
+		rxm_handle_sar_comp(rxm_ep, tx_buf);
 		return 0;
 	case RXM_RNDV_TX:
-		tx_rndv_buf = comp->op_context;
+		tx_buf = comp->op_context;
 		assert(comp->flags & FI_SEND);
 		if (rxm_ep->rndv_ops == &rxm_rndv_ops_write)
-			RXM_UPDATE_STATE(FI_LOG_CQ, tx_rndv_buf,
+			RXM_UPDATE_STATE(FI_LOG_CQ, tx_buf,
 					 RXM_RNDV_WRITE_DATA_WAIT);
 		else
-			RXM_UPDATE_STATE(FI_LOG_CQ, tx_rndv_buf,
+			RXM_UPDATE_STATE(FI_LOG_CQ, tx_buf,
 					 RXM_RNDV_READ_DONE_WAIT);
 		return 0;
 	case RXM_RNDV_READ_DONE_WAIT:
@@ -1453,13 +1438,13 @@ ssize_t rxm_handle_comp(struct rxm_ep *rxm_ep, struct fi_cq_data_entry *comp)
 		rxm_rndv_send_rd_done(rx_buf);
 		return 0;
 	case RXM_RNDV_WRITE:
-		tx_rndv_buf = comp->op_context;
+		tx_buf = comp->op_context;
 		assert(comp->flags & FI_WRITE);
-		if (++tx_rndv_buf->write_rndv.rndv_rma_index <
-		    tx_rndv_buf->write_rndv.rndv_rma_count)
+		if (++tx_buf->write_rndv.rndv_rma_index <
+		    tx_buf->write_rndv.rndv_rma_count)
 			return 0;
 
-		rxm_rndv_send_wr_done(rxm_ep, tx_rndv_buf);
+		rxm_rndv_send_wr_done(rxm_ep, tx_buf);
 		return 0;
 	case RXM_RNDV_READ_DONE_SENT:
 		assert(comp->flags & FI_SEND);
@@ -1484,14 +1469,15 @@ ssize_t rxm_handle_comp(struct rxm_ep *rxm_ep, struct fi_cq_data_entry *comp)
 		assert(0);
 		return 0;
 	case RXM_ATOMIC_RESP_WAIT:
-		/* Optional atomic request completion; TX completion
-		 * processing is performed when atomic response is received */
+		/* BUG: need to wait for completion, even if a response has
+		 * been received.
+		 */
 		assert(comp->flags & FI_SEND);
 		return 0;
 	case RXM_ATOMIC_RESP_SENT:
-		tx_atomic_buf = comp->op_context;
+		tx_buf = comp->op_context;
 		assert(comp->flags & FI_SEND);
-		ofi_buf_free(tx_atomic_buf);
+		ofi_buf_free(tx_buf);	/* BUG: should have consumed tx credit */
 		return 0;
 	default:
 		assert(0);
@@ -1499,32 +1485,29 @@ ssize_t rxm_handle_comp(struct rxm_ep *rxm_ep, struct fi_cq_data_entry *comp)
 	}
 }
 
-static int rxm_get_recv_entry(struct rxm_rx_buf *rx_buf)
+static void rxm_get_recv_entry(struct rxm_rx_buf *rx_buf,
+			       struct ofi_cq_rbuf_entry *cq_entry)
 {
 	struct rxm_recv_match_attr match_attr;
+	struct rxm_conn *conn;
 	struct rxm_recv_queue *recv_queue;
 	struct dlist_entry *entry;
 
 	assert(!rx_buf->recv_entry);
 	if (rx_buf->ep->rxm_info->caps & (FI_SOURCE | FI_DIRECTED_RECV)) {
-		if (rx_buf->ep->srx_ctx)
-			rx_buf->conn = rxm_key2conn(rx_buf->ep, rx_buf->
-						    pkt.ctrl_hdr.conn_id);
-		if (!rx_buf->conn)
-			return -FI_EOTHER;
-		match_attr.addr = rx_buf->conn->handle.fi_addr;
+		conn = cq_entry->ep_context;
+		match_attr.addr = conn->peer->fi_addr;
 	} else {
 		match_attr.addr = FI_ADDR_UNSPEC;
 	}
 
 	match_attr.ignore = 0;
-	if (rx_buf->pkt.hdr.op == ofi_op_msg) {
-		match_attr.tag = 0;
-		recv_queue = &rx_buf->ep->recv_queue;
-	} else {
-		assert(rx_buf->pkt.hdr.op == ofi_op_tagged);
+	if (rx_buf->pkt.hdr.op == ofi_op_tagged) {
 		match_attr.tag = rx_buf->pkt.hdr.tag;
 		recv_queue = &rx_buf->ep->trecv_queue;
+	} else {
+		match_attr.tag = 0;
+		recv_queue = &rx_buf->ep->recv_queue;
 	}
 
 	/* See comment with rxm_get_dyn_rbuf */
@@ -1543,8 +1526,45 @@ static int rxm_get_recv_entry(struct rxm_rx_buf *rx_buf)
 	} else {
 		recv_queue->dyn_rbuf_unexp_cnt++;
 	}
+}
 
+static void rxm_fake_rx_hdr(struct rxm_rx_buf *rx_buf,
+			    struct ofi_cq_rbuf_entry *entry)
+{
+	struct rxm_conn *conn;
+
+	conn = entry->ep_context;
+
+	OFI_DBG_SET(rx_buf->pkt.hdr.version, OFI_OP_VERSION);
+	OFI_DBG_SET(rx_buf->pkt.ctrl_hdr.version, RXM_CTRL_VERSION);
+	rx_buf->pkt.ctrl_hdr.type = rxm_ctrl_eager;
+	rx_buf->pkt.ctrl_hdr.conn_id = conn->peer->index;
+	rx_buf->pkt.hdr.op = ofi_op_tagged;
+	rx_buf->pkt.hdr.tag = entry->tag;
+	rx_buf->pkt.hdr.size = entry->len;
+	rx_buf->pkt.hdr.flags = 0;
+}
+
+static ssize_t
+rxm_get_dyn_unexp(struct rxm_rx_buf *rx_buf, struct iovec *iov, size_t *count)
+{
+	*count = 1;
+
+	if (rx_buf->pkt.hdr.size > rxm_buffer_size) {
+		rx_buf->data = malloc(rx_buf->pkt.hdr.size);
+		if (!rx_buf->data)
+			goto trunc;
+	}
+
+	iov[0].iov_base = rx_buf->data;
+	iov[0].iov_len = rx_buf->pkt.hdr.size;
 	return 0;
+
+trunc:
+	rx_buf->data = &rx_buf->pkt.data;
+	iov[0].iov_base = rx_buf->data;
+	iov[0].iov_len = rxm_buffer_size;
+	return -FI_ETRUNC;
 }
 
 /*
@@ -1570,72 +1590,54 @@ static int rxm_get_recv_entry(struct rxm_rx_buf *rx_buf)
  * receives in order.  If there are any unexpected messages outstanding, we
  * need to fail all matches until they have been read from the CQ.
  */
-ssize_t rxm_get_dyn_rbuf(struct fi_cq_data_entry *entry, struct iovec *iov,
+ssize_t rxm_get_dyn_rbuf(struct ofi_cq_rbuf_entry *entry, struct iovec *iov,
 			 size_t *count)
 {
 	struct rxm_rx_buf *rx_buf;
-	int ret;
 
 	rx_buf = entry->op_context;
+	assert(!(rx_buf->ep->rxm_info->mode & FI_BUFFERED_RECV));
+
+	/* Messages tagged at the tcp layer do not carry an rxm header */
+	if (entry->flags & FI_TAGGED)
+		rxm_fake_rx_hdr(rx_buf, entry);
+
 	assert((rx_buf->pkt.hdr.version == OFI_OP_VERSION) &&
 		(rx_buf->pkt.ctrl_hdr.version == RXM_CTRL_VERSION));
-	assert(!(rx_buf->ep->rxm_info->mode & FI_BUFFERED_RECV));
 
 	switch (rx_buf->pkt.ctrl_hdr.type) {
 	case rxm_ctrl_eager:
-		ret = rxm_get_recv_entry(rx_buf);
-		if (ret)
-			return ret;
-
+		rxm_get_recv_entry(rx_buf, entry);
 		if (rx_buf->recv_entry) {
 			*count = rx_buf->recv_entry->rxm_iov.count;
 			memcpy(iov, rx_buf->recv_entry->rxm_iov.iov, *count *
 			       sizeof(*iov));
 		} else {
-			*count = 1;
-			iov[0].iov_base = &rx_buf->pkt + 1;
-			iov[0].iov_len = rxm_eager_limit;
+			rxm_get_dyn_unexp(rx_buf, iov, count);
 		}
 		break;
 	case rxm_ctrl_rndv_req:
-		/* find matching receive to maintain message ordering, but we
-		 * only need to receive rendezvous header to complete message
-		 */
-		ret = rxm_get_recv_entry(rx_buf);
-		if (ret)
-			return ret;
+		/* Find matching receive to maintain message ordering. */
+		rxm_get_recv_entry(rx_buf, entry);
 
-		*count = 1;
-		iov[0].iov_base = &rx_buf->pkt + 1;
-		iov[0].iov_len = sizeof(struct rxm_rndv_hdr);
-		break;
+		/* fall through */
 	case rxm_ctrl_atomic:
-		*count = 1;
-		iov[0].iov_base = &rx_buf->pkt + 1;
-		iov[0].iov_len = sizeof(struct rxm_atomic_hdr);
-		break;
 	case rxm_ctrl_atomic_resp:
-		*count = 1;
-		iov[0].iov_base = &rx_buf->pkt + 1;
-		iov[0].iov_len = sizeof(struct rxm_atomic_resp_hdr);
-		break;
 	case rxm_ctrl_rndv_wr_data:
-		*count = 1;
-		iov[0].iov_base = &rx_buf->pkt + 1;
-		iov[0].iov_len = sizeof(struct rxm_rndv_hdr);
-		break;
 	case rxm_ctrl_rndv_wr_done:
 	case rxm_ctrl_rndv_rd_done:
 	case rxm_ctrl_credit:
-		*count = 0;
-		iov[0].iov_base = NULL;
-		iov[0].iov_len = 0;
+		*count = 1;
+		iov[0].iov_base = &rx_buf->pkt.data;
+		iov[0].iov_len = rxm_buffer_size;
 		break;
 	case rxm_ctrl_seg:
 	default:
 		FI_WARN(&rxm_prov, FI_LOG_CQ,
 			"Unexpected request for dynamic rbuf\n");
-		*count = 0;
+		*count = 1;
+		iov[0].iov_base = &rx_buf->pkt.data;
+		iov[0].iov_len = rxm_buffer_size;
 		break;
 	}
 
@@ -1697,12 +1699,8 @@ void rxm_cq_write_error_all(struct rxm_ep *rxm_ep, int err)
 
 void rxm_handle_comp_error(struct rxm_ep *rxm_ep)
 {
-	struct rxm_tx_base_buf *base_buf;
-	struct rxm_tx_eager_buf *eager_buf;
-	struct rxm_tx_sar_buf *sar_buf;
-	struct rxm_tx_rndv_buf *rndv_buf;
+	struct rxm_tx_buf *tx_buf;
 	struct rxm_rx_buf *rx_buf;
-	struct rxm_rma_buf *rma_buf;
 	struct util_cq *cq;
 	struct util_cntr *cntr;
 	struct fi_cq_err_entry err_entry = {0};
@@ -1725,42 +1723,53 @@ void rxm_handle_comp_error(struct rxm_ep *rxm_ep)
 
 	switch (RXM_GET_PROTO_STATE(err_entry.op_context)) {
 	case RXM_TX:
-		eager_buf = err_entry.op_context;
-		err_entry.op_context = eager_buf->app_context;
-		err_entry.flags = ofi_tx_cq_flags(eager_buf->pkt.hdr.op);
-		ofi_buf_free(eager_buf);
+	case RXM_RNDV_TX:
+	case RXM_RNDV_WRITE_DONE_SENT:
+	case RXM_ATOMIC_RESP_WAIT:
+		tx_buf = err_entry.op_context;
+		err_entry.op_context = tx_buf->app_context;
+		err_entry.flags = ofi_tx_cq_flags(tx_buf->pkt.hdr.op);
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 		break;
+	case RXM_RNDV_READ_DONE_RECVD:
+		/* We received the response, so ignore the send error */
+		rxm_rndv_tx_finish(rxm_ep, err_entry.op_context);
+		return;
+	case RXM_RNDV_WRITE_DONE_RECVD:
+		/* We received the response, so ignore the send error */
+		rxm_rndv_rx_finish(err_entry.op_context);
+		return;
 	case RXM_INJECT_TX:
-		assert(0);
+		rxm_free_rx_buf(rxm_ep, err_entry.op_context);
+		if (cntr)
+			rxm_cntr_incerr(cntr);
+		return;
+	case RXM_CREDIT_TX:
+	case RXM_ATOMIC_RESP_SENT: /* BUG: should have consumed tx credit */
+		tx_buf = err_entry.op_context;
+		ofi_buf_free(tx_buf);
 		return;
 	case RXM_RMA:
-		rma_buf = err_entry.op_context;
-		err_entry.op_context = rma_buf->app_context;
+		tx_buf = err_entry.op_context;
+		err_entry.op_context = tx_buf->app_context;
 		/* err_entry.flags pass through from msg ep */
-		if (!(rma_buf->flags & FI_INJECT) && !rxm_ep->rdm_mr_local &&
+		if (!(tx_buf->flags & FI_INJECT) && !rxm_ep->rdm_mr_local &&
 		    rxm_ep->msg_mr_local) {
-			rxm_msg_mr_closev(rma_buf->mr.mr, rma_buf->mr.count);
+			rxm_msg_mr_closev(tx_buf->rma.mr, tx_buf->rma.count);
 		}
-		ofi_buf_free(rma_buf);
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 		break;
 	case RXM_SAR_TX:
-		sar_buf = err_entry.op_context;
-		err_entry.op_context = sar_buf->app_context;
-		err_entry.flags = ofi_tx_cq_flags(sar_buf->pkt.hdr.op);
-		if (!rxm_complete_sar(rxm_ep, sar_buf))
+		tx_buf = err_entry.op_context;
+		err_entry.op_context = tx_buf->app_context;
+		err_entry.flags = ofi_tx_cq_flags(tx_buf->pkt.hdr.op);
+		if (!rxm_complete_sar(rxm_ep, tx_buf))
 			return;
 		break;
-	case RXM_CREDIT_TX:
-		base_buf = err_entry.op_context;
-		err_entry.op_context = 0;
-		err_entry.flags = ofi_tx_cq_flags(base_buf->pkt.hdr.op);
-		break;
 	case RXM_RNDV_WRITE:
-		/* fall through */
-	case RXM_RNDV_TX:
-		rndv_buf = err_entry.op_context;
-		err_entry.op_context = rndv_buf->app_context;
-		err_entry.flags = ofi_tx_cq_flags(rndv_buf->pkt.hdr.op);
+		tx_buf = err_entry.op_context;
+		err_entry.op_context = tx_buf->app_context;
+		err_entry.flags = ofi_tx_cq_flags(tx_buf->pkt.hdr.op);
 		break;
 
 	/* Incoming application data error */
@@ -1778,7 +1787,7 @@ void rxm_handle_comp_error(struct rxm_ep *rxm_ep)
 		}
 		/* fall through */
 	case RXM_RNDV_READ_DONE_SENT:
-	case RXM_RNDV_WRITE_DATA_SENT:
+	case RXM_RNDV_WRITE_DATA_SENT: /* BUG: should fail initial send */
 	case RXM_RNDV_READ:
 		rx_buf = (struct rxm_rx_buf *) err_entry.op_context;
 		assert(rx_buf->recv_entry);
@@ -1807,10 +1816,10 @@ void rxm_handle_comp_error(struct rxm_ep *rxm_ep)
 	}
 }
 
-static int rxm_post_recv(struct rxm_rx_buf *rx_buf)
+int rxm_post_recv(struct rxm_rx_buf *rx_buf)
 {
 	struct rxm_domain *domain;
-	int ret, level;
+	int ret;
 
 	if (rx_buf->ep->srx_ctx)
 		rx_buf->conn = NULL;
@@ -1826,9 +1835,7 @@ static int rxm_post_recv(struct rxm_rx_buf *rx_buf)
 		return 0;
 
 	if (ret != -FI_EAGAIN) {
-		level = (rx_buf->conn->handle.state == RXM_CMAP_SHUTDOWN) ?
-			FI_LOG_DEBUG : FI_LOG_WARN;
-		FI_LOG(&rxm_prov, level, FI_LOG_EP_CTRL,
+		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
 		       "unable to post recv buf: %d\n", ret);
 	}
 	return ret;
@@ -1841,7 +1848,7 @@ int rxm_prepost_recv(struct rxm_ep *rxm_ep, struct fid_ep *rx_ep)
 	size_t i;
 
 	for (i = 0; i < rxm_ep->msg_info->rx_attr->size; i++) {
-		rx_buf = rxm_rx_buf_alloc(rxm_ep, rx_ep, true);
+		rx_buf = rxm_rx_buf_alloc(rxm_ep, rx_ep);
 		if (!rx_buf)
 			return -FI_ENOMEM;
 
@@ -1860,27 +1867,9 @@ void rxm_ep_do_progress(struct util_ep *util_ep)
 	struct fi_cq_data_entry comp;
 	struct dlist_entry *conn_entry_tmp;
 	struct rxm_conn *rxm_conn;
-	struct rxm_rx_buf *buf;
-	ssize_t ret;
 	size_t comp_read = 0;
 	uint64_t timestamp;
-
-	while (!dlist_empty(&rxm_ep->repost_ready_list)) {
-		dlist_pop_front(&rxm_ep->repost_ready_list, struct rxm_rx_buf,
-				buf, repost_entry);
-
-		/* Discard rx buffer if its msg_ep was closed */
-		if (!rxm_ep->srx_ctx && !buf->conn->msg_ep) {
-			ofi_buf_free(&buf->hdr);
-			continue;
-		}
-
-		ret = rxm_post_recv(buf);
-		if (ret) {
-			if (ret == -FI_EAGAIN)
-				ofi_buf_free(&buf->hdr);
-		}
-	}
+	ssize_t ret;
 
 	do {
 		ret = fi_cq_read(rxm_ep->msg_cq, &comp, 1);
@@ -1906,15 +1895,15 @@ void rxm_ep_do_progress(struct util_ep *util_ep)
 			if (timestamp - rxm_ep->msg_cq_last_poll >
 				rxm_cm_progress_interval) {
 				rxm_ep->msg_cq_last_poll = timestamp;
-				rxm_msg_eq_progress(rxm_ep);
+				rxm_conn_progress(rxm_ep);
 			}
 		}
 	} while ((ret > 0) && (++comp_read < rxm_ep->comp_per_progress));
 
-	if (!dlist_empty(&rxm_ep->deferred_tx_conn_queue)) {
-		dlist_foreach_container_safe(&rxm_ep->deferred_tx_conn_queue,
+	if (!dlist_empty(&rxm_ep->deferred_queue)) {
+		dlist_foreach_container_safe(&rxm_ep->deferred_queue,
 					     struct rxm_conn, rxm_conn,
-					     deferred_conn_entry, conn_entry_tmp) {
+					     deferred_entry, conn_entry_tmp) {
 			rxm_ep_progress_deferred_queue(rxm_ep, rxm_conn);
 		}
 	}
diff --git a/prov/rxm/src/rxm_domain.c b/prov/rxm/src/rxm_domain.c
index 0bcd8e3..cc1b86d 100644
--- a/prov/rxm/src/rxm_domain.c
+++ b/prov/rxm/src/rxm_domain.c
@@ -380,29 +380,29 @@ static struct fi_ops_mr rxm_domain_mr_ops = {
 
 static ssize_t rxm_send_credits(struct fid_ep *ep, size_t credits)
 {
-	struct rxm_conn *rxm_conn =
-		container_of(ep->fid.context, struct rxm_conn, handle);
-	struct rxm_ep *rxm_ep = rxm_conn->handle.cmap->ep;
+	struct rxm_conn *rxm_conn = ep->fid.context;
+	struct rxm_ep *rxm_ep = rxm_conn->ep;
 	struct rxm_deferred_tx_entry *def_tx_entry;
-	struct rxm_tx_base_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 	struct iovec iov;
 	struct fi_msg msg;
 	ssize_t ret;
 
-	tx_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_CREDIT);
+	tx_buf = ofi_buf_alloc(rxm_ep->tx_pool);
 	if (!tx_buf) {
 		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
 			"Ran out of buffers from TX credit buffer pool.\n");
 		return -FI_ENOMEM;
 	}
 
+	tx_buf->hdr.state = RXM_CREDIT_TX;
 	rxm_ep_format_tx_buf_pkt(rxm_conn, 0, rxm_ctrl_credit, 0, 0, FI_SEND,
 				 &tx_buf->pkt);
 	tx_buf->pkt.ctrl_hdr.type = rxm_ctrl_credit;
 	tx_buf->pkt.ctrl_hdr.msg_id = ofi_buf_index(tx_buf);
 	tx_buf->pkt.ctrl_hdr.ctrl_data = credits;
 
-	if (rxm_conn->handle.state != RXM_CMAP_CONNECTED)
+	if (rxm_conn->state != RXM_CM_CONNECTED)
 		goto defer;
 
 	iov.iov_base = &tx_buf->pkt;
@@ -427,7 +427,7 @@ defer:
 	}
 
 	def_tx_entry->credit_msg.tx_buf = tx_buf;
-	rxm_ep_enqueue_deferred_tx_queue_priority(def_tx_entry);
+	rxm_queue_deferred_tx(def_tx_entry, OFI_LIST_HEAD);
 	return FI_SUCCESS;
 }
 
@@ -484,7 +484,7 @@ struct ofi_ops_dynamic_rbuf rxm_dynamic_rbuf = {
 static void rxm_config_dyn_rbuf(struct rxm_domain *domain, struct fi_info *info,
 				struct fi_info *msg_info)
 {
-	int ret = 0;
+	int ret = 1;
 
 	/* Collective support requires rxm generated and consumed messages.
 	 * Although we could update the code to handle receiving collective
@@ -547,7 +547,7 @@ int rxm_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 	rxm_domain->util_domain.mr_map.mode &= ~FI_MR_PROV_KEY;
 
 	rxm_domain->max_atomic_size = rxm_ep_max_atomic_size(info);
-	rxm_domain->rx_post_size = rxm_buffer_size;
+	rxm_domain->rx_post_size = rxm_packet_size;
 
 	*domain = &rxm_domain->util_domain.domain_fid;
 	(*domain)->fid.ops = &rxm_domain_fi_ops;
diff --git a/prov/rxm/src/rxm_ep.c b/prov/rxm/src/rxm_ep.c
index 61ae89f..897c071 100644
--- a/prov/rxm/src/rxm_ep.c
+++ b/prov/rxm/src/rxm_ep.c
@@ -109,24 +109,22 @@ static int rxm_match_unexp_msg_tag_addr(struct dlist_entry *item, const void *ar
 
 static int rxm_buf_reg(struct ofi_bufpool_region *region)
 {
-	struct rxm_buf_pool *pool = region->pool->attr.context;
-	struct rxm_ep *rxm_ep = pool->rxm_ep;
+	struct rxm_ep *rxm_ep = region->pool->attr.context;
 	struct rxm_domain *rxm_domain;
 	int ret;
 	bool hmem_enabled = !!(rxm_ep->util_ep.caps & FI_HMEM);
 
 	if (hmem_enabled) {
 		ret = ofi_hmem_host_register(region->mem_region,
-					region->pool->region_size);
+					     region->pool->region_size);
 		if (ret != FI_SUCCESS)
 			return ret;
 	}
 
-	if ((pool->type == RXM_BUF_POOL_TX_INJECT) ||
-	    !pool->rxm_ep->msg_mr_local)
+	if (!rxm_ep->msg_mr_local)
 		return 0;
 
-	rxm_domain = container_of(pool->rxm_ep->util_ep.domain,
+	rxm_domain = container_of(rxm_ep->util_ep.domain,
 				  struct rxm_domain, util_domain);
 
 	ret = rxm_msg_mr_reg_internal(rxm_domain, region->mem_region,
@@ -143,179 +141,43 @@ static int rxm_buf_reg(struct ofi_bufpool_region *region)
 	return ret;
 }
 
-static void rxm_buf_init(struct ofi_bufpool_region *region, void *buf)
+static void rxm_init_rx_buf(struct ofi_bufpool_region *region, void *buf)
 {
-	struct rxm_buf_pool *pool = region->pool->attr.context;
-	struct rxm_pkt *pkt;
-	struct rxm_rx_buf *rx_buf;
-	struct rxm_tx_base_buf *tx_base_buf;
-	struct rxm_tx_eager_buf *tx_eager_buf;
-	struct rxm_tx_sar_buf *tx_sar_buf;
-	struct rxm_tx_rndv_buf *tx_rndv_buf;
-	struct rxm_tx_atomic_buf *tx_atomic_buf;
-	struct rxm_rma_buf *rma_buf;
-	void *mr_desc;
-	uint8_t type;
-
-	if ((pool->type != RXM_BUF_POOL_TX_INJECT) &&
-	    pool->rxm_ep->msg_mr_local) {
-		mr_desc = fi_mr_desc((struct fid_mr *) region->context);
-	} else {
-		mr_desc = NULL;
-	}
-
-	switch (pool->type) {
-	case RXM_BUF_POOL_RX:
-		rx_buf = buf;
-		rx_buf->ep = pool->rxm_ep;
-
-		rx_buf->hdr.desc = mr_desc;
-		pkt = NULL;
-		type = rxm_ctrl_eager; /* This can be any value */
-		break;
-	case RXM_BUF_POOL_TX:
-		tx_eager_buf = buf;
-		tx_eager_buf->hdr.state = RXM_TX;
-
-		tx_eager_buf->hdr.desc = mr_desc;
-		pkt = &tx_eager_buf->pkt;
-		type = rxm_ctrl_eager;
-		break;
-	case RXM_BUF_POOL_TX_INJECT:
-		tx_base_buf = buf;
-		tx_base_buf->hdr.state = RXM_INJECT_TX;
-
-		pkt = &tx_base_buf->pkt;
-		type = rxm_ctrl_eager;
-		break;
-	case RXM_BUF_POOL_TX_SAR:
-		tx_sar_buf = buf;
-		tx_sar_buf->hdr.state = RXM_SAR_TX;
-
-		tx_sar_buf->hdr.desc = mr_desc;
-		pkt = &tx_sar_buf->pkt;
-		type = rxm_ctrl_seg;
-		break;
-	case RXM_BUF_POOL_TX_CREDIT:
-		tx_base_buf = buf;
-		tx_base_buf->hdr.state = RXM_CREDIT_TX;
-
-		tx_base_buf->hdr.desc = mr_desc;
-		pkt = &tx_base_buf->pkt;
-		type = rxm_ctrl_credit;
-		break;
-	case RXM_BUF_POOL_TX_RNDV_REQ:
-		tx_rndv_buf = buf;
-
-		tx_rndv_buf->hdr.desc = mr_desc;
-		pkt = &tx_rndv_buf->pkt;
-		type = rxm_ctrl_rndv_req;
-		break;
-	case RXM_BUF_POOL_TX_ATOMIC:
-		tx_atomic_buf = buf;
+	struct rxm_ep *ep = region->pool->attr.context;
+	struct rxm_rx_buf *rx_buf = buf;
 
-		tx_atomic_buf->hdr.desc = mr_desc;
-		pkt = &tx_atomic_buf->pkt;
-		type = rxm_ctrl_atomic;
-		break;
-	case RXM_BUF_POOL_TX_RNDV_RD_DONE:
-		tx_base_buf = buf;
-		tx_base_buf->pkt.hdr.op = ofi_op_msg;
+	rx_buf->hdr.desc = ep->msg_mr_local ?
+			   fi_mr_desc((struct fid_mr *) region->context) : NULL;
+	rx_buf->ep = ep;
+	rx_buf->data = &rx_buf->pkt.data;
+}
 
-		tx_base_buf->hdr.desc = mr_desc;
-		pkt = &tx_base_buf->pkt;
-		type = rxm_ctrl_rndv_rd_done;
-		break;
-	case RXM_BUF_POOL_TX_RNDV_WR_DONE:
-		tx_base_buf = buf;
-		tx_base_buf->pkt.hdr.op = ofi_op_msg;
+static void rxm_init_tx_buf(struct ofi_bufpool_region *region, void *buf)
+{
+	struct rxm_ep *ep = region->pool->attr.context;
+	struct rxm_tx_buf *tx_buf = buf;
 
-		tx_base_buf->hdr.desc = mr_desc;
-		pkt = &tx_base_buf->pkt;
-		type = rxm_ctrl_rndv_wr_done;
-		break;
-	case RXM_BUF_POOL_TX_RNDV_WR_DATA:
-		tx_base_buf = buf;
-		tx_base_buf->pkt.hdr.op = ofi_op_msg;
+	tx_buf->hdr.desc = ep->msg_mr_local ?
+			   fi_mr_desc((struct fid_mr *) region->context) : NULL;
 
-		tx_base_buf->hdr.desc = mr_desc;
-		pkt = &tx_base_buf->pkt;
-		type = rxm_ctrl_rndv_wr_data;
-		break;
-	case RXM_BUF_POOL_RMA:
-		rma_buf = buf;
-		rma_buf->pkt.hdr.op = ofi_op_msg;
-		rma_buf->hdr.state = RXM_RMA;
-
-		rma_buf->hdr.desc = mr_desc;
-		pkt = &rma_buf->pkt;
-		type = rxm_ctrl_eager;
-		break;
-	default:
-		assert(0);
-		pkt = NULL;
-		break;
-	}
-
-	if (pkt) {
-		pkt->ctrl_hdr.version = RXM_CTRL_VERSION;
-		pkt->hdr.version = OFI_OP_VERSION;
-		pkt->ctrl_hdr.type = type;
-	}
+	tx_buf->pkt.ctrl_hdr.version = RXM_CTRL_VERSION;
+	tx_buf->pkt.hdr.version = OFI_OP_VERSION;
 }
 
 static void rxm_buf_close(struct ofi_bufpool_region *region)
 {
-	struct rxm_buf_pool *pool = region->pool->attr.context;
-	struct rxm_ep *rxm_ep = pool->rxm_ep;
-	bool hmem_enabled = !!(rxm_ep->util_ep.caps & FI_HMEM);
+	struct rxm_ep *ep = region->pool->attr.context;
 
-	if (hmem_enabled)
+	if (ep->util_ep.caps & FI_HMEM)
 		ofi_hmem_host_unregister(region->mem_region);
 
-	if ((rxm_ep->msg_mr_local) && (pool->type != RXM_BUF_POOL_TX_INJECT)) {
+	if (ep->msg_mr_local) {
 		/* We would get a (fid_mr *) in context but
 		 * it is safe to cast it into (fid *) */
 		fi_close(region->context);
 	}
 }
 
-static void rxm_buf_pool_destroy(struct rxm_buf_pool *pool)
-{
-	/* This indicates whether the pool is allocated or not */
-	if (pool->rxm_ep) {
-		ofi_bufpool_destroy(pool->pool);
-	}
-}
-
-static int rxm_buf_pool_create(struct rxm_ep *rxm_ep, size_t size,
-			       size_t max_cnt, size_t chunk_count,
-			       struct rxm_buf_pool *pool,
-			       enum rxm_buf_pool_type type)
-{
-	int ret;
-	struct ofi_bufpool_attr attr = {
-		.size		= size,
-		.alignment	= 16,
-		.max_cnt	= max_cnt,
-		.chunk_cnt	= chunk_count,
-		.alloc_fn	= rxm_buf_reg,
-		.free_fn	= rxm_buf_close,
-		.init_fn	= rxm_buf_init,
-		.context	= pool,
-		.flags		= OFI_BUFPOOL_NO_TRACK | OFI_BUFPOOL_HUGEPAGES,
-	};
-
-	pool->rxm_ep = rxm_ep;
-	pool->type = type;
-	ret = ofi_bufpool_create_attr(&attr, &pool->pool);
-	if (ret)
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to create buf pool\n");
-
-	return ret;
-}
-
 static void rxm_recv_entry_init(struct rxm_recv_entry *entry, void *arg)
 {
 	struct rxm_recv_queue *recv_queue = arg;
@@ -374,76 +236,49 @@ static void rxm_recv_queue_close(struct rxm_recv_queue *recv_queue)
 	/* It indicates that the recv_queue were allocated */
 	if (recv_queue->fs) {
 		rxm_recv_fs_free(recv_queue->fs);
+		recv_queue->fs = NULL;
 	}
 	// TODO cleanup recv_list and unexp msg list
 }
 
-static int rxm_ep_txrx_pool_create(struct rxm_ep *rxm_ep)
-{
-	int ret, i;
-	size_t entry_sizes[] = {
-		[RXM_BUF_POOL_RX] = rxm_eager_limit +
-				    sizeof(struct rxm_rx_buf),
-		[RXM_BUF_POOL_TX] = rxm_eager_limit +
-				    sizeof(struct rxm_tx_eager_buf),
-		[RXM_BUF_POOL_TX_INJECT] = rxm_ep->inject_limit +
-					   sizeof(struct rxm_tx_base_buf),
-		[RXM_BUF_POOL_TX_RNDV_RD_DONE] = sizeof(struct rxm_tx_base_buf),
-		[RXM_BUF_POOL_TX_RNDV_WR_DONE] = sizeof(struct rxm_tx_base_buf),
-		[RXM_BUF_POOL_TX_RNDV_REQ] = sizeof(struct rxm_rndv_hdr) +
-					 rxm_ep->buffered_min +
-					 sizeof(struct rxm_tx_rndv_buf),
-		[RXM_BUF_POOL_TX_RNDV_WR_DATA] = sizeof(struct rxm_rndv_hdr) +
-						   sizeof(struct rxm_tx_base_buf),
-		[RXM_BUF_POOL_TX_ATOMIC] = rxm_eager_limit +
-					 sizeof(struct rxm_tx_atomic_buf),
-		[RXM_BUF_POOL_TX_SAR] = rxm_eager_limit +
-					sizeof(struct rxm_tx_sar_buf),
-		[RXM_BUF_POOL_TX_CREDIT] = sizeof(struct rxm_tx_base_buf),
-		[RXM_BUF_POOL_RMA] = rxm_eager_limit +
-				     sizeof(struct rxm_rma_buf),
-	};
-
-	dlist_init(&rxm_ep->repost_ready_list);
+static int rxm_ep_create_pools(struct rxm_ep *rxm_ep)
+{
+	struct ofi_bufpool_attr attr = {0};
+	int ret;
 
-	rxm_ep->buf_pools = calloc(1, RXM_BUF_POOL_MAX *
-				      sizeof(*rxm_ep->buf_pools));
-	if (!rxm_ep->buf_pools)
-		return -FI_ENOMEM;
+	attr.size = rxm_buffer_size + sizeof(struct rxm_rx_buf);
+	attr.alignment = 16;
+	attr.chunk_cnt = 1024;
+	attr.alloc_fn = rxm_buf_reg;
+	attr.free_fn = rxm_buf_close;
+	attr.init_fn = rxm_init_rx_buf;
+	attr.context = rxm_ep;
+	attr.flags = OFI_BUFPOOL_NO_TRACK;
 
-	for (i = RXM_BUF_POOL_START; i < RXM_BUF_POOL_MAX; i++) {
-		if ((i == RXM_BUF_POOL_TX_INJECT) &&
-		    (rxm_ep->util_ep.domain->threading != FI_THREAD_SAFE))
-			continue;
+	ret = ofi_bufpool_create_attr(&attr, &rxm_ep->rx_pool);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"Unable to create rx buf pool\n");
+		return ret;
+	}
 
-		ret = rxm_buf_pool_create(rxm_ep, entry_sizes[i],
-					  (i == RXM_BUF_POOL_RX ||
-					   i == RXM_BUF_POOL_TX_ATOMIC) ? 0 :
-					  rxm_ep->rxm_info->tx_attr->size,
-					  1024,
-					  &rxm_ep->buf_pools[i], i);
-		if (ret)
-			goto err;
+	attr.size = rxm_buffer_size + sizeof(struct rxm_tx_buf);
+	attr.init_fn = rxm_init_tx_buf;
+	ret = ofi_bufpool_create_attr(&attr, &rxm_ep->tx_pool);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"Unable to create rx buf pool\n");
+		goto free_rx_pool;
 	}
 
-	return FI_SUCCESS;
+	return 0;
 
-err:
-	while (--i >= RXM_BUF_POOL_START)
-		rxm_buf_pool_destroy(&rxm_ep->buf_pools[i]);
-	free(rxm_ep->buf_pools);
+free_rx_pool:
+	ofi_bufpool_destroy(rxm_ep->rx_pool);
+	rxm_ep->rx_pool = NULL;
 	return ret;
 }
 
-static void rxm_ep_txrx_pool_destroy(struct rxm_ep *rxm_ep)
-{
-	size_t i;
-
-	for (i = RXM_BUF_POOL_START; i < RXM_BUF_POOL_MAX; i++)
-		rxm_buf_pool_destroy(&rxm_ep->buf_pools[i]);
-	free(rxm_ep->buf_pools);
-}
-
 static int rxm_multi_recv_pool_init(struct rxm_ep *rxm_ep)
 {
 	struct ofi_bufpool_attr attr = {
@@ -489,21 +324,25 @@ err_recv_tag:
 	return ret;
 }
 
-static void rxm_ep_rx_queue_close(struct rxm_ep *rxm_ep)
-{
-	rxm_recv_queue_close(&rxm_ep->trecv_queue);
-	rxm_recv_queue_close(&rxm_ep->recv_queue);
-}
-
 /* It is safe to call this function, even if `rxm_ep_txrx_res_open`
  * has not yet been called */
-static void rxm_ep_txrx_res_close(struct rxm_ep *rxm_ep)
+static void rxm_ep_txrx_res_close(struct rxm_ep *ep)
 {
-	rxm_ep_rx_queue_close(rxm_ep);
-	if (rxm_ep->multi_recv_pool)
-		ofi_bufpool_destroy(rxm_ep->multi_recv_pool);
-	if (rxm_ep->buf_pools)
-		rxm_ep_txrx_pool_destroy(rxm_ep);
+	rxm_recv_queue_close(&ep->trecv_queue);
+	rxm_recv_queue_close(&ep->recv_queue);
+
+	if (ep->multi_recv_pool) {
+		ofi_bufpool_destroy(ep->multi_recv_pool);
+		ep->multi_recv_pool = NULL;
+	}
+	if (ep->rx_pool) {
+		ofi_bufpool_destroy(ep->rx_pool);
+		ep->rx_pool = NULL;
+	}
+	if (ep->tx_pool) {
+		ofi_bufpool_destroy(ep->tx_pool);
+		ep->tx_pool = NULL;
+	}
 }
 
 static int rxm_setname(fid_t fid, void *addr, size_t addrlen)
@@ -652,7 +491,7 @@ static int rxm_ep_setopt(fid_t fid, int level, int optname,
 			rxm_ep->min_multi_recv_size);
 		break;
 	case FI_OPT_BUFFERED_MIN:
-		if (rxm_ep->buf_pools) {
+		if (rxm_ep->rx_pool) {
 			FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
 				"Endpoint already enabled. Can't set opt now!\n");
 			ret = -FI_EOPBADSTATE;
@@ -670,7 +509,7 @@ static int rxm_ep_setopt(fid_t fid, int level, int optname,
 		}
 		break;
 	case FI_OPT_BUFFERED_LIMIT:
-		if (rxm_ep->buf_pools) {
+		if (rxm_ep->rx_pool) {
 			FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
 				"Endpoint already enabled. Can't set opt now!\n");
 			ret = -FI_EOPBADSTATE;
@@ -705,7 +544,7 @@ static struct fi_ops_ep rxm_ops_ep = {
 	.tx_size_left = fi_no_tx_size_left,
 };
 
-/* Caller must hold recv_queue->lock */
+/* Caller must hold recv_queue->lock -- TODO which lock? */
 static struct rxm_rx_buf *
 rxm_get_unexp_msg(struct rxm_recv_queue *recv_queue, fi_addr_t addr,
 		  uint64_t tag, uint64_t ignore)
@@ -761,8 +600,8 @@ static int rxm_handle_unexp_sar(struct rxm_recv_queue *recv_queue,
 			continue;
 
 		if (!rx_buf->conn) {
-			rx_buf->conn = rxm_key2conn(rx_buf->ep,
-							rx_buf->pkt.ctrl_hdr.conn_id);
+			rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					(int) rx_buf->pkt.ctrl_hdr.conn_id);
 		}
 		if (recv_entry->sar.conn != rx_buf->conn)
 			continue;
@@ -896,6 +735,31 @@ rxm_multi_recv_entry_get(struct rxm_ep *rxm_ep, const struct iovec *iov,
 	return recv_entry;
 }
 
+struct rxm_tx_buf *rxm_get_tx_buf(struct rxm_ep *ep)
+{
+	struct rxm_tx_buf *buf;
+
+	assert(fastlock_held(&ep->util_ep.lock));
+	if (!ep->tx_credit)
+		return NULL;
+
+	buf = ofi_buf_alloc(ep->tx_pool);
+	if (buf) {
+		OFI_DBG_SET(buf->user_tx, true);
+		ep->tx_credit--;
+	}
+	return buf;
+}
+
+void rxm_free_rx_buf(struct rxm_ep *ep, struct rxm_tx_buf *buf)
+{
+	assert(fastlock_held(&ep->util_ep.lock));
+	assert(buf->user_tx);
+	OFI_DBG_SET(buf->user_tx, false);
+	ep->tx_credit++;
+	ofi_buf_free(buf);
+}
+
 /*
  * We don't expect to have unexpected messages when the app is using
  * multi-recv buffers.  Optimize for that case.
@@ -1105,23 +969,6 @@ void rxm_rndv_hdr_init(struct rxm_ep *rxm_ep, void *buf,
 }
 
 static ssize_t
-rxm_ep_msg_inject_send(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		       struct rxm_pkt *tx_pkt, size_t pkt_size,
-		       ofi_cntr_inc_func cntr_inc_func)
-{
-	FI_DBG(&rxm_prov, FI_LOG_EP_DATA, "Posting inject with length: %zu"
-	       " tag: 0x%" PRIx64 "\n", pkt_size, tx_pkt->hdr.tag);
-
-	assert((tx_pkt->hdr.flags & FI_REMOTE_CQ_DATA) || !tx_pkt->hdr.flags);
-	assert(pkt_size <= rxm_ep->inject_limit);
-
-	ssize_t ret = fi_inject(rxm_conn->msg_ep, tx_pkt, pkt_size, 0);
-	if (ret == -FI_EAGAIN)
-		rxm_ep_do_progress(&rxm_ep->util_ep);
-	return ret;
-}
-
-static ssize_t
 rxm_ep_msg_normal_send(struct rxm_conn *rxm_conn, struct rxm_pkt *tx_pkt,
 		       size_t pkt_size, void *desc, void *context)
 {
@@ -1139,34 +986,32 @@ rxm_alloc_rndv_buf(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 		   void **desc, size_t data_len, uint64_t data,
 		   uint64_t flags, uint64_t tag, uint8_t op,
 		   enum fi_hmem_iface iface, uint64_t device,
-		   struct rxm_tx_rndv_buf **rndv_buf)
+		   struct rxm_tx_buf **rndv_buf)
 {
 	struct fid_mr *rxm_mr_msg_mr[RXM_IOV_LIMIT];
 	struct fid_mr **mr_iov;
 	size_t len, i;
 	ssize_t ret;
 
-	*rndv_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_RNDV_REQ);
-	if (!*rndv_buf) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-			"Ran out of buffers from RNDV buffer pool\n");
+	*rndv_buf = rxm_get_tx_buf(rxm_ep);
+	if (!*rndv_buf)
 		return -FI_EAGAIN;
-	}
 
+	(*rndv_buf)->pkt.ctrl_hdr.type = rxm_ctrl_rndv_req;
 	rxm_ep_format_tx_buf_pkt(rxm_conn, data_len, op, data, tag,
 				 flags, &(*rndv_buf)->pkt);
 	(*rndv_buf)->pkt.ctrl_hdr.msg_id = ofi_buf_index(*rndv_buf);
 	(*rndv_buf)->app_context = context;
 	(*rndv_buf)->flags = flags;
-	(*rndv_buf)->count = count;
+	(*rndv_buf)->rma.count = count;
 
 	if (!rxm_ep->rdm_mr_local) {
-		ret = rxm_msg_mr_regv(rxm_ep, iov, (*rndv_buf)->count, data_len,
+		ret = rxm_msg_mr_regv(rxm_ep, iov, (*rndv_buf)->rma.count, data_len,
 				      rxm_ep->rndv_ops->tx_mr_access,
-				      (*rndv_buf)->mr);
+				      (*rndv_buf)->rma.mr);
 		if (ret)
 			goto err;
-		mr_iov = (*rndv_buf)->mr;
+		mr_iov = (*rndv_buf)->rma.mr;
 	} else {
 		for (i = 0; i < count; i++)
 			rxm_mr_msg_mr[i] = ((struct rxm_mr *) desc[i])->msg_mr;
@@ -1183,7 +1028,7 @@ rxm_alloc_rndv_buf(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 	}
 
 	rxm_rndv_hdr_init(rxm_ep, &(*rndv_buf)->pkt.data, iov,
-			  (*rndv_buf)->count, mr_iov);
+			  (*rndv_buf)->rma.count, mr_iov);
 
 	len = sizeof(struct rxm_pkt) + sizeof(struct rxm_rndv_hdr);
 
@@ -1199,63 +1044,66 @@ rxm_alloc_rndv_buf(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 	return len;
 
 err:
-	ofi_buf_free(*rndv_buf);
+	rxm_free_rx_buf(rxm_ep, *rndv_buf);
 	return ret;
 }
 
 static ssize_t
 rxm_ep_rndv_tx_send(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		   struct rxm_tx_rndv_buf *tx_buf, size_t pkt_size)
+		    struct rxm_tx_buf *tx_buf, size_t pkt_size)
 {
 	ssize_t ret;
 
-	RXM_UPDATE_STATE(FI_LOG_EP_DATA, tx_buf, RXM_RNDV_TX);
 	if (pkt_size <= rxm_ep->inject_limit) {
 		if (rxm_ep->rndv_ops == &rxm_rndv_ops_write)
-			RXM_UPDATE_STATE(FI_LOG_EP_DATA, tx_buf, RXM_RNDV_WRITE_DATA_WAIT);
+			RXM_UPDATE_STATE(FI_LOG_EP_DATA, tx_buf,
+					 RXM_RNDV_WRITE_DATA_WAIT);
 		else
-			RXM_UPDATE_STATE(FI_LOG_EP_DATA, tx_buf, RXM_RNDV_READ_DONE_WAIT);
-		ret = rxm_ep_msg_inject_send(rxm_ep, rxm_conn, &tx_buf->pkt,
-					     pkt_size, ofi_cntr_inc_noop);
-	} else {
-		tx_buf->hdr.state = RXM_RNDV_TX;
+			RXM_UPDATE_STATE(FI_LOG_EP_DATA, tx_buf,
+					 RXM_RNDV_READ_DONE_WAIT);
 
+		ret = fi_inject(rxm_conn->msg_ep, &tx_buf->pkt, pkt_size, 0);
+	} else {
+		RXM_UPDATE_STATE(FI_LOG_EP_DATA, tx_buf, RXM_RNDV_TX);
 		ret = rxm_ep_msg_normal_send(rxm_conn, &tx_buf->pkt, pkt_size,
 					     tx_buf->hdr.desc, tx_buf);
 	}
+
 	if (ret)
 		goto err;
+
 	return FI_SUCCESS;
+
 err:
 	FI_DBG(&rxm_prov, FI_LOG_EP_DATA,
 	       "Transmit for MSG provider failed\n");
 	if (!rxm_ep->rdm_mr_local)
-		rxm_msg_mr_closev(tx_buf->mr, tx_buf->count);
-	ofi_buf_free(tx_buf);
+		rxm_msg_mr_closev(tx_buf->rma.mr, tx_buf->rma.count);
+	rxm_free_rx_buf(rxm_ep, tx_buf);
 	return ret;
 }
 
 static size_t
 rxm_ep_sar_calc_segs_cnt(struct rxm_ep *rxm_ep, size_t data_len)
 {
-	return (data_len + rxm_eager_limit - 1) / rxm_eager_limit;
+	return (data_len + rxm_buffer_size - 1) / rxm_buffer_size;
 }
 
-static struct rxm_tx_sar_buf *
+static struct rxm_tx_buf *
 rxm_ep_sar_tx_prepare_segment(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 			      void *app_context, size_t total_len,
 			      size_t seg_len, size_t seg_no, uint64_t data,
 			      uint64_t flags, uint64_t tag, uint8_t op,
 			      enum rxm_sar_seg_type seg_type, uint64_t *msg_id)
 {
-	struct rxm_tx_sar_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 
-	tx_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_SAR);
-	if (!tx_buf) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-			"Ran out of buffers from SAR buffer pool\n");
+	tx_buf = rxm_get_tx_buf(rxm_ep);
+	if (!tx_buf)
 		return NULL;
-	};
+
+	tx_buf->hdr.state = RXM_SAR_TX;
+	tx_buf->pkt.ctrl_hdr.type = rxm_ctrl_seg;
 
 	rxm_ep_format_tx_buf_pkt(rxm_conn, total_len, op, data, tag, flags,
 				 &tx_buf->pkt);
@@ -1275,15 +1123,14 @@ rxm_ep_sar_tx_prepare_segment(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 
 static void
 rxm_ep_sar_tx_cleanup(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		      struct rxm_tx_sar_buf *tx_buf)
+		      struct rxm_tx_buf *tx_buf)
 {
-	struct rxm_tx_sar_buf *first_tx_buf;
+	struct rxm_tx_buf *first_tx_buf;
 
-	first_tx_buf = ofi_bufpool_get_ibuf(rxm_ep->
-					    buf_pools[RXM_BUF_POOL_TX_SAR].pool,
+	first_tx_buf = ofi_bufpool_get_ibuf(rxm_ep->tx_pool,
 					    tx_buf->pkt.ctrl_hdr.msg_id);
-	ofi_buf_free(first_tx_buf);
-	ofi_buf_free(tx_buf);
+	rxm_free_rx_buf(rxm_ep, first_tx_buf);
+	rxm_free_rx_buf(rxm_ep, tx_buf);
 }
 
 static ssize_t
@@ -1293,10 +1140,10 @@ rxm_ep_sar_tx_prepare_and_send_segment(struct rxm_ep *rxm_ep,
 		size_t seg_no, size_t segs_cnt, uint64_t data, uint64_t flags,
 		uint64_t tag, uint8_t op, const struct iovec *iov,
 		uint8_t count, size_t *iov_offset,
-		struct rxm_tx_sar_buf **out_tx_buf,
+		struct rxm_tx_buf **out_tx_buf,
 		enum fi_hmem_iface iface, uint64_t device)
 {
-	struct rxm_tx_sar_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 	enum rxm_sar_seg_type seg_type = RXM_SAR_SEG_MIDDLE;
 	ssize_t ret __attribute__((unused));
 
@@ -1327,32 +1174,34 @@ rxm_ep_sar_tx_prepare_and_send_segment(struct rxm_ep *rxm_ep,
 }
 
 static ssize_t
-rxm_ep_sar_tx_send(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		   void *context, uint8_t count, const struct iovec *iov,
-		   size_t data_len, size_t segs_cnt, uint64_t data,
-		   uint64_t flags, uint64_t tag, uint8_t op,
-		   enum fi_hmem_iface iface, uint64_t device)
+rxm_send_sar(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
+	     const struct iovec *iov, void **desc, uint8_t count,
+	     void *context, uint64_t data, uint64_t flags, uint64_t tag,
+	     uint8_t op, size_t data_len, size_t segs_cnt)
 {
-	struct rxm_tx_sar_buf *tx_buf, *first_tx_buf;
+	struct rxm_tx_buf *tx_buf, *first_tx_buf;
 	size_t i, iov_offset = 0, remain_len = data_len;
-	ssize_t ret;
 	struct rxm_deferred_tx_entry *def_tx;
+	enum fi_hmem_iface iface;
+	uint64_t device;
 	uint64_t msg_id = 0;
+	ssize_t ret;
 
 	assert(segs_cnt >= 2);
+	iface = rxm_mr_desc_to_hmem_iface_dev(desc, count, &device);
 
 	first_tx_buf = rxm_ep_sar_tx_prepare_segment(rxm_ep, rxm_conn, context,
-						     data_len, rxm_eager_limit,
+						     data_len, rxm_buffer_size,
 						     0, data, flags, tag, op,
 						     RXM_SAR_SEG_FIRST, &msg_id);
 	if (!first_tx_buf)
 		return -FI_EAGAIN;
 
-	ret = ofi_copy_from_hmem_iov(first_tx_buf->pkt.data, rxm_eager_limit,
+	ret = ofi_copy_from_hmem_iov(first_tx_buf->pkt.data, rxm_buffer_size,
 				     iface, device, iov, count, iov_offset);
-	assert(ret == rxm_eager_limit);
+	assert(ret == rxm_buffer_size);
 
-	iov_offset += rxm_eager_limit;
+	iov_offset += rxm_buffer_size;
 
 	ret = fi_send(rxm_conn->msg_ep, &first_tx_buf->pkt,
 		      sizeof(struct rxm_pkt) + first_tx_buf->pkt.ctrl_hdr.seg_size,
@@ -1360,37 +1209,37 @@ rxm_ep_sar_tx_send(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 	if (ret) {
 		if (ret == -FI_EAGAIN)
 			rxm_ep_do_progress(&rxm_ep->util_ep);
-		ofi_buf_free(first_tx_buf);
+		rxm_free_rx_buf(rxm_ep, first_tx_buf);
 		return ret;
 	}
 
-	remain_len -= rxm_eager_limit;
+	remain_len -= rxm_buffer_size;
 
 	for (i = 1; i < segs_cnt; i++) {
 		ret = rxm_ep_sar_tx_prepare_and_send_segment(
-					rxm_ep, rxm_conn, context, data_len, remain_len,
-					msg_id, rxm_eager_limit, i, segs_cnt, data,
-					flags, tag, op, iov, count, &iov_offset, &tx_buf,
-					iface, device);
+				rxm_ep, rxm_conn, context, data_len, remain_len,
+				msg_id, rxm_buffer_size, i, segs_cnt, data,
+				flags, tag, op, iov, count, &iov_offset, &tx_buf,
+				iface, device);
 		if (ret) {
 			if (ret == -FI_EAGAIN)
 				goto defer;
 			goto free;
 		}
-		remain_len -= rxm_eager_limit;
+		remain_len -= rxm_buffer_size;
 	}
 
 	return 0;
 
 free:
-	ofi_buf_free(first_tx_buf);
+	rxm_free_rx_buf(rxm_ep, first_tx_buf);
 	return ret;
 defer:
 	def_tx = rxm_ep_alloc_deferred_tx_entry(rxm_ep,
 			rxm_conn, RXM_DEFERRED_TX_SAR_SEG);
 	if (!def_tx) {
 		if (tx_buf)
-			ofi_buf_free(tx_buf);
+			rxm_free_rx_buf(rxm_ep, tx_buf);
 		return -FI_ENOMEM;
 	}
 	memcpy(def_tx->sar_seg.payload.iov,
@@ -1410,7 +1259,7 @@ defer:
 	def_tx->sar_seg.msg_id = msg_id;
 	def_tx->sar_seg.iface = iface;
 	def_tx->sar_seg.device = device;
-	rxm_ep_enqueue_deferred_tx_queue(def_tx);
+	rxm_queue_deferred_tx(def_tx, OFI_LIST_TAIL);
 	return 0;
 }
 
@@ -1420,113 +1269,93 @@ rxm_ep_emulate_inject(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 		      uint64_t data, uint64_t flags, uint64_t tag,
 		      uint8_t op)
 {
-	struct rxm_tx_eager_buf *tx_buf;
+	struct rxm_tx_buf *tx_buf;
 	ssize_t ret;
-	enum fi_hmem_iface iface = FI_HMEM_SYSTEM;
-	const struct iovec iov = {
-		.iov_base = (void *)buf,
-		.iov_len = len,
-	};
 
-	tx_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX);
-	if (!tx_buf) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-			"Ran out of buffers from Eager buffer pool\n");
+	tx_buf = rxm_get_tx_buf(rxm_ep);
+	if (!tx_buf)
 		return -FI_EAGAIN;
-	}
-	/* This is needed so that we don't report bogus context in fi_cq_err_entry */
-	tx_buf->app_context = NULL;
-
-	rxm_ep_format_tx_buf_pkt(rxm_conn, len, op, data, tag, flags, &tx_buf->pkt);
-
-	ret = ofi_copy_from_hmem_iov(tx_buf->pkt.data, len, iface, 0, &iov, 1,
-				     0);
-	assert(ret == len);
 
+	tx_buf->hdr.state = RXM_INJECT_TX;
+	tx_buf->pkt.ctrl_hdr.type = rxm_ctrl_eager;
 	tx_buf->flags = flags;
 
+	rxm_ep_format_tx_buf_pkt(rxm_conn, len, op, data, tag, flags,
+				 &tx_buf->pkt);
+	memcpy(tx_buf->pkt.data, buf, len);
+
 	ret = rxm_ep_msg_normal_send(rxm_conn, &tx_buf->pkt, pkt_size,
 				     tx_buf->hdr.desc, tx_buf);
 	if (ret) {
 		if (ret == -FI_EAGAIN)
 			rxm_ep_do_progress(&rxm_ep->util_ep);
-		ofi_buf_free(tx_buf);
+		rxm_free_rx_buf(rxm_ep, tx_buf);
 	}
 	return ret;
 }
 
-static ssize_t
-rxm_ep_inject_send_fast(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-			const void *buf, size_t len, struct rxm_pkt *inject_pkt)
+static bool
+rxm_use_msg_tinject(struct rxm_ep *ep, uint8_t op)
 {
-	size_t pkt_size = sizeof(struct rxm_pkt) + len;
-	ssize_t ret;
+	struct rxm_domain *domain;
 
-	assert(len <= rxm_ep->rxm_info->tx_attr->inject_size);
+	domain = container_of(ep->util_ep.domain, struct rxm_domain,
+			      util_domain);
+	return domain->dyn_rbuf && (op == ofi_op_tagged);
+}
 
-	if (pkt_size <= rxm_ep->inject_limit && !rxm_ep->util_ep.tx_cntr) {
-		inject_pkt->hdr.size = len;
-		memcpy(inject_pkt->data, buf, len);
-		ret = rxm_ep_msg_inject_send(rxm_ep, rxm_conn, inject_pkt,
-					     pkt_size,
-					     rxm_ep->util_ep.tx_cntr_inc);
-	} else {
-		ret = rxm_ep_emulate_inject(rxm_ep, rxm_conn, buf, len,
-					    pkt_size, inject_pkt->hdr.data,
-					    inject_pkt->hdr.flags,
-					    inject_pkt->hdr.tag,
-					    inject_pkt->hdr.op);
-	}
-	return ret;
+static ssize_t
+rxm_msg_tinject(struct fid_ep *msg_ep, const void *buf, size_t len,
+		bool cq_data, uint64_t data, uint64_t tag)
+{
+	return cq_data ?
+		fi_tinject(msg_ep, buf, len, 0, tag) :
+		fi_tinjectdata(msg_ep, buf, len, data, 0, tag);
 }
 
 static ssize_t
 rxm_ep_inject_send(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		   const void *buf, size_t len, uint64_t data,
-		   uint64_t flags, uint64_t tag, uint8_t op)
+		   const void *buf, size_t len)
 {
-	struct rxm_tx_base_buf *tx_buf;
-	size_t pkt_size = sizeof(struct rxm_pkt) + len;
+	struct rxm_pkt *inject_pkt = rxm_ep->inject_pkt;
+	size_t pkt_size = sizeof(*inject_pkt) + len;
 	ssize_t ret;
 
 	assert(len <= rxm_ep->rxm_info->tx_attr->inject_size);
 
-	if (pkt_size <= rxm_ep->inject_limit &&
-	    !rxm_ep->util_ep.tx_cntr) {
-		tx_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX_INJECT);
-		if (!tx_buf) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-				"Ran out of eager inject buffers\n");
-			ret = -FI_EAGAIN;
-			goto unlock;
+	inject_pkt->ctrl_hdr.conn_id = rxm_conn->remote_index;
+	if (pkt_size <= rxm_ep->inject_limit && !rxm_ep->util_ep.tx_cntr) {
+		if (rxm_use_msg_tinject(rxm_ep, inject_pkt->hdr.op)) {
+			return rxm_msg_tinject(rxm_conn->msg_ep, buf, len,
+					       inject_pkt->hdr.flags &
+							FI_REMOTE_CQ_DATA,
+					       inject_pkt->hdr.data,
+					       inject_pkt->hdr.tag);
 		}
-		rxm_ep_format_tx_buf_pkt(rxm_conn, len, op, data, tag,
-					 flags, &tx_buf->pkt);
-		memcpy(tx_buf->pkt.data, buf, len);
-
-		ret = rxm_ep_msg_inject_send(rxm_ep, rxm_conn, &tx_buf->pkt,
-					     pkt_size,
-					     rxm_ep->util_ep.tx_cntr_inc);
-		ofi_buf_free(tx_buf);
+
+		inject_pkt->hdr.size = len;
+		memcpy(inject_pkt->data, buf, len);
+		ret = fi_inject(rxm_conn->msg_ep, inject_pkt, pkt_size, 0);
 	} else {
 		ret = rxm_ep_emulate_inject(rxm_ep, rxm_conn, buf, len,
-					    pkt_size, data, flags, tag, op);
+					    pkt_size, inject_pkt->hdr.data,
+					    inject_pkt->hdr.flags,
+					    inject_pkt->hdr.tag,
+					    inject_pkt->hdr.op);
 	}
-unlock:
 	return ret;
-
 }
 
 static bool
 rxm_use_direct_send(struct rxm_ep *ep, size_t iov_count, uint64_t flags)
 {
-	return ep->enable_direct_send && !(flags & FI_INJECT) &&
+	return ep->enable_direct_send &&
 		(iov_count < ep->msg_info->tx_attr->iov_limit);
 }
 
 static ssize_t
 rxm_direct_send(struct rxm_ep *ep, struct rxm_conn *rxm_conn,
-		struct rxm_tx_eager_buf *tx_buf,
+		struct rxm_tx_buf *tx_buf,
 		const struct iovec *iov, void **desc, size_t count)
 {
 	struct iovec send_iov[RXM_IOV_LIMIT];
@@ -1537,7 +1366,7 @@ rxm_direct_send(struct rxm_ep *ep, struct rxm_conn *rxm_conn,
 
 	send_iov[0].iov_base = &tx_buf->pkt;
 	send_iov[0].iov_len = sizeof(tx_buf->pkt);
-	memcpy(send_iov + 1, iov, sizeof(*iov) * count);
+	memcpy(&send_iov[1], iov, sizeof(*iov) * count);
 
 	if (ep->msg_mr_local) {
 		send_desc[0] = tx_buf->hdr.desc;
@@ -1557,18 +1386,127 @@ rxm_direct_send(struct rxm_ep *ep, struct rxm_conn *rxm_conn,
 	return ret;
 }
 
+static bool
+rxm_use_msg_tsend(struct rxm_ep *ep, size_t iov_count, uint8_t op)
+{
+	struct rxm_domain *domain;
+
+	domain = container_of(ep->util_ep.domain, struct rxm_domain,
+			      util_domain);
+
+	return domain->dyn_rbuf && (op == ofi_op_tagged) &&
+	       (iov_count <= ep->msg_info->tx_attr->iov_limit);
+}
+
 static ssize_t
-rxm_ep_send_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
-		   const struct iovec *iov, void **desc, size_t count,
-		   void *context, uint64_t data, uint64_t flags, uint64_t tag,
-		   uint8_t op)
+rxm_msg_tsend(struct rxm_ep *ep, struct rxm_conn *conn,
+	      struct rxm_tx_buf *tx_buf,
+	      const struct iovec *iov, size_t count,
+	      uint64_t data, uint64_t tag)
 {
-	struct rxm_tx_eager_buf *eager_buf;
-	struct rxm_tx_rndv_buf *rndv_buf;
-	size_t data_len, total_len;
+	struct fi_msg_tagged msg;
+
+	assert(!(ep->msg_info->domain_attr->mr_mode & FI_MR_LOCAL));
+
+	if (count == 0) {
+		return !(tx_buf->flags & FI_REMOTE_CQ_DATA) ?
+			fi_tsend(conn->msg_ep, NULL, 0, NULL, 0, tag, tx_buf) :
+			fi_tsenddata(conn->msg_ep, NULL, 0, NULL, data, 0,
+				     tag, tx_buf);
+	}
+
+	if (count == 1) {
+		return !(tx_buf->flags & FI_REMOTE_CQ_DATA) ?
+			fi_tsend(conn->msg_ep, iov[0].iov_base, iov[0].iov_len,
+				 NULL, 0, tag, tx_buf) :
+			fi_tsenddata(conn->msg_ep, iov[0].iov_base,
+				     iov[0].iov_len, NULL, data, 0, tag,
+				     tx_buf);
+	}
+
+	if (!(tx_buf->flags & FI_REMOTE_CQ_DATA)) {
+		return fi_tsendv(conn->msg_ep, iov, NULL, count, 0, tag,
+				 tx_buf);
+	}
+
+	msg.addr = 0;
+	msg.context = tx_buf;
+	msg.data = data;
+	msg.desc = NULL;
+	msg.ignore = 0;
+	msg.iov_count = count;
+	msg.msg_iov = iov;
+	msg.tag = tag;
+
+	return fi_tsendmsg(conn->msg_ep, &msg, ep->msg_info->tx_attr->op_flags |
+			   FI_REMOTE_CQ_DATA);
+}
+
+static ssize_t
+rxm_send_eager(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
+	       const struct iovec *iov, void **desc, size_t count,
+	       void *context, uint64_t data, uint64_t flags, uint64_t tag,
+	       uint8_t op, size_t data_len, size_t total_len)
+{
+	struct rxm_tx_buf *eager_buf;
+	enum fi_hmem_iface iface;
+	uint64_t device;
 	ssize_t ret;
+
+	eager_buf = rxm_get_tx_buf(rxm_ep);
+	if (!eager_buf)
+		return -FI_EAGAIN;
+
+	eager_buf->hdr.state = RXM_TX;
+	eager_buf->pkt.ctrl_hdr.type = rxm_ctrl_eager;
+	eager_buf->app_context = context;
+	eager_buf->flags = flags;
+
+	if (rxm_use_msg_tsend(rxm_ep, count, op)) {
+		/* hdr isn't sent, but op is accessed handling completion */
+		eager_buf->pkt.hdr.op = op;
+		ret = rxm_msg_tsend(rxm_ep, rxm_conn, eager_buf, iov, count,
+				    data, tag);
+	} else if (rxm_use_direct_send(rxm_ep, count, flags)) {
+		rxm_ep_format_tx_buf_pkt(rxm_conn, data_len, op, data, tag,
+					 flags, &eager_buf->pkt);
+
+		ret = rxm_direct_send(rxm_ep, rxm_conn, eager_buf,
+				      iov, desc, count);
+	} else {
+		rxm_ep_format_tx_buf_pkt(rxm_conn, data_len, op, data, tag,
+					 flags, &eager_buf->pkt);
+
+		iface = rxm_mr_desc_to_hmem_iface_dev(desc, count, &device);
+		ret = ofi_copy_from_hmem_iov(eager_buf->pkt.data,
+					     eager_buf->pkt.hdr.size,
+					     iface, device, iov, count, 0);
+		assert(ret == eager_buf->pkt.hdr.size);
+
+		ret = rxm_ep_msg_normal_send(rxm_conn, &eager_buf->pkt,
+					     total_len, eager_buf->hdr.desc,
+					     eager_buf);
+	}
+
+	if (ret) {
+		if (ret == -FI_EAGAIN)
+			rxm_ep_do_progress(&rxm_ep->util_ep);
+		rxm_free_rx_buf(rxm_ep, eager_buf);
+	}
+	return ret;
+}
+
+static ssize_t
+rxm_send_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
+		const struct iovec *iov, void **desc, size_t count,
+		void *context, uint64_t data, uint64_t flags, uint64_t tag,
+		uint8_t op)
+{
+	struct rxm_tx_buf *rndv_buf;
+	size_t data_len, total_len;
 	enum fi_hmem_iface iface;
 	uint64_t device;
+	ssize_t ret;
 
 	data_len = ofi_total_iov_len(iov, count);
 	total_len = sizeof(struct rxm_pkt) + data_len;
@@ -1578,47 +1516,17 @@ rxm_ep_send_common(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 		(data_len > rxm_ep->rxm_info->tx_attr->inject_size)) ||
 	       (data_len <= rxm_ep->rxm_info->tx_attr->inject_size));
 
-	iface = rxm_mr_desc_to_hmem_iface_dev(desc, count, &device);
-
-	if (data_len <= rxm_eager_limit) {
-		eager_buf = rxm_tx_buf_alloc(rxm_ep, RXM_BUF_POOL_TX);
-		if (!eager_buf) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_DATA,
-				"Ran out of buffers from Eager buffer pool\n");
-			return -FI_EAGAIN;
-		}
-
-		eager_buf->app_context = context;
-		eager_buf->flags = flags;
-		rxm_ep_format_tx_buf_pkt(rxm_conn, data_len, op, data, tag,
-					 flags, &eager_buf->pkt);
-
-		if (rxm_use_direct_send(rxm_ep, count, flags)) {
-			ret = rxm_direct_send(rxm_ep, rxm_conn, eager_buf,
-					      iov, desc, count);
-		} else {
-			ret = ofi_copy_from_hmem_iov(eager_buf->pkt.data,
-						     eager_buf->pkt.hdr.size,
-						     iface, device, iov,
-						     count, 0);
-			assert(ret == eager_buf->pkt.hdr.size);
-
-			ret = rxm_ep_msg_normal_send(rxm_conn, &eager_buf->pkt,
-						     total_len,
-						     eager_buf->hdr.desc,
-						     eager_buf);
-		}
-		if (ret) {
-			if (ret == -FI_EAGAIN)
-				rxm_ep_do_progress(&rxm_ep->util_ep);
-			ofi_buf_free(eager_buf);
-		}
+	if (data_len <= rxm_ep->eager_limit) {
+		ret = rxm_send_eager(rxm_ep, rxm_conn, iov, desc, count,
+				     context, data, flags, tag, op,
+				     data_len, total_len);
 	} else if (data_len <= rxm_ep->sar_limit) {
-		ret = rxm_ep_sar_tx_send(rxm_ep, rxm_conn, context,
-					 count, iov, data_len,
-					 rxm_ep_sar_calc_segs_cnt(rxm_ep, data_len),
-					 data, flags, tag, op, iface, device);
+		ret = rxm_send_sar(rxm_ep, rxm_conn, iov, desc, (uint8_t) count,
+				   context, data, flags, tag, op, data_len,
+				   rxm_ep_sar_calc_segs_cnt(rxm_ep, data_len));
 	} else {
+		iface = rxm_mr_desc_to_hmem_iface_dev(desc, count, &device);
+
 		ret = rxm_alloc_rndv_buf(rxm_ep, rxm_conn, context,
 					 (uint8_t) count, iov, desc,
 					 data_len, data, flags, tag, op,
@@ -1666,7 +1574,7 @@ static ssize_t
 rxm_ep_progress_sar_deferred_segments(struct rxm_deferred_tx_entry *def_tx_entry)
 {
 	ssize_t ret = 0;
-	struct rxm_tx_sar_buf *tx_buf = def_tx_entry->sar_seg.cur_seg_tx_buf;
+	struct rxm_tx_buf *tx_buf = def_tx_entry->sar_seg.cur_seg_tx_buf;
 
 	if (tx_buf) {
 		ret = fi_send(def_tx_entry->rxm_conn->msg_ep, &tx_buf->pkt,
@@ -1681,7 +1589,7 @@ rxm_ep_progress_sar_deferred_segments(struct rxm_deferred_tx_entry *def_tx_entry
 		}
 
 		def_tx_entry->sar_seg.next_seg_no++;
-		def_tx_entry->sar_seg.remain_len -= rxm_eager_limit;
+		def_tx_entry->sar_seg.remain_len -= rxm_buffer_size;
 
 		if (def_tx_entry->sar_seg.next_seg_no ==
 		    def_tx_entry->sar_seg.segs_cnt) {
@@ -1698,7 +1606,7 @@ rxm_ep_progress_sar_deferred_segments(struct rxm_deferred_tx_entry *def_tx_entry
 				def_tx_entry->sar_seg.app_context,
 				def_tx_entry->sar_seg.total_len,
 				def_tx_entry->sar_seg.remain_len,
-				def_tx_entry->sar_seg.msg_id, rxm_eager_limit,
+				def_tx_entry->sar_seg.msg_id, rxm_buffer_size,
 				def_tx_entry->sar_seg.next_seg_no,
 				def_tx_entry->sar_seg.segs_cnt,
 				def_tx_entry->sar_seg.payload.data,
@@ -1720,7 +1628,7 @@ rxm_ep_progress_sar_deferred_segments(struct rxm_deferred_tx_entry *def_tx_entry
 			return ret;
 		}
 		def_tx_entry->sar_seg.next_seg_no++;
-		def_tx_entry->sar_seg.remain_len -= rxm_eager_limit;
+		def_tx_entry->sar_seg.remain_len -= rxm_buffer_size;
 	}
 
 	return 0;
@@ -1734,7 +1642,7 @@ void rxm_ep_progress_deferred_queue(struct rxm_ep *rxm_ep,
 	struct fi_msg msg;
 	ssize_t ret = 0;
 
-	if (rxm_conn->handle.state != RXM_CMAP_CONNECTED)
+	if (rxm_conn->state != RXM_CM_CONNECTED)
 		return;
 
 	while (!dlist_empty(&rxm_conn->deferred_tx_queue) && !ret) {
@@ -1859,7 +1767,7 @@ void rxm_ep_progress_deferred_queue(struct rxm_ep *rxm_ep,
 			break;
 		}
 
-		rxm_ep_dequeue_deferred_tx_queue(def_tx_entry);
+		rxm_dequeue_deferred_tx(def_tx_entry);
 		free(def_tx_entry);
 	}
 }
@@ -1877,9 +1785,9 @@ rxm_ep_sendmsg(struct fid_ep *ep_fid, const struct fi_msg *msg, uint64_t flags)
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, msg->msg_iov, msg->desc,
-				 msg->iov_count, msg->context, msg->data,
-				 flags | rxm_ep->util_ep.tx_msg_flags, 0, ofi_op_msg);
+	ret = rxm_send_common(rxm_ep, rxm_conn, msg->msg_iov, msg->desc,
+			      msg->iov_count, msg->context, msg->data,
+			      flags | rxm_ep->util_ep.tx_msg_flags, 0, ofi_op_msg);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
@@ -1902,8 +1810,8 @@ static ssize_t rxm_ep_send(struct fid_ep *ep_fid, const void *buf, size_t len,
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context,
-				  0, rxm_ep->util_ep.tx_op_flags, 0, ofi_op_msg);
+	ret = rxm_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context,
+			      0, rxm_ep->util_ep.tx_op_flags, 0, ofi_op_msg);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
@@ -1923,15 +1831,15 @@ static ssize_t rxm_ep_sendv(struct fid_ep *ep_fid, const struct iovec *iov,
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, iov, desc, count, context,
-				  0, rxm_ep->util_ep.tx_op_flags, 0, ofi_op_msg);
+	ret = rxm_send_common(rxm_ep, rxm_conn, iov, desc, count, context,
+			      0, rxm_ep->util_ep.tx_op_flags, 0, ofi_op_msg);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_inject(struct fid_ep *ep_fid, const void *buf, size_t len,
-			     fi_addr_t dest_addr)
+static ssize_t rxm_ep_inject(struct fid_ep *ep_fid, const void *buf,
+			     size_t len, fi_addr_t dest_addr)
 {
 	struct rxm_conn *rxm_conn;
 	struct rxm_ep *rxm_ep;
@@ -1943,30 +1851,17 @@ static ssize_t rxm_ep_inject(struct fid_ep *ep_fid, const void *buf, size_t len,
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len, 0,
-				  rxm_ep->util_ep.inject_op_flags,
-				  0, ofi_op_msg);
+	rxm_ep->inject_pkt->hdr.op = ofi_op_msg;
+	rxm_ep->inject_pkt->hdr.flags = 0;
+	rxm_ep->inject_pkt->hdr.tag = 0;
+	rxm_ep->inject_pkt->hdr.data = 0;
+
+	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_inject_fast(struct fid_ep *ep_fid, const void *buf,
-				  size_t len, fi_addr_t dest_addr)
-{
-	struct rxm_conn *rxm_conn;
-	struct rxm_ep *rxm_ep;
-	ssize_t ret;
-
-	rxm_ep = container_of(ep_fid, struct rxm_ep, util_ep.ep_fid.fid);
-	ret = rxm_get_conn(rxm_ep, dest_addr, &rxm_conn);
-	if (ret)
-		return ret;
-
-	return rxm_ep_inject_send_fast(rxm_ep, rxm_conn, buf, len,
-				       rxm_conn->inject_pkt);
-}
-
 static ssize_t rxm_ep_senddata(struct fid_ep *ep_fid, const void *buf, size_t len,
 			       void *desc, uint64_t data, fi_addr_t dest_addr,
 			       void *context)
@@ -1985,16 +1880,17 @@ static ssize_t rxm_ep_senddata(struct fid_ep *ep_fid, const void *buf, size_t le
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context, data,
-				 rxm_ep->util_ep.tx_op_flags | FI_REMOTE_CQ_DATA,
-				 0, ofi_op_msg);
+	ret = rxm_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context, data,
+			      rxm_ep->util_ep.tx_op_flags | FI_REMOTE_CQ_DATA,
+			      0, ofi_op_msg);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_injectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
-				 uint64_t data, fi_addr_t dest_addr)
+static ssize_t
+rxm_ep_injectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
+		  uint64_t data, fi_addr_t dest_addr)
 {
 	struct rxm_conn *rxm_conn;
 	struct rxm_ep *rxm_ep;
@@ -2006,32 +1902,17 @@ static ssize_t rxm_ep_injectdata(struct fid_ep *ep_fid, const void *buf, size_t
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len, data,
-				  rxm_ep->util_ep.inject_op_flags |
-				  FI_REMOTE_CQ_DATA, 0, ofi_op_msg);
+	rxm_ep->inject_pkt->hdr.op = ofi_op_msg;
+	rxm_ep->inject_pkt->hdr.flags = FI_REMOTE_CQ_DATA;
+	rxm_ep->inject_pkt->hdr.tag = 0;
+	rxm_ep->inject_pkt->hdr.data = data;
+
+	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_injectdata_fast(struct fid_ep *ep_fid, const void *buf, size_t len,
-				      uint64_t data, fi_addr_t dest_addr)
-{
-	struct rxm_conn *rxm_conn;
-	struct rxm_ep *rxm_ep;
-	ssize_t ret;
-
-	rxm_ep = container_of(ep_fid, struct rxm_ep, util_ep.ep_fid.fid);
-	ret = rxm_get_conn(rxm_ep, dest_addr, &rxm_conn);
-	if (ret)
-		return ret;
-
-	rxm_conn->inject_data_pkt->hdr.data = data;
-
-	return rxm_ep_inject_send_fast(rxm_ep, rxm_conn, buf, len,
-				       rxm_conn->inject_data_pkt);
-}
-
 static struct fi_ops_msg rxm_ops_msg = {
 	.size = sizeof(struct fi_ops_msg),
 	.recv = rxm_ep_recv,
@@ -2045,18 +1926,6 @@ static struct fi_ops_msg rxm_ops_msg = {
 	.injectdata = rxm_ep_injectdata,
 };
 
-static struct fi_ops_msg rxm_ops_msg_thread_unsafe = {
-	.size = sizeof(struct fi_ops_msg),
-	.recv = rxm_ep_recv,
-	.recvv = rxm_ep_recvv,
-	.recvmsg = rxm_ep_recvmsg,
-	.send = rxm_ep_send,
-	.sendv = rxm_ep_sendv,
-	.sendmsg = rxm_ep_sendmsg,
-	.inject = rxm_ep_inject_fast,
-	.senddata = rxm_ep_senddata,
-	.injectdata = rxm_ep_injectdata_fast,
-};
 
 static ssize_t
 rxm_ep_post_trecv(struct rxm_ep *rxm_ep, const struct iovec *iov,
@@ -2225,10 +2094,10 @@ rxm_ep_tsendmsg(struct fid_ep *ep_fid, const struct fi_msg_tagged *msg,
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, msg->msg_iov, msg->desc,
-				  msg->iov_count, msg->context, msg->data,
-				  flags | rxm_ep->util_ep.tx_msg_flags, msg->tag,
-				  ofi_op_tagged);
+	ret = rxm_send_common(rxm_ep, rxm_conn, msg->msg_iov, msg->desc,
+			      msg->iov_count, msg->context, msg->data,
+			      flags | rxm_ep->util_ep.tx_msg_flags, msg->tag,
+			      ofi_op_tagged);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
@@ -2252,8 +2121,8 @@ static ssize_t rxm_ep_tsend(struct fid_ep *ep_fid, const void *buf, size_t len,
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context, 0,
-				 rxm_ep->util_ep.tx_op_flags, tag, ofi_op_tagged);
+	ret = rxm_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context, 0,
+			      rxm_ep->util_ep.tx_op_flags, tag, ofi_op_tagged);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
@@ -2273,15 +2142,16 @@ static ssize_t rxm_ep_tsendv(struct fid_ep *ep_fid, const struct iovec *iov,
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, iov, desc, count, context, 0,
-				 rxm_ep->util_ep.tx_op_flags, tag, ofi_op_tagged);
+	ret = rxm_send_common(rxm_ep, rxm_conn, iov, desc, count, context, 0,
+			      rxm_ep->util_ep.tx_op_flags, tag, ofi_op_tagged);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_tinject(struct fid_ep *ep_fid, const void *buf, size_t len,
-			      fi_addr_t dest_addr, uint64_t tag)
+static ssize_t
+rxm_ep_tinject(struct fid_ep *ep_fid, const void *buf, size_t len,
+	       fi_addr_t dest_addr, uint64_t tag)
 {
 	struct rxm_conn *rxm_conn;
 	struct rxm_ep *rxm_ep;
@@ -2293,32 +2163,17 @@ static ssize_t rxm_ep_tinject(struct fid_ep *ep_fid, const void *buf, size_t len
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len, 0,
-				  rxm_ep->util_ep.inject_op_flags, tag,
-				  ofi_op_tagged);
+	rxm_ep->inject_pkt->hdr.op = ofi_op_tagged;
+	rxm_ep->inject_pkt->hdr.flags = 0;
+	rxm_ep->inject_pkt->hdr.tag = tag;
+	rxm_ep->inject_pkt->hdr.data = 0;
+
+	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_tinject_fast(struct fid_ep *ep_fid, const void *buf, size_t len,
-				   fi_addr_t dest_addr, uint64_t tag)
-{
-	struct rxm_conn *rxm_conn;
-	struct rxm_ep *rxm_ep;
-	ssize_t ret;
-
-	rxm_ep = container_of(ep_fid, struct rxm_ep, util_ep.ep_fid.fid);
-	ret = rxm_get_conn(rxm_ep, dest_addr, &rxm_conn);
-	if (ret)
-		return ret;
-
-	rxm_conn->tinject_pkt->hdr.tag = tag;
-
-	return rxm_ep_inject_send_fast(rxm_ep, rxm_conn, buf, len,
-				       rxm_conn->tinject_pkt);
-}
-
 static ssize_t rxm_ep_tsenddata(struct fid_ep *ep_fid, const void *buf, size_t len,
 				void *desc, uint64_t data, fi_addr_t dest_addr,
 				uint64_t tag, void *context)
@@ -2337,16 +2192,17 @@ static ssize_t rxm_ep_tsenddata(struct fid_ep *ep_fid, const void *buf, size_t l
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context, data,
-				 rxm_ep->util_ep.tx_op_flags | FI_REMOTE_CQ_DATA,
-				 tag, ofi_op_tagged);
+	ret = rxm_send_common(rxm_ep, rxm_conn, &iov, &desc, 1, context, data,
+			      rxm_ep->util_ep.tx_op_flags | FI_REMOTE_CQ_DATA,
+			tag, ofi_op_tagged);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_tinjectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
-				  uint64_t data, fi_addr_t dest_addr, uint64_t tag)
+static ssize_t
+rxm_ep_tinjectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
+		   uint64_t data, fi_addr_t dest_addr, uint64_t tag)
 {
 	struct rxm_conn *rxm_conn;
 	struct rxm_ep *rxm_ep;
@@ -2358,33 +2214,17 @@ static ssize_t rxm_ep_tinjectdata(struct fid_ep *ep_fid, const void *buf, size_t
 	if (ret)
 		goto unlock;
 
-	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len, data,
-				  rxm_ep->util_ep.inject_op_flags |
-				  FI_REMOTE_CQ_DATA, tag, ofi_op_tagged);
+	rxm_ep->inject_pkt->hdr.op = ofi_op_tagged;
+	rxm_ep->inject_pkt->hdr.flags = FI_REMOTE_CQ_DATA;
+	rxm_ep->inject_pkt->hdr.tag = tag;
+	rxm_ep->inject_pkt->hdr.data = data;
+
+	ret = rxm_ep_inject_send(rxm_ep, rxm_conn, buf, len);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
 }
 
-static ssize_t rxm_ep_tinjectdata_fast(struct fid_ep *ep_fid, const void *buf, size_t len,
-				       uint64_t data, fi_addr_t dest_addr, uint64_t tag)
-{
-	struct rxm_conn *rxm_conn;
-	struct rxm_ep *rxm_ep;
-	ssize_t ret;
-
-	rxm_ep = container_of(ep_fid, struct rxm_ep, util_ep.ep_fid.fid);
-	ret = rxm_get_conn(rxm_ep, dest_addr, &rxm_conn);
-	if (ret)
-		return ret;
-
-	rxm_conn->tinject_data_pkt->hdr.tag = tag;
-	rxm_conn->tinject_data_pkt->hdr.data = data;
-
-	return rxm_ep_inject_send_fast(rxm_ep, rxm_conn, buf, len,
-				       rxm_conn->tinject_data_pkt);
-}
-
 static struct fi_ops_tagged rxm_ops_tagged = {
 	.size = sizeof(struct fi_ops_tagged),
 	.recv = rxm_ep_trecv,
@@ -2398,19 +2238,6 @@ static struct fi_ops_tagged rxm_ops_tagged = {
 	.injectdata = rxm_ep_tinjectdata,
 };
 
-static struct fi_ops_tagged rxm_ops_tagged_thread_unsafe = {
-	.size = sizeof(struct fi_ops_tagged),
-	.recv = rxm_ep_trecv,
-	.recvv = rxm_ep_trecvv,
-	.recvmsg = rxm_ep_trecvmsg,
-	.send = rxm_ep_tsend,
-	.sendv = rxm_ep_tsendv,
-	.sendmsg = rxm_ep_tsendmsg,
-	.inject = rxm_ep_tinject_fast,
-	.senddata = rxm_ep_tsenddata,
-	.injectdata = rxm_ep_tinjectdata_fast,
-};
-
 static struct fi_ops_collective rxm_ops_collective = {
 	.size = sizeof(struct fi_ops_collective),
 	.barrier = ofi_ep_barrier,
@@ -2439,75 +2266,76 @@ static struct fi_ops_collective rxm_ops_collective_none = {
 	.msg = fi_coll_no_msg,
 };
 
-static int rxm_ep_msg_res_close(struct rxm_ep *rxm_ep)
-{
-	int ret = 0;
-
-	if (rxm_ep->srx_ctx) {
-		ret = fi_close(&rxm_ep->srx_ctx->fid);
-		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, \
-				"Unable to close msg shared ctx\n");
-		}
-	}
-
-	fi_freeinfo(rxm_ep->msg_info);
-	return ret;
-}
 
-static int rxm_listener_close(struct rxm_ep *rxm_ep)
+static int rxm_listener_close(struct rxm_ep *ep)
 {
-	int ret, retv = 0;
+	int ret;
 
-	if (rxm_ep->msg_pep) {
-		ret = fi_close(&rxm_ep->msg_pep->fid);
+	if (ep->msg_pep) {
+		ret = fi_close(&ep->msg_pep->fid);
 		if (ret) {
 			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
 				"Unable to close msg pep\n");
-			retv = ret;
+			return ret;
 		}
+		ep->msg_pep = NULL;
 	}
-	if (rxm_ep->msg_eq) {
-		ret = fi_close(&rxm_ep->msg_eq->fid);
+
+	if (ep->msg_eq) {
+		ret = fi_close(&ep->msg_eq->fid);
 		if (ret) {
 			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
 				"Unable to close msg EQ\n");
-			retv = ret;
+			return ret;
 		}
+		ep->msg_eq = NULL;
 	}
-	return retv;
+	return 0;
 }
 
 static int rxm_ep_close(struct fid *fid)
 {
-	int ret, retv = 0;
-	struct rxm_ep *rxm_ep;
+	struct rxm_ep *ep;
+	int ret;
 
-	rxm_ep = container_of(fid, struct rxm_ep, util_ep.ep_fid.fid);
-	if (rxm_ep->cmap)
-		rxm_cmap_free(rxm_ep->cmap);
+	ep = container_of(fid, struct rxm_ep, util_ep.ep_fid.fid);
 
-	ret = rxm_listener_close(rxm_ep);
+	/* Stop listener thread to halt event processing before closing all
+	 * connections.
+	 */
+	rxm_stop_listen(ep);
+	rxm_freeall_conns(ep);
+	ret = rxm_listener_close(ep);
 	if (ret)
-		retv = ret;
+		return ret;
 
-	rxm_ep_txrx_res_close(rxm_ep);
-	ret = rxm_ep_msg_res_close(rxm_ep);
-	if (ret)
-		retv = ret;
+	rxm_ep_txrx_res_close(ep);
+	if (ep->srx_ctx) {
+		ret = fi_close(&ep->srx_ctx->fid);
+		if (ret) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, \
+				"Unable to close msg shared ctx\n");
+			return ret;
+		}
+		ep->srx_ctx = NULL;
+	}
 
-	if (rxm_ep->msg_cq) {
-		ret = fi_close(&rxm_ep->msg_cq->fid);
+	if (ep->msg_cq) {
+		ret = fi_close(&ep->msg_cq->fid);
 		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to close msg CQ\n");
-			retv = ret;
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"Unable to close msg CQ\n");
+			return ret;
 		}
+		ep->msg_cq = NULL;
 	}
 
-	ofi_endpoint_close(&rxm_ep->util_ep);
-	fi_freeinfo(rxm_ep->rxm_info);
-	free(rxm_ep);
-	return retv;
+	free(ep->inject_pkt);
+	ofi_endpoint_close(&ep->util_ep);
+	fi_freeinfo(ep->msg_info);
+	fi_freeinfo(ep->rxm_info);
+	free(ep);
+	return 0;
 }
 
 static int rxm_ep_trywait_cq(void *arg)
@@ -2553,6 +2381,12 @@ static int rxm_ep_wait_fd_add(struct rxm_ep *rxm_ep, struct util_wait *wait)
 				rxm_ep_trywait_eq);
 }
 
+static bool rxm_needs_atomic_progress(const struct fi_info *info)
+{
+	return (info->caps & FI_ATOMIC) && info->domain_attr &&
+		info->domain_attr->data_progress == FI_PROGRESS_AUTO;
+}
+
 static int rxm_msg_cq_fd_needed(struct rxm_ep *rxm_ep)
 {
 	return (rxm_needs_atomic_progress(rxm_ep->rxm_info) ||
@@ -2639,50 +2473,51 @@ err:
 	return ret;
 }
 
-static void rxm_ep_sar_init(struct rxm_ep *rxm_ep)
+static void rxm_ep_init_proto(struct rxm_ep *ep)
 {
 	struct rxm_domain *domain;
 	size_t param;
 
-	/* SAR segment size is capped at 64k. */
-	if (rxm_eager_limit > UINT16_MAX)
-		goto disable_sar;
-
-	domain = container_of(rxm_ep->util_ep.domain, struct rxm_domain,
+	domain = container_of(ep->util_ep.domain, struct rxm_domain,
 			      util_domain);
-	if (domain->dyn_rbuf) {
-		FI_INFO(&rxm_prov, FI_LOG_CORE, "Dynamic receive buffer "
-			"enabled, disabling SAR protocol\n");
-		goto disable_sar;
+
+	if (ep->enable_direct_send && domain->dyn_rbuf) {
+		if (!fi_param_get_size_t(&rxm_prov, "eager_limit", &param))
+			ep->eager_limit = param;
 	}
 
-	if (!fi_param_get_size_t(&rxm_prov, "sar_limit", &param)) {
-		if (param <= rxm_eager_limit) {
-			FI_WARN(&rxm_prov, FI_LOG_CORE,
-				"Requested SAR limit (%zd) less or equal to "
-				"eager limit (%zd) - disabling.",
-				param, rxm_eager_limit);
-			goto disable_sar;
-		}
+	if (ep->eager_limit < rxm_buffer_size)
+		ep->eager_limit = rxm_buffer_size;
 
-		rxm_ep->sar_limit = param;
-	} else {
-		rxm_ep->sar_limit = rxm_eager_limit * 8;
+	/* SAR segment size is capped at 64k. */
+	if (domain->dyn_rbuf || ep->eager_limit > UINT16_MAX) {
+		ep->sar_limit = ep->eager_limit;
+		return;
 	}
 
-	return;
-
-disable_sar:
-	rxm_ep->sar_limit = rxm_eager_limit;
+	if (!fi_param_get_size_t(&rxm_prov, "sar_limit", &param)) {
+		if (param <= ep->eager_limit)
+			ep->sar_limit = ep->eager_limit;
+		else
+			ep->sar_limit = param;
+	} else {
+		ep->sar_limit = ep->eager_limit * 8;
+	}
 }
 
+/* Direct send works with verbs, provided that msg_mr_local == rdm_mr_local.
+ * However, it fails consistently on HFI, with the receiving side getting
+ * corrupted data beyond the first iov.  Only enable if MR_LOCAL is not
+ * required (feature of tcp provider).
+ */
 static void rxm_config_direct_send(struct rxm_ep *ep)
 {
-	int ret = 0;
+	int ret = 1;
 
-	if (ep->msg_mr_local == ep->rdm_mr_local)
-		fi_param_get_bool(&rxm_prov, "enable_direct_send", &ret);
+	if (ep->msg_mr_local)
+		return;
 
+	fi_param_get_bool(&rxm_prov, "enable_direct_send", &ret);
 	ep->enable_direct_send = (ret != 0);
 }
 
@@ -2697,13 +2532,12 @@ static void rxm_ep_settings_init(struct rxm_ep *rxm_ep)
 			   rxm_ep->msg_info->rx_attr->size) / 2;
 	rxm_ep->comp_per_progress = (rxm_ep->comp_per_progress > max_prog_val) ?
 				    max_prog_val : rxm_ep->comp_per_progress;
-	ofi_atomic_initialize32(&rxm_ep->atomic_tx_credits,
-				rxm_ep->rxm_info->tx_attr->size);
 
 	rxm_ep->msg_mr_local = ofi_mr_local(rxm_ep->msg_info);
 	rxm_ep->rdm_mr_local = ofi_mr_local(rxm_ep->rxm_info);
 
 	rxm_ep->inject_limit = rxm_ep->msg_info->tx_attr->inject_size;
+	rxm_ep->tx_credit = rxm_ep->rxm_info->tx_attr->size;
 
 	/* Favor a default buffered_min size that's small enough to be
 	 * injected by FI_EP_MSG provider */
@@ -2713,16 +2547,16 @@ static void rxm_ep_settings_init(struct rxm_ep *rxm_ep)
 		rxm_ep->buffered_min = MIN((rxm_ep->inject_limit -
 					    (sizeof(struct rxm_pkt) +
 					     sizeof(struct rxm_rndv_hdr))),
-					   rxm_eager_limit);
+					   rxm_buffer_size);
 
 	assert(!rxm_ep->min_multi_recv_size);
-	rxm_ep->min_multi_recv_size = rxm_eager_limit;
+	rxm_ep->min_multi_recv_size = rxm_buffer_size;
 
 	assert(!rxm_ep->buffered_limit);
-	rxm_ep->buffered_limit = rxm_eager_limit;
+	rxm_ep->buffered_limit = rxm_buffer_size;
 
-	rxm_ep_sar_init(rxm_ep);
 	rxm_config_direct_send(rxm_ep);
+	rxm_ep_init_proto(rxm_ep);
 
  	FI_INFO(&rxm_prov, FI_LOG_CORE,
 		"Settings:\n"
@@ -2730,26 +2564,23 @@ static void rxm_ep_settings_init(struct rxm_ep *rxm_ep)
 		"\t\t Completions per progress: MSG - %zu\n"
 	        "\t\t Buffered min: %zu\n"
 	        "\t\t Min multi recv size: %zu\n"
-	        "\t\t FI_EP_MSG provider inject size: %zu\n"
-	        "\t\t rxm inject size: %zu\n"
-		"\t\t Protocol limits: Eager: %zu, "
-				      "SAR: %zu\n",
+	        "\t\t inject size: %zu\n"
+		"\t\t Protocol limits: Eager: %zu, SAR: %zu\n",
 		rxm_ep->msg_mr_local, rxm_ep->rdm_mr_local,
 		rxm_ep->comp_per_progress, rxm_ep->buffered_min,
 		rxm_ep->min_multi_recv_size, rxm_ep->inject_limit,
-		rxm_ep->rxm_info->tx_attr->inject_size,
-		rxm_eager_limit, rxm_ep->sar_limit);
+		rxm_ep->eager_limit, rxm_ep->sar_limit);
 }
 
 static int rxm_ep_txrx_res_open(struct rxm_ep *rxm_ep)
 {
 	int ret;
 
-	ret = rxm_ep_txrx_pool_create(rxm_ep);
+	ret = rxm_ep_create_pools(rxm_ep);
 	if (ret)
 		return ret;
 
-	dlist_init(&rxm_ep->deferred_tx_conn_queue);
+	dlist_init(&rxm_ep->deferred_queue);
 
 	ret = rxm_ep_rx_queue_init(rxm_ep);
 	if (ret)
@@ -2757,7 +2588,10 @@ static int rxm_ep_txrx_res_open(struct rxm_ep *rxm_ep)
 
 	return FI_SUCCESS;
 err:
-	rxm_ep_txrx_pool_destroy(rxm_ep);
+	ofi_bufpool_destroy(rxm_ep->rx_pool);
+	ofi_bufpool_destroy(rxm_ep->tx_pool);
+	rxm_ep->rx_pool = NULL;
+	rxm_ep->tx_pool = NULL;
 	return ret;
 }
 
@@ -2806,23 +2640,13 @@ static int rxm_ep_ctrl(struct fid *fid, int command, void *arg)
 		 * and then progressing both MSG EQ and MSG CQ once the latter
 		 * is opened) */
 		assert(!(rxm_ep->rxm_info->caps & FI_ATOMIC) ||
-		       !rxm_ep->cmap || !rxm_ep->cmap->cm_thread);
+		       !rxm_ep->cm_thread);
 
 		ret = rxm_ep_msg_cq_open(rxm_ep);
 		if (ret)
 			return ret;
 
-		/* fi_listen should be called before cmap alloc as cmap alloc
-		 * calls fi_getname on pep which would succeed only if fi_listen
-		 * was called first */
-		ret = fi_listen(rxm_ep->msg_pep);
-		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"unable to set msg PEP to listen state\n");
-			return ret;
-		}
-
-		ret = rxm_conn_cmap_alloc(rxm_ep);
+		ret = rxm_start_listen(rxm_ep);
 		if (ret)
 			return ret;
 
@@ -2836,19 +2660,17 @@ static int rxm_ep_ctrl(struct fid *fid, int command, void *arg)
 
 		if (rxm_ep->srx_ctx) {
 			ret = rxm_prepost_recv(rxm_ep, rxm_ep->srx_ctx);
-			if (ret) {
-				rxm_cmap_free(rxm_ep->cmap);
-				FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-					"unable to prepost recv bufs\n");
+			if (ret)
 				goto err;
-			}
 		}
 		break;
 	default:
 		return -FI_ENOSYS;
 	}
 	return 0;
+
 err:
+	/* TODO: cleanup all allocated resources on error */
 	rxm_ep_txrx_res_close(rxm_ep);
 	return ret;
 }
@@ -2925,12 +2747,6 @@ static int rxm_open_core_res(struct rxm_ep *ep)
 	if (ret)
 		goto err2;
 
-	/* Zero out the port as we will create multiple MSG EPs for a
-	 * single RXM EP, and we don't want address conflicts.
-	 */
-	if (ep->msg_info->src_addr)
-		ofi_addr_set_port(ep->msg_info->src_addr, 0);
-
 	return 0;
 err2:
 	if (ep->srx_ctx) {
@@ -2979,8 +2795,8 @@ rxm_prepare_deferred_rndv_write(struct rxm_deferred_tx_entry **def_tx_entry,
 			       void *buf)
 {
 	uint8_t i;
-	struct rxm_tx_rndv_buf *tx_buf = buf;
-	struct rxm_ep *rxm_ep = tx_buf->write_rndv.conn->handle.cmap->ep;
+	struct rxm_tx_buf *tx_buf = buf;
+	struct rxm_ep *rxm_ep = tx_buf->write_rndv.conn->ep;
 
 	*def_tx_entry = rxm_ep_alloc_deferred_tx_entry(rxm_ep, tx_buf->write_rndv.conn,
 						       RXM_DEFERRED_TX_RNDV_WRITE);
@@ -3047,7 +2863,6 @@ int rxm_endpoint(struct fid_domain *domain, struct fi_info *info,
 					&rxm_ep->util_ep, context,
 					&rxm_ep_progress);
 	}
-
 	if (ret)
 		goto err1;
 
@@ -3057,6 +2872,16 @@ int rxm_endpoint(struct fid_domain *domain, struct fi_info *info,
 
 	rxm_ep_settings_init(rxm_ep);
 
+	rxm_ep->inject_pkt = calloc(1, sizeof(*rxm_ep->inject_pkt) +
+				       rxm_ep->inject_limit);
+	if (!rxm_ep->inject_pkt) {
+		ret = -FI_ENOMEM;
+		goto err2;
+	}
+	rxm_ep->inject_pkt->ctrl_hdr.version = RXM_CTRL_VERSION;
+	rxm_ep->inject_pkt->ctrl_hdr.type = rxm_ctrl_eager;
+	rxm_ep->inject_pkt->hdr.version = OFI_OP_VERSION;
+
 	*ep_fid = &rxm_ep->util_ep.ep_fid;
 	(*ep_fid)->fid.ops = &rxm_ep_fi_ops;
 	(*ep_fid)->ops = &rxm_ops_ep;
@@ -3076,18 +2901,15 @@ int rxm_endpoint(struct fid_domain *domain, struct fi_info *info,
 		rxm_ep->rndv_ops = &rxm_rndv_ops_read;
 	dlist_init(&rxm_ep->rndv_wait_list);
 
-	if (rxm_ep->util_ep.domain->threading != FI_THREAD_SAFE) {
-		(*ep_fid)->msg = &rxm_ops_msg_thread_unsafe;
-		(*ep_fid)->tagged = &rxm_ops_tagged_thread_unsafe;
-	} else {
-		(*ep_fid)->msg = &rxm_ops_msg;
-		(*ep_fid)->tagged = &rxm_ops_tagged;
-	}
+	(*ep_fid)->msg = &rxm_ops_msg;
+	(*ep_fid)->tagged = &rxm_ops_tagged;
 	(*ep_fid)->rma = &rxm_ops_rma;
 
 	if (rxm_ep->rxm_info->caps & FI_ATOMIC)
 		(*ep_fid)->atomic = &rxm_ops_atomic;
 
+	dlist_init(&rxm_ep->loopback_list);
+
 	return 0;
 err2:
 	ofi_endpoint_close(&rxm_ep->util_ep);
diff --git a/prov/rxm/src/rxm_init.c b/prov/rxm/src/rxm_init.c
index 1d0fb10..ee2ea4e 100644
--- a/prov/rxm/src/rxm_init.c
+++ b/prov/rxm/src/rxm_init.c
@@ -48,10 +48,13 @@
 			   FI_READ | FI_WRITE | FI_REMOTE_READ |	\
 			   FI_REMOTE_WRITE | FI_HMEM)
 
-size_t rxm_msg_tx_size		= 128;
-size_t rxm_msg_rx_size		= 128;
-size_t rxm_eager_limit		= 16384;
-size_t rxm_buffer_size		= 16384 + sizeof(struct rxm_pkt);
+size_t rxm_msg_tx_size;
+size_t rxm_msg_rx_size;
+size_t rxm_def_rx_size = 2048;
+size_t rxm_def_tx_size = 2048;
+
+size_t rxm_buffer_size = 16384;
+size_t rxm_packet_size;
 
 int force_auto_progress		= 0;
 int rxm_use_write_rndv		= 0;
@@ -109,7 +112,7 @@ static bool rxm_use_srx(const struct fi_info *hints,
 	info = base_info ? base_info : hints;
 
 	return info && info->fabric_attr && info->fabric_attr->prov_name &&
-	       !strncasecmp(info->fabric_attr->prov_name, "tcp", 3);
+	       strcasestr(info->fabric_attr->prov_name, "tcp");
 }
 
 int rxm_info_to_core(uint32_t version, const struct fi_info *hints,
@@ -155,13 +158,18 @@ int rxm_info_to_core(uint32_t version, const struct fi_info *hints,
 		FI_DBG(&rxm_prov, FI_LOG_FABRIC,
 		       "Requesting shared receive context from core provider\n");
 		core_info->ep_attr->rx_ctx_cnt = FI_SHARED_CONTEXT;
+		core_info->rx_attr->size = rxm_msg_rx_size ?
+					   rxm_msg_rx_size : RXM_MSG_SRX_SIZE;
+	} else {
+		core_info->rx_attr->size = rxm_msg_rx_size ?
+					   rxm_msg_rx_size : RXM_MSG_RXTX_SIZE;
 	}
 
 	core_info->tx_attr->op_flags &= ~RXM_TX_OP_FLAGS;
-	core_info->tx_attr->size = rxm_msg_tx_size;
+	core_info->tx_attr->size = rxm_msg_tx_size ?
+				   rxm_msg_tx_size : RXM_MSG_RXTX_SIZE;
 
 	core_info->rx_attr->op_flags &= ~FI_MULTI_RECV;
-	core_info->rx_attr->size = rxm_msg_rx_size;
 
 	return 0;
 }
@@ -196,7 +204,12 @@ int rxm_info_to_rxm(uint32_t version, const struct fi_info *core_info,
 					     sizeof(struct rxm_pkt);
 	}
 
-	info->tx_attr->size 		= base_info->tx_attr->size;
+	/* User hints will override the modified info attributes through
+	 * ofi_alter_info.  Set default sizes lower than supported maximums.
+	 */
+	info->tx_attr->size = MIN(base_info->tx_attr->size, rxm_def_tx_size);
+	info->rx_attr->size = MIN(base_info->rx_attr->size, rxm_def_rx_size);
+
 	info->tx_attr->iov_limit 	= MIN(base_info->tx_attr->iov_limit,
 					      core_info->tx_attr->iov_limit);
 	info->tx_attr->rma_iov_limit	= MIN(base_info->tx_attr->rma_iov_limit,
@@ -206,7 +219,6 @@ int rxm_info_to_rxm(uint32_t version, const struct fi_info *core_info,
 	info->rx_attr->mode		= info->rx_attr->mode & ~FI_RX_CQ_DATA;
 	info->rx_attr->msg_order 	= core_info->rx_attr->msg_order;
 	info->rx_attr->comp_order 	= base_info->rx_attr->comp_order;
-	info->rx_attr->size 		= base_info->rx_attr->size;
 	info->rx_attr->iov_limit 	= MIN(base_info->rx_attr->iov_limit,
 					      core_info->rx_attr->iov_limit);
 
@@ -241,33 +253,38 @@ int rxm_info_to_rxm(uint32_t version, const struct fi_info *core_info,
 static void rxm_init_infos(void)
 {
 	struct fi_info *cur;
-	size_t eager_size, tx_size = 0, rx_size = 0;
+	size_t buf_size, tx_size = 0, rx_size = 0;
 
 	/* Historically, 'buffer_size' was the name given for the eager message
 	 * size.  Maintain the name for backwards compatability.
 	 */
-	if (!fi_param_get_size_t(&rxm_prov, "buffer_size", &eager_size)) {
+	if (!fi_param_get_size_t(&rxm_prov, "buffer_size", &buf_size)) {
 		/* We need enough space to carry extra headers */
-		if (eager_size < sizeof(struct rxm_rndv_hdr) ||
-		    eager_size < sizeof(struct rxm_atomic_hdr)) {
+		if (buf_size < sizeof(struct rxm_rndv_hdr) ||
+		    buf_size < sizeof(struct rxm_atomic_hdr)) {
 			FI_WARN(&rxm_prov, FI_LOG_CORE,
 				"Requested buffer size too small\n");
-			eager_size = MAX(sizeof(struct rxm_rndv_hdr),
-					 sizeof(struct rxm_atomic_hdr));
+			buf_size = MAX(sizeof(struct rxm_rndv_hdr),
+				       sizeof(struct rxm_atomic_hdr));
 		}
 
-		rxm_eager_limit = eager_size;
-		if (rxm_eager_limit > INT32_MAX)
-			rxm_eager_limit = INT32_MAX;
+		if (buf_size > INT32_MAX)
+			buf_size = INT32_MAX;
 
-		rxm_buffer_size = rxm_eager_limit + sizeof(struct rxm_pkt);
+		rxm_buffer_size = buf_size;
 	}
 
+	rxm_packet_size = sizeof(struct rxm_pkt) + rxm_buffer_size;
+
 	fi_param_get_size_t(&rxm_prov, "tx_size", &tx_size);
 	fi_param_get_size_t(&rxm_prov, "rx_size", &rx_size);
+	if (tx_size)
+		rxm_def_tx_size = tx_size;
+	if (rx_size)
+		rxm_def_rx_size = rx_size;
 
 	for (cur = (struct fi_info *) rxm_util_prov.info; cur; cur = cur->next) {
-		cur->tx_attr->inject_size = rxm_eager_limit;
+		cur->tx_attr->inject_size = rxm_buffer_size;
 		if (tx_size)
 			cur->tx_attr->size = tx_size;
 		if (rx_size)
@@ -386,6 +403,7 @@ static int rxm_getinfo(uint32_t version, const char *node, const char *service,
 			port_save = ofi_addr_get_port(ai->ai_addr);
 			freeaddrinfo(ai);
 			service = NULL;
+			flags &= ~FI_SOURCE;
 		} else {
 			port_save = ofi_addr_get_port(hints->src_addr);
 			ofi_addr_set_port(hints->src_addr, 0);
@@ -415,6 +433,7 @@ static int rxm_getinfo(uint32_t version, const char *node, const char *service,
 static void rxm_fini(void)
 {
 #if HAVE_RXM_DL
+	ofi_hmem_cleanup();
 	ofi_mem_fini();
 #endif
 }
@@ -440,7 +459,8 @@ static void rxm_get_def_wait(void)
 	fi_param_define(&rxm_prov, "def_tcp_wait_obj", FI_PARAM_STRING,
 			"See def_wait_obj for description.  If set, this "
 			"overrides the def_wait_obj when running over the "
-			"tcp provider.");
+			"tcp provider.  See def_wait_obj for valid values. "
+			"(default: UNSPEC, tcp provider will select).");
 
 	fi_param_get_str(&rxm_prov, "def_wait_obj", &wait_str);
 	if (wait_str && !strcasecmp(wait_str, "pollfd"))
@@ -457,16 +477,23 @@ static void rxm_get_def_wait(void)
 RXM_INI
 {
 	fi_param_define(&rxm_prov, "buffer_size", FI_PARAM_SIZE_T,
-			"Defines the transmit buffer size / inject size "
-			"(default: 16 KB). Eager protocol would be used to "
-			"transmit messages of size less than eager limit "
-			"(FI_OFI_RXM_BUFFER_SIZE - RxM header size (%zu B)). "
-			"Any message whose size is greater than eager limit would"
-			" be transmitted via rendezvous or SAR "
-			"(Segmentation And Reassembly) protocol depending on "
-			"the value of FI_OFI_RXM_SAR_LIMIT). Also, transmit data "
-			" would be copied up to eager limit.",
-			sizeof(struct rxm_pkt));
+			"Defines the allocated buffer size used for bounce "
+			"buffers, including buffers posted at the receive side "
+			"to handle unexpected messages.  This value "
+			"corresponds to the rxm inject limit, and is also "
+			"typically used as the eager message size. "
+			"(default %zu)", rxm_buffer_size);
+
+	fi_param_define(&rxm_prov, "eager_limit", FI_PARAM_SIZE_T,
+			"Specifies the maximum size transfer that the eager "
+			"protocol will be used.  For transfers smaller than "
+			"this limit, data may be copied into a bounce "
+			"buffer on the transmit side and received into "
+			"bounce buffer at the receiver.  The eager_limit must "
+			"be equal to the buffer_size when using rxm over "
+			"verbs, but may differ in the case of tcp."
+			"(default: %zu)", rxm_buffer_size);
+			/* rxm_buffer_size is correct here */
 
 	fi_param_define(&rxm_prov, "comp_per_progress", FI_PARAM_INT,
 			"Defines the maximum number of MSG provider CQ entries "
@@ -474,13 +501,14 @@ RXM_INI
 			"(RxM CQ read).");
 
 	fi_param_define(&rxm_prov, "sar_limit", FI_PARAM_SIZE_T,
-			"Set this environment variable to enable and control "
-			"RxM SAR (Segmentation And Reassembly) protocol "
-			"(default: 128 KB). This value should be set greater than "
-			" eager limit (FI_OFI_RXM_BUFFER_SIZE - RxM protocol "
-			"header size (%zu B)) for SAR to take effect. Messages "
-			"of size greater than this would be transmitted via "
-			"rendezvous protocol.", sizeof(struct rxm_pkt));
+			"Specifies the maximum size transfer that the SAR "
+			"Segmentation And Reassembly) protocol "
+			"For transfers smaller than SAR, data may be copied "
+			"into multiple bounce buffers on the transmit side "
+			"and received into bounce buffers at the receiver. "
+			"The sar_limit value must be greater than the "
+			"eager_limit to take effect.  (default %zu).",
+			rxm_buffer_size * 8);
 
 	fi_param_define(&rxm_prov, "use_srx", FI_PARAM_BOOL,
 			"Set this environment variable to control the RxM "
@@ -490,20 +518,18 @@ RXM_INI
 			"latency as a side-effect.");
 
 	fi_param_define(&rxm_prov, "tx_size", FI_PARAM_SIZE_T,
-			"Defines default tx context size (default: 65536).");
+			"Defines default tx context size (default: 2048).");
 
 	fi_param_define(&rxm_prov, "rx_size", FI_PARAM_SIZE_T,
-			"Defines default rx context size (default: 65536).");
+			"Defines default rx context size (default: 2048).");
 
 	fi_param_define(&rxm_prov, "msg_tx_size", FI_PARAM_SIZE_T,
 			"Defines FI_EP_MSG tx size that would be requested "
-			"(default: 128). Setting this to 0 would get default "
-			"value defined by the MSG provider.");
+			"(default: 128).");
 
 	fi_param_define(&rxm_prov, "msg_rx_size", FI_PARAM_SIZE_T,
-			"Defines FI_EP_MSG rx size that would be requested "
-			"(default: 128). Setting this to 0 would get default "
-			"value defined by the MSG provider.");
+			"Defines FI_EP_MSG rx or srx size that would be requested. "
+			"(default: 128, 4096 with srx");
 
 	fi_param_define(&rxm_prov, "cm_progress_interval", FI_PARAM_INT,
 			"Defines the number of microseconds to wait between "
@@ -534,7 +560,7 @@ RXM_INI
 			"This allows direct placement of received messages "
 			"into application buffers, bypassing RxM bounce "
 			"buffers.  This feature targets using tcp sockets "
-			"for the message transport.  (default: false)");
+			"for the message transport.  (default: true)");
 
 	fi_param_define(&rxm_prov, "enable_direct_send", FI_PARAM_BOOL,
 			"Enable support to pass application buffers directly "
@@ -542,7 +568,7 @@ RXM_INI
 			"copying application buffers through bounce buffers "
 			"before passing them to the core provider.  This "
 			"feature targets small to medium size message "
-			"transfers over the tcp provider.  (default: false)");
+			"transfers over the tcp provider.  (default: true)");
 
 	rxm_init_infos();
 	fi_param_get_size_t(&rxm_prov, "msg_tx_size", &rxm_msg_tx_size);
@@ -565,6 +591,7 @@ RXM_INI
 
 #if HAVE_RXM_DL
 	ofi_mem_init();
+	ofi_hmem_init();
 #endif
 
 	return &rxm_prov;
diff --git a/prov/rxm/src/rxm_rma.c b/prov/rxm/src/rxm_rma.c
index 15c69af..987953a 100644
--- a/prov/rxm/src/rxm_rma.c
+++ b/prov/rxm/src/rxm_rma.c
@@ -37,7 +37,7 @@
 static ssize_t
 rxm_ep_rma_reg_iov(struct rxm_ep *rxm_ep, const struct iovec *msg_iov,
 		   void **desc, void **desc_storage, size_t iov_count,
-		   uint64_t access, struct rxm_rma_buf *rma_buf)
+		   uint64_t access, struct rxm_tx_buf *rma_buf)
 {
 	size_t i, ret;
 
@@ -46,13 +46,13 @@ rxm_ep_rma_reg_iov(struct rxm_ep *rxm_ep, const struct iovec *msg_iov,
 
 	if (!rxm_ep->rdm_mr_local) {
 		ret = rxm_msg_mr_regv(rxm_ep, msg_iov, iov_count, SIZE_MAX,
-				      access, rma_buf->mr.mr);
+				      access, rma_buf->rma.mr);
 		if (OFI_UNLIKELY(ret))
 			return ret;
 
 		for (i = 0; i < iov_count; i++)
-			desc_storage[i] = fi_mr_desc(rma_buf->mr.mr[i]);
-		rma_buf->mr.count = iov_count;
+			desc_storage[i] = fi_mr_desc(rma_buf->rma.mr[i]);
+		rma_buf->rma.count = iov_count;
 	} else {
 		for (i = 0; i < iov_count; i++)
 			desc_storage[i] =
@@ -67,7 +67,7 @@ rxm_ep_rma_common(struct rxm_ep *rxm_ep, const struct fi_msg_rma *msg,
 		  const struct fi_msg_rma *msg, uint64_t flags),
 		  uint64_t comp_flags)
 {
-	struct rxm_rma_buf *rma_buf;
+	struct rxm_tx_buf *rma_buf;
 	struct fi_msg_rma msg_rma = *msg;
 	struct rxm_conn *rxm_conn;
 	void *mr_desc[RXM_IOV_LIMIT] = { 0 };
@@ -81,12 +81,14 @@ rxm_ep_rma_common(struct rxm_ep *rxm_ep, const struct fi_msg_rma *msg,
 	if (OFI_UNLIKELY(ret))
 		goto unlock;
 
-	rma_buf = ofi_buf_alloc(rxm_ep->buf_pools[RXM_BUF_POOL_RMA].pool);
+	rma_buf = rxm_get_tx_buf(rxm_ep);
 	if (!rma_buf) {
 		ret = -FI_EAGAIN;
 		goto unlock;
 	}
 
+	rma_buf->hdr.state = RXM_RMA;
+	rma_buf->pkt.ctrl_hdr.type = rxm_ctrl_eager;
 	rma_buf->app_context = msg->context;
 	rma_buf->flags = flags;
 
@@ -100,13 +102,13 @@ rxm_ep_rma_common(struct rxm_ep *rxm_ep, const struct fi_msg_rma *msg,
 	msg_rma.context = rma_buf;
 
 	ret = rma_msg(rxm_conn->msg_ep, &msg_rma, flags);
-	if (OFI_LIKELY(!ret))
+	if (!ret)
 		goto unlock;
 
 	if ((rxm_ep->msg_mr_local) && (!rxm_ep->rdm_mr_local))
-		rxm_msg_mr_closev(rma_buf->mr.mr, rma_buf->mr.count);
+		rxm_msg_mr_closev(rma_buf->rma.mr, rma_buf->rma.count);
 release:
-	ofi_buf_free(rma_buf);
+	rxm_free_rx_buf(rxm_ep, rma_buf);
 unlock:
 	ofi_ep_lock_release(&rxm_ep->util_ep);
 	return ret;
@@ -180,7 +182,8 @@ static ssize_t rxm_ep_read(struct fid_ep *ep_fid, void *buf, size_t len,
 }
 
 static void
-rxm_ep_format_rma_msg(struct rxm_rma_buf *rma_buf, const struct fi_msg_rma *orig_msg,
+rxm_ep_format_rma_msg(struct rxm_tx_buf *rma_buf,
+		      const struct fi_msg_rma *orig_msg,
 		      struct iovec *rxm_iov, struct fi_msg_rma *rxm_msg)
 {
 	ssize_t ret __attribute__((unused));
@@ -214,17 +217,19 @@ rxm_ep_rma_emulate_inject_msg(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 			      size_t total_size, const struct fi_msg_rma *msg,
 			      uint64_t flags)
 {
-	struct rxm_rma_buf *rma_buf;
+	struct rxm_tx_buf *rma_buf;
 	ssize_t ret;
 	struct iovec rxm_msg_iov = { 0 };
 	struct fi_msg_rma rxm_rma_msg = { 0 };
 
 	assert(msg->rma_iov_count <= rxm_ep->rxm_info->tx_attr->rma_iov_limit);
 
-	rma_buf = ofi_buf_alloc(rxm_ep->buf_pools[RXM_BUF_POOL_RMA].pool);
+	rma_buf = rxm_get_tx_buf(rxm_ep);
 	if (!rma_buf)
 		return -FI_EAGAIN;
 
+	rma_buf->hdr.state = RXM_RMA;
+	rma_buf->pkt.ctrl_hdr.type = rxm_ctrl_eager;
 	rma_buf->pkt.hdr.size = total_size;
 	rma_buf->app_context = msg->context;
 	rma_buf->flags = flags;
@@ -233,10 +238,10 @@ rxm_ep_rma_emulate_inject_msg(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 	flags = (flags & ~FI_INJECT) | FI_COMPLETION;
 
 	ret = fi_writemsg(rxm_conn->msg_ep, &rxm_rma_msg, flags);
-	if (OFI_UNLIKELY(ret)) {
+	if (ret) {
 		if (ret == -FI_EAGAIN)
 			rxm_ep_do_progress(&rxm_ep->util_ep);
-		ofi_buf_free(rma_buf);
+		rxm_free_rx_buf(rxm_ep, rma_buf);
 	}
 	return ret;
 }
@@ -286,7 +291,7 @@ rxm_ep_rma_inject_common(struct rxm_ep *rxm_ep, const struct fi_msg_rma *msg,
 	if (OFI_UNLIKELY(ret))
 		goto unlock;
 
-	if ((total_size > rxm_ep->msg_info->tx_attr->inject_size) ||
+	if ((total_size > rxm_ep->rxm_info->tx_attr->inject_size) ||
 	    rxm_ep->util_ep.wr_cntr ||
 	    (flags & FI_COMPLETION) || (msg->iov_count > 1) ||
 	    (msg->rma_iov_count > 1)) {
@@ -444,8 +449,7 @@ static ssize_t rxm_ep_inject_write(struct fid_ep *ep_fid, const void *buf,
 	if (OFI_UNLIKELY(ret))
 		goto unlock;
 
-	if (len > rxm_ep->msg_info->tx_attr->inject_size ||
-	    rxm_ep->util_ep.wr_cntr) {
+	if (len > rxm_ep->inject_limit || rxm_ep->util_ep.wr_cntr) {
 		ret = rxm_ep_rma_emulate_inject(rxm_ep, rxm_conn, buf, len, 0,
 						dest_addr, addr, key,
 						FI_INJECT);
@@ -479,8 +483,7 @@ static ssize_t rxm_ep_inject_writedata(struct fid_ep *ep_fid, const void *buf,
 	if (OFI_UNLIKELY(ret))
 		goto unlock;
 
-	if (len > rxm_ep->msg_info->tx_attr->inject_size ||
-	    rxm_ep->util_ep.wr_cntr) {
+	if (len > rxm_ep->inject_limit || rxm_ep->util_ep.wr_cntr) {
 		ret = rxm_ep_rma_emulate_inject(
 			rxm_ep, rxm_conn, buf, len, data, dest_addr,
 			addr, key, FI_REMOTE_CQ_DATA | FI_INJECT);
diff --git a/prov/shm/src/smr.h b/prov/shm/src/smr.h
index 1800a81..b2f0a32 100644
--- a/prov/shm/src/smr.h
+++ b/prov/shm/src/smr.h
@@ -68,12 +68,14 @@
 
 struct smr_env {
 	size_t sar_threshold;
+	int disable_cma;
 };
 
 extern struct smr_env smr_env;
 extern struct fi_provider smr_prov;
 extern struct fi_info smr_info;
 extern struct util_prov smr_util_prov;
+extern int smr_global_ep_idx; //protected by the ep_list_lock
 
 int smr_fabric(struct fi_fabric_attr *attr, struct fid_fabric **fabric,
 		void *context);
@@ -180,6 +182,12 @@ static inline enum fi_hmem_iface smr_get_mr_hmem_iface(struct util_domain *domai
 	return ((struct ofi_mr *) *desc)->iface;
 }
 
+static inline uint64_t smr_get_mr_flags(void **desc)
+{
+	assert(desc && *desc);
+	return ((struct ofi_mr *) *desc)->flags;
+}
+
 struct smr_unexp_msg {
 	struct dlist_entry entry;
 	struct smr_cmd cmd;
@@ -197,13 +205,10 @@ struct smr_queue {
 
 struct smr_fabric {
 	struct util_fabric	util_fabric;
-	int			dom_idx;
 };
 
 struct smr_domain {
 	struct util_domain	util_domain;
-	int			dom_idx;
-	int			ep_idx;
 	int			fast_rma;
 };
 
@@ -212,13 +217,6 @@ struct smr_domain {
 
 #define SMR_ZE_SOCK_PATH	"/dev/shm/ze_"
 
-static inline const char *smr_no_prefix(const char *addr)
-{
-	char *start;
-
-	return (start = strstr(addr, "://")) ? start + 3 : addr;
-}
-
 #define SMR_RMA_ORDER (OFI_ORDER_RAR_SET | OFI_ORDER_RAW_SET | FI_ORDER_RAS |	\
 		       OFI_ORDER_WAR_SET | OFI_ORDER_WAW_SET | FI_ORDER_WAS |	\
 		       FI_ORDER_SAR | FI_ORDER_SAW)
@@ -328,6 +326,9 @@ int smr_format_ze_ipc(struct smr_ep *ep, int64_t id, struct smr_cmd *cmd,
 		      const struct iovec *iov, uint64_t device,
 		      size_t total_len, struct smr_region *smr,
 		      struct smr_resp *resp, struct smr_tx_entry *pend);
+int smr_format_ipc(struct smr_cmd *cmd, void *ptr,
+                   size_t len, struct smr_region *smr,
+                   struct smr_resp *resp, enum fi_hmem_iface iface);
 int smr_format_mmap(struct smr_ep *ep, struct smr_cmd *cmd,
 		    const struct iovec *iov, size_t count, size_t total_len,
 		    struct smr_tx_entry *pend, struct smr_resp *resp);
diff --git a/prov/shm/src/smr_atomic.c b/prov/shm/src/smr_atomic.c
index 14ff545..69aef06 100644
--- a/prov/shm/src/smr_atomic.c
+++ b/prov/shm/src/smr_atomic.c
@@ -252,6 +252,7 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 	smr_format_rma_ioc(cmd, rma_ioc, rma_count);
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
+	smr_signal(peer_smr);
 unlock_cq:
 	fastlock_release(&ep->util_ep.tx_cq->cq_lock);
 unlock_region:
@@ -373,6 +374,7 @@ static ssize_t smr_atomic_inject(struct fid_ep *ep_fid, const void *buf,
 	smr_format_rma_ioc(cmd, &rma_ioc, 1);
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
+	smr_signal(peer_smr);
 
 	ofi_ep_tx_cntr_inc_func(&ep->util_ep, ofi_op_atomic);
 unlock_region:
diff --git a/prov/shm/src/smr_av.c b/prov/shm/src/smr_av.c
index 7ce7916..2961f84 100644
--- a/prov/shm/src/smr_av.c
+++ b/prov/shm/src/smr_av.c
@@ -62,7 +62,6 @@ static int smr_av_insert(struct fid_av *av_fid, const void *addr, size_t count,
 	struct smr_av *smr_av;
 	struct smr_ep *smr_ep;
 	struct dlist_entry *av_entry;
-	const char *ep_name;
 	fi_addr_t util_addr;
 	int64_t shm_id = -1;
 	int i, ret;
@@ -73,17 +72,19 @@ static int smr_av_insert(struct fid_av *av_fid, const void *addr, size_t count,
 
 	for (i = 0; i < count; i++, addr = (char *) addr + strlen(addr) + 1) {
 		if (smr_av->used < SMR_MAX_PEERS) {
-			ep_name = smr_no_prefix(addr);
+			util_addr = FI_ADDR_NOTAVAIL;
 			ret = smr_map_add(&smr_prov, smr_av->smr_map,
-					  ep_name, &shm_id);
-			if (!ret)
+					  addr, &shm_id);
+			if (!ret) {
+				fastlock_acquire(&util_av->lock);
 				ret = ofi_av_insert_addr(util_av, &shm_id,
 							 &util_addr);
+				fastlock_release(&util_av->lock);
+			}
 		} else {
 			FI_WARN(&smr_prov, FI_LOG_AV,
 				"AV insert failed. The maximum number of AV "
 				"entries shm supported has been reached.\n");
-			util_addr = FI_ADDR_NOTAVAIL;
 			ret = -FI_ENOMEM;
 		}
 
@@ -159,21 +160,19 @@ static int smr_av_lookup(struct fid_av *av, fi_addr_t fi_addr, void *addr,
 {
 	struct util_av *util_av;
 	struct smr_av *smr_av;
-	struct smr_region *peer_smr;
 	int64_t id;
+	char *name;
 
 	util_av = container_of(av, struct util_av, av_fid);
 	smr_av = container_of(util_av, struct smr_av, util_av);
 
 	id = smr_addr_lookup(util_av, fi_addr);
-	peer_smr = smr_map_get(smr_av->smr_map, id);
+	name = smr_av->smr_map->peers[id].peer.name;
 
-	if (!peer_smr)
-		return -FI_ENODATA;
+	strncpy((char *) addr, name, *addrlen);
 
-	strncpy((char *)addr, smr_name(peer_smr), *addrlen);
-	((char *) addr)[MIN(*addrlen - 1, strlen(smr_name(peer_smr)))] = '\0';
-	*addrlen = strlen(smr_name(peer_smr) + 1);
+	((char *) addr)[MIN(*addrlen - 1, strlen(name))] = '\0';
+	*addrlen = strlen(name) + 1;
 	return 0;
 }
 
diff --git a/prov/shm/src/smr_domain.c b/prov/shm/src/smr_domain.c
index 5970002..afa17ba 100644
--- a/prov/shm/src/smr_domain.c
+++ b/prov/shm/src/smr_domain.c
@@ -101,7 +101,6 @@ int smr_domain_open(struct fid_fabric *fabric, struct fi_info *info,
 
 	smr_fabric = container_of(fabric, struct smr_fabric, util_fabric.fabric_fid);
 	fastlock_acquire(&smr_fabric->util_fabric.lock);
-	smr_domain->dom_idx = smr_fabric->dom_idx++;
 	smr_domain->fast_rma = smr_fast_rma_enabled(info->domain_attr->mr_mode,
 						    info->tx_attr->msg_order);
 	fastlock_release(&smr_fabric->util_fabric.lock);
diff --git a/prov/shm/src/smr_ep.c b/prov/shm/src/smr_ep.c
index cf55b3d..e4b0a98 100644
--- a/prov/shm/src/smr_ep.c
+++ b/prov/shm/src/smr_ep.c
@@ -45,6 +45,7 @@ extern struct fi_ops_rma smr_rma_ops;
 extern struct fi_ops_atomic smr_atomic_ops;
 DEFINE_LIST(sock_name_list);
 pthread_mutex_t sock_list_lock = PTHREAD_MUTEX_INITIALIZER;
+int smr_global_ep_idx = 0;
 
 int smr_setname(fid_t fid, void *addr, size_t addrlen)
 {
@@ -199,16 +200,18 @@ static void smr_send_name(struct smr_ep *ep, int64_t id)
 
 	cmd->msg.hdr.op = SMR_OP_MAX + ofi_ctrl_connreq;
 	cmd->msg.hdr.id = id;
+	cmd->msg.hdr.data = ep->region->pid;
 
 	tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
 	cmd->msg.hdr.src_data = smr_get_offset(peer_smr, tx_buf);
 
-	cmd->msg.hdr.size = strlen(smr_name(ep->region)) + 1;
-	memcpy(tx_buf->data, smr_name(ep->region), cmd->msg.hdr.size);
+	cmd->msg.hdr.size = strlen(ep->name) + 1;
+	memcpy(tx_buf->data, ep->name, cmd->msg.hdr.size);
 
 	smr_peer_data(ep->region)[id].name_sent = 1;
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
+	smr_signal(peer_smr);
 
 out:
 	fastlock_release(&peer_smr->lock);
@@ -388,6 +391,19 @@ int smr_format_ze_ipc(struct smr_ep *ep, int64_t id, struct smr_cmd *cmd,
 	return FI_SUCCESS;
 }
 
+int smr_format_ipc(struct smr_cmd *cmd, void *ptr,
+                   size_t len, struct smr_region *smr,
+                   struct smr_resp *resp, enum fi_hmem_iface iface)
+{
+    cmd->msg.hdr.op_src = smr_src_ipc;
+    cmd->msg.hdr.src_data = smr_get_offset(smr, resp);
+    cmd->msg.hdr.size = len;
+    cmd->msg.data.ipc_info.iface = iface;
+
+    return ofi_hmem_get_handle(cmd->msg.data.ipc_info.iface, ptr,
+                               (void **)&cmd->msg.data.ipc_info.ipc_handle);
+}
+
 int smr_format_mmap(struct smr_ep *ep, struct smr_cmd *cmd,
 		    const struct iovec *iov, size_t count, size_t total_len,
 		    struct smr_tx_entry *pend, struct smr_resp *resp)
@@ -543,7 +559,7 @@ void smr_format_sar(struct smr_cmd *cmd, enum fi_hmem_iface iface, uint64_t devi
 	sar_msg->sar[0].status = SMR_SAR_FREE;
 	sar_msg->sar[1].status = SMR_SAR_FREE;
 	if (cmd->msg.hdr.op != ofi_op_read_req)
-		smr_copy_to_sar(sar_msg, NULL, cmd, iface, device ,iov, count,
+		smr_copy_to_sar(sar_msg, resp, cmd, iface, device ,iov, count,
 				&pending->bytes_done, &pending->next);
 }
 
@@ -762,7 +778,7 @@ static int smr_recvmsg_fd(int sock, int64_t *peer_id, int *fds, int nfds)
 	cmsg = CMSG_FIRSTHDR(&msg);
 	assert(cmsg && cmsg->cmsg_len == CMSG_LEN(ctrl_size) &&
 	       cmsg->cmsg_level == SOL_SOCKET &&
-	       cmsg->cmsg_type == SCM_RIGHTS);
+	       cmsg->cmsg_type == SCM_RIGHTS && CMSG_DATA(cmsg));
 	memcpy(fds, CMSG_DATA(cmsg), ctrl_size);
 out:
 	free(ctrl_buf);
@@ -773,7 +789,7 @@ static void *smr_start_listener(void *args)
 {
 	struct smr_ep *ep = (struct smr_ep *) args;
 	struct sockaddr_un sockaddr;
-	void *ctx[SMR_MAX_PEERS + 1];
+	struct ofi_epollfds_event events[SMR_MAX_PEERS + 1];
 	int i, ret, poll_fds, sock = -1;
 	int peer_fds[ZE_MAX_DEVICES];
 	socklen_t len;
@@ -781,7 +797,7 @@ static void *smr_start_listener(void *args)
 
 	ep->region->flags |= SMR_FLAG_IPC_SOCK;
 	while (1) {
-		poll_fds = ofi_epoll_wait(ep->sock_info->epollfd, ctx,
+		poll_fds = ofi_epoll_wait(ep->sock_info->epollfd, events,
 					  SMR_MAX_PEERS + 1, -1);
 
 		if (poll_fds < 0) {
@@ -791,7 +807,7 @@ static void *smr_start_listener(void *args)
 		}
 
 		for (i = 0; i < poll_fds; i++) {
-			if (!ctx[i])
+			if (!events[i].data.ptr)
 				goto out;
 
 			sock = accept(ep->sock_info->listen_sock,
@@ -937,7 +953,6 @@ out:
 
 static void smr_init_ipc_socket(struct smr_ep *ep)
 {
-	struct smr_domain *domain;
 	struct smr_sock_name *sock_name;
 	struct sockaddr_un sockaddr = {0};
 	int ret;
@@ -950,10 +965,8 @@ static void smr_init_ipc_socket(struct smr_ep *ep)
 	if (ep->sock_info->listen_sock < 0)
 		goto free;
 
-	domain = container_of(ep->util_ep.domain, struct smr_domain, util_domain);
 	snprintf(smr_sock_name(ep->region), SMR_SOCK_NAME_MAX,
-		 "%ld:%d:%d", (long) ep->region->pid, domain->dom_idx,
-		 ep->ep_idx);
+		 "%ld:%d", (long) ep->region->pid, ep->ep_idx);
 
 	sockaddr.sun_family = AF_UNIX;
 	snprintf(sockaddr.sun_path, SMR_SOCK_NAME_MAX,
@@ -1031,18 +1044,20 @@ static int smr_ep_ctrl(struct fid *fid, int command, void *arg)
 		if (!ep->util_ep.av)
 			return -FI_ENOAV;
 
-		attr.name = ep->name;
+		attr.name = smr_no_prefix(ep->name);
 		attr.rx_count = ep->rx_size;
 		attr.tx_count = ep->tx_size;
 		ret = smr_create(&smr_prov, av->smr_map, &attr, &ep->region);
 		if (ret)
 			return ret;
 
-		if (ep->util_ep.caps & FI_HMEM) {
+		if (ep->util_ep.caps & FI_HMEM || smr_env.disable_cma) {
 			ep->region->cma_cap_peer = SMR_CMA_CAP_OFF;
 			ep->region->cma_cap_self = SMR_CMA_CAP_OFF;
-			if (ze_hmem_p2p_enabled())
-				smr_init_ipc_socket(ep);
+			if (ep->util_ep.caps & FI_HMEM) {
+				if (ze_hmem_p2p_enabled())
+					smr_init_ipc_socket(ep);
+			}
 		}
 
 		smr_exchange_all_peers(ep->region);
@@ -1061,20 +1076,22 @@ static struct fi_ops smr_ep_fi_ops = {
 	.ops_open = fi_no_ops_open,
 };
 
-static int smr_endpoint_name(char *name, char *addr, size_t addrlen,
-			     int dom_idx, int ep_idx)
+static int smr_endpoint_name(struct smr_ep *ep, char *name, char *addr,
+			     size_t addrlen)
 {
-	const char *start;
 	memset(name, 0, SMR_NAME_MAX);
 	if (!addr || addrlen > SMR_NAME_MAX)
 		return -FI_EINVAL;
 
-	start = smr_no_prefix((const char *) addr);
-	if (strstr(addr, SMR_PREFIX) || dom_idx || ep_idx)
-		snprintf(name, SMR_NAME_MAX - 1, "%s:%d:%d:%d", start, getuid(),
-			 dom_idx, ep_idx);
+	pthread_mutex_lock(&ep_list_lock);
+	ep->ep_idx = smr_global_ep_idx++;
+	pthread_mutex_unlock(&ep_list_lock);
+
+	if (strstr(addr, SMR_PREFIX))
+		snprintf(name, SMR_NAME_MAX - 1, "%s:%d:%d", addr, getuid(),
+			 ep->ep_idx);
 	else
-		snprintf(name, SMR_NAME_MAX - 1, "%s", start);
+		snprintf(name, SMR_NAME_MAX - 1, "%s", addr);
 
 	return 0;
 }
@@ -1083,7 +1100,6 @@ int smr_endpoint(struct fid_domain *domain, struct fi_info *info,
 		  struct fid_ep **ep_fid, void *context)
 {
 	struct smr_ep *ep;
-	struct smr_domain *smr_domain;
 	int ret;
 	char name[SMR_NAME_MAX];
 
@@ -1091,13 +1107,7 @@ int smr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	if (!ep)
 		return -FI_ENOMEM;
 
-	smr_domain = container_of(domain, struct smr_domain, util_domain.domain_fid);
-
-	fastlock_acquire(&smr_domain->util_domain.lock);
-	ep->ep_idx = smr_domain->ep_idx++;
-	fastlock_release(&smr_domain->util_domain.lock);
-	ret = smr_endpoint_name(name, info->src_addr, info->src_addrlen,
-			        smr_domain->dom_idx, ep->ep_idx);
+	ret = smr_endpoint_name(ep, name, info->src_addr, info->src_addrlen);
 	if (ret)
 		goto err2;
 	ret = smr_setname(&ep->util_ep.ep_fid.fid, name, SMR_NAME_MAX);
diff --git a/prov/shm/src/smr_init.c b/prov/shm/src/smr_init.c
index 035d089..e24d6f5 100644
--- a/prov/shm/src/smr_init.c
+++ b/prov/shm/src/smr_init.c
@@ -40,6 +40,7 @@
 extern struct sigaction *old_action;
 struct smr_env smr_env = {
 	.sar_threshold = SIZE_MAX,
+	.disable_cma = false,
 };
 
 static void smr_init_env(void)
@@ -47,6 +48,7 @@ static void smr_init_env(void)
 	fi_param_get_size_t(&smr_prov, "sar_threshold", &smr_env.sar_threshold);
 	fi_param_get_size_t(&smr_prov, "tx_size", &smr_info.tx_attr->size);
 	fi_param_get_size_t(&smr_prov, "rx_size", &smr_info.rx_attr->size);
+	fi_param_get_bool(&smr_prov, "disable_cma", &smr_env.disable_cma);
 }
 
 static void smr_resolve_addr(const char *node, const char *service,
@@ -214,6 +216,8 @@ SHM_INI
 	fi_param_define(&smr_prov, "rx_size", FI_PARAM_SIZE_T,
 			"Max number of outstanding rx operations \
 			 Default: 1024");
+	fi_param_define(&smr_prov, "disable_cma", FI_PARAM_BOOL,
+			"Manually disables CMA. Default: false");
 
 	smr_init_env();
 
diff --git a/prov/shm/src/smr_msg.c b/prov/shm/src/smr_msg.c
index 515ffac..0893df5 100644
--- a/prov/shm/src/smr_msg.c
+++ b/prov/shm/src/smr_msg.c
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2013-2021 Intel Corporation. All rights reserved
+ * (C) Copyright 2021 Amazon.com, Inc. or its affiliates.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -167,6 +168,7 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 	int64_t id, peer_id;
 	ssize_t ret = 0;
 	size_t total_len;
+	bool use_ipc;
 
 	assert(iov_count <= SMR_IOV_LIMIT);
 
@@ -196,10 +198,17 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	smr_generic_format(cmd, peer_id, op, tag, data, op_flags);
 
-	if (total_len <= SMR_MSG_DATA_LEN && !(op_flags & FI_DELIVERY_COMPLETE)) {
+	/* Do not inline/inject if IPC is available so device to device
+	 * transfer may occur if possible. */
+	use_ipc = ofi_hmem_is_ipc_enabled(iface) && (iov_count == 1) &&
+		  desc && (smr_get_mr_flags(desc) & FI_HMEM_DEVICE_ONLY);
+
+	if (total_len <= SMR_MSG_DATA_LEN &&
+	    !(op_flags & FI_DELIVERY_COMPLETE) && !use_ipc) {
 		smr_format_inline(cmd, iface, device, iov, iov_count);
 	} else if (total_len <= SMR_INJECT_SIZE &&
-		   !(op_flags & FI_DELIVERY_COMPLETE)) {
+		   !(op_flags & FI_DELIVERY_COMPLETE) &&
+		   !use_ipc) {
 		tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
 		smr_format_inject(cmd, iface, device, iov, iov_count, peer_smr, tx_buf);
 	} else {
@@ -213,11 +222,14 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 			smr_format_iov(cmd, iov, iov_count, total_len, ep->region,
 				       resp);
 		} else {
-			if (iface == FI_HMEM_ZE && iov_count == 1 &&
+			if (use_ipc && iface == FI_HMEM_ZE &&
 			    smr_ze_ipc_enabled(ep->region, peer_smr)) {
 				ret = smr_format_ze_ipc(ep, id, cmd, iov,
 					device, total_len, ep->region,
 					resp, pend);
+			} else if (use_ipc && iface != FI_HMEM_ZE) {
+				ret = smr_format_ipc(cmd, iov[0].iov_base, total_len,
+						     ep->region, resp, iface);
 			} else if (total_len <= smr_env.sar_threshold ||
 				   iface != FI_HMEM_SYSTEM) {
 				if (!peer_smr->sar_cnt) {
@@ -256,6 +268,7 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 commit:
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
+	smr_signal(peer_smr);
 unlock_cq:
 	fastlock_release(&ep->util_ep.tx_cq->cq_lock);
 unlock_region:
@@ -347,6 +360,7 @@ static ssize_t smr_generic_inject(struct fid_ep *ep_fid, const void *buf,
 	ofi_ep_tx_cntr_inc_func(&ep->util_ep, op);
 	peer_smr->cmd_cnt--;
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
+	smr_signal(peer_smr);
 unlock:
 	fastlock_release(&peer_smr->lock);
 
diff --git a/prov/shm/src/smr_progress.c b/prov/shm/src/smr_progress.c
index 9636ce5..8dcee52 100644
--- a/prov/shm/src/smr_progress.c
+++ b/prov/shm/src/smr_progress.c
@@ -36,11 +36,12 @@
 
 #include "ofi_iov.h"
 #include "ofi_hmem.h"
+#include "ofi_atom.h"
 #include "smr.h"
 
 
-static inline void smr_try_progress_to_sar(struct smr_sar_msg *sar_msg,
-				struct smr_resp *resp,
+static inline void smr_try_progress_to_sar(struct smr_region *smr,
+				struct smr_sar_msg *sar_msg, struct smr_resp *resp,
 				struct smr_cmd *cmd, enum fi_hmem_iface iface,
 				uint64_t device, struct iovec *iov,
 				size_t iov_count, size_t *bytes_done, int *next)
@@ -48,10 +49,11 @@ static inline void smr_try_progress_to_sar(struct smr_sar_msg *sar_msg,
 	while (*bytes_done < cmd->msg.hdr.size &&
 	       smr_copy_to_sar(sar_msg, resp, cmd, iface, device, iov,
 			       iov_count, bytes_done, next));
+	smr_signal(smr);
 }
 
-static inline void smr_try_progress_from_sar(struct smr_sar_msg *sar_msg,
-				struct smr_resp *resp,
+static inline void smr_try_progress_from_sar(struct smr_region *smr,
+				struct smr_sar_msg *sar_msg, struct smr_resp *resp,
 				struct smr_cmd *cmd, enum fi_hmem_iface iface,
 				uint64_t device, struct iovec *iov,
 				size_t iov_count, size_t *bytes_done, int *next)
@@ -59,6 +61,7 @@ static inline void smr_try_progress_from_sar(struct smr_sar_msg *sar_msg,
 	while (*bytes_done < cmd->msg.hdr.size &&
 	       smr_copy_from_sar(sar_msg, resp, cmd, iface, device, iov,
 				 iov_count, bytes_done, next));
+	smr_signal(smr);
 }
 
 static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
@@ -69,6 +72,7 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 	struct smr_inject_buf *tx_buf = NULL;
 	struct smr_sar_msg *sar_msg = NULL;
 	uint8_t *src;
+	ssize_t hmem_copy_ret;
 
 	peer_smr = smr_peer_region(ep->region, pending->peer_id);
 
@@ -76,7 +80,8 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 	case smr_src_iov:
 		break;
 	case smr_src_ipc:
-		close(pending->fd);
+		if (pending->iface == FI_HMEM_ZE)
+			close(pending->fd);
 		break;
 	case smr_src_sar:
 		sar_msg = smr_get_ptr(peer_smr, pending->cmd.msg.data.sar);
@@ -86,13 +91,13 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 			break;
 
 		if (pending->cmd.msg.hdr.op == ofi_op_read_req)
-			smr_try_progress_from_sar(sar_msg, resp,
+			smr_try_progress_from_sar(peer_smr, sar_msg, resp,
 					&pending->cmd, pending->iface,
 					pending->device, pending->iov,
 				        pending->iov_count, &pending->bytes_done,
 					&pending->next);
 		else
-			smr_try_progress_to_sar(sar_msg, resp,
+			smr_try_progress_to_sar(peer_smr, sar_msg, resp,
 					&pending->cmd, pending->iface,
 					pending->device, pending->iov,
 					pending->iov_count, &pending->bytes_done,
@@ -107,14 +112,24 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 			break;
 		if (pending->cmd.msg.hdr.op == ofi_op_read_req) {
 			if (!*err) {
-				pending->bytes_done = ofi_copy_to_iov(pending->iov,
-						pending->iov_count, 0,
-						pending->map_ptr,
-						pending->cmd.msg.hdr.size);
-				if (pending->bytes_done != pending->cmd.msg.hdr.size) {
+				hmem_copy_ret =
+					ofi_copy_to_hmem_iov(pending->iface,
+							     pending->device,
+							     pending->iov,
+							     pending->iov_count,
+							     0, pending->map_ptr,
+							     pending->cmd.msg.hdr.size);
+				if (hmem_copy_ret < 0) {
+					FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+						"Copy from mmapped file failed with code %d\n",
+						(int)(-hmem_copy_ret));
+					*err = hmem_copy_ret;
+				} else if (hmem_copy_ret != pending->cmd.msg.hdr.size) {
 					FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 						"Incomplete copy from mmapped file\n");
-					*err = -FI_EIO;
+					*err = -FI_ETRUNC;
+				} else {
+					pending->bytes_done = (size_t) hmem_copy_ret;
 				}
 			}
 			munmap(pending->map_ptr, pending->cmd.msg.hdr.size);
@@ -133,13 +148,21 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 
 		src = pending->cmd.msg.hdr.op == ofi_op_atomic_compare ?
 		      tx_buf->buf : tx_buf->data;
-		pending->bytes_done = ofi_copy_to_iov(pending->iov, pending->iov_count,
-				       0, src, pending->cmd.msg.hdr.size);
+		hmem_copy_ret  = ofi_copy_to_hmem_iov(pending->iface, pending->device,
+						      pending->iov, pending->iov_count,
+						      0, src, pending->cmd.msg.hdr.size);
 
-		if (pending->bytes_done != pending->cmd.msg.hdr.size) {
+		if (hmem_copy_ret < 0) {
+			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+				"RMA read/fetch failed with code %d\n",
+				(int)(-hmem_copy_ret));
+			*err = hmem_copy_ret;
+		} else if (hmem_copy_ret != pending->cmd.msg.hdr.size) {
 			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 				"Incomplete rma read/fetch buffer copied\n");
-			*err = FI_EIO;
+			*err = -FI_ETRUNC;
+		} else {
+			pending->bytes_done = (size_t) hmem_copy_ret;
 		}
 		break;
 	default:
@@ -150,8 +173,10 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 	//Skip locking on transfers from self since we already have
 	//the ep->region->lock
 	if (peer_smr != ep->region) {
-		if (fastlock_tryacquire(&peer_smr->lock))
+		if (fastlock_tryacquire(&peer_smr->lock)) {
+			smr_signal(ep->region);
 			return -FI_EAGAIN;
+		}
 	}
 
 	peer_smr->cmd_cnt++;
@@ -166,7 +191,7 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 	if (peer_smr != ep->region)
 		fastlock_release(&peer_smr->lock);
 
-	return 0;
+	return FI_SUCCESS;
 }
 
 static void smr_progress_resp(struct smr_ep *ep)
@@ -206,14 +231,24 @@ static int smr_progress_inline(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 			       uint64_t device, struct iovec *iov,
 			       size_t iov_count, size_t *total_len)
 {
-	*total_len = ofi_copy_to_hmem_iov(iface, device, iov, iov_count, 0,
-					  cmd->msg.data.msg, cmd->msg.hdr.size);
-	if (*total_len != cmd->msg.hdr.size) {
+	ssize_t hmem_copy_ret;
+
+	hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov, iov_count, 0,
+					     cmd->msg.data.msg, cmd->msg.hdr.size);
+	if (hmem_copy_ret < 0) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-			"recv truncated");
-		return -FI_EIO;
+			"inline recv failed with code %d\n",
+			(int)(-hmem_copy_ret));
+		return hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"inline recv truncated\n");
+		return -FI_ETRUNC;
 	}
-	return 0;
+
+	*total_len = hmem_copy_ret;
+
+	return FI_SUCCESS;
 }
 
 static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
@@ -223,6 +258,7 @@ static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 {
 	struct smr_inject_buf *tx_buf;
 	size_t inj_offset;
+	ssize_t hmem_copy_ret;
 
 	inj_offset = (size_t) cmd->msg.hdr.src_data;
 	tx_buf = smr_get_ptr(ep->region, inj_offset);
@@ -233,20 +269,30 @@ static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 	}
 
 	if (cmd->msg.hdr.op == ofi_op_read_req) {
-		*total_len = ofi_copy_from_hmem_iov(tx_buf->data, cmd->msg.hdr.size,
-						    iface, device, iov, iov_count, 0);
+		hmem_copy_ret = ofi_copy_from_hmem_iov(tx_buf->data,
+						       cmd->msg.hdr.size,
+						       iface, device, iov,
+						       iov_count, 0);
 	} else {
-		*total_len = ofi_copy_to_hmem_iov(iface, device, iov, iov_count, 0,
-						  tx_buf->data, cmd->msg.hdr.size);
+		hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov,
+						     iov_count, 0, tx_buf->data,
+						     cmd->msg.hdr.size);
 		smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
 	}
 
-	if (*total_len != cmd->msg.hdr.size) {
+	if (hmem_copy_ret < 0) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-			"recv truncated");
-		return -FI_EIO;
+			"inject recv failed with code %d\n",
+			(int)(-hmem_copy_ret));
+		return hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"inject recv truncated\n");
+		return -FI_ETRUNC;
 	}
 
+	*total_len = hmem_copy_ret;
+
 	return FI_SUCCESS;
 }
 
@@ -275,18 +321,21 @@ static int smr_progress_iov(struct smr_cmd *cmd, struct iovec *iov,
 out:
 	//Status must be set last (signals peer: op done, valid resp entry)
 	resp->status = ret;
+	smr_signal(peer_smr);
 
 	return -ret;
 }
 
 static int smr_mmap_peer_copy(struct smr_ep *ep, struct smr_cmd *cmd,
-				 struct iovec *iov, size_t iov_count,
-				 size_t *total_len)
+			      enum fi_hmem_iface iface, uint64_t device,
+			      struct iovec *iov, size_t iov_count,
+			      size_t *total_len)
 {
 	char shm_name[SMR_NAME_MAX];
 	void *mapped_ptr;
 	int fd, num;
 	int ret = 0;
+	ssize_t hmem_copy_ret;
 
 	num = smr_mmap_name(shm_name,
 			ep->region->map->peers[cmd->msg.hdr.id].peer.name,
@@ -311,26 +360,28 @@ static int smr_mmap_peer_copy(struct smr_ep *ep, struct smr_cmd *cmd,
 	}
 
 	if (cmd->msg.hdr.op == ofi_op_read_req) {
-		*total_len = ofi_total_iov_len(iov, iov_count);
-		if (ofi_copy_from_iov(mapped_ptr, *total_len, iov, iov_count, 0)
-		    != *total_len) {
-			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-				"mmap iov copy in error\n");
-			ret = -FI_EIO;
-			goto munmap;
-		}
+		hmem_copy_ret = ofi_copy_from_hmem_iov(mapped_ptr,
+						    cmd->msg.hdr.size, iface,
+						    device, iov, iov_count, 0);
 	} else {
-		*total_len = ofi_copy_to_iov(iov, iov_count, 0, mapped_ptr,
-				      cmd->msg.hdr.size);
-		if (*total_len != cmd->msg.hdr.size) {
-			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-				"mmap iov copy out error\n");
-			ret = -FI_EIO;
-			goto munmap;
-		}
+		hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov,
+						  iov_count, 0, mapped_ptr,
+						  cmd->msg.hdr.size);
 	}
 
-munmap:
+	if (hmem_copy_ret < 0) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"mmap copy iov failed with code %d\n",
+			(int)(-hmem_copy_ret));
+		ret = hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"mmap copy iov truncated\n");
+		ret = -FI_ETRUNC;
+	}
+
+	*total_len = hmem_copy_ret;
+
 	munmap(mapped_ptr, cmd->msg.hdr.size);
 unlink_close:
 	shm_unlink(shm_name);
@@ -338,7 +389,8 @@ unlink_close:
 	return ret;
 }
 
-static int smr_progress_mmap(struct smr_cmd *cmd, struct iovec *iov,
+static int smr_progress_mmap(struct smr_cmd *cmd, enum fi_hmem_iface iface,
+			     uint64_t device, struct iovec *iov,
 			     size_t iov_count, size_t *total_len,
 			     struct smr_ep *ep)
 {
@@ -349,10 +401,12 @@ static int smr_progress_mmap(struct smr_cmd *cmd, struct iovec *iov,
 	peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.id);
 	resp = smr_get_ptr(peer_smr, cmd->msg.hdr.src_data);
 
-	ret = smr_mmap_peer_copy(ep, cmd, iov, iov_count, total_len);
+	ret = smr_mmap_peer_copy(ep, cmd, iface, device,
+				 iov, iov_count, total_len);
 
 	//Status must be set last (signals peer: op done, valid resp entry)
 	resp->status = ret;
+	smr_signal(peer_smr);
 
 	return ret;
 }
@@ -377,10 +431,10 @@ static struct smr_sar_entry *smr_progress_sar(struct smr_cmd *cmd,
 	(void) ofi_truncate_iov(sar_iov, &iov_count, cmd->msg.hdr.size);
 
 	if (cmd->msg.hdr.op == ofi_op_read_req)
-		smr_try_progress_to_sar(sar_msg, resp, cmd, iface, device,
+		smr_try_progress_to_sar(peer_smr, sar_msg, resp, cmd, iface, device,
 					sar_iov, iov_count, total_len, &next);
 	else
-		smr_try_progress_from_sar(sar_msg, resp, cmd, iface, device,
+		smr_try_progress_from_sar(peer_smr, sar_msg, resp, cmd, iface, device,
 					  sar_iov, iov_count, total_len, &next);
 
 	if (*total_len == cmd->msg.hdr.size)
@@ -420,11 +474,15 @@ static int smr_progress_ipc(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 	uint64_t ipc_device;
 	int64_t id;
 	int ret, fd, ipc_fd;
+	ssize_t hmem_copy_ret;
 
 	peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.id);
 	resp = smr_get_ptr(peer_smr, cmd->msg.hdr.src_data);
 
-	if (iface == FI_HMEM_ZE) {
+	//TODO disable IPC if more than 1 interface is initialized
+	assert(iface == cmd->msg.data.ipc_info.iface || iface == FI_HMEM_SYSTEM);
+
+	if (cmd->msg.data.ipc_info.iface == FI_HMEM_ZE) {
 		id = cmd->msg.hdr.id;
 		ipc_device = cmd->msg.data.ipc_info.device;
 		fd = ep->sock_info->peers[id].device_fds[ipc_device];
@@ -432,7 +490,7 @@ static int smr_progress_ipc(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 				(void **) &cmd->msg.data.ipc_info.fd_handle,
 				&ipc_fd, ipc_device, &base);
 	} else {
-		ret = ofi_hmem_open_handle(iface,
+		ret = ofi_hmem_open_handle(cmd->msg.data.ipc_info.iface,
 				(void **) &cmd->msg.data.ipc_info.ipc_handle,
 				device, &base);
 	}
@@ -440,27 +498,37 @@ static int smr_progress_ipc(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 		goto out;
 
 	ptr = base;
-	if (iface == FI_HMEM_ZE)
+	if (cmd->msg.data.ipc_info.iface == FI_HMEM_ZE)
 		ptr = (char *) ptr + (uintptr_t) cmd->msg.data.ipc_info.offset;
 
 	if (cmd->msg.hdr.op == ofi_op_read_req) {
-		*total_len = ofi_copy_from_hmem_iov(ptr, cmd->msg.hdr.size,
-						    iface, device, iov,
-						    iov_count, 0);
+		hmem_copy_ret = ofi_copy_from_hmem_iov(ptr, cmd->msg.hdr.size,
+						       cmd->msg.data.ipc_info.iface,
+						       device, iov, iov_count, 0);
 	} else {
-		*total_len = ofi_copy_to_hmem_iov(iface, device, iov,
-						  iov_count, 0, ptr,
-						  cmd->msg.hdr.size);
+		hmem_copy_ret = ofi_copy_to_hmem_iov(cmd->msg.data.ipc_info.iface,
+						     device, iov, iov_count, 0,
+						     ptr, cmd->msg.hdr.size);
 	}
-	if (!ret)
-		*total_len = cmd->msg.hdr.size;
 
-	if (iface == FI_HMEM_ZE)
+	if (cmd->msg.data.ipc_info.iface == FI_HMEM_ZE)
 		close(ipc_fd);
-	ret = ofi_hmem_close_handle(iface, base);
+
+	/* Truncation error takes precedence over close_handle error */
+	ret = ofi_hmem_close_handle(cmd->msg.data.ipc_info.iface, base);
+
+	if (hmem_copy_ret < 0) {
+		ret = hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		ret = -FI_ETRUNC;
+	}
+
+	*total_len = hmem_copy_ret;
+
 out:
 	//Status must be set last (signals peer: op done, valid resp entry)
 	resp->status = ret;
+	smr_signal(peer_smr);
 
 	return -ret;
 }
@@ -532,9 +600,9 @@ static int smr_progress_inline_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
 	if (*len != cmd->msg.hdr.size) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 			"recv truncated");
-		return -FI_EIO;
+		return -FI_ETRUNC;
 	}
-	return 0;
+	return FI_SUCCESS;
 }
 
 static int smr_progress_inject_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
@@ -572,7 +640,7 @@ static int smr_progress_inject_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
 	if (*len != cmd->msg.hdr.size) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 			"recv truncated");
-		err = -FI_EIO;
+		err = -FI_ETRUNC;
 	}
 
 out:
@@ -610,7 +678,8 @@ static int smr_progress_msg_common(struct smr_ep *ep, struct smr_cmd *cmd,
 					      &total_len, ep, 0);
 		break;
 	case smr_src_mmap:
-		entry->err = smr_progress_mmap(cmd, entry->iov, entry->iov_count,
+		entry->err = smr_progress_mmap(cmd, entry->iface, entry->device,
+					       entry->iov, entry->iov_count,
 					       &total_len, ep);
 		break;
 	case smr_src_sar:
@@ -677,8 +746,14 @@ static void smr_progress_connreq(struct smr_ep *ep, struct smr_cmd *cmd)
 
 	peer_smr = smr_peer_region(ep->region, idx);
 
+	if (peer_smr->pid != (int) cmd->msg.hdr.data) {
+		//TODO track and update/complete in error any transfers
+		//to or from old mapping
+		munmap(peer_smr, peer_smr->total_size);
+		smr_map_to_region(&smr_prov, &ep->region->map->peers[idx]);
+		peer_smr = smr_peer_region(ep->region, idx);
+	}
 	smr_peer_data(peer_smr)[cmd->msg.hdr.id].addr.id = idx;
-
 	smr_peer_data(ep->region)[idx].addr.id = cmd->msg.hdr.id;
 
 	smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
@@ -798,6 +873,7 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 			peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.id);
 			resp = smr_get_ptr(peer_smr, cmd->msg.hdr.data);
 			resp->status = -err;
+			smr_signal(peer_smr);
 		} else {
 			ep->region->cmd_cnt++;
 		}
@@ -806,7 +882,8 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 		err = smr_progress_iov(cmd, iov, iov_count, &total_len, ep, ret);
 		break;
 	case smr_src_mmap:
-		err = smr_progress_mmap(cmd, iov, iov_count, &total_len, ep);
+		err = smr_progress_mmap(cmd, iface, device, iov,
+					iov_count, &total_len, ep);
 		break;
 	case smr_src_sar:
 		if (smr_progress_sar(cmd, NULL, iface, device, iov, iov_count,
@@ -889,6 +966,7 @@ static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd)
 		peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.id);
 		resp = smr_get_ptr(peer_smr, cmd->msg.hdr.data);
 		resp->status = -err;
+		smr_signal(peer_smr);
 	} else {
 		ep->region->cmd_cnt++;
 	}
@@ -947,6 +1025,7 @@ static void smr_progress_cmd(struct smr_ep *ep)
 			ret = -FI_EINVAL;
 		}
 		if (ret) {
+			smr_signal(ep->region);
 			if (ret != -FI_EAGAIN) {
 				FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 					"error processing command\n");
@@ -976,12 +1055,12 @@ static void smr_progress_sar_list(struct smr_ep *ep)
 		peer_smr = smr_peer_region(ep->region, sar_entry->cmd.msg.hdr.id);
 		resp = smr_get_ptr(peer_smr, sar_entry->cmd.msg.hdr.src_data);
 		if (sar_entry->cmd.msg.hdr.op == ofi_op_read_req)
-			smr_try_progress_to_sar(sar_msg, resp, &sar_entry->cmd,
+			smr_try_progress_to_sar(peer_smr, sar_msg, resp, &sar_entry->cmd,
 					sar_entry->iface, sar_entry->device,
 					sar_entry->iov, sar_entry->iov_count,
 					&sar_entry->bytes_done, &sar_entry->next);
 		else
-			smr_try_progress_from_sar(sar_msg, resp, &sar_entry->cmd,
+			smr_try_progress_from_sar(peer_smr, sar_msg, resp, &sar_entry->cmd,
 					sar_entry->iface, sar_entry->device,
 					sar_entry->iov, sar_entry->iov_count,
 					&sar_entry->bytes_done, &sar_entry->next);
@@ -1013,10 +1092,11 @@ void smr_ep_progress(struct util_ep *util_ep)
 
 	ep = container_of(util_ep, struct smr_ep, util_ep);
 
-	smr_progress_resp(ep);
-	smr_progress_cmd(ep);
-
-	smr_progress_sar_list(ep);
+	if (ofi_atomic_cas_bool32(&ep->region->signal, 1, 0)) {
+		smr_progress_resp(ep);
+		smr_progress_cmd(ep);
+		smr_progress_sar_list(ep);
+	}
 }
 
 int smr_progress_unexp_queue(struct smr_ep *ep, struct smr_rx_entry *entry,
diff --git a/prov/shm/src/smr_rma.c b/prov/shm/src/smr_rma.c
index 54c569f..949cd1e 100644
--- a/prov/shm/src/smr_rma.c
+++ b/prov/shm/src/smr_rma.c
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2013-2021 Intel Corporation. All rights reserved
+ * (C) Copyright 2021 Amazon.com, Inc. or its affiliates.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -103,6 +104,7 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 	uint16_t comp_flags;
 	ssize_t ret = 0;
 	size_t total_len;
+	bool use_ipc;
 
 	assert(iov_count <= SMR_IOV_LIMIT);
 	assert(rma_count <= SMR_IOV_LIMIT);
@@ -147,12 +149,17 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 
 	total_len = ofi_total_iov_len(iov, iov_count);
 
+	/* Do not inline/inject if IPC is available so device to device
+	 * transfer may occur if possible. */
+	use_ipc = ofi_hmem_is_ipc_enabled(iface) && (iov_count == 1) &&
+		  desc && (smr_get_mr_flags(desc) & FI_HMEM_DEVICE_ONLY);
+
 	smr_generic_format(cmd, peer_id, op, 0, data, op_flags);
 	if (total_len <= SMR_MSG_DATA_LEN && op == ofi_op_write &&
-	    !(op_flags & FI_DELIVERY_COMPLETE)) {
+	    !(op_flags & FI_DELIVERY_COMPLETE) && !use_ipc) {
 		smr_format_inline(cmd, iface, device, iov, iov_count);
 	} else if (total_len <= SMR_INJECT_SIZE &&
-		   !(op_flags & FI_DELIVERY_COMPLETE)) {
+		   !(op_flags & FI_DELIVERY_COMPLETE) && !use_ipc) {
 		tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
 		smr_format_inject(cmd, iface, device, iov, iov_count, peer_smr, tx_buf);
 		if (op == ofi_op_read_req) {
@@ -181,11 +188,14 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 			smr_format_iov(cmd, iov, iov_count, total_len, ep->region,
 				       resp);
 		} else {
-			if (iface == FI_HMEM_ZE && iov_count == 1 &&
+			if (use_ipc && iface == FI_HMEM_ZE &&
 			    smr_ze_ipc_enabled(ep->region, peer_smr)) {
 				ret = smr_format_ze_ipc(ep, id, cmd, iov,
 					device, total_len, ep->region,
 					resp, pend);
+			} else if (use_ipc && iface != FI_HMEM_ZE) {
+				ret = smr_format_ipc(cmd, iov[0].iov_base, total_len,
+						     ep->region, resp, iface);
 			} else if (total_len <= smr_env.sar_threshold ||
 			    iface != FI_HMEM_SYSTEM) {
 				if (!peer_smr->sar_cnt) {
@@ -224,6 +234,7 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 commit_comp:
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
+	smr_signal(peer_smr);
 
 	if (!comp)
 		goto unlock_cq;
@@ -415,6 +426,7 @@ ssize_t smr_generic_rma_inject(struct fid_ep *ep_fid, const void *buf,
 commit:
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
+	smr_signal(peer_smr);
 	ofi_ep_tx_cntr_inc_func(&ep->util_ep, ofi_op_write);
 unlock_region:
 	fastlock_release(&peer_smr->lock);
diff --git a/prov/sockets/include/sock.h b/prov/sockets/include/sock.h
index 9b8186e..57436ab 100644
--- a/prov/sockets/include/sock.h
+++ b/prov/sockets/include/sock.h
@@ -197,8 +197,8 @@ struct sock_conn {
 struct sock_conn_map {
 	struct sock_conn *table;
 	ofi_epoll_t epoll_set;
-	void **epoll_ctxs;
-	int epoll_ctxs_sz;
+	struct ofi_epollfds_event *epoll_events;
+	int epoll_size;
 	int used;
 	int size;
 	fastlock_t lock;
diff --git a/prov/sockets/src/sock_atomic.c b/prov/sockets/src/sock_atomic.c
index 4d93c42..e4cca45 100644
--- a/prov/sockets/src/sock_atomic.c
+++ b/prov/sockets/src/sock_atomic.c
@@ -133,7 +133,7 @@ ssize_t sock_ep_tx_atomic(struct fid_ep *ep,
 
 		total_len = src_len + cmp_len;
 	} else {
-		total_len = msg->iov_count * sizeof(union sock_iov);
+		total_len = (msg->iov_count + compare_count) * sizeof(union sock_iov);
 	}
 
 	total_len += (sizeof(struct sock_op_send) +
diff --git a/prov/sockets/src/sock_av.c b/prov/sockets/src/sock_av.c
index f83312a..e8ed506 100644
--- a/prov/sockets/src/sock_av.c
+++ b/prov/sockets/src/sock_av.c
@@ -194,7 +194,7 @@ static int sock_av_get_next_index(struct sock_av *av)
 	return -1;
 }
 
-static int sock_check_table_in(struct sock_av *_av, const struct sockaddr *addr,
+static int sock_check_table_in(struct sock_av *_av, const void *addr,
 			       fi_addr_t *fi_addr, int count, uint64_t flags,
 			       void *context)
 {
@@ -215,9 +215,10 @@ static int sock_check_table_in(struct sock_av *_av, const struct sockaddr *addr,
 
 	if (_av->attr.flags & FI_READ) {
 		for (i = 0; i < count; i++) {
+			struct sockaddr *sock_addr = (struct sockaddr *) ((char *)addr + i * _av->addrlen);
 			for (j = 0; j < _av->table_hdr->size; j++) {
 				if (_av->table[j].valid &&
-				    !ofi_valid_dest_ipaddr(&addr[i])) {
+				    !ofi_valid_dest_ipaddr(sock_addr)) {
 					sock_av_report_error(_av, fi_addr,
 							context, i, FI_EINVAL,
 							flags);
@@ -225,8 +226,8 @@ static int sock_check_table_in(struct sock_av *_av, const struct sockaddr *addr,
 				}
 
 				av_addr = &_av->table[j];
-				if (memcmp(&av_addr->addr, &addr[i],
-					   ofi_sizeofaddr(&addr[i])) == 0) {
+				if (memcmp(&av_addr->addr, sock_addr,
+					   ofi_sizeofaddr(sock_addr)) == 0) {
 					SOCK_LOG_DBG("Found addr in shared av\n");
 					if (fi_addr)
 						fi_addr[i] = (fi_addr_t)j;
@@ -239,7 +240,8 @@ static int sock_check_table_in(struct sock_av *_av, const struct sockaddr *addr,
 	}
 
 	for (i = 0, ret = 0; i < count; i++) {
-		if (!ofi_valid_dest_ipaddr(&addr[i])) {
+		struct sockaddr *sock_addr = (struct sockaddr *) ((char *)addr + i * _av->addrlen);
+		if (!ofi_valid_dest_ipaddr(sock_addr)) {
 			sock_av_report_error(_av, fi_addr, context, i, FI_EINVAL,
 					     flags);
 			continue;
@@ -260,13 +262,13 @@ static int sock_check_table_in(struct sock_av *_av, const struct sockaddr *addr,
 		}
 
 		av_addr = &_av->table[index];
-		inet_ntop(addr[i].sa_family, ofi_get_ipaddr(&addr[i]),
+		inet_ntop(sock_addr->sa_family, ofi_get_ipaddr(sock_addr),
 			  sa_ip, sizeof sa_ip);
 		SOCK_LOG_DBG("AV-INSERT: dst_addr family: %d, IP %s, port: %d\n",
-			      (&addr[i])->sa_family, sa_ip,
-			      ofi_addr_get_port(&addr[i]));
+			      sock_addr->sa_family, sa_ip,
+			      ofi_addr_get_port(sock_addr));
 
-		memcpy(&av_addr->addr, &addr[i], ofi_sizeofaddr(&addr[i]));
+		memcpy(&av_addr->addr, sock_addr, ofi_sizeofaddr(sock_addr));
 		if (fi_addr)
 			fi_addr[i] = (fi_addr_t)index;
 
@@ -286,8 +288,7 @@ static int sock_av_insert(struct fid_av *av, const void *addr, size_t count,
 	_av = container_of(av, struct sock_av, av_fid);
 
 	fastlock_acquire(&_av->table_lock);
-	ret = sock_check_table_in(_av, (const struct sockaddr *) addr,
-				   fi_addr, count, flags, context);
+	ret = sock_check_table_in(_av, addr, fi_addr, count, flags, context);
 	fastlock_release(&_av->table_lock);
 	return ret;
 }
diff --git a/prov/sockets/src/sock_conn.c b/prov/sockets/src/sock_conn.c
index 0d39956..fd71689 100644
--- a/prov/sockets/src/sock_conn.c
+++ b/prov/sockets/src/sock_conn.c
@@ -97,12 +97,13 @@ int sock_conn_map_init(struct sock_ep *ep, int init_size)
 {
 	struct sock_conn_map *map = &ep->attr->cmap;
 	int ret;
+
 	map->table = calloc(init_size, sizeof(*map->table));
 	if (!map->table)
 		return -FI_ENOMEM;
 
-	map->epoll_ctxs = calloc(init_size, sizeof(*map->epoll_ctxs));
-	if (!map->epoll_ctxs)
+	map->epoll_events = calloc(init_size, sizeof(*map->epoll_events));
+	if (!map->epoll_events)
 		goto err1;
 
 	ret = ofi_epoll_create(&map->epoll_set);
@@ -116,10 +117,11 @@ int sock_conn_map_init(struct sock_ep *ep, int init_size)
 	fastlock_init(&map->lock);
 	map->used = 0;
 	map->size = init_size;
+	map->epoll_size = init_size;
 	return 0;
 
 err2:
-	free(map->epoll_ctxs);
+	free(map->epoll_events);
 err1:
 	free(map->table);
 	return -FI_ENOMEM;
@@ -153,9 +155,9 @@ void sock_conn_map_destroy(struct sock_ep_attr *ep_attr)
 	}
 	free(cmap->table);
 	cmap->table = NULL;
-	free(cmap->epoll_ctxs);
-	cmap->epoll_ctxs = NULL;
-	cmap->epoll_ctxs_sz = 0;
+	free(cmap->epoll_events);
+	cmap->epoll_events = NULL;
+	cmap->epoll_size = 0;
 	cmap->used = cmap->size = 0;
 	ofi_epoll_close(cmap->epoll_set);
 	fastlock_destroy(&cmap->lock);
@@ -317,14 +319,14 @@ static void *sock_conn_listener_thread(void *arg)
 {
 	struct sock_conn_listener *conn_listener = arg;
 	struct sock_conn_handle *conn_handle;
-	void *ep_contexts[SOCK_EPOLL_WAIT_EVENTS];
+	struct ofi_epollfds_event events[SOCK_EPOLL_WAIT_EVENTS];
 	struct sock_ep_attr *ep_attr;
 	int num_fds, i, conn_fd;
 	union ofi_sock_ip remote;
 	socklen_t addr_size;
 
 	while (conn_listener->do_listen) {
-		num_fds = ofi_epoll_wait(conn_listener->epollfd, ep_contexts,
+		num_fds = ofi_epoll_wait(conn_listener->epollfd, events,
 		                        SOCK_EPOLL_WAIT_EVENTS, -1);
 		if (num_fds < 0) {
 			SOCK_LOG_ERROR("poll failed : %s\n", strerror(errno));
@@ -342,7 +344,7 @@ static void *sock_conn_listener_thread(void *arg)
 		}
 
 		for (i = 0; i < num_fds; i++) {
-			conn_handle = ep_contexts[i];
+			conn_handle = events[i].data.ptr;
 
 			if (conn_handle == NULL) { /* signal event */
 				fd_signal_reset(&conn_listener->signal);
diff --git a/prov/sockets/src/sock_ep_msg.c b/prov/sockets/src/sock_ep_msg.c
index 4261e8a..bd158cb 100644
--- a/prov/sockets/src/sock_ep_msg.c
+++ b/prov/sockets/src/sock_ep_msg.c
@@ -1171,13 +1171,13 @@ static void *sock_ep_cm_thread(void *arg)
 {
 	int num_fds, i;
 	struct sock_ep_cm_head *cm_head = arg;
-	void *ep_contexts[SOCK_EPOLL_WAIT_EVENTS];
+	struct ofi_epollfds_event events[SOCK_EPOLL_WAIT_EVENTS];
 	struct sock_conn_req_handle *handle;
 
 	while (cm_head->do_listen) {
 		sock_ep_cm_check_closing_rejected_list(cm_head);
 
-		num_fds = ofi_epoll_wait(cm_head->epollfd, ep_contexts,
+		num_fds = ofi_epoll_wait(cm_head->epollfd, events,
 		                        SOCK_EPOLL_WAIT_EVENTS, -1);
 		if (num_fds < 0) {
 			SOCK_LOG_ERROR("poll failed : %s\n", strerror(errno));
@@ -1195,7 +1195,7 @@ static void *sock_ep_cm_thread(void *arg)
 			goto skip;
 		}
 		for (i = 0; i < num_fds; i++) {
-			handle = ep_contexts[i];
+			handle = events[i].data.ptr;
 
 			if (handle == NULL) { /* Signal event */
 				fd_signal_reset(&cm_head->signal);
diff --git a/prov/sockets/src/sock_progress.c b/prov/sockets/src/sock_progress.c
index a9602bf..0d94897 100644
--- a/prov/sockets/src/sock_progress.c
+++ b/prov/sockets/src/sock_progress.c
@@ -2376,20 +2376,20 @@ static int sock_pe_progress_rx_ep(struct sock_pe *pe,
 	if (!map->used)
 		return 0;
 
-	if (map->epoll_ctxs_sz < map->used) {
+	if (map->epoll_size < map->used) {
 		uint64_t new_size = map->used * 2;
-		void *ctxs;
+		struct ofi_epollfds_event *events;
 
-		ctxs = realloc(map->epoll_ctxs,
-			       sizeof(*map->epoll_ctxs) * new_size);
-		if (ctxs) {
-			map->epoll_ctxs = ctxs;
-			map->epoll_ctxs_sz = new_size;
+		events = realloc(map->epoll_events,
+				 sizeof(*map->epoll_events) * new_size);
+		if (events) {
+			map->epoll_events = events;
+			map->epoll_size = new_size;
 		}
 	}
 
-	num_fds = ofi_epoll_wait(map->epoll_set, map->epoll_ctxs,
-	                        MIN(map->used, map->epoll_ctxs_sz), 0);
+	num_fds = ofi_epoll_wait(map->epoll_set, map->epoll_events,
+	                        MIN(map->used, map->epoll_size), 0);
 	if (num_fds < 0 || num_fds == 0) {
 		if (num_fds < 0)
 			SOCK_LOG_ERROR("epoll failed: %d\n", num_fds);
@@ -2398,7 +2398,7 @@ static int sock_pe_progress_rx_ep(struct sock_pe *pe,
 
 	fastlock_acquire(&map->lock);
 	for (i = 0; i < num_fds; i++) {
-		conn = map->epoll_ctxs[i];
+		conn = map->epoll_events[i].data.ptr;
 		if (!conn)
 			SOCK_LOG_ERROR("ofi_idm_lookup failed\n");
 
@@ -2597,9 +2597,9 @@ static void sock_pe_wait(struct sock_pe *pe)
 {
 	char tmp;
 	int ret;
-	void *ep_contexts[1];
+	struct ofi_epollfds_event event;
 
-	ret = ofi_epoll_wait(pe->epoll_set, ep_contexts, 1, -1);
+	ret = ofi_epoll_wait(pe->epoll_set, &event, 1, -1);
 	if (ret < 0)
 		SOCK_LOG_ERROR("poll failed : %s\n", strerror(ofi_sockerr()));
 
diff --git a/prov/tcp/Makefile.include b/prov/tcp/Makefile.include
index 94d9241..4fb43ca 100644
--- a/prov/tcp/Makefile.include
+++ b/prov/tcp/Makefile.include
@@ -12,7 +12,6 @@ _tcp_files = \
 	prov/tcp/src/tcpx_eq.c		\
 	prov/tcp/src/tcpx_init.c	\
 	prov/tcp/src/tcpx_progress.c	\
-	prov/tcp/src/tcpx_comm.c	\
 	prov/tcp/src/tcpx.h
 
 if HAVE_TCP_DL
diff --git a/prov/tcp/src/tcpx.h b/prov/tcp/src/tcpx.h
index d6aa0e3..a45e15d 100644
--- a/prov/tcp/src/tcpx.h
+++ b/prov/tcp/src/tcpx.h
@@ -58,43 +58,104 @@
 #include <ofi_signal.h>
 #include <ofi_util.h>
 #include <ofi_proto.h>
+#include <ofi_net.h>
 
 #ifndef _TCP_H_
 #define _TCP_H_
 
-#define TCPX_HDR_VERSION	3
-#define TCPX_CTRL_HDR_VERSION	3
-
-#define TCPX_MAX_CM_DATA_SIZE	(1 << 8)
-#define TCPX_IOV_LIMIT		(4)
-#define TCPX_MAX_INJECT_SZ	(64)
 
+#define TCPX_MAX_INJECT		128
 #define MAX_POLL_EVENTS		100
-
 #define TCPX_MIN_MULTI_RECV	16384
-
 #define TCPX_PORT_MAX_RANGE	(USHRT_MAX)
 
 extern struct fi_provider	tcpx_prov;
 extern struct util_prov		tcpx_util_prov;
 extern struct fi_info		tcpx_info;
 extern struct tcpx_port_range	port_range;
-extern int			tcpx_nodelay;
+extern int tcpx_nodelay;
+extern int tcpx_staging_sbuf_size;
+extern int tcpx_prefetch_rbuf_size;
+extern size_t tcpx_default_tx_size;
+extern size_t tcpx_default_rx_size;
+extern size_t tcpx_zerocopy_size;
+
 struct tcpx_xfer_entry;
 struct tcpx_ep;
 
-enum tcpx_xfer_op_codes {
-	TCPX_OP_MSG_SEND,
-	TCPX_OP_MSG_RECV,
-	TCPX_OP_MSG_RESP,
-	TCPX_OP_WRITE,
-	TCPX_OP_REMOTE_WRITE,
-	TCPX_OP_READ_REQ,
-	TCPX_OP_READ_RSP,
-	TCPX_OP_REMOTE_READ,
-	TCPX_OP_CODE_MAX,
+
+/*
+ * Wire protocol structures and definitions
+ */
+
+#define TCPX_CTRL_HDR_VERSION	3
+
+enum {
+	TCPX_MAX_CM_DATA_SIZE = (1 << 8)
+};
+
+struct tcpx_cm_msg {
+	struct ofi_ctrl_hdr hdr;
+	char data[TCPX_MAX_CM_DATA_SIZE];
+};
+
+#define TCPX_HDR_VERSION	3
+
+enum {
+	TCPX_IOV_LIMIT = 4
+};
+
+/* base_hdr::op_data */
+enum {
+	/* backward compatible value */
+	TCPX_OP_ACK = 2, /* indicates ack message - should be a flag */
+};
+
+/* Flags */
+#define TCPX_REMOTE_CQ_DATA	(1 << 0)
+/* not used TCPX_TRANSMIT_COMPLETE	(1 << 1) */
+#define TCPX_DELIVERY_COMPLETE	(1 << 2)
+#define TCPX_COMMIT_COMPLETE	(1 << 3)
+#define TCPX_TAGGED		(1 << 7)
+
+struct tcpx_base_hdr {
+	uint8_t			version;
+	uint8_t			op;
+	uint16_t		flags;
+	uint8_t			op_data;
+	uint8_t			rma_iov_cnt;
+	uint8_t			hdr_size;
+	union {
+		uint8_t		rsvd;
+		uint8_t		id; /* debug */
+	};
+	uint64_t		size;
+};
+
+struct tcpx_tag_hdr {
+	struct tcpx_base_hdr	base_hdr;
+	uint64_t		tag;
+};
+
+struct tcpx_cq_data_hdr {
+	struct tcpx_base_hdr 	base_hdr;
+	uint64_t		cq_data;
+};
+
+struct tcpx_tag_data_hdr {
+	struct tcpx_cq_data_hdr	cq_data_hdr;
+	uint64_t		tag;
 };
 
+/* Maximum header is scatter RMA with CQ data */
+#define TCPX_MAX_HDR (sizeof(struct tcpx_cq_data_hdr) + \
+		     sizeof(struct ofi_rma_iov) * TCPX_IOV_LIMIT)
+
+/*
+ * End wire protocol definitions
+ */
+
+
 enum tcpx_cm_state {
 	TCPX_CM_LISTENING,
 	TCPX_CM_CONNECTING,
@@ -105,11 +166,6 @@ enum tcpx_cm_state {
 	/* CM context is freed once connected */
 };
 
-struct tcpx_cm_msg {
-	struct ofi_ctrl_hdr hdr;
-	char data[TCPX_MAX_CM_DATA_SIZE];
-};
-
 struct tcpx_cm_context {
 	fid_t			fid;
 	enum tcpx_cm_state	state;
@@ -117,6 +173,9 @@ struct tcpx_cm_context {
 	struct tcpx_cm_msg	msg;
 };
 
+struct tcpx_cm_context *tcpx_alloc_cm_ctx(fid_t fid, enum tcpx_cm_state state);
+void tcpx_free_cm_ctx(struct tcpx_cm_context *cm_ctx);
+
 struct tcpx_port_range {
 	int high;
 	int low;
@@ -145,35 +204,21 @@ enum tcpx_state {
 	TCPX_DISCONNECTED,
 };
 
-struct tcpx_base_hdr {
-	uint8_t			version;
-	uint8_t			op;
-	uint16_t		flags;
-	uint8_t			op_data;
-	uint8_t			rma_iov_cnt;
-	uint8_t			payload_off;
-	uint8_t			rsvd;
-	uint64_t		size;
-};
-
-struct tcpx_cq_data_hdr {
-	struct tcpx_base_hdr 	base_hdr;
-	uint64_t		cq_data;
-};
-
-#define TCPX_MAX_HDR_SZ (sizeof(struct tcpx_base_hdr) + 	\
-			 sizeof(uint64_t) +			\
-			 sizeof(struct ofi_rma_iov) *		\
-			 TCPX_IOV_LIMIT +			\
-			 TCPX_MAX_INJECT_SZ)
-
-struct tcpx_cur_rx_msg {
+struct tcpx_cur_rx {
 	union {
 		struct tcpx_base_hdr	base_hdr;
-		uint8_t		       	max_hdr[TCPX_MAX_HDR_SZ];
+		uint8_t			max_hdr[TCPX_MAX_HDR];
 	} hdr;
 	size_t			hdr_len;
-	size_t			done_len;
+	size_t			hdr_done;
+	size_t			data_left;
+	struct tcpx_xfer_entry	*entry;
+	int			(*handler)(struct tcpx_ep *ep);
+};
+
+struct tcpx_cur_tx {
+	size_t			data_left;
+	struct tcpx_xfer_entry	*entry;
 };
 
 struct tcpx_rx_ctx {
@@ -184,36 +229,30 @@ struct tcpx_rx_ctx {
 	fastlock_t		lock;
 };
 
-typedef int (*tcpx_rx_process_fn_t)(struct tcpx_xfer_entry *rx_entry);
-
-enum {
-	STAGE_BUF_SIZE = 512
-};
-
-struct stage_buf {
-	uint8_t			buf[STAGE_BUF_SIZE];
-	size_t			bytes_avail;
-	size_t			cur_pos;
-};
-
 struct tcpx_ep {
 	struct util_ep		util_ep;
-	SOCKET			sock;
-	struct tcpx_cur_rx_msg	cur_rx_msg;
-	struct tcpx_xfer_entry	*cur_rx_entry;
-	tcpx_rx_process_fn_t 	cur_rx_proc_fn;
+	struct ofi_bsock	bsock;
+	struct tcpx_cur_rx	cur_rx;
+	struct tcpx_cur_tx	cur_tx;
+	OFI_DBG_VAR(uint8_t, tx_id)
+	OFI_DBG_VAR(uint8_t, rx_id)
+
 	struct dlist_entry	ep_entry;
 	struct slist		rx_queue;
 	struct slist		tx_queue;
-	struct slist		tx_rsp_pend_queue;
+	struct slist		priority_queue;
+	struct slist		need_ack_queue;
+	struct slist		async_queue;
 	struct slist		rma_read_queue;
+	int			rx_avail;
 	struct tcpx_rx_ctx	*srx_ctx;
 	enum tcpx_state		state;
+	struct tcpx_cm_context	*cm_ctx;
+
 	/* lock for protecting tx/rx queues, rma list, state*/
 	fastlock_t		lock;
 	int (*start_op[ofi_op_write + 1])(struct tcpx_ep *ep);
 	void (*hdr_bswap)(struct tcpx_base_hdr *hdr);
-	struct stage_buf	stage_buf;
 	size_t			min_multi_recv_size;
 	bool			pollout_set;
 };
@@ -222,21 +261,27 @@ struct tcpx_fabric {
 	struct util_fabric	util_fabric;
 };
 
+#define TCPX_INTERNAL_MASK	GENMASK_ULL(63, 59)
+#define TCPX_NEED_ACK		BIT_ULL(59)
+#define TCPX_INTERNAL_XFER	BIT_ULL(60)
 #define TCPX_NEED_DYN_RBUF 	BIT_ULL(61)
+#define TCPX_ASYNC		BIT_ULL(62)
 
 struct tcpx_xfer_entry {
 	struct slist_entry	entry;
 	union {
 		struct tcpx_base_hdr	base_hdr;
 		struct tcpx_cq_data_hdr cq_data_hdr;
-		uint8_t		       	max_hdr[TCPX_MAX_HDR_SZ];
+		struct tcpx_tag_data_hdr tag_data_hdr;
+		struct tcpx_tag_hdr	tag_hdr;
+		uint8_t		       	max_hdr[TCPX_MAX_HDR + TCPX_MAX_INJECT];
 	} hdr;
 	size_t			iov_cnt;
 	struct iovec		iov[TCPX_IOV_LIMIT+1];
 	struct tcpx_ep		*ep;
 	uint64_t		flags;
+	uint32_t		async_index;
 	void			*context;
-	uint64_t		rem_len;
 	void			*mrecv_msg_start;
 };
 
@@ -254,15 +299,9 @@ static inline struct ofi_ops_dynamic_rbuf *tcpx_dynamic_rbuf(struct tcpx_ep *ep)
 	return domain->dynamic_rbuf;
 }
 
-struct tcpx_buf_pool {
-	struct ofi_bufpool	*pool;
-	enum tcpx_xfer_op_codes	op_type;
-};
-
 struct tcpx_cq {
 	struct util_cq		util_cq;
-	/* buf_pools protected by util.cq_lock */
-	struct tcpx_buf_pool	buf_pools[TCPX_OP_CODE_MAX];
+	struct ofi_bufpool	*xfer_pool;
 };
 
 struct tcpx_eq {
@@ -299,27 +338,16 @@ void tcpx_cq_report_success(struct util_cq *cq,
 void tcpx_cq_report_error(struct util_cq *cq,
 			  struct tcpx_xfer_entry *xfer_entry,
 			  int err);
+void tcpx_get_cq_info(struct tcpx_xfer_entry *entry, uint64_t *flags,
+		      uint64_t *data, uint64_t *tag);
 
-
-ssize_t tcpx_recv_hdr(SOCKET sock, struct stage_buf *stage_buf,
-		      struct tcpx_cur_rx_msg *cur_rx_msg);
-int tcpx_recv_msg_data(struct tcpx_xfer_entry *recv_entry);
-int tcpx_send_msg(struct tcpx_xfer_entry *tx_entry);
-int tcpx_read_to_buffer(SOCKET sock, struct stage_buf *stage_buf);
-
-struct tcpx_xfer_entry *tcpx_xfer_entry_alloc(struct tcpx_cq *cq,
-					      enum tcpx_xfer_op_codes type);
-struct tcpx_xfer_entry *tcpx_srx_entry_alloc(struct tcpx_rx_ctx *srx_ctx,
-					     struct tcpx_ep *ep);
-void tcpx_xfer_entry_free(struct tcpx_cq *tcpx_cq,
-			  struct tcpx_xfer_entry *xfer_entry);
-void tcpx_srx_entry_free(struct tcpx_rx_ctx *srx_ctx,
-			 struct tcpx_xfer_entry *xfer_entry);
-void tcpx_rx_entry_free(struct tcpx_xfer_entry *rx_entry);
+void tcpx_reset_rx(struct tcpx_ep *ep);
 
 void tcpx_progress_tx(struct tcpx_ep *ep);
 void tcpx_progress_rx(struct tcpx_ep *ep);
+void tcpx_progress_async(struct tcpx_ep *ep);
 int tcpx_try_func(void *util_ep);
+int tcpx_update_epoll(struct tcpx_ep *ep);
 
 void tcpx_hdr_none(struct tcpx_base_hdr *hdr);
 void tcpx_hdr_bswap(struct tcpx_base_hdr *hdr);
@@ -338,4 +366,106 @@ int tcpx_op_read_req(struct tcpx_ep *tcpx_ep);
 int tcpx_op_write(struct tcpx_ep *tcpx_ep);
 int tcpx_op_read_rsp(struct tcpx_ep *tcpx_ep);
 
+
+static inline void
+tcpx_set_ack_flags(struct tcpx_xfer_entry *xfer, uint64_t flags)
+{
+	if (flags & (FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)) {
+		xfer->hdr.base_hdr.flags |= TCPX_DELIVERY_COMPLETE;
+		xfer->flags |= TCPX_NEED_ACK;
+	}
+}
+
+static inline void
+tcpx_set_commit_flags(struct tcpx_xfer_entry *xfer, uint64_t flags)
+{
+	tcpx_set_ack_flags(xfer, flags);
+	if (flags & FI_COMMIT_COMPLETE) {
+		xfer->hdr.base_hdr.flags |= TCPX_COMMIT_COMPLETE;
+		xfer->flags |= TCPX_NEED_ACK;
+	}
+}
+
+static inline struct tcpx_xfer_entry *
+tcpx_alloc_xfer(struct tcpx_cq *cq)
+{
+	struct tcpx_xfer_entry *xfer;
+
+	cq->util_cq.cq_fastlock_acquire(&cq->util_cq.cq_lock);
+	xfer = ofi_buf_alloc(cq->xfer_pool);
+	cq->util_cq.cq_fastlock_release(&cq->util_cq.cq_lock);
+
+	return xfer;
+}
+
+static inline void
+tcpx_free_xfer(struct tcpx_cq *cq, struct tcpx_xfer_entry *xfer)
+{
+	xfer->hdr.base_hdr.flags = 0;
+	xfer->flags = 0;
+	xfer->context = 0;
+
+	cq->util_cq.cq_fastlock_acquire(&cq->util_cq.cq_lock);
+	ofi_buf_free(xfer);
+	cq->util_cq.cq_fastlock_release(&cq->util_cq.cq_lock);
+}
+
+static inline struct tcpx_xfer_entry *
+tcpx_alloc_rx(struct tcpx_ep *ep)
+{
+	struct tcpx_xfer_entry *xfer;
+	struct tcpx_cq *cq;
+
+	cq = container_of(ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
+	xfer = tcpx_alloc_xfer(cq);
+	if (xfer)
+		xfer->ep = ep;
+
+	return xfer;
+}
+
+static inline void
+tcpx_free_rx(struct tcpx_xfer_entry *xfer)
+{
+	struct tcpx_cq *cq;
+	struct tcpx_rx_ctx *srx;
+
+	if (xfer->ep->srx_ctx) {
+		srx = xfer->ep->srx_ctx;
+		fastlock_acquire(&srx->lock);
+		ofi_buf_free(xfer);
+		fastlock_release(&srx->lock);
+	} else {
+		cq = container_of(xfer->ep->util_ep.rx_cq,
+				  struct tcpx_cq, util_cq);
+		tcpx_free_xfer(cq, xfer);
+	}
+}
+
+static inline struct tcpx_xfer_entry *
+tcpx_alloc_tx(struct tcpx_ep *ep)
+{
+	struct tcpx_xfer_entry *xfer;
+	struct tcpx_cq *cq;
+
+	cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
+
+	xfer = tcpx_alloc_xfer(cq);
+	if (xfer) {
+		xfer->hdr.base_hdr.version = TCPX_HDR_VERSION;
+		xfer->hdr.base_hdr.op_data = 0;
+		xfer->ep = ep;
+	}
+
+	return xfer;
+}
+
+static inline void
+tcpx_free_tx(struct tcpx_xfer_entry *xfer)
+{
+	struct tcpx_cq *cq;
+	cq = container_of(xfer->ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
+	tcpx_free_xfer(cq, xfer);
+}
+
 #endif //_TCP_H_
diff --git a/prov/tcp/src/tcpx_attr.c b/prov/tcp/src/tcpx_attr.c
index f0f59d6..d2805b0 100644
--- a/prov/tcp/src/tcpx_attr.c
+++ b/prov/tcp/src/tcpx_attr.c
@@ -55,7 +55,7 @@ static struct fi_tx_attr tcpx_tx_attr = {
 	.op_flags = TCPX_TX_OP_FLAGS,
 	.comp_order = FI_ORDER_STRICT,
 	.msg_order = TCPX_MSG_ORDER,
-	.inject_size = 64,
+	.inject_size = TCPX_MAX_INJECT,
 	.size = 1024,
 	.iov_limit = TCPX_IOV_LIMIT,
 	.rma_iov_limit = TCPX_IOV_LIMIT,
@@ -67,7 +67,7 @@ static struct fi_rx_attr tcpx_rx_attr = {
 	.comp_order = FI_ORDER_STRICT,
 	.msg_order = TCPX_MSG_ORDER,
 	.total_buffered_recv = 0,
-	.size = 1024,
+	.size = 65536,
 	.iov_limit = TCPX_IOV_LIMIT
 };
 
@@ -97,9 +97,10 @@ static struct fi_domain_attr tcpx_domain_attr = {
 	.ep_cnt = 8192,
 	.tx_ctx_cnt = 8192,
 	.rx_ctx_cnt = 8192,
-	.max_ep_srx_ctx = 128,
+	.max_ep_srx_ctx = 8192,
 	.max_ep_tx_ctx = 1,
-	.max_ep_rx_ctx = 1
+	.max_ep_rx_ctx = 1,
+	.mr_iov_limit = 1,
 };
 
 static struct fi_fabric_attr tcpx_fabric_attr = {
@@ -117,8 +118,26 @@ struct fi_info tcpx_info = {
 	.fabric_attr = &tcpx_fabric_attr
 };
 
+
+/* User hints will still override the modified dest_info attributes
+ * through ofi_alter_info
+ */
+static int
+tcpx_alter_defaults(uint32_t version, const struct fi_info *hints,
+		    const struct fi_info *base_info,
+		    struct fi_info *dest_info)
+{
+	dest_info->tx_attr->size = tcpx_default_tx_size;
+	if (hints && hints->ep_attr &&
+	    hints->ep_attr->rx_ctx_cnt != FI_SHARED_CONTEXT)
+		dest_info->rx_attr->size = tcpx_default_rx_size;
+	return 0;
+}
+
+
 struct util_prov tcpx_util_prov = {
 	.prov = &tcpx_prov,
 	.info = &tcpx_info,
+	.alter_defaults = &tcpx_alter_defaults,
 	.flags = 0,
 };
diff --git a/prov/tcp/src/tcpx_comm.c b/prov/tcp/src/tcpx_comm.c
deleted file mode 100644
index d790b01..0000000
--- a/prov/tcp/src/tcpx_comm.c
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Copyright (c) 2017 Intel Corporation. All rights reserved.
- *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * BSD license below:
- *
- *	   Redistribution and use in source and binary forms, with or
- *	   without modification, are permitted provided that the following
- *	   conditions are met:
- *
- *		- Redistributions of source code must retain the above
- *		  copyright notice, this list of conditions and the following
- *		  disclaimer.
- *
- *		- Redistributions in binary form must reproduce the above
- *		  copyright notice, this list of conditions and the following
- *		  disclaimer in the documentation and/or other materials
- *		  provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
- */
-
-#include <rdma/fi_errno.h>
-#include <ofi_prov.h>
-#include <sys/types.h>
-#include <ofi_util.h>
-#include <ofi_iov.h>
-#include "tcpx.h"
-
-int tcpx_send_msg(struct tcpx_xfer_entry *tx_entry)
-{
-	ssize_t bytes_sent;
-	struct msghdr msg = {0};
-
-	msg.msg_iov = tx_entry->iov;
-	msg.msg_iovlen = tx_entry->iov_cnt;
-
-	bytes_sent = ofi_sendmsg_tcp(tx_entry->ep->sock, &msg, MSG_NOSIGNAL);
-	if (bytes_sent < 0)
-		return ofi_sockerr() == EPIPE ? -FI_ENOTCONN : -ofi_sockerr();
-
-	tx_entry->rem_len -= bytes_sent;
-	if (tx_entry->rem_len) {
-		ofi_consume_iov(tx_entry->iov, &tx_entry->iov_cnt, bytes_sent);
-		return -FI_EAGAIN;
-	}
-	return FI_SUCCESS;
-}
-
-static ssize_t tcpx_read_from_buffer(struct stage_buf *stage_buf,
-				     uint8_t *buf, size_t len)
-{
-	size_t rem_size;
-	ssize_t ret;
-
-	assert(stage_buf->cur_pos < stage_buf->bytes_avail);
-	rem_size = stage_buf->bytes_avail - stage_buf->cur_pos;
-	ret = (rem_size >= len) ? len : rem_size;
-	memcpy(buf, &stage_buf->buf[stage_buf->cur_pos], ret);
-	stage_buf->cur_pos += ret;
-	return ret;
-}
-
-ssize_t tcpx_recv_hdr(SOCKET sock, struct stage_buf *stage_buf,
-		      struct tcpx_cur_rx_msg *cur_rx_msg)
-{
-	ssize_t bytes_recvd, bytes_read;
-	size_t rem_len;
-	void *rem_buf;
-
-	rem_buf = (uint8_t *) &cur_rx_msg->hdr + cur_rx_msg->done_len;
-	rem_len = cur_rx_msg->hdr_len - cur_rx_msg->done_len;
-
-	if (stage_buf->cur_pos < stage_buf->bytes_avail) {
-		bytes_read = tcpx_read_from_buffer(stage_buf, rem_buf, rem_len);
-		rem_len -= bytes_read;
-		if (!rem_len)
-			return bytes_read;
-
-		rem_buf = (char *) rem_buf + bytes_read;
-	} else {
-		bytes_read = 0;
-	}
-
-	bytes_recvd = ofi_recv_socket(sock, rem_buf, rem_len, 0);
-	if (bytes_recvd < 0)
-		return bytes_read ? bytes_read : -ofi_sockerr();
-	else if (bytes_recvd == 0)
-		return -FI_ENOTCONN;
-
-	return bytes_read + bytes_recvd;
-}
-
-static ssize_t tcpx_readv_from_buffer(struct stage_buf *stage_buf,
-				      struct iovec *iov,
-				      int iov_cnt)
-{
-	ssize_t ret = 0;
-	size_t bytes_read;
-	int i;
-
-	if (iov_cnt == 1)
-		return tcpx_read_from_buffer(stage_buf, iov[0].iov_base,
-					     iov[0].iov_len);
-
-	for (i = 0; i < iov_cnt; i++) {
-		bytes_read = tcpx_read_from_buffer(stage_buf, iov[i].iov_base,
-						   iov[i].iov_len);
-		ret += bytes_read;
-		if ((bytes_read < iov[i].iov_len) ||
-		    !(stage_buf->bytes_avail - stage_buf->cur_pos))
-			break;
-	}
-	return ret;
-}
-
-int tcpx_recv_msg_data(struct tcpx_xfer_entry *rx_entry)
-{
-	struct stage_buf *stage_buf;
-	ssize_t bytes_recvd, bytes_read;
-
-	if (!rx_entry->iov_cnt || !rx_entry->iov[0].iov_len)
-		return FI_SUCCESS;
-
-	stage_buf = &rx_entry->ep->stage_buf;
-	if (stage_buf->cur_pos < stage_buf->bytes_avail) {
-		bytes_read = tcpx_readv_from_buffer(stage_buf,
-						    rx_entry->iov,
-						    rx_entry->iov_cnt);
-		ofi_consume_iov(rx_entry->iov, &rx_entry->iov_cnt, bytes_read);
-		if (!rx_entry->iov_cnt || !rx_entry->iov[0].iov_len)
-			return FI_SUCCESS;
-	} else {
-		bytes_read = 0;
-	}
-
-	bytes_recvd = ofi_readv_socket(rx_entry->ep->sock, rx_entry->iov,
-				       rx_entry->iov_cnt);
-	if (bytes_recvd < 0)
-		return bytes_read ? -FI_EAGAIN : -ofi_sockerr();
-	else if (bytes_recvd == 0)
-		return -FI_ENOTCONN;
-
-	ofi_consume_iov(rx_entry->iov, &rx_entry->iov_cnt, bytes_recvd);
-	return (!rx_entry->iov_cnt || !rx_entry->iov[0].iov_len) ?
-		FI_SUCCESS : -FI_EAGAIN;
-}
-
-int tcpx_read_to_buffer(SOCKET sock, struct stage_buf *stage_buf)
-{
-	int bytes_recvd;
-
-	bytes_recvd = ofi_recv_socket(sock, stage_buf->buf,
-				      sizeof(stage_buf->buf), 0);
-	if (bytes_recvd <= 0)
-		return (bytes_recvd) ? -ofi_sockerr(): -FI_ENOTCONN;
-
-	stage_buf->bytes_avail = bytes_recvd;
-	stage_buf->cur_pos = 0;
-	return FI_SUCCESS;
-}
diff --git a/prov/tcp/src/tcpx_conn_mgr.c b/prov/tcp/src/tcpx_conn_mgr.c
index 8ed3299..a0ebb5c 100644
--- a/prov/tcp/src/tcpx_conn_mgr.c
+++ b/prov/tcp/src/tcpx_conn_mgr.c
@@ -39,6 +39,38 @@
 #include <ofi_util.h>
 
 
+struct tcpx_cm_context *tcpx_alloc_cm_ctx(fid_t fid, enum tcpx_cm_state state)
+{
+	struct tcpx_cm_context *cm_ctx;
+	struct tcpx_ep *ep;
+
+	cm_ctx = calloc(1, sizeof(*cm_ctx));
+	if (!cm_ctx)
+		return cm_ctx;
+
+	cm_ctx->fid = fid;
+	if (fid && fid->fclass == FI_CLASS_EP) {
+		ep = container_of(cm_ctx->fid, struct tcpx_ep,
+				  util_ep.ep_fid.fid);
+		ep->cm_ctx = cm_ctx;
+	}
+	cm_ctx->state = state;
+	return cm_ctx;
+}
+
+void tcpx_free_cm_ctx(struct tcpx_cm_context *cm_ctx)
+{
+	struct tcpx_ep *ep;
+
+	if (cm_ctx->fid && cm_ctx->fid->fclass == FI_CLASS_EP) {
+		ep = container_of(cm_ctx->fid, struct tcpx_ep,
+				  util_ep.ep_fid.fid);
+		ep->cm_ctx = NULL;
+	}
+
+	free(cm_ctx);
+}
+
 /* The underlying socket has the POLLIN event set.  The entire
  * CM message should be readable, as it fits within a single MTU
  * and is the first data transferred over the socket.
@@ -130,7 +162,10 @@ static int tx_cm_data(SOCKET fd, uint8_t type, struct tcpx_cm_context *cm_ctx)
 	return FI_SUCCESS;
 }
 
-static int tcpx_ep_enable(struct tcpx_ep *ep)
+static int tcpx_ep_enable(struct tcpx_ep *ep,
+			  struct fi_eq_cm_entry *cm_entry,
+			  size_t cm_entry_sz)
+
 {
 	int ret = 0;
 
@@ -153,7 +188,7 @@ static int tcpx_ep_enable(struct tcpx_ep *ep)
 
 	if (ep->util_ep.rx_cq) {
 		ret = ofi_wait_add_fd(ep->util_ep.rx_cq->wait,
-				      ep->sock, POLLIN, tcpx_try_func,
+				      ep->bsock.sock, POLLIN, tcpx_try_func,
 				      (void *) &ep->util_ep,
 				      &ep->util_ep.ep_fid.fid);
 		if (ret) {
@@ -165,7 +200,7 @@ static int tcpx_ep_enable(struct tcpx_ep *ep)
 
 	if (ep->util_ep.tx_cq) {
 		ret = ofi_wait_add_fd(ep->util_ep.tx_cq->wait,
-				      ep->sock, POLLIN, tcpx_try_func,
+				      ep->bsock.sock, POLLIN, tcpx_try_func,
 				      (void *) &ep->util_ep,
 				      &ep->util_ep.ep_fid.fid);
 		if (ret) {
@@ -175,9 +210,14 @@ static int tcpx_ep_enable(struct tcpx_ep *ep)
 		}
 	}
 
-	/* TODO: Move writing CONNECTED event here */
+	ret = (int) fi_eq_write(&ep->util_ep.eq->eq_fid, FI_CONNECTED, cm_entry,
+				cm_entry_sz, 0);
+	if (ret < 0) {
+		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "Error writing to EQ\n");
+		return ret;
+	}
 
-	return ret;
+	return 0;
 unlock:
 	fastlock_release(&ep->lock);
 	return ret;
@@ -194,7 +234,7 @@ static void tcpx_cm_recv_resp(struct util_wait *wait,
 	assert(cm_ctx->fid->fclass == FI_CLASS_EP);
 	ep = container_of(cm_ctx->fid, struct tcpx_ep, util_ep.ep_fid.fid);
 
-	ret = rx_cm_data(ep->sock, ofi_ctrl_connresp, cm_ctx);
+	ret = rx_cm_data(ep->bsock.sock, ofi_ctrl_connresp, cm_ctx);
 	if (ret) {
 		if (ret == -FI_EAGAIN)
 			return;
@@ -203,11 +243,11 @@ static void tcpx_cm_recv_resp(struct util_wait *wait,
 				FI_LOG_INFO : FI_LOG_WARN;
 		FI_LOG(&tcpx_prov, level, FI_LOG_EP_CTRL,
 			"Failed to receive connect response\n");
-		ofi_wait_del_fd(wait, ep->sock);
+		ofi_wait_del_fd(wait, ep->bsock.sock);
 		goto err1;
 	}
 
-	ret = ofi_wait_del_fd(wait, ep->sock);
+	ret = ofi_wait_del_fd(wait, ep->bsock.sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"Could not remove fd from wait\n");
@@ -224,24 +264,22 @@ static void tcpx_cm_recv_resp(struct util_wait *wait,
 	ep->hdr_bswap = (cm_ctx->msg.hdr.conn_data == 1) ?
 			tcpx_hdr_none : tcpx_hdr_bswap;
 
-	ret = tcpx_ep_enable(ep);
+	ret = tcpx_ep_enable(ep, cm_entry,
+			     sizeof(*cm_entry) + cm_ctx->cm_data_sz);
 	if (ret)
 		goto err2;
 
-	ret = (int) fi_eq_write(&ep->util_ep.eq->eq_fid, FI_CONNECTED, cm_entry,
-				sizeof(*cm_entry) + cm_ctx->cm_data_sz, 0);
-	if (ret < 0)
-		goto err2;
-
 	free(cm_entry);
-	free(cm_ctx);
+	tcpx_free_cm_ctx(cm_ctx);
 	return;
 
 err2:
 	free(cm_entry);
 err1:
+	fastlock_acquire(&ep->lock);
 	tcpx_ep_disable(ep, -ret);
-	free(cm_ctx);
+	fastlock_release(&ep->lock);
+	tcpx_free_cm_ctx(cm_ctx);
 }
 
 int tcpx_eq_wait_try_func(void *arg)
@@ -260,7 +298,7 @@ static void tcpx_cm_send_resp(struct util_wait *wait,
 	assert(cm_ctx->fid->fclass == FI_CLASS_EP);
 	ep = container_of(cm_ctx->fid, struct tcpx_ep, util_ep.ep_fid.fid);
 
-	ret = tx_cm_data(ep->sock, ofi_ctrl_connresp, cm_ctx);
+	ret = tx_cm_data(ep->bsock.sock, ofi_ctrl_connresp, cm_ctx);
 	if (ret) {
 		if (ret == -FI_EAGAIN)
 			return;
@@ -269,7 +307,7 @@ static void tcpx_cm_send_resp(struct util_wait *wait,
 		goto delfd;
 	}
 
-	ret = ofi_wait_del_fd(wait, ep->sock);
+	ret = ofi_wait_del_fd(wait, ep->bsock.sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"Could not remove fd from wait\n");
@@ -277,24 +315,22 @@ static void tcpx_cm_send_resp(struct util_wait *wait,
 	}
 
 	cm_entry.fid =  cm_ctx->fid;
-	ret = (int) fi_eq_write(&ep->util_ep.eq->eq_fid, FI_CONNECTED,
-				&cm_entry, sizeof(cm_entry), 0);
-	if (ret < 0)
-		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "Error writing to EQ\n");
 
-	ret = tcpx_ep_enable(ep);
+	ret = tcpx_ep_enable(ep, &cm_entry, sizeof(cm_entry));
 	if (ret)
 		goto disable;
 
 	FI_DBG(&tcpx_prov, FI_LOG_EP_CTRL, "Connection Accept Successful\n");
-	free(cm_ctx);
+	tcpx_free_cm_ctx(cm_ctx);
 	return;
 
 delfd:
-	ofi_wait_del_fd(wait, ep->sock);
+	ofi_wait_del_fd(wait, ep->bsock.sock);
 disable:
+	fastlock_acquire(&ep->lock);
 	tcpx_ep_disable(ep, -ret);
-	free(cm_ctx);
+	fastlock_release(&ep->lock);
+	tcpx_free_cm_ctx(cm_ctx);
 }
 
 static void tcpx_cm_recv_req(struct util_wait *wait,
@@ -333,6 +369,7 @@ static void tcpx_cm_recv_req(struct util_wait *wait,
 		goto err2;
 
 	len = cm_entry->info->dest_addrlen = handle->pep->info->src_addrlen;
+	free(cm_entry->info->dest_addr);
 	cm_entry->info->dest_addr = malloc(len);
 	if (!cm_entry->info->dest_addr)
 		goto err3;
@@ -355,7 +392,7 @@ static void tcpx_cm_recv_req(struct util_wait *wait,
 	}
 
 	free(cm_entry);
-	free(cm_ctx);
+	tcpx_free_cm_ctx(cm_ctx);
 	return;
 err3:
 	fi_freeinfo(cm_entry->info);
@@ -363,7 +400,7 @@ err2:
 	free(cm_entry);
 err1:
 	ofi_close_socket(handle->sock);
-	free(cm_ctx);
+	tcpx_free_cm_ctx(cm_ctx);
 	free(handle);
 }
 
@@ -378,18 +415,20 @@ static void tcpx_cm_send_req(struct util_wait *wait,
 	ep = container_of(cm_ctx->fid, struct tcpx_ep, util_ep.ep_fid.fid);
 
 	len = sizeof(status);
-	ret = getsockopt(ep->sock, SOL_SOCKET, SO_ERROR, (char *) &status, &len);
+	ret = getsockopt(ep->bsock.sock, SOL_SOCKET, SO_ERROR,
+			 (char *) &status, &len);
 	if (ret < 0 || status) {
-		ret = (ret < 0)? -ofi_sockerr() : status;
-		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "connection failure\n");
+		ret = (ret < 0)? -ofi_sockerr() : -status;
+		FI_WARN_SPARSE(&tcpx_prov, FI_LOG_EP_CTRL,
+				"connection failure (sockerr %d)\n", ret);
 		goto delfd;
 	}
 
-	ret = tx_cm_data(ep->sock, ofi_ctrl_connreq, cm_ctx);
+	ret = tx_cm_data(ep->bsock.sock, ofi_ctrl_connreq, cm_ctx);
 	if (ret)
 		goto delfd;
 
-	ret = ofi_wait_del_fd(wait, ep->sock);
+	ret = ofi_wait_del_fd(wait, ep->bsock.sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"Could not remove fd from wait: %s\n",
@@ -398,7 +437,7 @@ static void tcpx_cm_send_req(struct util_wait *wait,
 	}
 
 	cm_ctx->state = TCPX_CM_REQ_SENT;
-	ret = ofi_wait_add_fd(wait, ep->sock, POLLIN,
+	ret = ofi_wait_add_fd(wait, ep->bsock.sock, POLLIN,
 			      tcpx_eq_wait_try_func, NULL, cm_ctx);
 	if (ret)
 		goto disable;
@@ -406,10 +445,12 @@ static void tcpx_cm_send_req(struct util_wait *wait,
 	return;
 
 delfd:
-	ofi_wait_del_fd(wait, ep->sock);
+	ofi_wait_del_fd(wait, ep->bsock.sock);
 disable:
+	fastlock_acquire(&ep->lock);
 	tcpx_ep_disable(ep, -ret);
-	free(cm_ctx);
+	fastlock_release(&ep->lock);
+	tcpx_free_cm_ctx(cm_ctx);
 }
 
 static void tcpx_accept(struct util_wait *wait,
@@ -441,15 +482,13 @@ static void tcpx_accept(struct util_wait *wait,
 		goto err1;
 	}
 
-	rx_req_cm_ctx = calloc(1, sizeof(*rx_req_cm_ctx));
+	rx_req_cm_ctx = tcpx_alloc_cm_ctx(&handle->handle, TCPX_CM_WAIT_REQ);
 	if (!rx_req_cm_ctx)
 		goto err2;
 
 	handle->sock = sock;
 	handle->handle.fclass = FI_CLASS_CONNREQ;
 	handle->pep = pep;
-	rx_req_cm_ctx->fid = &handle->handle;
-	rx_req_cm_ctx->state = TCPX_CM_WAIT_REQ;
 
 	ret = ofi_wait_add_fd(wait, sock, POLLIN,
 			      tcpx_eq_wait_try_func,
@@ -513,8 +552,8 @@ void tcpx_conn_mgr_run(struct util_eq *eq)
 {
 	struct util_wait_fd *wait_fd;
 	struct tcpx_eq *tcpx_eq;
-	void *wait_contexts[MAX_POLL_EVENTS];
-	int num_fds = 0, i;
+	struct ofi_epollfds_event events[MAX_POLL_EVENTS];
+	int count, i;
 
 	assert(eq->wait != NULL);
 
@@ -523,23 +562,19 @@ void tcpx_conn_mgr_run(struct util_eq *eq)
 
 	tcpx_eq = container_of(eq, struct tcpx_eq, util_eq);
 	fastlock_acquire(&tcpx_eq->close_lock);
-	num_fds = (wait_fd->util_wait.wait_obj == FI_WAIT_FD) ?
-		  ofi_epoll_wait(wait_fd->epoll_fd, wait_contexts,
-				 MAX_POLL_EVENTS, 0) :
-		  ofi_pollfds_wait(wait_fd->pollfds, wait_contexts,
-				   MAX_POLL_EVENTS, 0);
-	if (num_fds < 0) {
-		fastlock_release(&tcpx_eq->close_lock);
-		return;
-	}
+	count = (wait_fd->util_wait.wait_obj == FI_WAIT_FD) ?
+		ofi_epoll_wait(wait_fd->epoll_fd, events, MAX_POLL_EVENTS, 0) :
+		ofi_pollfds_wait(wait_fd->pollfds, events, MAX_POLL_EVENTS, 0);
+	if (count < 0)
+		goto unlock;
 
-	for ( i = 0; i < num_fds; i++) {
+	for (i = 0; i < count; i++) {
 		/* skip wake up signals */
-		if (&wait_fd->util_wait.wait_fid.fid == wait_contexts[i])
+		if (&wait_fd->util_wait.wait_fid.fid == events[i].data.ptr)
 			continue;
 
-		process_cm_ctx(eq->wait, (struct tcpx_cm_context *)
-			       wait_contexts[i]);
+		process_cm_ctx(eq->wait, events[i].data.ptr);
 	}
+unlock:
 	fastlock_release(&tcpx_eq->close_lock);
 }
diff --git a/prov/tcp/src/tcpx_cq.c b/prov/tcp/src/tcpx_cq.c
index a2d55ac..463a69b 100644
--- a/prov/tcp/src/tcpx_cq.c
+++ b/prov/tcp/src/tcpx_cq.c
@@ -40,12 +40,13 @@
 
 void tcpx_cq_progress(struct util_cq *cq)
 {
-	void *wait_contexts[MAX_POLL_EVENTS];
+	struct ofi_epollfds_event events[MAX_POLL_EVENTS];
 	struct fid_list_entry *fid_entry;
 	struct util_wait_fd *wait_fd;
 	struct dlist_entry *item;
 	struct tcpx_ep *ep;
 	struct fid *fid;
+	uint32_t inevent, outevent, errevent;
 	int nfds, i;
 
 	wait_fd = container_of(cq->wait, struct util_wait_fd, util_wait);
@@ -55,24 +56,42 @@ void tcpx_cq_progress(struct util_cq *cq)
 		fid_entry = container_of(item, struct fid_list_entry, entry);
 		ep = container_of(fid_entry->fid, struct tcpx_ep,
 				  util_ep.ep_fid.fid);
-		tcpx_try_func(&ep->util_ep);
+
 		fastlock_acquire(&ep->lock);
-		tcpx_progress_tx(ep);
-		if (ep->stage_buf.cur_pos < ep->stage_buf.bytes_avail)
+		/* We need to progress receives in the case where we're waiting
+		 * on the application to post a buffer to consume a receive
+		 * that we've already read from the kernel.  If the message is
+		 * of length 0, there's no additional data to read, so failing
+		 * to progress can result in application hangs.
+		 */
+		if (ofi_bsock_readable(&ep->bsock) ||
+		    (ep->cur_rx.handler && !ep->cur_rx.entry)) {
+			assert(ep->state == TCPX_CONNECTED);
 			tcpx_progress_rx(ep);
+		}
+
+		(void) tcpx_update_epoll(ep);
 		fastlock_release(&ep->lock);
 	}
 
-	nfds = (wait_fd->util_wait.wait_obj == FI_WAIT_FD) ?
-	       ofi_epoll_wait(wait_fd->epoll_fd, wait_contexts,
-			      MAX_POLL_EVENTS, 0) :
-	       ofi_pollfds_wait(wait_fd->pollfds, wait_contexts,
-				MAX_POLL_EVENTS, 0);
+	if (wait_fd->util_wait.wait_obj == FI_WAIT_FD) {
+		nfds = ofi_epoll_wait(wait_fd->epoll_fd, events,
+				      MAX_POLL_EVENTS, 0);
+		inevent = POLLIN;
+		outevent = POLLOUT;
+		errevent = POLLERR;
+	} else {
+		nfds = ofi_pollfds_wait(wait_fd->pollfds, events,
+					MAX_POLL_EVENTS, 0);
+		inevent = OFI_EPOLL_IN;
+		outevent = OFI_EPOLL_OUT;
+		errevent = OFI_EPOLL_ERR;
+	}
 	if (nfds <= 0)
 		goto unlock;
 
 	for (i = 0; i < nfds; i++) {
-		fid = wait_contexts[i];
+		fid = events[i].data.ptr;
 		if (fid->fclass != FI_CLASS_EP) {
 			fd_signal_reset(&wait_fd->signal);
 			continue;
@@ -80,28 +99,25 @@ void tcpx_cq_progress(struct util_cq *cq)
 
 		ep = container_of(fid, struct tcpx_ep, util_ep.ep_fid.fid);
 		fastlock_acquire(&ep->lock);
-		tcpx_progress_rx(ep);
+		if (events[i].events & errevent)
+			tcpx_progress_async(ep);
+		if (events[i].events & inevent)
+			tcpx_progress_rx(ep);
+		if (events[i].events & outevent)
+			tcpx_progress_tx(ep);
 		fastlock_release(&ep->lock);
 	}
 unlock:
 	cq->cq_fastlock_release(&cq->ep_list_lock);
 }
 
-static void tcpx_buf_pools_destroy(struct tcpx_buf_pool *buf_pools)
-{
-	int i;
-
-	for (i = 0; i < TCPX_OP_CODE_MAX; i++)
-		ofi_bufpool_destroy(buf_pools[i].pool);
-}
-
 static int tcpx_cq_close(struct fid *fid)
 {
 	int ret;
 	struct tcpx_cq *tcpx_cq;
 
 	tcpx_cq = container_of(fid, struct tcpx_cq, util_cq.cq_fid.fid);
-	tcpx_buf_pools_destroy(tcpx_cq->buf_pools);
+	ofi_bufpool_destroy(tcpx_cq->xfer_pool);
 	ret = ofi_cq_cleanup(&tcpx_cq->util_cq);
 	if (ret)
 		return ret;
@@ -110,61 +126,53 @@ static int tcpx_cq_close(struct fid *fid)
 	return 0;
 }
 
-struct tcpx_xfer_entry *tcpx_xfer_entry_alloc(struct tcpx_cq *tcpx_cq,
-					      enum tcpx_xfer_op_codes type)
-{
-	struct tcpx_xfer_entry *xfer_entry;
-
-	tcpx_cq->util_cq.cq_fastlock_acquire(&tcpx_cq->util_cq.cq_lock);
-	if (!ofi_cirque_isfull(tcpx_cq->util_cq.cirq))
-		xfer_entry = ofi_buf_alloc(tcpx_cq->buf_pools[type].pool);
-	else
-		xfer_entry = NULL;
-	tcpx_cq->util_cq.cq_fastlock_release(&tcpx_cq->util_cq.cq_lock);
-
-	return xfer_entry;
-}
-
-void tcpx_xfer_entry_free(struct tcpx_cq *tcpx_cq,
-			  struct tcpx_xfer_entry *xfer_entry)
+void tcpx_get_cq_info(struct tcpx_xfer_entry *entry, uint64_t *flags,
+		      uint64_t *data, uint64_t *tag)
 {
-	if (xfer_entry->ep->cur_rx_entry == xfer_entry)
-		xfer_entry->ep->cur_rx_entry = NULL;
-
-	xfer_entry->hdr.base_hdr.flags = 0;
-
-	xfer_entry->flags = 0;
-	xfer_entry->context = 0;
-	xfer_entry->rem_len = 0;
+	if (entry->hdr.base_hdr.flags & TCPX_REMOTE_CQ_DATA) {
+		*data = entry->hdr.cq_data_hdr.cq_data;
+
+		if (entry->hdr.base_hdr.flags & TCPX_TAGGED) {
+			*flags |= FI_REMOTE_CQ_DATA | FI_TAGGED;
+			*tag = entry->hdr.tag_data_hdr.tag;
+		} else {
+			*flags |= FI_REMOTE_CQ_DATA;
+			*tag = 0;
+		}
 
-	tcpx_cq->util_cq.cq_fastlock_acquire(&tcpx_cq->util_cq.cq_lock);
-	ofi_buf_free(xfer_entry);
-	tcpx_cq->util_cq.cq_fastlock_release(&tcpx_cq->util_cq.cq_lock);
+	} else if (entry->hdr.base_hdr.flags & TCPX_TAGGED) {
+		*flags |= FI_TAGGED;
+		*data = 0;
+		*tag = entry->hdr.tag_hdr.tag;
+	} else {
+		*data = 0;
+		*tag = 0;
+	}
 }
 
 void tcpx_cq_report_success(struct util_cq *cq,
 			    struct tcpx_xfer_entry *xfer_entry)
 {
-	uint64_t data = 0;
-	uint64_t flags = 0;
-	void *buf = NULL;
-	size_t len = 0;
-
-	flags = xfer_entry->flags;
+	uint64_t flags, data, tag;
+	size_t len;
 
-	if (!(flags & FI_COMPLETION))
+	if (!(xfer_entry->flags & FI_COMPLETION) ||
+	    (xfer_entry->flags & TCPX_INTERNAL_XFER))
 		return;
 
-	len = xfer_entry->hdr.base_hdr.size -
-	      xfer_entry->hdr.base_hdr.payload_off;
-
-	if (xfer_entry->hdr.base_hdr.flags & OFI_REMOTE_CQ_DATA) {
-		flags |= FI_REMOTE_CQ_DATA;
-		data = xfer_entry->hdr.cq_data_hdr.cq_data;
+	flags = xfer_entry->flags & ~TCPX_INTERNAL_MASK;
+	if (flags & FI_RECV) {
+		len = xfer_entry->hdr.base_hdr.size -
+		      xfer_entry->hdr.base_hdr.hdr_size;
+		tcpx_get_cq_info(xfer_entry, &flags, &data, &tag);
+	} else {
+		len = 0;
+		data = 0;
+		tag = 0;
 	}
 
 	ofi_cq_write(cq, xfer_entry->context,
-		     flags, len, buf, data, 0);
+		     flags, len, NULL, data, tag);
 	if (cq->wait)
 		ofi_cq_signal(&cq->cq_fid);
 }
@@ -174,19 +182,22 @@ void tcpx_cq_report_error(struct util_cq *cq,
 			  int err)
 {
 	struct fi_cq_err_entry err_entry;
-	uint64_t data = 0;
 
-	if (xfer_entry->hdr.base_hdr.flags & OFI_REMOTE_CQ_DATA) {
-		xfer_entry->flags |= FI_REMOTE_CQ_DATA;
-		data = xfer_entry->hdr.cq_data_hdr.cq_data;
+	if (xfer_entry->flags & TCPX_INTERNAL_XFER)
+		return;
+
+	err_entry.flags = xfer_entry->flags & ~TCPX_INTERNAL_MASK;
+	if (err_entry.flags & FI_RECV) {
+		tcpx_get_cq_info(xfer_entry, &err_entry.flags, &err_entry.data,
+				 &err_entry.tag);
+	} else {
+		err_entry.data = 0;
+		err_entry.tag = 0;
 	}
 
 	err_entry.op_context = xfer_entry->context;
-	err_entry.flags = xfer_entry->flags;
 	err_entry.len = 0;
 	err_entry.buf = NULL;
-	err_entry.data = data;
-	err_entry.tag = 0;
 	err_entry.olen = 0;
 	err_entry.err = err;
 	err_entry.prov_errno = ofi_sockerr();
@@ -226,69 +237,6 @@ static struct fi_ops tcpx_cq_fi_ops = {
 	.ops_open = fi_no_ops_open,
 };
 
-static void tcpx_buf_pool_init(struct ofi_bufpool_region *region, void *buf)
-{
-	struct tcpx_buf_pool *pool = region->pool->attr.context;
-	struct tcpx_xfer_entry *xfer_entry = buf;
-
-	xfer_entry->hdr.base_hdr.version = TCPX_HDR_VERSION;
-	xfer_entry->hdr.base_hdr.op_data = pool->op_type;
-
-	switch (pool->op_type) {
-	case TCPX_OP_MSG_RECV:
-	case TCPX_OP_MSG_SEND:
-	case TCPX_OP_MSG_RESP:
-		xfer_entry->hdr.base_hdr.op = ofi_op_msg;
-		break;
-	case TCPX_OP_WRITE:
-	case TCPX_OP_REMOTE_WRITE:
-		xfer_entry->hdr.base_hdr.op = ofi_op_write;
-		break;
-	case TCPX_OP_READ_REQ:
-		xfer_entry->hdr.base_hdr.op = ofi_op_read_req;
-		break;
-	case TCPX_OP_READ_RSP:
-		xfer_entry->hdr.base_hdr.op = ofi_op_read_rsp;
-		break;
-	case TCPX_OP_REMOTE_READ:
-		break;
-	default:
-		assert(0);
-		break;
-	}
-}
-
-static int tcpx_buf_pools_create(struct tcpx_buf_pool *buf_pools)
-{
-	int i, ret;
-	struct ofi_bufpool_attr attr = {
-		.size = sizeof(struct tcpx_xfer_entry),
-		.alignment = 16,
-		.chunk_cnt = 1024,
-		.init_fn = tcpx_buf_pool_init,
-		.flags = OFI_BUFPOOL_HUGEPAGES,
-	};
-
-	for (i = 0; i < TCPX_OP_CODE_MAX; i++) {
-		buf_pools[i].op_type = i;
-
-		attr.context = &buf_pools[i];
-		ret = ofi_bufpool_create_attr(&attr, &buf_pools[i].pool);
-		if (ret) {
-			FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
-				"Unable to create buf pool\n");
-			goto err;
-		}
-	}
-	return 0;
-
-err:
-	while (i--)
-		ofi_bufpool_destroy(buf_pools[i].pool);
-
-	return -ret;
-}
-
 int tcpx_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 		 struct fid_cq **cq_fid, void *context)
 {
@@ -303,7 +251,9 @@ int tcpx_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 	if (!attr->size)
 		attr->size = TCPX_DEF_CQ_SIZE;
 
-	ret = tcpx_buf_pools_create(tcpx_cq->buf_pools);
+	ret = ofi_bufpool_create(&tcpx_cq->xfer_pool,
+				 sizeof(struct tcpx_xfer_entry), 16, 0,
+				 1024, 0);
 	if (ret)
 		goto free_cq;
 
@@ -324,7 +274,7 @@ int tcpx_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 	return 0;
 
 destroy_pool:
-	tcpx_buf_pools_destroy(tcpx_cq->buf_pools);
+	ofi_bufpool_destroy(tcpx_cq->xfer_pool);
 free_cq:
 	free(tcpx_cq);
 	return ret;
diff --git a/prov/tcp/src/tcpx_domain.c b/prov/tcp/src/tcpx_domain.c
index 2c3a22a..697210a 100644
--- a/prov/tcp/src/tcpx_domain.c
+++ b/prov/tcp/src/tcpx_domain.c
@@ -87,14 +87,12 @@ static int tcpx_srx_ctx(struct fid_domain *domain, struct fi_rx_attr *attr,
 		goto err1;
 
 	ret = ofi_bufpool_create(&srx_ctx->buf_pool,
-				 sizeof(struct tcpx_xfer_entry), 16, 0, 1024,
-				 OFI_BUFPOOL_HUGEPAGES);
+				 sizeof(struct tcpx_xfer_entry),
+				 16, attr->size, 1024, 0);
 	if (ret)
 		goto err2;
 
-	if (attr)
-		srx_ctx->op_flags = attr->op_flags;
-
+	srx_ctx->op_flags = attr->op_flags;
 	*rx_ep = &srx_ctx->rx_fid;
 	return FI_SUCCESS;
 err2:
diff --git a/prov/tcp/src/tcpx_ep.c b/prov/tcp/src/tcpx_ep.c
index e21037b..f2ad048 100644
--- a/prov/tcp/src/tcpx_ep.c
+++ b/prov/tcp/src/tcpx_ep.c
@@ -41,6 +41,8 @@
 
 extern struct fi_ops_rma tcpx_rma_ops;
 extern struct fi_ops_msg tcpx_msg_ops;
+extern struct fi_ops_tagged tcpx_tagged_ops;
+
 
 void tcpx_hdr_none(struct tcpx_base_hdr *hdr)
 {
@@ -49,25 +51,49 @@ void tcpx_hdr_none(struct tcpx_base_hdr *hdr)
 
 void tcpx_hdr_bswap(struct tcpx_base_hdr *hdr)
 {
-	struct ofi_rma_iov *rma_iov;
-	uint8_t *ptr = (uint8_t *)hdr + sizeof(*hdr);
-	int i;
+	uint64_t *cur;
+	int i, cnt;
 
 	hdr->flags = ntohs(hdr->flags);
 	hdr->size = ntohll(hdr->size);
 
-	if (hdr->flags & OFI_REMOTE_CQ_DATA) {
-		*((uint64_t *)ptr) = ntohll(*((uint64_t *) ptr));
-		ptr += sizeof(uint64_t);
-	}
+	cnt = (hdr->hdr_size - sizeof(*hdr)) >> 3;
+	cur = (uint64_t *) (hdr + 1);
+	for (i = 0; i < cnt; i++)
+		cur[i] = ntohll(cur[i]);
+}
+
+#ifdef MSG_ZEROCOPY
+static void tcpx_set_zerocopy(SOCKET sock)
+{
+	int val = 1;
+
+	if (tcpx_zerocopy_size == SIZE_MAX)
+		return;
+
+	(void) setsockopt(sock, SOL_SOCKET, SO_ZEROCOPY, &val, sizeof(val));
+}
+
+static void tcpx_config_bsock(struct ofi_bsock *bsock)
+{
+	int ret, val = 0;
+	socklen_t len = sizeof(val);
 
-	rma_iov = (struct ofi_rma_iov *)ptr;
-	for ( i = 0; i < hdr->rma_iov_cnt; i++) {
-		rma_iov[i].addr = ntohll(rma_iov[i].addr);
-		rma_iov[i].len = ntohll(rma_iov[i].len);
-		rma_iov[i].key = ntohll(rma_iov[i].key);
+	if (tcpx_zerocopy_size == SIZE_MAX)
+		return;
+
+	ret = getsockopt(bsock->sock, SOL_SOCKET, SO_ZEROCOPY, &val, &len);
+	if (!ret && val) {
+		bsock->zerocopy_size = tcpx_zerocopy_size;
+		FI_INFO(&tcpx_prov, FI_LOG_EP_CTRL,
+			"zero copy enabled for transfers > %zu\n",
+			bsock->zerocopy_size);
 	}
 }
+#else
+#define tcpx_set_zerocopy(sock)
+#define tcpx_config_bsock(bsock)
+#endif
 
 static int tcpx_setup_socket(SOCKET sock, struct fi_info *info)
 {
@@ -110,11 +136,11 @@ static int tcpx_ep_connect(struct fid_ep *ep, const void *addr,
 	struct tcpx_cm_context *cm_ctx;
 	int ret;
 
-	if (!addr || !tcpx_ep->sock || paramlen > TCPX_MAX_CM_DATA_SIZE ||
-	    tcpx_ep->state != TCPX_IDLE)
+	if (!addr || (tcpx_ep->bsock.sock == INVALID_SOCKET) ||
+	    (paramlen > TCPX_MAX_CM_DATA_SIZE) || (tcpx_ep->state != TCPX_IDLE))
 		return -FI_EINVAL;
 
-	cm_ctx = calloc(1, sizeof(*cm_ctx));
+	cm_ctx = tcpx_alloc_cm_ctx(&ep->fid, TCPX_CM_CONNECTING);
 	if (!cm_ctx) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"cannot allocate memory \n");
@@ -122,7 +148,7 @@ static int tcpx_ep_connect(struct fid_ep *ep, const void *addr,
 	}
 
 	tcpx_ep->state = TCPX_CONNECTING;
-	ret = connect(tcpx_ep->sock, (struct sockaddr *) addr,
+	ret = connect(tcpx_ep->bsock.sock, (struct sockaddr *) addr,
 		      (socklen_t) ofi_sizeofaddr(addr));
 	if (ret && !OFI_SOCK_TRY_CONN_AGAIN(ofi_sockerr())) {
 		tcpx_ep->state = TCPX_IDLE;
@@ -130,25 +156,24 @@ static int tcpx_ep_connect(struct fid_ep *ep, const void *addr,
 		goto free;
 	}
 
-	cm_ctx->fid = &tcpx_ep->util_ep.ep_fid.fid;
-	cm_ctx->state = TCPX_CM_CONNECTING;
-
 	if (paramlen) {
 		cm_ctx->cm_data_sz = paramlen;
 		memcpy(cm_ctx->msg.data, param, paramlen);
 	}
 
-	ret = ofi_wait_add_fd(tcpx_ep->util_ep.eq->wait, tcpx_ep->sock,
-			      POLLOUT, tcpx_eq_wait_try_func, NULL,cm_ctx);
+	ret = ofi_wait_add_fd(tcpx_ep->util_ep.eq->wait, tcpx_ep->bsock.sock,
+			      POLLOUT, tcpx_eq_wait_try_func, NULL, cm_ctx);
 	if (ret)
 		goto disable;
 
 	return 0;
 
 disable:
+	fastlock_acquire(&tcpx_ep->lock);
 	tcpx_ep_disable(tcpx_ep, -ret);
+	fastlock_release(&tcpx_ep->lock);
 free:
-	free(cm_ctx);
+	tcpx_free_cm_ctx(cm_ctx);
 	return ret;
 }
 
@@ -158,10 +183,12 @@ static int tcpx_ep_accept(struct fid_ep *ep, const void *param, size_t paramlen)
 	struct tcpx_cm_context *cm_ctx;
 	int ret;
 
-	if (tcpx_ep->sock == INVALID_SOCKET || tcpx_ep->state != TCPX_RCVD_REQ)
+	if (tcpx_ep->bsock.sock == INVALID_SOCKET ||
+	    tcpx_ep->state != TCPX_RCVD_REQ)
 		return -FI_EINVAL;
 
-	cm_ctx = calloc(1, sizeof(*cm_ctx));
+	cm_ctx = tcpx_alloc_cm_ctx(&tcpx_ep->util_ep.ep_fid.fid,
+				   TCPX_CM_RESP_READY);
 	if (!cm_ctx) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"cannot allocate memory \n");
@@ -169,14 +196,13 @@ static int tcpx_ep_accept(struct fid_ep *ep, const void *param, size_t paramlen)
 	}
 
 	tcpx_ep->state = TCPX_ACCEPTING;
-	cm_ctx->fid = &tcpx_ep->util_ep.ep_fid.fid;
-	cm_ctx->state = TCPX_CM_RESP_READY;
+
 	if (paramlen) {
 		cm_ctx->cm_data_sz = paramlen;
 		memcpy(cm_ctx->msg.data, param, paramlen);
 	}
 
-	ret = ofi_wait_add_fd(tcpx_ep->util_ep.eq->wait, tcpx_ep->sock,
+	ret = ofi_wait_add_fd(tcpx_ep->util_ep.eq->wait, tcpx_ep->bsock.sock,
 			      POLLOUT, tcpx_eq_wait_try_func, NULL, cm_ctx);
 	if (ret)
 		goto free;
@@ -185,7 +211,7 @@ static int tcpx_ep_accept(struct fid_ep *ep, const void *param, size_t paramlen)
 
 free:
 	tcpx_ep->state = TCPX_RCVD_REQ;
-	free(cm_ctx);
+	tcpx_free_cm_ctx(cm_ctx);
 	return ret;
 }
 
@@ -200,31 +226,49 @@ static void tcpx_ep_flush_queue(struct slist *queue,
 					  entry);
 		slist_remove_head(queue);
 		tcpx_cq_report_error(&tcpx_cq->util_cq, xfer_entry, FI_ECANCELED);
-		tcpx_xfer_entry_free(tcpx_cq, xfer_entry);
+		tcpx_free_xfer(tcpx_cq, xfer_entry);
 	}
 }
 
-/* must hold ep->lock */
 static void tcpx_ep_flush_all_queues(struct tcpx_ep *ep)
 {
-	struct tcpx_cq *tcpx_cq;
+	struct tcpx_cq *cq;
+
+	assert(fastlock_held(&ep->lock));
+	cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
+	if (ep->cur_tx.entry) {
+		ep->hdr_bswap(&ep->cur_tx.entry->hdr.base_hdr);
+		tcpx_cq_report_error(&cq->util_cq, ep->cur_tx.entry,
+				     FI_ECANCELED);
+		tcpx_free_xfer(cq, ep->cur_tx.entry);
+		ep->cur_tx.entry = NULL;
+	}
 
-	tcpx_cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
-	tcpx_ep_flush_queue(&ep->tx_queue, tcpx_cq);
-	tcpx_ep_flush_queue(&ep->rma_read_queue, tcpx_cq);
-	tcpx_ep_flush_queue(&ep->tx_rsp_pend_queue, tcpx_cq);
+	tcpx_ep_flush_queue(&ep->tx_queue, cq);
+	tcpx_ep_flush_queue(&ep->priority_queue, cq);
+	tcpx_ep_flush_queue(&ep->rma_read_queue, cq);
+	tcpx_ep_flush_queue(&ep->need_ack_queue, cq);
+	tcpx_ep_flush_queue(&ep->async_queue, cq);
 
-	tcpx_cq = container_of(ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
-	tcpx_ep_flush_queue(&ep->rx_queue, tcpx_cq);
+	cq = container_of(ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
+	if (ep->cur_rx.entry) {
+		tcpx_cq_report_error(&cq->util_cq, ep->cur_rx.entry,
+				     FI_ECANCELED);
+		tcpx_free_xfer(cq, ep->cur_rx.entry);
+		tcpx_reset_rx(ep);
+	}
+	tcpx_ep_flush_queue(&ep->rx_queue, cq);
+	ofi_bsock_discard(&ep->bsock);
 }
 
-/* must hold ep->lock */
 void tcpx_ep_disable(struct tcpx_ep *ep, int cm_err)
 {
 	struct util_wait_fd *wait;
 	struct fi_eq_cm_entry cm_entry = {0};
 	struct fi_eq_err_entry err_entry = {0};
+	int ret;
 
+	assert(fastlock_held(&ep->lock));
 	switch (ep->state) {
 	case TCPX_RCVD_REQ:
 		break;
@@ -236,26 +280,30 @@ void tcpx_ep_disable(struct tcpx_ep *ep, int cm_err)
 		if (ep->util_ep.tx_cq) {
 			wait = container_of(ep->util_ep.tx_cq->wait,
 					    struct util_wait_fd, util_wait);
-			ofi_wait_fdset_del(wait, ep->sock);
+			ofi_wait_fdset_del(wait, ep->bsock.sock);
 		}
 
 		if (ep->util_ep.rx_cq) {
 			wait = container_of(ep->util_ep.rx_cq->wait,
 					    struct util_wait_fd, util_wait);
-			ofi_wait_fdset_del(wait, ep->sock);
+			ofi_wait_fdset_del(wait, ep->bsock.sock);
 		}
 		/* fall through */
 	case TCPX_ACCEPTING:
 	case TCPX_CONNECTING:
 		wait = container_of(ep->util_ep.eq->wait,
 				    struct util_wait_fd, util_wait);
-		ofi_wait_fdset_del(wait, ep->sock);
+		ofi_wait_fdset_del(wait, ep->bsock.sock);
 		break;
 
 	default:
 		return;
 	}
 
+	ret = ofi_shutdown(ep->bsock.sock, SHUT_RDWR);
+	if (ret && ofi_sockerr() != ENOTCONN)
+		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA, "shutdown failed\n");
+
 	tcpx_ep_flush_all_queues(ep);
 
 	if (cm_err) {
@@ -276,14 +324,9 @@ void tcpx_ep_disable(struct tcpx_ep *ep, int cm_err)
 static int tcpx_ep_shutdown(struct fid_ep *ep, uint64_t flags)
 {
 	struct tcpx_ep *tcpx_ep;
-	int ret;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
-
-	ret = ofi_shutdown(tcpx_ep->sock, SHUT_RDWR);
-	if (ret && ofi_sockerr() != ENOTCONN) {
-		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA, "ep shutdown unsuccessful\n");
-	}
+	(void) ofi_bsock_flush(&tcpx_ep->bsock);
 
 	fastlock_acquire(&tcpx_ep->lock);
 	tcpx_ep_disable(tcpx_ep, 0);
@@ -346,10 +389,10 @@ static int tcpx_pep_sock_create(struct tcpx_pep *pep)
 		return -FI_EIO;
 	}
 	ret = tcpx_setup_socket(pep->sock, pep->info);
-	if (ret) {
+	if (ret)
 		goto err;
-	}
 
+	tcpx_set_zerocopy(pep->sock);
 	ret = fi_fd_nonblock(pep->sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
@@ -387,7 +430,7 @@ static int tcpx_ep_getname(fid_t fid, void *addr, size_t *addrlen)
 	int ret;
 
 	tcpx_ep = container_of(fid, struct tcpx_ep, util_ep.ep_fid);
-	ret = ofi_getsockname(tcpx_ep->sock, addr, (socklen_t *)addrlen);
+	ret = ofi_getsockname(tcpx_ep->bsock.sock, addr, (socklen_t *) addrlen);
 	if (ret)
 		return -ofi_sockerr();
 
@@ -401,7 +444,7 @@ static int tcpx_ep_getpeer(struct fid_ep *ep, void *addr, size_t *addrlen)
 	int ret;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
-	ret = ofi_getpeername(tcpx_ep->sock, addr, (socklen_t *)addrlen);
+	ret = ofi_getpeername(tcpx_ep->bsock.sock, addr, (socklen_t *) addrlen);
 	if (ret)
 		return -ofi_sockerr();
 
@@ -421,28 +464,23 @@ static struct fi_ops_cm tcpx_cm_ops = {
 	.join = fi_no_join,
 };
 
-void tcpx_rx_entry_free(struct tcpx_xfer_entry *rx_entry)
+void tcpx_reset_rx(struct tcpx_ep *ep)
 {
-	struct tcpx_cq *tcpx_cq;
-
-	assert(rx_entry->hdr.base_hdr.op_data == TCPX_OP_MSG_RECV);
-
-	if (rx_entry->ep->srx_ctx) {
-		tcpx_srx_entry_free(rx_entry->ep->srx_ctx, rx_entry);
-	} else {
-		tcpx_cq = container_of(rx_entry->ep->util_ep.rx_cq,
-				       struct tcpx_cq, util_cq);
-		tcpx_xfer_entry_free(tcpx_cq, rx_entry);
-	}
+	ep->cur_rx.handler = NULL;
+	ep->cur_rx.entry = NULL;
+	ep->cur_rx.hdr_done = 0;
+	ep->cur_rx.hdr_len = sizeof(ep->cur_rx.hdr.base_hdr);
+	OFI_DBG_SET(ep->cur_rx.hdr.base_hdr.version, 0);
 }
 
-/* Must hold ep->lock. */
 static void tcpx_ep_cancel_rx(struct tcpx_ep *ep, void *context)
 {
 	struct slist_entry *cur, *prev;
 	struct tcpx_xfer_entry *xfer_entry;
 	struct tcpx_cq *cq;
 
+	assert(fastlock_held(&ep->lock));
+
 	/* To cancel an active receive, we would need to flush the socket of
 	 * all data associated with that message.  Since some of that data
 	 * may not have arrived yet, this would require additional state
@@ -452,7 +490,7 @@ static void tcpx_ep_cancel_rx(struct tcpx_ep *ep, void *context)
 	slist_foreach(&ep->rx_queue, cur, prev) {
 		xfer_entry = container_of(cur, struct tcpx_xfer_entry, entry);
 		if (xfer_entry->context == context) {
-			if (ep->cur_rx_entry != xfer_entry)
+			if (ep->cur_rx.entry != xfer_entry)
 				goto found;
 			break;
 		}
@@ -464,8 +502,9 @@ found:
 	cq = container_of(ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
 
 	slist_remove(&ep->rx_queue, cur, prev);
+	ep->rx_avail++;
 	tcpx_cq_report_error(&cq->util_cq, xfer_entry, FI_ECANCELED);
-	tcpx_xfer_entry_free(cq, xfer_entry);
+	tcpx_free_xfer(cq, xfer_entry);
 }
 
 /* We currently only support canceling receives, which is the common case.
@@ -499,17 +538,20 @@ static int tcpx_ep_close(struct fid *fid)
 		fastlock_acquire(&eq->close_lock);
 
 	if (ep->util_ep.rx_cq)
-		ofi_wait_del_fd(ep->util_ep.rx_cq->wait, ep->sock);
+		ofi_wait_del_fd(ep->util_ep.rx_cq->wait, ep->bsock.sock);
 
 	if (ep->util_ep.tx_cq)
-		ofi_wait_del_fd(ep->util_ep.tx_cq->wait, ep->sock);
+		ofi_wait_del_fd(ep->util_ep.tx_cq->wait, ep->bsock.sock);
 
 	if (ep->util_ep.eq && ep->util_ep.eq->wait)
-		ofi_wait_del_fd(ep->util_ep.eq->wait, ep->sock);
+		ofi_wait_del_fd(ep->util_ep.eq->wait, ep->bsock.sock);
 
 	if (eq)
 		fastlock_release(&eq->close_lock);
 
+	if (ep->cm_ctx)
+		tcpx_free_cm_ctx(ep->cm_ctx);
+
 	/* Lock not technically needed, since we're freeing the EP.  But it's
 	 * harmless to acquire and silences static code analysis tools.
 	 */
@@ -521,7 +563,7 @@ static int tcpx_ep_close(struct fid *fid)
 		ofi_eq_remove_fid_events(ep->util_ep.eq,
 					 &ep->util_ep.ep_fid.fid);
 	}
-	ofi_close_socket(ep->sock);
+	ofi_close_socket(ep->bsock.sock);
 	ofi_endpoint_close(&ep->util_ep);
 	fastlock_destroy(&ep->lock);
 
@@ -655,36 +697,51 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 	if (ret)
 		goto err1;
 
+	ofi_bsock_init(&ep->bsock, tcpx_staging_sbuf_size,
+		       tcpx_prefetch_rbuf_size);
 	if (info->handle) {
 		if (((fid_t) info->handle)->fclass == FI_CLASS_PEP) {
 			pep = container_of(info->handle, struct tcpx_pep,
 					   util_pep.pep_fid.fid);
 
-			ep->sock = pep->sock;
+			ep->bsock.sock = pep->sock;
 			pep->sock = INVALID_SOCKET;
 		} else {
 			ep->state = TCPX_RCVD_REQ;
 			handle = container_of(info->handle,
 					      struct tcpx_conn_handle, handle);
-			ep->sock = handle->sock;
+			ep->bsock.sock = handle->sock;
 			ep->hdr_bswap = handle->endian_match ?
 					tcpx_hdr_none : tcpx_hdr_bswap;
 			free(handle);
 
-			ret = tcpx_setup_socket(ep->sock, info);
+			ret = tcpx_setup_socket(ep->bsock.sock, info);
 			if (ret)
 				goto err3;
 		}
 	} else {
-		ep->sock = ofi_socket(ofi_get_sa_family(info), SOCK_STREAM, 0);
-		if (ep->sock == INVALID_SOCKET) {
+		ep->bsock.sock = ofi_socket(ofi_get_sa_family(info), SOCK_STREAM, 0);
+		if (ep->bsock.sock == INVALID_SOCKET) {
 			ret = -ofi_sockerr();
 			goto err2;
 		}
 
-		ret = tcpx_setup_socket(ep->sock, info);
+		ret = tcpx_setup_socket(ep->bsock.sock, info);
 		if (ret)
 			goto err3;
+
+		tcpx_set_zerocopy(ep->bsock.sock);
+
+		if (info->src_addr && (!ofi_is_any_addr(info->src_addr) ||
+					ofi_addr_get_port(info->src_addr))) {
+			ret = bind(ep->bsock.sock, info->src_addr,
+				(socklen_t) info->src_addrlen);
+			if (ret) {
+				FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "bind failed\n");
+				ret = -ofi_sockerr();
+				goto err3;
+			}
+		}
 	}
 
 	ret = fastlock_init(&ep->lock);
@@ -693,12 +750,18 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 
 	slist_init(&ep->rx_queue);
 	slist_init(&ep->tx_queue);
+	slist_init(&ep->priority_queue);
 	slist_init(&ep->rma_read_queue);
-	slist_init(&ep->tx_rsp_pend_queue);
+	slist_init(&ep->need_ack_queue);
+	slist_init(&ep->async_queue);
+
+	if (info->ep_attr->rx_ctx_cnt != FI_SHARED_CONTEXT)
+		ep->rx_avail = info->rx_attr->size;
 
-	ep->cur_rx_msg.done_len = 0;
-	ep->cur_rx_msg.hdr_len = sizeof(ep->cur_rx_msg.hdr.base_hdr);
+	ep->cur_rx.hdr_done = 0;
+	ep->cur_rx.hdr_len = sizeof(ep->cur_rx.hdr.base_hdr);
 	ep->min_multi_recv_size = TCPX_MIN_MULTI_RECV;
+	tcpx_config_bsock(&ep->bsock);
 
 	*ep_fid = &ep->util_ep.ep_fid;
 	(*ep_fid)->fid.ops = &tcpx_ep_fi_ops;
@@ -706,6 +769,8 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 	(*ep_fid)->cm = &tcpx_cm_ops;
 	(*ep_fid)->msg = &tcpx_msg_ops;
 	(*ep_fid)->rma = &tcpx_rma_ops;
+	if (tcpx_dynamic_rbuf(ep))
+		(*ep_fid)->tagged = &tcpx_tagged_ops;
 
 	ep->start_op[ofi_op_msg] = tcpx_op_msg;
 	ep->start_op[ofi_op_tagged] = tcpx_op_invalid;
@@ -714,7 +779,7 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 	ep->start_op[ofi_op_write] = tcpx_op_write;
 	return 0;
 err3:
-	ofi_close_socket(ep->sock);
+	ofi_close_socket(ep->bsock.sock);
 err2:
 	ofi_endpoint_close(&ep->util_ep);
 err1:
diff --git a/prov/tcp/src/tcpx_init.c b/prov/tcp/src/tcpx_init.c
index c0f1bab..4871e53 100644
--- a/prov/tcp/src/tcpx_init.c
+++ b/prov/tcp/src/tcpx_init.c
@@ -54,9 +54,18 @@ struct tcpx_port_range port_range = {
 
 int tcpx_nodelay = -1;
 
+int tcpx_staging_sbuf_size = 9000;
+int tcpx_prefetch_rbuf_size = 9000;
+size_t tcpx_default_tx_size = 256;
+size_t tcpx_default_rx_size = 256;
+size_t tcpx_zerocopy_size = SIZE_MAX;
+
 
 static void tcpx_init_env(void)
 {
+	size_t tx_size;
+	size_t rx_size;
+
 	fi_param_define(&tcpx_prov, "iface", FI_PARAM_STRING,
 			"Specify interface name");
 
@@ -66,10 +75,35 @@ static void tcpx_init_env(void)
 	fi_param_define(&tcpx_prov,"port_high_range", FI_PARAM_INT,
 			"define port high range");
 
+	fi_param_define(&tcpx_prov,"tx_size", FI_PARAM_SIZE_T,
+			"define default tx context size (default: %zu)",
+			tcpx_default_tx_size);
+
+	fi_param_define(&tcpx_prov,"rx_size", FI_PARAM_SIZE_T,
+			"define default rx context size (default: %zu)",
+			tcpx_default_rx_size);
+
 	fi_param_define(&tcpx_prov, "nodelay", FI_PARAM_BOOL,
 			"overrides default TCP_NODELAY socket setting");
 	fi_param_get_bool(&tcpx_prov, "nodelay", &tcpx_nodelay);
 
+	fi_param_define(&tcpx_prov, "staging_sbuf_size", FI_PARAM_INT,
+			"size of buffer used to coalesce iovec's or "
+			"send requests before posting to the kernel, "
+			"set to 0 to disable");
+	fi_param_define(&tcpx_prov, "prefetch_rbuf_size", FI_PARAM_INT,
+			"size of buffer used to prefetch received data from "
+			"the kernel, set to 0 to disable");
+	fi_param_define(&tcpx_prov, "zerocopy_size", FI_PARAM_SIZE_T,
+			"lower threshold where zero copy transfers will be "
+			"used, if supported by the platform, set to -1 to "
+			"disable (default: %zu)", tcpx_zerocopy_size);
+	fi_param_get_int(&tcpx_prov, "staging_sbuf_size",
+			 &tcpx_staging_sbuf_size);
+	fi_param_get_int(&tcpx_prov, "prefetch_rbuf_size",
+			 &tcpx_prefetch_rbuf_size);
+	fi_param_get_size_t(&tcpx_prov, "zerocopy_size", &tcpx_zerocopy_size);
+
 	fi_param_get_int(&tcpx_prov, "port_high_range", &port_range.high);
 	fi_param_get_int(&tcpx_prov, "port_low_range", &port_range.low);
 
@@ -83,6 +117,15 @@ static void tcpx_init_env(void)
 		port_range.low  = 0;
 		port_range.high = 0;
 	}
+
+	if (!fi_param_get_size_t(&tcpx_prov, "tx_size", &tx_size)) {
+		tcpx_default_tx_size = tx_size;
+	}
+
+	if (!fi_param_get_size_t(&tcpx_prov, "rx_size", &rx_size)) {
+		tcpx_default_rx_size = rx_size;
+	}
+
 }
 
 static void fi_tcp_fini(void)
diff --git a/prov/tcp/src/tcpx_msg.c b/prov/tcp/src/tcpx_msg.c
index 3e8581b..10861a3 100644
--- a/prov/tcp/src/tcpx_msg.c
+++ b/prov/tcp/src/tcpx_msg.c
@@ -48,42 +48,94 @@
 #include <arpa/inet.h>
 #include <netdb.h>
 
+
 static inline struct tcpx_xfer_entry *
-tcpx_alloc_recv_entry(struct tcpx_ep *tcpx_ep)
+tcpx_alloc_send_entry(struct tcpx_ep *ep)
 {
-	struct tcpx_xfer_entry *recv_entry;
-	struct tcpx_cq *tcpx_cq;
+	struct tcpx_xfer_entry *send_entry;
 
-	tcpx_cq = container_of(tcpx_ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
+	send_entry = tcpx_alloc_tx(ep);
+	if (send_entry)
+		send_entry->hdr.base_hdr.op = ofi_op_msg;
 
-	recv_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_MSG_RECV);
-	if (recv_entry)
-		recv_entry->ep = tcpx_ep;
+	return send_entry;
+}
 
-	return recv_entry;
+static inline void
+tcpx_init_tx_sizes(struct tcpx_xfer_entry *tx_entry, size_t hdr_len,
+		   size_t data_len)
+{
+	tx_entry->hdr.base_hdr.size = hdr_len + data_len;
+	tx_entry->hdr.base_hdr.hdr_size = (uint8_t) hdr_len;
 }
 
-static inline struct tcpx_xfer_entry *
-tcpx_alloc_send_entry(struct tcpx_ep *tcpx_ep)
+static inline void
+tcpx_init_tx_inject(struct tcpx_xfer_entry *tx_entry, size_t hdr_len,
+		    const void *buf, size_t data_len)
 {
-	struct tcpx_xfer_entry *send_entry;
-	struct tcpx_cq *tcpx_cq;
+	assert(data_len <= TCPX_MAX_INJECT);
+	tcpx_init_tx_sizes(tx_entry, hdr_len, data_len);
 
-	tcpx_cq = container_of(tcpx_ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
+	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
+	memcpy((uint8_t *) &tx_entry->hdr + hdr_len, (uint8_t *) buf,
+		data_len);
+	tx_entry->iov[0].iov_len = hdr_len + data_len;
+	tx_entry->iov_cnt = 1;
+}
 
-	send_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_MSG_SEND);
-	if (send_entry)
-		send_entry->ep = tcpx_ep;
+static inline void
+tcpx_init_tx_buf(struct tcpx_xfer_entry *tx_entry, size_t hdr_len,
+		 const void *buf, size_t data_len)
+{
+	if (data_len <= TCPX_MAX_INJECT) {
+		tcpx_init_tx_inject(tx_entry, hdr_len, buf, data_len);
+		return;
+	}
 
-	return send_entry;
+	tcpx_init_tx_sizes(tx_entry, hdr_len, data_len);
+	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
+	tx_entry->iov[0].iov_len = hdr_len;
+	tx_entry->iov[1].iov_base = (void *) buf;
+	tx_entry->iov[1].iov_len = data_len;
+	tx_entry->iov_cnt = 2;
 }
 
-static inline void tcpx_queue_recv(struct tcpx_ep *tcpx_ep,
+static inline void
+tcpx_init_tx_iov(struct tcpx_xfer_entry *tx_entry, size_t hdr_len,
+		 const struct iovec *iov, size_t count)
+{
+	size_t data_len;
+
+	assert(count <= TCPX_IOV_LIMIT);
+	data_len = ofi_total_iov_len(iov, count);
+	tcpx_init_tx_sizes(tx_entry, hdr_len, data_len);
+
+	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
+	if (data_len <= TCPX_MAX_INJECT) {
+		ofi_copy_iov_buf(iov, count, 0, (uint8_t *) &tx_entry->hdr +
+				 hdr_len, TCPX_MAX_INJECT, OFI_COPY_IOV_TO_BUF);
+		tx_entry->iov[0].iov_len = hdr_len + data_len;
+		tx_entry->iov_cnt = 1;
+	} else {
+		tx_entry->iov[0].iov_len = hdr_len;
+		tx_entry->iov_cnt = count + 1;
+		memcpy(&tx_entry->iov[1], &iov[0], count * sizeof(struct iovec));
+	}
+}
+
+static inline bool tcpx_queue_recv(struct tcpx_ep *ep,
 				   struct tcpx_xfer_entry *recv_entry)
 {
-	fastlock_acquire(&tcpx_ep->lock);
-	slist_insert_tail(&recv_entry->entry, &tcpx_ep->rx_queue);
-	fastlock_release(&tcpx_ep->lock);
+	bool ret;
+
+	fastlock_acquire(&ep->lock);
+	ret = ep->rx_avail;
+	if (ret) {
+		slist_insert_tail(&recv_entry->entry, &ep->rx_queue);
+		ep->rx_avail--;
+	}
+	fastlock_release(&ep->lock);
+	return ret;
 }
 
 static ssize_t tcpx_recvmsg(struct fid_ep *ep, const struct fi_msg *msg,
@@ -96,7 +148,7 @@ static ssize_t tcpx_recvmsg(struct fid_ep *ep, const struct fi_msg *msg,
 
 	assert(msg->iov_count <= TCPX_IOV_LIMIT);
 
-	recv_entry = tcpx_alloc_recv_entry(tcpx_ep);
+	recv_entry = tcpx_alloc_rx(tcpx_ep);
 	if (!recv_entry)
 		return -FI_EAGAIN;
 
@@ -108,7 +160,10 @@ static ssize_t tcpx_recvmsg(struct fid_ep *ep, const struct fi_msg *msg,
 			    FI_MSG | FI_RECV;
 	recv_entry->context = msg->context;
 
-	tcpx_queue_recv(tcpx_ep, recv_entry);
+	if (!tcpx_queue_recv(tcpx_ep, recv_entry)) {
+		tcpx_free_rx(recv_entry);
+		return -FI_EAGAIN;
+	}
 	return FI_SUCCESS;
 }
 
@@ -120,7 +175,7 @@ static ssize_t tcpx_recv(struct fid_ep *ep, void *buf, size_t len, void *desc,
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
 
-	recv_entry = tcpx_alloc_recv_entry(tcpx_ep);
+	recv_entry = tcpx_alloc_rx(tcpx_ep);
 	if (!recv_entry)
 		return -FI_EAGAIN;
 
@@ -132,7 +187,10 @@ static ssize_t tcpx_recv(struct fid_ep *ep, void *buf, size_t len, void *desc,
 			    FI_MSG | FI_RECV;
 	recv_entry->context = context;
 
-	tcpx_queue_recv(tcpx_ep, recv_entry);
+	if (!tcpx_queue_recv(tcpx_ep, recv_entry)) {
+		tcpx_free_rx(recv_entry);
+		return -FI_EAGAIN;
+	}
 	return FI_SUCCESS;
 }
 
@@ -146,7 +204,7 @@ static ssize_t tcpx_recvv(struct fid_ep *ep, const struct iovec *iov, void **des
 
 	assert(count <= TCPX_IOV_LIMIT);
 
-	recv_entry = tcpx_alloc_recv_entry(tcpx_ep);
+	recv_entry = tcpx_alloc_rx(tcpx_ep);
 	if (!recv_entry)
 		return -FI_EAGAIN;
 
@@ -157,73 +215,48 @@ static ssize_t tcpx_recvv(struct fid_ep *ep, const struct iovec *iov, void **des
 			    FI_MSG | FI_RECV;
 	recv_entry->context = context;
 
-	tcpx_queue_recv(tcpx_ep, recv_entry);
+	if (!tcpx_queue_recv(tcpx_ep, recv_entry)) {
+		tcpx_free_rx(recv_entry);
+		return -FI_EAGAIN;
+	}
 	return FI_SUCCESS;
 }
 
+static inline void tcpx_queue_send(struct tcpx_ep *ep,
+				   struct tcpx_xfer_entry *tx_entry)
+{
+	fastlock_acquire(&ep->lock);
+	tcpx_tx_queue_insert(ep, tx_entry);
+	fastlock_release(&ep->lock);
+}
+
 static ssize_t tcpx_sendmsg(struct fid_ep *ep, const struct fi_msg *msg,
 			    uint64_t flags)
 {
 	struct tcpx_ep *tcpx_ep;
-	struct tcpx_cq *tcpx_cq;
 	struct tcpx_xfer_entry *tx_entry;
-	uint64_t data_len;
-	size_t offset = 0;
-	uint64_t *cq_data;
+	size_t hdr_len;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
-	tcpx_cq = container_of(tcpx_ep->util_ep.tx_cq, struct tcpx_cq,
-			       util_cq);
-
-	tx_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_MSG_SEND);
+	tx_entry = tcpx_alloc_send_entry(tcpx_ep);
 	if (!tx_entry)
 		return -FI_EAGAIN;
 
-	assert(msg->iov_count <= TCPX_IOV_LIMIT);
-	data_len = ofi_total_iov_len(msg->msg_iov, msg->iov_count);
-	assert(!(flags & FI_INJECT) || (data_len <= TCPX_MAX_INJECT_SZ));
-
-	offset = sizeof(tx_entry->hdr.base_hdr);
-
 	if (flags & FI_REMOTE_CQ_DATA) {
-		tx_entry->hdr.base_hdr.flags |= OFI_REMOTE_CQ_DATA;
-		cq_data = (uint64_t *)((uint8_t *)&tx_entry->hdr + offset);
-		*cq_data = msg->data;
-		offset += sizeof(msg->data);
-	}
-
-	tx_entry->hdr.base_hdr.payload_off = (uint8_t)offset;
-	tx_entry->hdr.base_hdr.size = offset + data_len;
-	if (flags & FI_INJECT) {
-		ofi_copy_iov_buf(msg->msg_iov, msg->iov_count, 0,
-				 (uint8_t *)&tx_entry->hdr + offset,
-				 data_len,
-				 OFI_COPY_IOV_TO_BUF);
-		tx_entry->iov_cnt = 1;
-		offset += data_len;
+		tx_entry->hdr.base_hdr.flags = TCPX_REMOTE_CQ_DATA;
+		tx_entry->hdr.cq_data_hdr.cq_data = msg->data;
+		hdr_len = sizeof(tx_entry->hdr.cq_data_hdr);
 	} else {
-		memcpy(&tx_entry->iov[1], &msg->msg_iov[0],
-		       msg->iov_count * sizeof(struct iovec));
-
-		tx_entry->iov_cnt = msg->iov_count + 1;
+		hdr_len = sizeof(tx_entry->hdr.base_hdr);
 	}
-	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
-	tx_entry->iov[0].iov_len = offset;
 
+	tcpx_init_tx_iov(tx_entry, hdr_len, msg->msg_iov, msg->iov_count);
 	tx_entry->flags = ((tcpx_ep->util_ep.tx_op_flags & FI_COMPLETION) |
 			    flags | FI_MSG | FI_SEND);
-
-	if (flags & (FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE))
-		tx_entry->hdr.base_hdr.flags |= OFI_DELIVERY_COMPLETE;
-
-	tx_entry->ep = tcpx_ep;
+	tcpx_set_ack_flags(tx_entry, flags);
 	tx_entry->context = msg->context;
-	tx_entry->rem_len = tx_entry->hdr.base_hdr.size;
 
-	tcpx_ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	fastlock_acquire(&tcpx_ep->lock);
-	tcpx_tx_queue_insert(tcpx_ep, tx_entry);
-	fastlock_release(&tcpx_ep->lock);
+	tcpx_queue_send(tcpx_ep, tx_entry);
 	return FI_SUCCESS;
 }
 
@@ -239,29 +272,13 @@ static ssize_t tcpx_send(struct fid_ep *ep, const void *buf, size_t len,
 	if (!tx_entry)
 		return -FI_EAGAIN;
 
-	tx_entry->hdr.base_hdr.size = len + sizeof(tx_entry->hdr.base_hdr);
-	tx_entry->hdr.base_hdr.payload_off = (uint8_t)
-					     sizeof(tx_entry->hdr.base_hdr);
-
-	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
-	tx_entry->iov[0].iov_len = sizeof(tx_entry->hdr.base_hdr);
-
-	tx_entry->iov[1].iov_base = (void *) buf;
-	tx_entry->iov[1].iov_len = len;
-	tx_entry->iov_cnt = 2;
+	tcpx_init_tx_buf(tx_entry, sizeof(tx_entry->hdr.base_hdr), buf, len);
 	tx_entry->context = context;
-	tx_entry->rem_len = tx_entry->hdr.base_hdr.size;
 	tx_entry->flags = (tcpx_ep->util_ep.tx_op_flags & FI_COMPLETION) |
 			   FI_MSG | FI_SEND;
+	tcpx_set_ack_flags(tx_entry, tcpx_ep->util_ep.tx_op_flags);
 
-	if (tcpx_ep->util_ep.tx_op_flags &
-	    (FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE))
-		tx_entry->hdr.base_hdr.flags |= OFI_DELIVERY_COMPLETE;
-
-	tcpx_ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	fastlock_acquire(&tcpx_ep->lock);
-	tcpx_tx_queue_insert(tcpx_ep, tx_entry);
-	fastlock_release(&tcpx_ep->lock);
+	tcpx_queue_send(tcpx_ep, tx_entry);
 	return FI_SUCCESS;
 }
 
@@ -271,7 +288,6 @@ static ssize_t tcpx_sendv(struct fid_ep *ep, const struct iovec *iov,
 {
 	struct tcpx_ep *tcpx_ep;
 	struct tcpx_xfer_entry *tx_entry;
-	uint64_t data_len;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
 
@@ -279,30 +295,13 @@ static ssize_t tcpx_sendv(struct fid_ep *ep, const struct iovec *iov,
 	if (!tx_entry)
 		return -FI_EAGAIN;
 
-	assert(count <= TCPX_IOV_LIMIT);
-	data_len = ofi_total_iov_len(iov, count);
-	tx_entry->hdr.base_hdr.size = data_len + sizeof(tx_entry->hdr.base_hdr);
-	tx_entry->hdr.base_hdr.payload_off = (uint8_t)
-					     sizeof(tx_entry->hdr.base_hdr);
-
-	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
-	tx_entry->iov[0].iov_len = sizeof(tx_entry->hdr.base_hdr);
-	tx_entry->iov_cnt = count + 1;
-	memcpy(&tx_entry->iov[1], &iov[0], count * sizeof(struct iovec));
-
+	tcpx_init_tx_iov(tx_entry, sizeof(tx_entry->hdr.base_hdr), iov, count);
 	tx_entry->context = context;
-	tx_entry->rem_len = tx_entry->hdr.base_hdr.size;
 	tx_entry->flags = (tcpx_ep->util_ep.tx_op_flags & FI_COMPLETION) |
 			   FI_MSG | FI_SEND;
+	tcpx_set_ack_flags(tx_entry, tcpx_ep->util_ep.tx_op_flags);
 
-	if (tcpx_ep->util_ep.tx_op_flags &
-	    (FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE))
-		tx_entry->hdr.base_hdr.flags |= OFI_DELIVERY_COMPLETE;
-
-	tcpx_ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	fastlock_acquire(&tcpx_ep->lock);
-	tcpx_tx_queue_insert(tcpx_ep, tx_entry);
-	fastlock_release(&tcpx_ep->lock);
+	tcpx_queue_send(tcpx_ep, tx_entry);
 	return FI_SUCCESS;
 }
 
@@ -312,7 +311,6 @@ static ssize_t tcpx_inject(struct fid_ep *ep, const void *buf, size_t len,
 {
 	struct tcpx_ep *tcpx_ep;
 	struct tcpx_xfer_entry *tx_entry;
-	size_t offset;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
 
@@ -320,23 +318,11 @@ static ssize_t tcpx_inject(struct fid_ep *ep, const void *buf, size_t len,
 	if (!tx_entry)
 		return -FI_EAGAIN;
 
-	assert(len <= TCPX_MAX_INJECT_SZ);
-	tx_entry->hdr.base_hdr.size = len + sizeof(tx_entry->hdr.base_hdr);
+	tcpx_init_tx_inject(tx_entry, sizeof(tx_entry->hdr.base_hdr), buf, len);
 
-	offset = sizeof(tx_entry->hdr.base_hdr);
-	tx_entry->hdr.base_hdr.payload_off = (uint8_t) offset;
-	memcpy((uint8_t *)&tx_entry->hdr + offset, (uint8_t *) buf, len);
-
-	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
-	tx_entry->iov[0].iov_len = len + sizeof(tx_entry->hdr.base_hdr);
-	tx_entry->iov_cnt = 1;
-	tx_entry->rem_len = tx_entry->hdr.base_hdr.size;
 	tx_entry->flags = FI_MSG | FI_SEND;
 
-	tcpx_ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	fastlock_acquire(&tcpx_ep->lock);
-	tcpx_tx_queue_insert(tcpx_ep, tx_entry);
-	fastlock_release(&tcpx_ep->lock);
+	tcpx_queue_send(tcpx_ep, tx_entry);
 	return FI_SUCCESS;
 }
 
@@ -355,34 +341,17 @@ static ssize_t tcpx_senddata(struct fid_ep *ep, const void *buf, size_t len,
 
 	tx_entry->hdr.cq_data_hdr.base_hdr.size =
 		len + sizeof(tx_entry->hdr.cq_data_hdr);
-	tx_entry->hdr.cq_data_hdr.base_hdr.flags = OFI_REMOTE_CQ_DATA;
-
+	tx_entry->hdr.cq_data_hdr.base_hdr.flags = TCPX_REMOTE_CQ_DATA;
 	tx_entry->hdr.cq_data_hdr.cq_data = data;
 
-	tx_entry->hdr.cq_data_hdr.base_hdr.payload_off =
-		(uint8_t) sizeof(tx_entry->hdr.cq_data_hdr);
-
-	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
-	tx_entry->iov[0].iov_len = sizeof(tx_entry->hdr.cq_data_hdr);
-
-
-	tx_entry->iov[1].iov_base = (void *) buf;
-	tx_entry->iov[1].iov_len = len;
-	tx_entry->iov_cnt = 2;
-
+	tcpx_init_tx_buf(tx_entry, sizeof(tx_entry->hdr.cq_data_hdr),
+			 buf, len);
 	tx_entry->context = context;
-	tx_entry->rem_len = tx_entry->hdr.base_hdr.size;
 	tx_entry->flags = (tcpx_ep->util_ep.tx_op_flags & FI_COMPLETION) |
 			   FI_MSG | FI_SEND;
+	tcpx_set_ack_flags(tx_entry, tcpx_ep->util_ep.tx_op_flags);
 
-	if (tcpx_ep->util_ep.tx_op_flags &
-	    (FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE))
-		tx_entry->hdr.base_hdr.flags |= OFI_DELIVERY_COMPLETE;
-
-	tcpx_ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	fastlock_acquire(&tcpx_ep->lock);
-	tcpx_tx_queue_insert(tcpx_ep, tx_entry);
-	fastlock_release(&tcpx_ep->lock);
+	tcpx_queue_send(tcpx_ep, tx_entry);
 	return FI_SUCCESS;
 }
 
@@ -398,28 +367,15 @@ static ssize_t tcpx_injectdata(struct fid_ep *ep, const void *buf, size_t len,
 	if (!tx_entry)
 		return -FI_EAGAIN;
 
-	assert(len <= TCPX_MAX_INJECT_SZ);
-
-	tx_entry->hdr.cq_data_hdr.base_hdr.flags = OFI_REMOTE_CQ_DATA;
+	tx_entry->hdr.cq_data_hdr.base_hdr.flags = TCPX_REMOTE_CQ_DATA;
 	tx_entry->hdr.cq_data_hdr.cq_data = data;
 
-	tx_entry->hdr.base_hdr.size = len + sizeof(tx_entry->hdr.cq_data_hdr);
-	tx_entry->hdr.base_hdr.payload_off = (uint8_t)
-					     sizeof(tx_entry->hdr.cq_data_hdr);
-
-	memcpy((uint8_t *) &tx_entry->hdr + sizeof(tx_entry->hdr.cq_data_hdr),
-	       (uint8_t *) buf, len);
+	tcpx_init_tx_inject(tx_entry, sizeof(tx_entry->hdr.cq_data_hdr),
+			    buf, len);
 
-	tx_entry->iov[0].iov_base = (void *) &tx_entry->hdr;
-	tx_entry->iov[0].iov_len = len + sizeof(tx_entry->hdr.cq_data_hdr);
-	tx_entry->iov_cnt = 1;
-	tx_entry->rem_len = tx_entry->hdr.base_hdr.size;
 	tx_entry->flags = FI_MSG | FI_SEND;
 
-	tcpx_ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	fastlock_acquire(&tcpx_ep->lock);
-	tcpx_tx_queue_insert(tcpx_ep, tx_entry);
-	fastlock_release(&tcpx_ep->lock);
+	tcpx_queue_send(tcpx_ep, tx_entry);
 	return FI_SUCCESS;
 }
 
@@ -435,3 +391,187 @@ struct fi_ops_msg tcpx_msg_ops = {
 	.senddata = tcpx_senddata,
 	.injectdata = tcpx_injectdata,
 };
+
+
+/* There's no application driven need for tagged message operations over
+ * connected endpoints.  The tcp provider exposes the ability to send
+ * tagged messages using the tcp header, with the expectation that the
+ * peer side is using dynamic receive buffers to match the tagged messages
+ * with application buffers.  This provides an optimized path for rxm
+ * over tcp, that allows rxm to drop its header in certain cases an only
+ * use a minimal tcp header.
+ */
+static ssize_t
+tcpx_tsendmsg(struct fid_ep *fid_ep, const struct fi_msg_tagged *msg,
+	      uint64_t flags)
+{
+	struct tcpx_ep *ep;
+	struct tcpx_xfer_entry *tx_entry;
+	size_t hdr_len;
+
+	ep = container_of(fid_ep, struct tcpx_ep, util_ep.ep_fid);
+	tx_entry = tcpx_alloc_send_entry(ep);
+	if (!tx_entry)
+		return -FI_EAGAIN;
+
+	if (flags & FI_REMOTE_CQ_DATA) {
+		tx_entry->hdr.base_hdr.flags = TCPX_REMOTE_CQ_DATA |
+						TCPX_TAGGED;
+		tx_entry->hdr.tag_data_hdr.cq_data_hdr.cq_data = msg->data;
+		tx_entry->hdr.tag_data_hdr.tag = msg->tag;
+		hdr_len = sizeof(tx_entry->hdr.tag_data_hdr);
+	} else {
+		tx_entry->hdr.base_hdr.flags = TCPX_TAGGED;
+		tx_entry->hdr.tag_hdr.tag = msg->tag;
+		hdr_len = sizeof(tx_entry->hdr.tag_hdr);
+	}
+
+	tcpx_init_tx_iov(tx_entry, hdr_len, msg->msg_iov, msg->iov_count);
+	tx_entry->flags = ((ep->util_ep.tx_op_flags & FI_COMPLETION) |
+			    flags | FI_TAGGED | FI_SEND);
+	tcpx_set_ack_flags(tx_entry, flags);
+	tx_entry->context = msg->context;
+
+	tcpx_queue_send(ep, tx_entry);
+	return FI_SUCCESS;
+}
+
+static ssize_t
+tcpx_tsend(struct fid_ep *fid_ep, const void *buf, size_t len,
+	   void *desc, fi_addr_t dest_addr, uint64_t tag, void *context)
+{
+	struct tcpx_ep *ep;
+	struct tcpx_xfer_entry *tx_entry;
+
+	ep = container_of(fid_ep, struct tcpx_ep, util_ep.ep_fid);
+	tx_entry = tcpx_alloc_send_entry(ep);
+	if (!tx_entry)
+		return -FI_EAGAIN;
+
+	tx_entry->hdr.base_hdr.flags = TCPX_TAGGED;
+	tx_entry->hdr.tag_hdr.tag = tag;
+
+	tcpx_init_tx_buf(tx_entry, sizeof(tx_entry->hdr.tag_hdr), buf, len);
+	tx_entry->context = context;
+	tx_entry->flags = (ep->util_ep.tx_op_flags & FI_COMPLETION) |
+			   FI_TAGGED | FI_SEND;
+	tcpx_set_ack_flags(tx_entry, ep->util_ep.tx_op_flags);
+
+	tcpx_queue_send(ep, tx_entry);
+	return FI_SUCCESS;
+}
+
+static ssize_t
+tcpx_tsendv(struct fid_ep *fid_ep, const struct iovec *iov, void **desc,
+	    size_t count, fi_addr_t dest_addr, uint64_t tag, void *context)
+{
+	struct tcpx_ep *ep;
+	struct tcpx_xfer_entry *tx_entry;
+
+	ep = container_of(fid_ep, struct tcpx_ep, util_ep.ep_fid);
+	tx_entry = tcpx_alloc_send_entry(ep);
+	if (!tx_entry)
+		return -FI_EAGAIN;
+
+	tx_entry->hdr.base_hdr.flags = TCPX_TAGGED;
+	tx_entry->hdr.tag_hdr.tag = tag;
+
+	tcpx_init_tx_iov(tx_entry, sizeof(tx_entry->hdr.tag_hdr), iov, count);
+	tx_entry->context = context;
+	tx_entry->flags = (ep->util_ep.tx_op_flags & FI_COMPLETION) |
+			   FI_TAGGED | FI_SEND;
+	tcpx_set_ack_flags(tx_entry, ep->util_ep.tx_op_flags);
+
+	tcpx_queue_send(ep, tx_entry);
+	return FI_SUCCESS;
+}
+
+
+static ssize_t
+tcpx_tinject(struct fid_ep *fid_ep, const void *buf, size_t len,
+	     fi_addr_t dest_addr, uint64_t tag)
+{
+	struct tcpx_ep *ep;
+	struct tcpx_xfer_entry *tx_entry;
+
+	ep = container_of(fid_ep, struct tcpx_ep, util_ep.ep_fid);
+	tx_entry = tcpx_alloc_send_entry(ep);
+	if (!tx_entry)
+		return -FI_EAGAIN;
+
+	tx_entry->hdr.base_hdr.flags = TCPX_TAGGED;
+	tx_entry->hdr.tag_hdr.tag = tag;
+
+	tcpx_init_tx_inject(tx_entry, sizeof(tx_entry->hdr.tag_hdr), buf, len);
+
+	tx_entry->flags = FI_TAGGED | FI_SEND;
+
+	tcpx_queue_send(ep, tx_entry);
+	return FI_SUCCESS;
+}
+
+static ssize_t
+tcpx_tsenddata(struct fid_ep *fid_ep, const void *buf, size_t len, void *desc,
+	       uint64_t data, fi_addr_t dest_addr, uint64_t tag, void *context)
+{
+	struct tcpx_ep *ep;
+	struct tcpx_xfer_entry *tx_entry;
+
+	ep = container_of(fid_ep, struct tcpx_ep, util_ep.ep_fid);
+	tx_entry = tcpx_alloc_send_entry(ep);
+	if (!tx_entry)
+		return -FI_EAGAIN;
+
+	tx_entry->hdr.base_hdr.flags = TCPX_TAGGED | TCPX_REMOTE_CQ_DATA;
+	tx_entry->hdr.tag_data_hdr.tag = tag;
+	tx_entry->hdr.tag_data_hdr.cq_data_hdr.cq_data = data;
+
+	tcpx_init_tx_buf(tx_entry, sizeof(tx_entry->hdr.tag_data_hdr),
+			 buf, len);
+	tx_entry->context = context;
+	tx_entry->flags = (ep->util_ep.tx_op_flags & FI_COMPLETION) |
+			   FI_TAGGED | FI_SEND;
+	tcpx_set_ack_flags(tx_entry, ep->util_ep.tx_op_flags);
+
+	tcpx_queue_send(ep, tx_entry);
+	return FI_SUCCESS;
+}
+
+static ssize_t
+tcpx_tinjectdata(struct fid_ep *fid_ep, const void *buf, size_t len,
+		 uint64_t data, fi_addr_t dest_addr, uint64_t tag)
+{
+	struct tcpx_ep *ep;
+	struct tcpx_xfer_entry *tx_entry;
+
+	ep = container_of(fid_ep, struct tcpx_ep, util_ep.ep_fid);
+
+	tx_entry = tcpx_alloc_send_entry(ep);
+	if (!tx_entry)
+		return -FI_EAGAIN;
+
+	tx_entry->hdr.base_hdr.flags = TCPX_TAGGED | TCPX_REMOTE_CQ_DATA;
+	tx_entry->hdr.tag_data_hdr.tag = tag;
+	tx_entry->hdr.tag_data_hdr.cq_data_hdr.cq_data = data;
+
+	tcpx_init_tx_inject(tx_entry, sizeof(tx_entry->hdr.tag_data_hdr),
+			    buf, len);
+
+	tx_entry->flags = FI_TAGGED | FI_SEND;
+
+	tcpx_queue_send(ep, tx_entry);
+	return FI_SUCCESS;
+}
+
+struct fi_ops_tagged tcpx_tagged_ops = {
+	.size = sizeof(struct fi_ops_msg),
+	.recv = fi_no_tagged_recv,
+	.recvv = fi_no_tagged_recvv,
+	.recvmsg = fi_no_tagged_recvmsg,
+	.send = tcpx_tsend,
+	.sendv = tcpx_tsendv,
+	.sendmsg = tcpx_tsendmsg,
+	.inject = tcpx_tinject,
+	.senddata = tcpx_tsenddata,
+	.injectdata = tcpx_tinjectdata,
+};
diff --git a/prov/tcp/src/tcpx_progress.c b/prov/tcp/src/tcpx_progress.c
index 07181c8..f5ca4e9 100644
--- a/prov/tcp/src/tcpx_progress.c
+++ b/prov/tcp/src/tcpx_progress.c
@@ -43,86 +43,169 @@
 #include <ofi_iov.h>
 
 
-static void tcpx_process_tx_entry(struct tcpx_xfer_entry *tx_entry)
+static int tcpx_send_msg(struct tcpx_ep *ep)
 {
-	struct tcpx_cq *tcpx_cq;
+	struct tcpx_xfer_entry *tx_entry;
+	ssize_t ret;
+	size_t len;
+
+	assert(ep->cur_tx.entry);
+	tx_entry = ep->cur_tx.entry;
+	ret = ofi_bsock_sendv(&ep->bsock, tx_entry->iov, tx_entry->iov_cnt,
+			      &len);
+	if (ret < 0 && ret != -FI_EINPROGRESS)
+		return ret;
+
+	if (ret == -FI_EINPROGRESS) {
+		/* If a transfer generated multiple async sends, we only
+		 * need to track the last async index to know when the entire
+		 * transfer has completed.
+		 */
+		tx_entry->async_index = ep->bsock.async_index;
+		tx_entry->flags |= TCPX_ASYNC;
+	} else {
+		len = ret;
+	}
+
+	ep->cur_tx.data_left -= len;
+	if (ep->cur_tx.data_left) {
+		ofi_consume_iov(tx_entry->iov, &tx_entry->iov_cnt, len);
+		return -FI_EAGAIN;
+	}
+	return FI_SUCCESS;
+}
+
+static int tcpx_recv_msg_data(struct tcpx_ep *ep)
+{
+	struct tcpx_xfer_entry *rx_entry;
+	ssize_t ret;
+
+	if (!ep->cur_rx.data_left)
+		return FI_SUCCESS;
+
+	rx_entry = ep->cur_rx.entry;
+	ret = ofi_bsock_recvv(&ep->bsock, rx_entry->iov, rx_entry->iov_cnt);
+	if (ret < 0)
+		return ret;
+
+	ep->cur_rx.data_left -= ret;
+	if (!ep->cur_rx.data_left)
+		return FI_SUCCESS;
+
+	ofi_consume_iov(rx_entry->iov, &rx_entry->iov_cnt, ret);
+	if (!rx_entry->iov_cnt || !rx_entry->iov[0].iov_len)
+		return -FI_ETRUNC;
+
+	return -FI_EAGAIN;
+}
+
+void tcpx_progress_tx(struct tcpx_ep *ep)
+{
+	struct tcpx_xfer_entry *tx_entry;
+	struct tcpx_cq *cq;
 	int ret;
 
-	ret = tcpx_send_msg(tx_entry);
-	if (OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
-		return;
+	assert(fastlock_held(&ep->lock));
+	while (ep->cur_tx.entry) {
+		ret = tcpx_send_msg(ep);
+		if (OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
+			return;
 
-	/* Keep this path below as a single pass path.*/
-	tx_entry->ep->hdr_bswap(&tx_entry->hdr.base_hdr);
-	slist_remove_head(&tx_entry->ep->tx_queue);
+		tx_entry = ep->cur_tx.entry;
+		cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
 
-	if (ret) {
-		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN, "msg send failed\n");
-		tcpx_cq_report_error(tx_entry->ep->util_ep.tx_cq,
-				     tx_entry, -ret);
-	} else {
-		if (tx_entry->hdr.base_hdr.flags &
-		    (OFI_DELIVERY_COMPLETE | OFI_COMMIT_COMPLETE)) {
+		if (ret) {
+			FI_WARN(&tcpx_prov, FI_LOG_DOMAIN, "msg send failed\n");
+			tcpx_cq_report_error(&cq->util_cq, tx_entry, -ret);
+			tcpx_free_xfer(cq, tx_entry);
+		} else if (tx_entry->flags & TCPX_NEED_ACK) {
+			/* A SW ack guarantees the peer received the data, so
+			 * we can skip the async completion.
+			 */
 			slist_insert_tail(&tx_entry->entry,
-					  &tx_entry->ep->tx_rsp_pend_queue);
-			return;
+					  &ep->need_ack_queue);
+		} else if ((tx_entry->flags & TCPX_ASYNC) &&
+			   (ofi_val32_gt(tx_entry->async_index,
+					 ep->bsock.done_index))) {
+			slist_insert_tail(&tx_entry->entry,
+						&ep->async_queue);
+		} else {
+			tcpx_cq_report_success(&cq->util_cq, tx_entry);
+			tcpx_free_xfer(cq, tx_entry);
+		}
+
+		if (!slist_empty(&ep->priority_queue)) {
+			ep->cur_tx.entry = container_of(slist_remove_head(
+							&ep->priority_queue),
+					     struct tcpx_xfer_entry, entry);
+			assert(ep->cur_tx.entry->flags & TCPX_INTERNAL_XFER);
+		} else if (!slist_empty(&ep->tx_queue)) {
+			ep->cur_tx.entry = container_of(slist_remove_head(
+							&ep->tx_queue),
+					     struct tcpx_xfer_entry, entry);
+			assert(!(ep->cur_tx.entry->flags & TCPX_INTERNAL_XFER));
+		} else {
+			ep->cur_tx.entry = NULL;
+			break;
 		}
-		tcpx_cq_report_success(tx_entry->ep->util_ep.tx_cq, tx_entry);
+
+		ep->cur_tx.data_left = ep->cur_tx.entry->hdr.base_hdr.size;
+		OFI_DBG_SET(ep->cur_tx.entry->hdr.base_hdr.id, ep->tx_id++);
+		ep->hdr_bswap(&ep->cur_tx.entry->hdr.base_hdr);
 	}
 
-	tcpx_cq = container_of(tx_entry->ep->util_ep.tx_cq,
-			       struct tcpx_cq, util_cq);
-	tcpx_xfer_entry_free(tcpx_cq, tx_entry);
+	/* Buffered data is sent first by tcpx_send_msg, but if we don't
+	 * have other data to send, we need to try flushing any buffered data.
+	 */
+	(void) ofi_bsock_flush(&ep->bsock);
 }
 
-static int tcpx_prepare_rx_entry_resp(struct tcpx_xfer_entry *rx_entry)
+static int tcpx_queue_ack(struct tcpx_xfer_entry *rx_entry)
 {
-	struct tcpx_cq *tcpx_tx_cq;
-	struct tcpx_xfer_entry *resp_entry;
+	struct tcpx_ep *ep;
+	struct tcpx_cq *cq;
+	struct tcpx_xfer_entry *resp;
 
-	tcpx_tx_cq = container_of(rx_entry->ep->util_ep.tx_cq,
-			       struct tcpx_cq, util_cq);
+	ep = rx_entry->ep;
+	cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
 
-	resp_entry = tcpx_xfer_entry_alloc(tcpx_tx_cq, TCPX_OP_MSG_RESP);
-	if (!resp_entry)
-		return -FI_EAGAIN;
+	resp = tcpx_alloc_xfer(cq);
+	if (!resp)
+		return -FI_ENOMEM;
 
-	resp_entry->iov[0].iov_base = (void *) &resp_entry->hdr;
-	resp_entry->iov[0].iov_len = sizeof(resp_entry->hdr.base_hdr);
-	resp_entry->iov_cnt = 1;
+	resp->iov[0].iov_base = (void *) &resp->hdr;
+	resp->iov[0].iov_len = sizeof(resp->hdr.base_hdr);
+	resp->iov_cnt = 1;
 
-	resp_entry->hdr.base_hdr.op = ofi_op_msg;
-	resp_entry->hdr.base_hdr.size = sizeof(resp_entry->hdr.base_hdr);
-	resp_entry->hdr.base_hdr.payload_off =
-		(uint8_t)sizeof(resp_entry->hdr.base_hdr);
+	resp->hdr.base_hdr.version = TCPX_HDR_VERSION;
+	resp->hdr.base_hdr.op_data = TCPX_OP_ACK;
+	resp->hdr.base_hdr.op = ofi_op_msg;
+	resp->hdr.base_hdr.size = sizeof(resp->hdr.base_hdr);
+	resp->hdr.base_hdr.hdr_size = (uint8_t) sizeof(resp->hdr.base_hdr);
 
-	resp_entry->flags = 0;
-	resp_entry->context = NULL;
-	resp_entry->rem_len = sizeof(resp_entry->hdr.base_hdr);
-	resp_entry->ep = rx_entry->ep;
+	resp->flags = TCPX_INTERNAL_XFER;
+	resp->context = NULL;
+	resp->ep = ep;
 
-	resp_entry->ep->hdr_bswap(&resp_entry->hdr.base_hdr);
-	tcpx_tx_queue_insert(resp_entry->ep, resp_entry);
-	tcpx_cq_report_success(rx_entry->ep->util_ep.rx_cq, rx_entry);
-
-	tcpx_rx_entry_free(rx_entry);
+	tcpx_tx_queue_insert(ep, resp);
 	return FI_SUCCESS;
 }
 
 static int tcpx_update_rx_iov(struct tcpx_xfer_entry *rx_entry)
 {
-	struct fi_cq_data_entry cq_entry;
+	struct ofi_cq_rbuf_entry cq_entry;
 	int ret;
 
 	assert(tcpx_dynamic_rbuf(rx_entry->ep));
 
+	cq_entry.ep_context = rx_entry->ep->util_ep.ep_fid.fid.context;
 	cq_entry.op_context = rx_entry->context;
-	cq_entry.flags = rx_entry->flags;
-	cq_entry.len = (rx_entry->hdr.base_hdr.size -
-			 rx_entry->hdr.base_hdr.payload_off) -
-			rx_entry->rem_len;
+	cq_entry.flags = 0;
+	cq_entry.len = rx_entry->hdr.base_hdr.size -
+		       rx_entry->hdr.base_hdr.hdr_size;
 	cq_entry.buf = rx_entry->mrecv_msg_start;
-	cq_entry.data = 0;
+	tcpx_get_cq_info(rx_entry, &cq_entry.flags, &cq_entry.data,
+			 &cq_entry.tag);
 
 	rx_entry->iov_cnt = TCPX_IOV_LIMIT;
 	ret = (int) tcpx_dynamic_rbuf(rx_entry->ep)->
@@ -136,7 +219,7 @@ static int tcpx_update_rx_iov(struct tcpx_xfer_entry *rx_entry)
 
 	assert(rx_entry->iov_cnt <= TCPX_IOV_LIMIT);
 	ret = ofi_truncate_iov(rx_entry->iov, &rx_entry->iov_cnt,
-				rx_entry->rem_len);
+			       rx_entry->ep->cur_rx.data_left);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
 			"dynamically provided rbuf is too small\n");
@@ -146,83 +229,52 @@ static int tcpx_update_rx_iov(struct tcpx_xfer_entry *rx_entry)
 	return 0;
 }
 
-static int tcpx_process_recv(struct tcpx_xfer_entry *rx_entry)
+static int tcpx_process_recv(struct tcpx_ep *ep)
 {
+	struct tcpx_xfer_entry *rx_entry;
 	int ret;
 
+	rx_entry = ep->cur_rx.entry;
 retry:
-	ret = tcpx_recv_msg_data(rx_entry);
+	ret = tcpx_recv_msg_data(ep);
 	if (ret) {
 		if (OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
 			return ret;
 
-		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
-			"msg recv failed ret = %d (%s)\n", ret,
-			fi_strerror(-ret));
-		goto shutdown;
+		if (ret != -FI_ETRUNC)
+			goto err;
+		assert(rx_entry->flags & TCPX_NEED_DYN_RBUF);
 	}
 
 	if (rx_entry->flags & TCPX_NEED_DYN_RBUF) {
 		ret = tcpx_update_rx_iov(rx_entry);
 		if (ret)
-			goto shutdown;
+			goto err;
 
 		rx_entry->flags &= ~TCPX_NEED_DYN_RBUF;
-		rx_entry->rem_len = 0;
 		goto retry;
 	}
 
-	if (rx_entry->hdr.base_hdr.flags & OFI_DELIVERY_COMPLETE) {
-		if (tcpx_prepare_rx_entry_resp(rx_entry))
-			rx_entry->ep->cur_rx_proc_fn = tcpx_prepare_rx_entry_resp;
-	} else {
-		tcpx_cq_report_success(rx_entry->ep->util_ep.rx_cq, rx_entry);
-		tcpx_rx_entry_free(rx_entry);
+	if (rx_entry->hdr.base_hdr.flags & TCPX_DELIVERY_COMPLETE) {
+		ret = tcpx_queue_ack(rx_entry);
+		if (ret)
+			goto err;
 	}
+
+	tcpx_cq_report_success(ep->util_ep.rx_cq, rx_entry);
+	tcpx_free_rx(rx_entry);
+	tcpx_reset_rx(ep);
 	return 0;
 
-shutdown:
-	tcpx_ep_disable(rx_entry->ep, 0);
+err:
+	FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
+		"msg recv failed ret = %d (%s)\n", ret, fi_strerror(-ret));
 	tcpx_cq_report_error(rx_entry->ep->util_ep.rx_cq, rx_entry, -ret);
-	tcpx_rx_entry_free(rx_entry);
+	tcpx_free_rx(rx_entry);
+	tcpx_reset_rx(ep);
 	return ret;
 }
 
-static int tcpx_prepare_rx_write_resp(struct tcpx_xfer_entry *rx_entry)
-{
-	struct tcpx_cq *tcpx_rx_cq, *tcpx_tx_cq;
-	struct tcpx_xfer_entry *resp_entry;
-
-	tcpx_tx_cq = container_of(rx_entry->ep->util_ep.tx_cq,
-				  struct tcpx_cq, util_cq);
-
-	resp_entry = tcpx_xfer_entry_alloc(tcpx_tx_cq, TCPX_OP_MSG_RESP);
-	if (!resp_entry)
-		return -FI_EAGAIN;
-
-	resp_entry->iov[0].iov_base = (void *) &resp_entry->hdr;
-	resp_entry->iov[0].iov_len = sizeof(resp_entry->hdr.base_hdr);
-	resp_entry->iov_cnt = 1;
-
-	resp_entry->hdr.base_hdr.op = ofi_op_msg;
-	resp_entry->hdr.base_hdr.size = sizeof(resp_entry->hdr.base_hdr);
-	resp_entry->hdr.base_hdr.payload_off = (uint8_t)
-						sizeof(resp_entry->hdr.base_hdr);
-
-	resp_entry->flags &= ~FI_COMPLETION;
-	resp_entry->context = NULL;
-	resp_entry->rem_len = resp_entry->hdr.base_hdr.size;
-	resp_entry->ep = rx_entry->ep;
-	resp_entry->ep->hdr_bswap(&resp_entry->hdr.base_hdr);
-	tcpx_tx_queue_insert(resp_entry->ep, resp_entry);
-
-	tcpx_cq_report_success(rx_entry->ep->util_ep.rx_cq, rx_entry);
-	tcpx_rx_cq = container_of(rx_entry->ep->util_ep.rx_cq,
-				  struct tcpx_cq, util_cq);
-	tcpx_xfer_entry_free(tcpx_rx_cq, rx_entry);
-	return FI_SUCCESS;
-}
-
 static void tcpx_pmem_commit(struct tcpx_xfer_entry *rx_entry)
 {
 	struct ofi_rma_iov *rma_iov;
@@ -232,8 +284,7 @@ static void tcpx_pmem_commit(struct tcpx_xfer_entry *rx_entry)
 	if (!ofi_pmem_commit)
 		return ;
 
-	if (rx_entry->hdr.base_hdr.flags &
-	    OFI_REMOTE_CQ_DATA)
+	if (rx_entry->hdr.base_hdr.flags & TCPX_REMOTE_CQ_DATA)
 		offset = sizeof(rx_entry->hdr.base_hdr) + sizeof(uint64_t);
 	else
 		offset = sizeof(rx_entry->hdr.base_hdr);
@@ -247,122 +298,71 @@ static void tcpx_pmem_commit(struct tcpx_xfer_entry *rx_entry)
 	}
 }
 
-static int tcpx_process_remote_write(struct tcpx_xfer_entry *rx_entry)
+static int tcpx_process_remote_write(struct tcpx_ep *ep)
 {
-	struct tcpx_cq *tcpx_cq;
-	int ret = FI_SUCCESS;
+	struct tcpx_xfer_entry *rx_entry;
+	struct tcpx_cq *cq;
+	int ret;
 
-	ret = tcpx_recv_msg_data(rx_entry);
+	rx_entry = ep->cur_rx.entry;
+	ret = tcpx_recv_msg_data(ep);
 	if (OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
 		return ret;
 
-	if (ret) {
-		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN,
-			"remote write Failed ret = %d\n",
-			ret);
-
-		tcpx_ep_disable(rx_entry->ep, 0);
-		tcpx_cq_report_error(rx_entry->ep->util_ep.rx_cq, rx_entry, -ret);
-		tcpx_cq = container_of(rx_entry->ep->util_ep.rx_cq,
-				       struct tcpx_cq, util_cq);
-		tcpx_xfer_entry_free(tcpx_cq, rx_entry);
+	cq = container_of(ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
+	if (ret)
+		goto err;
 
-	} else if (rx_entry->hdr.base_hdr.flags &
-		  (OFI_DELIVERY_COMPLETE | OFI_COMMIT_COMPLETE)) {
+	if (rx_entry->hdr.base_hdr.flags &
+	    (TCPX_DELIVERY_COMPLETE | TCPX_COMMIT_COMPLETE)) {
 
-		if (rx_entry->hdr.base_hdr.flags & OFI_COMMIT_COMPLETE)
+		if (rx_entry->hdr.base_hdr.flags & TCPX_COMMIT_COMPLETE)
 			tcpx_pmem_commit(rx_entry);
 
-		if (tcpx_prepare_rx_write_resp(rx_entry))
-			rx_entry->ep->cur_rx_proc_fn = tcpx_prepare_rx_write_resp;
-	} else {
-		tcpx_cq_report_success(rx_entry->ep->util_ep.rx_cq, rx_entry);
-		tcpx_cq = container_of(rx_entry->ep->util_ep.rx_cq,
-				       struct tcpx_cq, util_cq);
-		tcpx_xfer_entry_free(tcpx_cq, rx_entry);
+		ret = tcpx_queue_ack(rx_entry);
+		if (ret)
+			goto err;
 	}
+
+	tcpx_cq_report_success(ep->util_ep.rx_cq, rx_entry);
+	tcpx_free_xfer(cq, rx_entry);
+	tcpx_reset_rx(ep);
+	return FI_SUCCESS;
+
+err:
+	FI_WARN(&tcpx_prov, FI_LOG_DOMAIN, "remote write failed %d\n", ret);
+	tcpx_free_xfer(cq, rx_entry);
+	tcpx_reset_rx(ep);
 	return ret;
 }
 
-static int tcpx_process_remote_read(struct tcpx_xfer_entry *rx_entry)
+static int tcpx_process_remote_read(struct tcpx_ep *ep)
 {
-	struct tcpx_cq *tcpx_cq;
-	int ret = FI_SUCCESS;
+	struct tcpx_xfer_entry *rx_entry;
+	struct tcpx_cq *cq;
+	int ret;
+
+	rx_entry = ep->cur_rx.entry;
+	cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
 
-	ret = tcpx_recv_msg_data(rx_entry);
+	ret = tcpx_recv_msg_data(ep);
 	if (OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
 		return ret;
 
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN,
 			"msg recv Failed ret = %d\n", ret);
-		tcpx_ep_disable(rx_entry->ep, 0);
-		tcpx_cq_report_error(rx_entry->ep->util_ep.tx_cq, rx_entry, -ret);
+		tcpx_cq_report_error(&cq->util_cq, rx_entry, -ret);
 	} else {
-		tcpx_cq_report_success(rx_entry->ep->util_ep.tx_cq, rx_entry);
+		tcpx_cq_report_success(&cq->util_cq, rx_entry);
 	}
 
 	slist_remove_head(&rx_entry->ep->rma_read_queue);
-	tcpx_cq = container_of(rx_entry->ep->util_ep.tx_cq,
-			       struct tcpx_cq, util_cq);
-	tcpx_xfer_entry_free(tcpx_cq, rx_entry);
+	tcpx_free_xfer(cq, rx_entry);
+	tcpx_reset_rx(ep);
 	return ret;
 }
 
-static void tcpx_copy_rma_iov_to_msg_iov(struct tcpx_xfer_entry *xfer_entry)
-{
-	struct ofi_rma_iov *rma_iov;
-	size_t offset;
-	int i;
-
-	if (xfer_entry->hdr.base_hdr.flags &
-	    OFI_REMOTE_CQ_DATA)
-		offset = sizeof(xfer_entry->hdr.base_hdr) + sizeof(uint64_t);
-	else
-		offset = sizeof(xfer_entry->hdr.base_hdr);
-
-	rma_iov = (struct ofi_rma_iov *) ((uint8_t *) &xfer_entry->hdr + offset);
-
-	xfer_entry->iov_cnt = xfer_entry->hdr.base_hdr.rma_iov_cnt;
-	for ( i = 0 ; i < xfer_entry->hdr.base_hdr.rma_iov_cnt; i++ ) {
-		xfer_entry->iov[i].iov_base = (void *) rma_iov[i].addr;
-		xfer_entry->iov[i].iov_len = rma_iov[i].len;
-	}
-}
-
-static int tcpx_prepare_rx_remote_read_resp(struct tcpx_xfer_entry *resp_entry)
-{
-	struct ofi_rma_iov *rma_iov;
-	int i;
-
-	resp_entry->iov[0].iov_base = (void *) &resp_entry->hdr;
-	resp_entry->iov[0].iov_len = sizeof(resp_entry->hdr.base_hdr);
-
-	rma_iov = (struct ofi_rma_iov *) ((uint8_t *)
-		  &resp_entry->hdr + sizeof(resp_entry->hdr.base_hdr));
-
-	resp_entry->iov_cnt = 1 + resp_entry->hdr.base_hdr.rma_iov_cnt;
-	resp_entry->hdr.base_hdr.size = resp_entry->iov[0].iov_len;
-	for ( i = 0 ; i < resp_entry->hdr.base_hdr.rma_iov_cnt ; i++ ) {
-		resp_entry->iov[i+1].iov_base =	(void *) (uintptr_t)rma_iov[i].addr;
-		resp_entry->iov[i+1].iov_len = rma_iov[i].len;
-		resp_entry->hdr.base_hdr.size += resp_entry->iov[i+1].iov_len;
-	}
-
-	resp_entry->hdr.base_hdr.op = ofi_op_read_rsp;
-	resp_entry->hdr.base_hdr.payload_off = (uint8_t)
-						sizeof(resp_entry->hdr.base_hdr);
-
-	resp_entry->flags &= ~FI_COMPLETION;
-	resp_entry->context = NULL;
-	resp_entry->rem_len = resp_entry->hdr.base_hdr.size;
-
-	resp_entry->ep->hdr_bswap(&resp_entry->hdr.base_hdr);
-	tcpx_tx_queue_insert(resp_entry->ep, resp_entry);
-	resp_entry->ep->cur_rx_entry = NULL;
-	return FI_SUCCESS;
-}
-
 static int tcpx_validate_rx_rma_data(struct tcpx_xfer_entry *rx_entry,
 				     uint64_t access)
 {
@@ -371,7 +371,7 @@ static int tcpx_validate_rx_rma_data(struct tcpx_xfer_entry *rx_entry,
 	size_t offset;
 	int i, ret;
 
-	if (rx_entry->hdr.base_hdr.flags & OFI_REMOTE_CQ_DATA)
+	if (rx_entry->hdr.base_hdr.flags & TCPX_REMOTE_CQ_DATA)
 		offset = sizeof(rx_entry->hdr.base_hdr) + sizeof(uint64_t);
 	else
 		offset = sizeof(rx_entry->hdr.base_hdr);
@@ -396,173 +396,203 @@ int tcpx_op_invalid(struct tcpx_ep *tcpx_ep)
 	return -FI_EINVAL;
 }
 
-/* Must hold ep lock */
-static struct tcpx_xfer_entry *tcpx_rx_entry_alloc(struct tcpx_ep *ep)
+static struct tcpx_xfer_entry *tcpx_get_rx_entry(struct tcpx_ep *ep)
 {
-	struct tcpx_xfer_entry *rx_entry;
-
-	if (slist_empty(&ep->rx_queue))
-		return NULL;
+	struct tcpx_xfer_entry *xfer;
+	struct tcpx_rx_ctx *srx;
+
+	if (ep->srx_ctx) {
+		srx = ep->srx_ctx;
+		fastlock_acquire(&srx->lock);
+		if (!slist_empty(&srx->rx_queue)) {
+			xfer = container_of(slist_remove_head(&srx->rx_queue),
+					    struct tcpx_xfer_entry, entry);
+			xfer->flags |= ep->util_ep.rx_op_flags & FI_COMPLETION;
+		} else {
+			xfer = NULL;
+		}
+		fastlock_release(&ep->srx_ctx->lock);
+	} else {
+		assert(fastlock_held(&ep->lock));
+		if (!slist_empty(&ep->rx_queue)) {
+			xfer = container_of(slist_remove_head(&ep->rx_queue),
+					    struct tcpx_xfer_entry, entry);
+			ep->rx_avail++;
+		} else {
+			xfer = NULL;
+		}
+	}
 
-	rx_entry = container_of(ep->rx_queue.head, struct tcpx_xfer_entry,
-				entry);
-	slist_remove_head(&ep->rx_queue);
-	return rx_entry;
+	return xfer;
 }
 
-static void tcpx_rx_setup(struct tcpx_ep *ep, struct tcpx_xfer_entry *rx_entry,
-			  tcpx_rx_process_fn_t process_fn)
+static int tcpx_handle_ack(struct tcpx_ep *ep)
 {
-	ep->cur_rx_entry = rx_entry;
-	ep->cur_rx_proc_fn = process_fn;
+	struct tcpx_xfer_entry *tx_entry;
+	struct tcpx_cq *cq;
+
+	if (ep->cur_rx.hdr.base_hdr.size !=
+	    sizeof(ep->cur_rx.hdr.base_hdr))
+		return -FI_EIO;
 
-	/* Reset to receive next message */
-	ep->cur_rx_msg.hdr_len = sizeof(ep->cur_rx_msg.hdr.base_hdr);
-	ep->cur_rx_msg.done_len = 0;
+	assert(!slist_empty(&ep->need_ack_queue));
+	tx_entry = container_of(slist_remove_head(&ep->need_ack_queue),
+				struct tcpx_xfer_entry, entry);
+
+	cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
+	tcpx_cq_report_success(ep->util_ep.tx_cq, tx_entry);
+	tcpx_free_xfer(cq, tx_entry);
+	tcpx_reset_rx(ep);
+	return FI_SUCCESS;
 }
 
 int tcpx_op_msg(struct tcpx_ep *tcpx_ep)
 {
 	struct tcpx_xfer_entry *rx_entry;
-	struct tcpx_xfer_entry *tx_entry;
-	struct tcpx_cq *tcpx_cq;
-	struct tcpx_cur_rx_msg *cur_rx_msg = &tcpx_ep->cur_rx_msg;
+	struct tcpx_cur_rx *msg = &tcpx_ep->cur_rx;
 	size_t msg_len;
 	int ret;
 
-	if (cur_rx_msg->hdr.base_hdr.op_data == TCPX_OP_MSG_RESP) {
-		assert(!slist_empty(&tcpx_ep->tx_rsp_pend_queue));
-		tx_entry = container_of(tcpx_ep->tx_rsp_pend_queue.head,
-					struct tcpx_xfer_entry, entry);
+	if (msg->hdr.base_hdr.op_data == TCPX_OP_ACK)
+		return tcpx_handle_ack(tcpx_ep);
 
-		tcpx_cq = container_of(tcpx_ep->util_ep.tx_cq, struct tcpx_cq,
-				       util_cq);
-		tcpx_cq_report_success(tx_entry->ep->util_ep.tx_cq, tx_entry);
+	msg_len = (msg->hdr.base_hdr.size - msg->hdr.base_hdr.hdr_size);
 
-		slist_remove_head(&tx_entry->ep->tx_rsp_pend_queue);
-		tcpx_xfer_entry_free(tcpx_cq, tx_entry);
-		tcpx_rx_setup(tcpx_ep, NULL, NULL);
+	rx_entry = tcpx_get_rx_entry(tcpx_ep);
+	if (!rx_entry)
 		return -FI_EAGAIN;
-	}
 
-	msg_len = (tcpx_ep->cur_rx_msg.hdr.base_hdr.size -
-		   tcpx_ep->cur_rx_msg.hdr.base_hdr.payload_off);
-
-	if (tcpx_ep->srx_ctx) {
-		rx_entry = tcpx_srx_entry_alloc(tcpx_ep->srx_ctx, tcpx_ep);
-		if (!rx_entry)
-			return -FI_EAGAIN;
-
-		rx_entry->flags |= tcpx_ep->util_ep.rx_op_flags & FI_COMPLETION;
-	} else {
-		rx_entry = tcpx_rx_entry_alloc(tcpx_ep);
-		if (!rx_entry)
-			return -FI_EAGAIN;
-	}
-
-	memcpy(&rx_entry->hdr, &tcpx_ep->cur_rx_msg.hdr,
-	       (size_t) tcpx_ep->cur_rx_msg.hdr.base_hdr.payload_off);
+	memcpy(&rx_entry->hdr, &msg->hdr,
+	       (size_t) msg->hdr.base_hdr.hdr_size);
 	rx_entry->ep = tcpx_ep;
-	rx_entry->hdr.base_hdr.op_data = TCPX_OP_MSG_RECV;
 	rx_entry->mrecv_msg_start = rx_entry->iov[0].iov_base;
 
-	if (tcpx_dynamic_rbuf(tcpx_ep))
+	if (tcpx_dynamic_rbuf(tcpx_ep)) {
 		rx_entry->flags |= TCPX_NEED_DYN_RBUF;
 
-	ret = ofi_truncate_iov(rx_entry->iov, &rx_entry->iov_cnt, msg_len);
-	if (ret) {
-		if (!tcpx_dynamic_rbuf(tcpx_ep))
+		if (msg->hdr.base_hdr.flags & TCPX_TAGGED) {
+			/* Raw message, no rxm header */
+			rx_entry->iov_cnt = 0;
+		} else {
+			/* Receiving only rxm header */
+			assert(msg_len >= ofi_total_iov_len(rx_entry->iov,
+							    rx_entry->iov_cnt));
+		}
+	} else {
+		ret = ofi_truncate_iov(rx_entry->iov, &rx_entry->iov_cnt,
+				       msg_len);
+		if (ret)
 			goto truncate_err;
-
-		rx_entry->rem_len = msg_len -
-				    ofi_total_iov_len(rx_entry->iov,
-						      rx_entry->iov_cnt);
 	}
 
-	if (cur_rx_msg->hdr.base_hdr.flags & OFI_REMOTE_CQ_DATA)
-		rx_entry->flags |= FI_REMOTE_CQ_DATA;
-
-	tcpx_rx_setup(tcpx_ep, rx_entry, tcpx_process_recv);
-	return FI_SUCCESS;
+	tcpx_ep->cur_rx.entry = rx_entry;
+	tcpx_ep->cur_rx.handler = tcpx_process_recv;
+	return tcpx_process_recv(tcpx_ep);
 
 truncate_err:
 	FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
 		"posted rx buffer size is not big enough\n");
-	tcpx_cq_report_error(rx_entry->ep->util_ep.rx_cq,
-				rx_entry, -ret);
-	tcpx_rx_entry_free(rx_entry);
+	tcpx_cq_report_error(rx_entry->ep->util_ep.rx_cq, rx_entry, -ret);
+	tcpx_free_rx(rx_entry);
 	return ret;
 }
 
-int tcpx_op_read_req(struct tcpx_ep *tcpx_ep)
+int tcpx_op_read_req(struct tcpx_ep *ep)
 {
-	struct tcpx_xfer_entry *rx_entry;
-	struct tcpx_cq *tcpx_cq;
-	int ret;
+	struct tcpx_xfer_entry *resp;
+	struct tcpx_cq *cq;
+	struct ofi_rma_iov *rma_iov;
+	int i, ret;
 
-	/* The read request will generate a response once done,
-	 * so the xfer_entry will become a transmit and returned
-	 * to the tx cq buffer pool.
-	 */
-	tcpx_cq = container_of(tcpx_ep->util_ep.tx_cq,
-			       struct tcpx_cq, util_cq);
+	cq = container_of(ep->util_ep.tx_cq, struct tcpx_cq, util_cq);
+	resp = tcpx_alloc_xfer(cq);
+	if (!resp)
+		return -FI_ENOMEM;
 
-	rx_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_REMOTE_READ);
-	if (!rx_entry)
-		return -FI_EAGAIN;
+	memcpy(&resp->hdr, &ep->cur_rx.hdr,
+	       (size_t) ep->cur_rx.hdr.base_hdr.hdr_size);
+	resp->hdr.base_hdr.op_data = 0;
+	resp->ep = ep;
 
-	memcpy(&rx_entry->hdr, &tcpx_ep->cur_rx_msg.hdr,
-	       (size_t) tcpx_ep->cur_rx_msg.hdr.base_hdr.payload_off);
-	rx_entry->hdr.base_hdr.op_data = TCPX_OP_REMOTE_READ;
-	rx_entry->ep = tcpx_ep;
-
-	ret = tcpx_validate_rx_rma_data(rx_entry, FI_REMOTE_READ);
+	ret = tcpx_validate_rx_rma_data(resp, FI_REMOTE_READ);
 	if (ret) {
-		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN,
-			"invalid rma data\n");
-		tcpx_xfer_entry_free(tcpx_cq, rx_entry);
+		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN, "invalid rma data\n");
+		tcpx_free_xfer(cq, resp);
 		return ret;
 	}
 
-	tcpx_rx_setup(tcpx_ep, rx_entry, tcpx_prepare_rx_remote_read_resp);
+	resp->iov[0].iov_base = (void *) &resp->hdr;
+	resp->iov[0].iov_len = sizeof(resp->hdr.base_hdr);
+
+	rma_iov = (struct ofi_rma_iov *) ((uint8_t *)
+		  &resp->hdr + sizeof(resp->hdr.base_hdr));
+
+	resp->iov_cnt = 1 + resp->hdr.base_hdr.rma_iov_cnt;
+	resp->hdr.base_hdr.size = resp->iov[0].iov_len;
+	for (i = 0; i < resp->hdr.base_hdr.rma_iov_cnt; i++) {
+		resp->iov[i + 1].iov_base = (void *) (uintptr_t)
+					    rma_iov[i].addr;
+		resp->iov[i + 1].iov_len = rma_iov[i].len;
+		resp->hdr.base_hdr.size += resp->iov[i + 1].iov_len;
+	}
+
+	resp->hdr.base_hdr.op = ofi_op_read_rsp;
+	resp->hdr.base_hdr.hdr_size = (uint8_t) sizeof(resp->hdr.base_hdr);
+
+	resp->flags |= TCPX_INTERNAL_XFER;
+	resp->context = NULL;
+
+	tcpx_tx_queue_insert(ep, resp);
+	tcpx_reset_rx(ep);
 	return FI_SUCCESS;
 }
 
-int tcpx_op_write(struct tcpx_ep *tcpx_ep)
+int tcpx_op_write(struct tcpx_ep *ep)
 {
 	struct tcpx_xfer_entry *rx_entry;
-	struct tcpx_cq *tcpx_cq;
-	int ret;
-
-	tcpx_cq = container_of(tcpx_ep->util_ep.rx_cq,
-			       struct tcpx_cq, util_cq);
+	struct tcpx_cq *cq;
+	struct ofi_rma_iov *rma_iov;
+	size_t offset;
+	int ret, i;
 
-	rx_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_REMOTE_WRITE);
+	cq = container_of(ep->util_ep.rx_cq, struct tcpx_cq, util_cq);
+	rx_entry = tcpx_alloc_xfer(cq);
 	if (!rx_entry)
-		return -FI_EAGAIN;
+		return -FI_ENOMEM;
 
 	rx_entry->flags = 0;
-	if (tcpx_ep->cur_rx_msg.hdr.base_hdr.flags & OFI_REMOTE_CQ_DATA)
-		rx_entry->flags = (FI_COMPLETION |
-				   FI_REMOTE_CQ_DATA | FI_REMOTE_WRITE);
+	if (ep->cur_rx.hdr.base_hdr.flags & TCPX_REMOTE_CQ_DATA)
+		rx_entry->flags = (FI_COMPLETION | FI_REMOTE_WRITE);
+	else
+		rx_entry->flags = TCPX_INTERNAL_XFER;
 
-	memcpy(&rx_entry->hdr, &tcpx_ep->cur_rx_msg.hdr,
-	       (size_t) tcpx_ep->cur_rx_msg.hdr.base_hdr.payload_off);
-	rx_entry->hdr.base_hdr.op_data = TCPX_OP_REMOTE_WRITE;
-	rx_entry->ep = tcpx_ep;
+	memcpy(&rx_entry->hdr, &ep->cur_rx.hdr,
+	       (size_t) ep->cur_rx.hdr.base_hdr.hdr_size);
+	rx_entry->hdr.base_hdr.op_data = 0;
+	rx_entry->ep = ep;
 
 	ret = tcpx_validate_rx_rma_data(rx_entry, FI_REMOTE_WRITE);
 	if (ret) {
-		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN,
-			"invalid rma data\n");
-		tcpx_xfer_entry_free(tcpx_cq, rx_entry);
+		FI_WARN(&tcpx_prov, FI_LOG_DOMAIN, "invalid rma data\n");
+		tcpx_free_xfer(cq, rx_entry);
 		return ret;
 	}
 
-	tcpx_copy_rma_iov_to_msg_iov(rx_entry);
-	tcpx_rx_setup(tcpx_ep, rx_entry, tcpx_process_remote_write);
-	return FI_SUCCESS;
+	offset = rx_entry->hdr.base_hdr.flags & TCPX_REMOTE_CQ_DATA ?
+		 sizeof(rx_entry->hdr.cq_data_hdr) :
+		 sizeof(rx_entry->hdr.base_hdr);
+	rma_iov = (struct ofi_rma_iov *) ((uint8_t *) &rx_entry->hdr + offset);
+
+	rx_entry->iov_cnt = rx_entry->hdr.base_hdr.rma_iov_cnt;
+	for (i = 0; i < rx_entry->hdr.base_hdr.rma_iov_cnt; i++) {
+		rx_entry->iov[i].iov_base = (void *) (uintptr_t)
+					      rma_iov[i].addr;
+		rx_entry->iov[i].iov_len = rma_iov[i].len;
+	}
 
+	ep->cur_rx.entry = rx_entry;
+	ep->cur_rx.handler = tcpx_process_remote_write;
+	return tcpx_process_remote_write(ep);
 }
 
 int tcpx_op_read_rsp(struct tcpx_ep *tcpx_ep)
@@ -574,162 +604,205 @@ int tcpx_op_read_rsp(struct tcpx_ep *tcpx_ep)
 		return -FI_EINVAL;
 
 	entry = tcpx_ep->rma_read_queue.head;
-	rx_entry = container_of(entry, struct tcpx_xfer_entry,
-				entry);
+	rx_entry = container_of(entry, struct tcpx_xfer_entry, entry);
 
-	memcpy(&rx_entry->hdr, &tcpx_ep->cur_rx_msg.hdr,
-	       (size_t) tcpx_ep->cur_rx_msg.hdr.base_hdr.payload_off);
-	rx_entry->hdr.base_hdr.op_data = TCPX_OP_READ_RSP;
+	memcpy(&rx_entry->hdr, &tcpx_ep->cur_rx.hdr,
+	       (size_t) tcpx_ep->cur_rx.hdr.base_hdr.hdr_size);
+	rx_entry->hdr.base_hdr.op_data = 0;
 
-	tcpx_rx_setup(tcpx_ep, rx_entry, tcpx_process_remote_read);
-	return FI_SUCCESS;
+	tcpx_ep->cur_rx.entry = rx_entry;
+	tcpx_ep->cur_rx.handler = tcpx_process_remote_read;
+	return tcpx_process_remote_read(tcpx_ep);
 }
 
-static int tcpx_get_next_rx_hdr(struct tcpx_ep *ep)
+static int tcpx_recv_hdr(struct tcpx_ep *ep)
 {
+	size_t len;
+	void *buf;
 	ssize_t ret;
 
-	ret = tcpx_recv_hdr(ep->sock, &ep->stage_buf, &ep->cur_rx_msg);
+	assert(ep->cur_rx.hdr_done < ep->cur_rx.hdr_len);
+
+next_hdr:
+	buf = (uint8_t *) &ep->cur_rx.hdr + ep->cur_rx.hdr_done;
+	len = ep->cur_rx.hdr_len - ep->cur_rx.hdr_done;
+	ret = ofi_bsock_recv(&ep->bsock, buf, len);
 	if (ret < 0)
 		return (int) ret;
 
-	ep->cur_rx_msg.done_len += ret;
-	if (ep->cur_rx_msg.done_len >= sizeof(ep->cur_rx_msg.hdr.base_hdr)) {
-		if (ep->cur_rx_msg.hdr.base_hdr.payload_off > TCPX_MAX_HDR_SZ) {
+	ep->cur_rx.hdr_done += ret;
+	if (ep->cur_rx.hdr_done == sizeof(ep->cur_rx.hdr.base_hdr)) {
+		assert(ep->cur_rx.hdr_len == sizeof(ep->cur_rx.hdr.base_hdr));
+
+		if (ep->cur_rx.hdr.base_hdr.hdr_size > TCPX_MAX_HDR) {
 			FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
 				"Payload offset is too large\n");
 			return -FI_EIO;
 		}
-		ep->cur_rx_msg.hdr_len = (size_t) ep->cur_rx_msg.hdr.
-						  base_hdr.payload_off;
+		ep->cur_rx.hdr_len = (size_t) ep->cur_rx.hdr.base_hdr.hdr_size;
+		if (ep->cur_rx.hdr_done < ep->cur_rx.hdr_len)
+			goto next_hdr;
 
-		if (ep->cur_rx_msg.hdr_len > ep->cur_rx_msg.done_len) {
-			ret = tcpx_recv_hdr(ep->sock, &ep->stage_buf,
-					    &ep->cur_rx_msg);
-			if (ret < 0)
-				return (int) ret;
-
-			ep->cur_rx_msg.done_len += ret;
-		}
+	} else if (ep->cur_rx.hdr_done < ep->cur_rx.hdr_len) {
+		return -FI_EAGAIN;
 	}
 
-	if (ep->cur_rx_msg.done_len < ep->cur_rx_msg.hdr_len)
+	if (ep->cur_rx.hdr_done < ep->cur_rx.hdr_len)
 		return -FI_EAGAIN;
 
-	ep->hdr_bswap(&ep->cur_rx_msg.hdr.base_hdr);
-	return FI_SUCCESS;
+	ep->hdr_bswap(&ep->cur_rx.hdr.base_hdr);
+	assert(ep->cur_rx.hdr.base_hdr.id == ep->rx_id++);
+	if (ep->cur_rx.hdr.base_hdr.op >= ARRAY_SIZE(ep->start_op)) {
+		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
+			"Received invalid opcode\n");
+		return -FI_EIO;
+	}
+
+	ep->cur_rx.data_left = ep->cur_rx.hdr.base_hdr.size -
+			       ep->cur_rx.hdr.base_hdr.hdr_size;
+	ep->cur_rx.handler = ep->start_op[ep->cur_rx.hdr.base_hdr.op];
+
+	return ep->cur_rx.handler(ep);
 }
 
-/* Must hold ep lock */
 void tcpx_progress_rx(struct tcpx_ep *ep)
 {
 	int ret;
 
-	if (!ep->cur_rx_entry &&
-	    (ep->stage_buf.cur_pos == ep->stage_buf.bytes_avail)) {
-		ret = tcpx_read_to_buffer(ep->sock, &ep->stage_buf);
-		if (ret)
-			goto err;
-	}
-
+	assert(fastlock_held(&ep->lock));
 	do {
-		if (!ep->cur_rx_entry) {
-			if (ep->cur_rx_msg.done_len < ep->cur_rx_msg.hdr_len) {
-				ret = tcpx_get_next_rx_hdr(ep);
-				if (ret)
-					goto err;
-			}
-
-			if (ep->cur_rx_msg.hdr.base_hdr.op >=
-			    ARRAY_SIZE(ep->start_op)) {
-				FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
-					"Received invalid opcode\n");
-				ret = -FI_ENOTCONN; /* force shutdown */
-				goto err;
-			}
-			ret = ep->start_op[ep->cur_rx_msg.hdr.base_hdr.op](ep);
-			if (ret)
-				goto err;
+		if (ep->cur_rx.hdr_done < ep->cur_rx.hdr_len) {
+			ret = tcpx_recv_hdr(ep);
+		} else {
+			ret = ep->cur_rx.handler(ep);
 		}
-		assert(ep->cur_rx_proc_fn);
-		ep->cur_rx_proc_fn(ep->cur_rx_entry);
 
-	} while (ep->stage_buf.cur_pos < ep->stage_buf.bytes_avail);
+	} while (!ret && ofi_bsock_readable(&ep->bsock));
 
-	return;
-err:
-	if (OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
-		return;
-
-	if (ret == -FI_ENOTCONN)
+	if (ret && !OFI_SOCK_TRY_SND_RCV_AGAIN(-ret))
 		tcpx_ep_disable(ep, 0);
 }
 
-/* Must hold ep lock */
-void tcpx_progress_tx(struct tcpx_ep *ep)
+void tcpx_progress_async(struct tcpx_ep *ep)
 {
-	struct tcpx_xfer_entry *tx_entry;
-	struct slist_entry *entry;
-
-	if (!slist_empty(&ep->tx_queue)) {
-		entry = ep->tx_queue.head;
-		tx_entry = container_of(entry, struct tcpx_xfer_entry, entry);
-		tcpx_process_tx_entry(tx_entry);
+	struct tcpx_xfer_entry *xfer;
+	uint32_t done;
+
+	done = ofi_bsock_async_done(&tcpx_prov, &ep->bsock);
+	while (!slist_empty(&ep->async_queue)) {
+		xfer = container_of(ep->async_queue.head,
+				    struct tcpx_xfer_entry, entry);
+		if (ofi_val32_gt(xfer->async_index, done))
+			break;
+
+		slist_remove_head(&ep->async_queue);
+		tcpx_cq_report_success(ep->util_ep.tx_cq, xfer);
+		tcpx_free_tx(xfer);
 	}
 }
 
-int tcpx_try_func(void *util_ep)
+static bool tcpx_tx_pending(struct tcpx_ep *ep)
+{
+	return ep->cur_tx.entry || ofi_bsock_tosend(&ep->bsock);
+}
+
+static int tcpx_mod_epoll(struct tcpx_ep *ep, struct util_wait_fd *wait_fd)
 {
 	uint32_t events;
-	struct util_wait_fd *wait_fd;
-	struct tcpx_ep *ep;
 	int ret;
 
-	ep = container_of(util_ep, struct tcpx_ep, util_ep);
-	wait_fd = container_of(((struct util_ep *) util_ep)->tx_cq->wait,
-			       struct util_wait_fd, util_wait);
-
-	fastlock_acquire(&ep->lock);
-	if (!slist_empty(&ep->tx_queue) && !ep->pollout_set) {
-		ep->pollout_set = true;
+	assert(fastlock_held(&ep->lock));
+	if (ep->pollout_set) {
 		events = (wait_fd->util_wait.wait_obj == FI_WAIT_FD) ?
 			 (OFI_EPOLL_IN | OFI_EPOLL_OUT) : (POLLIN | POLLOUT);
-		goto epoll_mod;
-	} else if (slist_empty(&ep->tx_queue) && ep->pollout_set) {
-		ep->pollout_set = false;
+	} else {
 		events = (wait_fd->util_wait.wait_obj == FI_WAIT_FD) ?
 			 OFI_EPOLL_IN : POLLIN;
-		goto epoll_mod;
 	}
-	fastlock_release(&ep->lock);
-	return FI_SUCCESS;
 
-epoll_mod:
 	ret = (wait_fd->util_wait.wait_obj == FI_WAIT_FD) ?
-	      ofi_epoll_mod(wait_fd->epoll_fd, ep->sock, events,
+	      ofi_epoll_mod(wait_fd->epoll_fd, ep->bsock.sock, events,
 			    &ep->util_ep.ep_fid.fid) :
-	      ofi_pollfds_mod(wait_fd->pollfds, ep->sock, events,
+	      ofi_pollfds_mod(wait_fd->pollfds, ep->bsock.sock, events,
 			      &ep->util_ep.ep_fid.fid);
 	if (ret)
 		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
 			"epoll modify failed\n");
+
+	return ret;
+}
+
+/* We may need to send data in response to received requests,
+ * such as delivery complete acks or RMA read responses.  So,
+ * even if this is the Rx CQ, we need to progress transmits.
+ * We also need to keep the rx and tx epoll wait fd's in sync,
+ * such that we ask for POLLOUT on both or neither.  This is
+ * required in case they share the same wait set and underlying
+ * epoll fd.  So we only maintain a single pollout_set state
+ * variable rather than trying to track them independently.
+ * The latter does not work if the epoll fd behind the tx
+ * and rx CQs is the same fd.
+ */
+int tcpx_update_epoll(struct tcpx_ep *ep)
+{
+	struct util_wait_fd *rx_wait, *tx_wait;
+	int ret;
+
+	assert(fastlock_held(&ep->lock));
+	if ((tcpx_tx_pending(ep) && ep->pollout_set) ||
+	    (!tcpx_tx_pending(ep) && !ep->pollout_set))
+		return FI_SUCCESS;
+
+	rx_wait = ep->util_ep.rx_cq ?
+		  container_of(ep->util_ep.rx_cq->wait,
+		  	       struct util_wait_fd, util_wait) : NULL;
+	tx_wait = ep->util_ep.tx_cq ?
+		  container_of(ep->util_ep.tx_cq->wait,
+		  	       struct util_wait_fd, util_wait) : NULL;
+
+	ep->pollout_set = tcpx_tx_pending(ep);
+	ret = tcpx_mod_epoll(ep, rx_wait);
+	if (!ret && rx_wait != tx_wait)
+		ret = tcpx_mod_epoll(ep, tx_wait);
+
+	if (ret)
+		ep->pollout_set = false;
+	return ret;
+}
+
+int tcpx_try_func(void *util_ep)
+{
+	struct tcpx_ep *ep;
+	int ret;
+
+	ep = container_of(util_ep, struct tcpx_ep, util_ep);
+	fastlock_acquire(&ep->lock);
+	if (ofi_bsock_readable(&ep->bsock)) {
+		ret = -FI_EAGAIN;
+	} else {
+		ret = tcpx_update_epoll(ep);
+	}
 	fastlock_release(&ep->lock);
 	return ret;
 }
 
-void tcpx_tx_queue_insert(struct tcpx_ep *tcpx_ep,
+void tcpx_tx_queue_insert(struct tcpx_ep *ep,
 			  struct tcpx_xfer_entry *tx_entry)
 {
-	int empty;
-	struct util_wait *wait = tcpx_ep->util_ep.tx_cq->wait;
-
-	empty = slist_empty(&tcpx_ep->tx_queue);
-	slist_insert_tail(&tx_entry->entry, &tcpx_ep->tx_queue);
+	struct util_wait *wait = ep->util_ep.tx_cq->wait;
 
-	if (empty) {
-		tcpx_process_tx_entry(tx_entry);
+	if (!ep->cur_tx.entry) {
+		ep->cur_tx.entry = tx_entry;
+		ep->cur_tx.data_left = tx_entry->hdr.base_hdr.size;
+		OFI_DBG_SET(tx_entry->hdr.base_hdr.id, ep->tx_id++);
+		ep->hdr_bswap(&tx_entry->hdr.base_hdr);
+		tcpx_progress_tx(ep);
 
-		if (!slist_empty(&tcpx_ep->tx_queue) && wait)
+		if (!ep->cur_tx.entry && wait)
 			wait->signal(wait);
+	} else if (tx_entry->flags & TCPX_INTERNAL_XFER) {
+		slist_insert_tail(&tx_entry->entry, &ep->priority_queue);
+	} else {
+		slist_insert_tail(&tx_entry->entry, &ep->tx_queue);
 	}
 }
diff --git a/prov/tcp/src/tcpx_rma.c b/prov/tcp/src/tcpx_rma.c
index e380954..ff9dbfa 100644
--- a/prov/tcp/src/tcpx_rma.c
+++ b/prov/tcp/src/tcpx_rma.c
@@ -59,6 +59,7 @@ static void tcpx_rma_read_send_entry_fill(struct tcpx_xfer_entry *send_entry,
 	offset = sizeof(send_entry->hdr.base_hdr);
 	rma_iov = (struct ofi_rma_iov *) ((uint8_t *) &send_entry->hdr + offset);
 
+	send_entry->hdr.base_hdr.op = ofi_op_read_req;
 	send_entry->hdr.base_hdr.rma_iov_cnt = msg->rma_iov_count;
 	memcpy(rma_iov, msg->rma_iov,
 	       msg->rma_iov_count * sizeof(msg->rma_iov[0]));
@@ -66,13 +67,11 @@ static void tcpx_rma_read_send_entry_fill(struct tcpx_xfer_entry *send_entry,
 	offset += (msg->rma_iov_count * sizeof(*rma_iov));
 
 	send_entry->hdr.base_hdr.size = offset;
-	send_entry->hdr.base_hdr.payload_off = (uint8_t)offset;
+	send_entry->hdr.base_hdr.hdr_size = (uint8_t) offset;
 
 	send_entry->iov[0].iov_base = (void *) &send_entry->hdr;
 	send_entry->iov[0].iov_len = offset;
 	send_entry->iov_cnt = 1;
-	send_entry->ep = tcpx_ep;
-	send_entry->rem_len = send_entry->hdr.base_hdr.size;
 }
 
 static void tcpx_rma_read_recv_entry_fill(struct tcpx_xfer_entry *recv_entry,
@@ -105,19 +104,18 @@ static ssize_t tcpx_rma_readmsg(struct fid_ep *ep, const struct fi_msg_rma *msg,
 	assert(msg->iov_count <= TCPX_IOV_LIMIT);
 	assert(msg->rma_iov_count <= TCPX_IOV_LIMIT);
 
-	send_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_READ_REQ);
+	send_entry = tcpx_alloc_tx(tcpx_ep);
 	if (!send_entry)
 		return -FI_EAGAIN;
 
-	recv_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_READ_RSP);
+	recv_entry = tcpx_alloc_xfer(tcpx_cq);
 	if (!recv_entry) {
-		tcpx_xfer_entry_free(tcpx_cq, send_entry);
+		tcpx_free_xfer(tcpx_cq, send_entry);
 		return -FI_EAGAIN;
 	}
 	tcpx_rma_read_send_entry_fill(send_entry, tcpx_ep, msg);
 	tcpx_rma_read_recv_entry_fill(recv_entry, tcpx_ep, msg, flags);
 
-	tcpx_ep->hdr_bswap(&send_entry->hdr.base_hdr);
 	fastlock_acquire(&tcpx_ep->lock);
 	slist_insert_tail(&recv_entry->entry, &tcpx_ep->rma_read_queue);
 	tcpx_tx_queue_insert(tcpx_ep, send_entry);
@@ -178,18 +176,13 @@ static ssize_t tcpx_rma_writemsg(struct fid_ep *ep, const struct fi_msg_rma *msg
 				 uint64_t flags)
 {
 	struct tcpx_ep *tcpx_ep;
-	struct tcpx_cq *tcpx_cq;
 	struct tcpx_xfer_entry *send_entry;
 	struct ofi_rma_iov *rma_iov;
 	uint64_t data_len;
-	uint64_t *cq_data;
 	size_t offset;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
-	tcpx_cq = container_of(tcpx_ep->util_ep.tx_cq, struct tcpx_cq,
-			       util_cq);
-
-	send_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_WRITE);
+	send_entry = tcpx_alloc_tx(tcpx_ep);
 	if (!send_entry)
 		return -FI_EAGAIN;
 
@@ -198,14 +191,16 @@ static ssize_t tcpx_rma_writemsg(struct fid_ep *ep, const struct fi_msg_rma *msg
 
 	data_len = ofi_total_iov_len(msg->msg_iov, msg->iov_count);
 
-	assert(!(flags & FI_INJECT) || (data_len <= TCPX_MAX_INJECT_SZ));
-	offset = sizeof(send_entry->hdr.base_hdr);
+	assert(!(flags & FI_INJECT) || (data_len <= TCPX_MAX_INJECT));
+
+	send_entry->hdr.base_hdr.op = ofi_op_write;
 
 	if (flags & FI_REMOTE_CQ_DATA) {
-		send_entry->hdr.base_hdr.flags |= OFI_REMOTE_CQ_DATA;
-		cq_data = (uint64_t *)((uint8_t *)&send_entry->hdr + offset);
-		*cq_data = msg->data;
-		offset += sizeof(msg->data);
+		send_entry->hdr.base_hdr.flags = TCPX_REMOTE_CQ_DATA;
+		send_entry->hdr.cq_data_hdr.cq_data = msg->data;
+		offset = sizeof(send_entry->hdr.cq_data_hdr);
+	} else {
+		offset = sizeof(send_entry->hdr.base_hdr);
 	}
 
 	rma_iov = (struct ofi_rma_iov *)((uint8_t *)&send_entry->hdr + offset);
@@ -215,7 +210,7 @@ static ssize_t tcpx_rma_writemsg(struct fid_ep *ep, const struct fi_msg_rma *msg
 
 	offset += (send_entry->hdr.base_hdr.rma_iov_cnt * sizeof(*rma_iov));
 
-	send_entry->hdr.base_hdr.payload_off = (uint8_t)offset;
+	send_entry->hdr.base_hdr.hdr_size = (uint8_t) offset;
 	send_entry->hdr.base_hdr.size = data_len + offset;
 	if (flags & FI_INJECT) {
 		ofi_copy_iov_buf(msg->msg_iov, msg->iov_count, 0,
@@ -235,18 +230,9 @@ static ssize_t tcpx_rma_writemsg(struct fid_ep *ep, const struct fi_msg_rma *msg
 
 	send_entry->flags = (tcpx_ep->util_ep.tx_op_flags & FI_COMPLETION) |
 			     flags | FI_RMA | FI_WRITE;
-
-	if (flags & (FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE))
-		send_entry->hdr.base_hdr.flags |= OFI_DELIVERY_COMPLETE;
-
-	if (flags & FI_COMMIT_COMPLETE)
-		send_entry->hdr.base_hdr.flags |= OFI_COMMIT_COMPLETE;
-
-	send_entry->ep = tcpx_ep;
+	tcpx_set_commit_flags(send_entry, flags);
 	send_entry->context = msg->context;
-	send_entry->rem_len = send_entry->hdr.base_hdr.size;
 
-	tcpx_ep->hdr_bswap(&send_entry->hdr.base_hdr);
 	fastlock_acquire(&tcpx_ep->lock);
 	tcpx_tx_queue_insert(tcpx_ep, send_entry);
 	fastlock_release(&tcpx_ep->lock);
@@ -336,25 +322,23 @@ static ssize_t tcpx_rma_inject_common(struct fid_ep *ep, const void *buf,
 				      uint64_t key, uint64_t flags)
 {
 	struct tcpx_ep *tcpx_ep;
-	struct tcpx_cq *tcpx_cq;
 	struct tcpx_xfer_entry *send_entry;
 	struct ofi_rma_iov *rma_iov;
 	uint64_t *cq_data;
 	size_t offset;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
-	tcpx_cq = container_of(tcpx_ep->util_ep.tx_cq, struct tcpx_cq,
-			       util_cq);
-
-	send_entry = tcpx_xfer_entry_alloc(tcpx_cq, TCPX_OP_WRITE);
+	send_entry = tcpx_alloc_tx(tcpx_ep);
 	if (!send_entry)
 		return -FI_EAGAIN;
 
-	assert(len <= TCPX_MAX_INJECT_SZ);
+	assert(len <= TCPX_MAX_INJECT);
 	offset = sizeof(send_entry->hdr.base_hdr);
 
+	send_entry->hdr.base_hdr.op = ofi_op_write;
+
 	if (flags & FI_REMOTE_CQ_DATA) {
-		send_entry->hdr.base_hdr.flags |= OFI_REMOTE_CQ_DATA;
+		send_entry->hdr.base_hdr.flags = TCPX_REMOTE_CQ_DATA;
 		cq_data = (uint64_t *)((uint8_t *)&send_entry->hdr + offset);
 		*cq_data = data;
 		offset += sizeof(data);
@@ -367,7 +351,7 @@ static ssize_t tcpx_rma_inject_common(struct fid_ep *ep, const void *buf,
 	send_entry->hdr.base_hdr.rma_iov_cnt = 1;
 	offset += sizeof(*rma_iov);
 
-	send_entry->hdr.base_hdr.payload_off = (uint8_t)offset;
+	send_entry->hdr.base_hdr.hdr_size = (uint8_t) offset;
 	memcpy((uint8_t *)&send_entry->hdr + offset, (uint8_t *)buf, len);
 	offset += len;
 
@@ -376,10 +360,7 @@ static ssize_t tcpx_rma_inject_common(struct fid_ep *ep, const void *buf,
 	send_entry->iov_cnt = 1;
 
 	send_entry->hdr.base_hdr.size = offset;
-	send_entry->ep = tcpx_ep;
-	send_entry->rem_len = send_entry->hdr.base_hdr.size;
 
-	tcpx_ep->hdr_bswap(&send_entry->hdr.base_hdr);
 	fastlock_acquire(&tcpx_ep->lock);
 	tcpx_tx_queue_insert(tcpx_ep, send_entry);
 	fastlock_release(&tcpx_ep->lock);
diff --git a/prov/tcp/src/tcpx_shared_ctx.c b/prov/tcp/src/tcpx_shared_ctx.c
index 4eeeb72..2e08fe7 100644
--- a/prov/tcp/src/tcpx_shared_ctx.c
+++ b/prov/tcp/src/tcpx_shared_ctx.c
@@ -38,33 +38,6 @@
 #include <unistd.h>
 #include <ofi_iov.h>
 
-void tcpx_srx_entry_free(struct tcpx_rx_ctx *srx_ctx,
-			 struct tcpx_xfer_entry *xfer_entry)
-{
-	if (xfer_entry->ep->cur_rx_entry == xfer_entry)
-		xfer_entry->ep->cur_rx_entry = NULL;
-
-	fastlock_acquire(&srx_ctx->lock);
-	ofi_buf_free(xfer_entry);
-	fastlock_release(&srx_ctx->lock);
-}
-
-struct tcpx_xfer_entry *
-tcpx_srx_entry_alloc(struct tcpx_rx_ctx *srx_ctx, struct tcpx_ep *ep)
-{
-	struct tcpx_xfer_entry *rx_entry = NULL;
-
-	fastlock_acquire(&srx_ctx->lock);
-	if (slist_empty(&srx_ctx->rx_queue))
-		goto out;
-
-	rx_entry = container_of(srx_ctx->rx_queue.head,
-				struct tcpx_xfer_entry, entry);
-	slist_remove_head(&srx_ctx->rx_queue);
-out:
-	fastlock_release(&srx_ctx->lock);
-	return rx_entry;
-}
 
 static ssize_t tcpx_srx_recvmsg(struct fid_ep *ep, const struct fi_msg *msg,
 				uint64_t flags)
diff --git a/prov/usnic/src/usdf_progress.c b/prov/usnic/src/usdf_progress.c
index 47204a1..b5db07e 100644
--- a/prov/usnic/src/usdf_progress.c
+++ b/prov/usnic/src/usdf_progress.c
@@ -93,7 +93,7 @@ usdf_fabric_progression_thread(void *v)
 	int num_blocked_waiting;
 	int sleep_time;
 	ofi_epoll_t epfd;
-	void *context;
+	struct ofi_epollfds_event event;
 	int ret;
 	int n;
 
@@ -111,14 +111,14 @@ usdf_fabric_progression_thread(void *v)
 			sleep_time = -1;
 		}
 
-		n = ofi_epoll_wait(epfd, &context, 1, sleep_time);
+		n = ofi_epoll_wait(epfd, &event, 1, sleep_time);
 		if (fp->fab_exit || (n < 0 && n != EINTR)) {
 			pthread_exit(NULL);
 		}
 
 		/* consume event if there was one */
 		if (n == 1) {
-			pip = context;
+			pip = event.data.ptr;
 			ret = pip->pi_rtn(pip->pi_context);
 			if (ret != 0) {
 				pthread_exit(NULL);
diff --git a/prov/usnic/src/usdf_wait.c b/prov/usnic/src/usdf_wait.c
index ea575e3..f94b1dc 100644
--- a/prov/usnic/src/usdf_wait.c
+++ b/prov/usnic/src/usdf_wait.c
@@ -274,7 +274,7 @@ static int usdf_wait_close(struct fid *waitset)
 static int usdf_wait_wait(struct fid_wait *fwait, int timeout)
 {
 	struct usdf_wait *wait;
-	void *context;
+	struct ofi_epollfds_event event;
 	int ret = FI_SUCCESS;
 	int nevents;
 
@@ -289,7 +289,7 @@ static int usdf_wait_wait(struct fid_wait *fwait, int timeout)
 		return ret;
 	}
 
-	nevents = ofi_epoll_wait(wait->object.epfd, &context, 1, timeout);
+	nevents = ofi_epoll_wait(wait->object.epfd, &event, 1, timeout);
 	if (nevents == 0) {
 		ret = -FI_ETIMEDOUT;
 	} else if (nevents < 0) {
diff --git a/prov/util/src/util_atomic.c b/prov/util/src/util_atomic.c
index a950570..5be30e7 100644
--- a/prov/util/src/util_atomic.c
+++ b/prov/util/src/util_atomic.c
@@ -49,11 +49,14 @@ static const size_t ofi_datatype_size_table[] = {
 	[FI_DOUBLE_COMPLEX] = sizeof(ofi_complex_double),
 	[FI_LONG_DOUBLE]    = sizeof(long double),
 	[FI_LONG_DOUBLE_COMPLEX] = sizeof(ofi_complex_long_double),
+	/* Compute 128-bit integer size, since compiler may not support type. */
+	[FI_INT128]  = sizeof(int64_t) * 2,
+	[FI_UINT128] = sizeof(uint64_t) * 2,
 };
 
 size_t ofi_datatype_size(enum fi_datatype datatype)
 {
-	if (datatype >= FI_DATATYPE_LAST) {
+	if (datatype >= ARRAY_SIZE(ofi_datatype_size_table)) {
 		errno = FI_EINVAL;
 		return 0;
 	}
diff --git a/prov/util/src/util_attr.c b/prov/util/src/util_attr.c
index d84ddbe..da80878 100644
--- a/prov/util/src/util_attr.c
+++ b/prov/util/src/util_attr.c
@@ -398,7 +398,7 @@ int ofi_check_fabric_attr(const struct fi_provider *prov,
 	 * user's hints, if one is specified.
 	 */
 	if (prov_attr->prov_name && user_attr->prov_name &&
-	    !strcasestr(user_attr->prov_name, prov_attr->prov_name)) {
+	    strcasestr(user_attr->prov_name, prov_attr->prov_name)) {
 		FI_INFO(prov, FI_LOG_CORE,
 			"Requesting provider %s, skipping %s\n",
 			prov_attr->prov_name, user_attr->prov_name);
@@ -989,10 +989,18 @@ int ofi_prov_check_dup_info(const struct util_prov *util_prov,
 				     api_version, user_info);
 	    	if (ret)
 			continue;
+
 		if (!(fi = fi_dupinfo(prov_info))) {
 			ret = -FI_ENOMEM;
 			goto err;
 		}
+
+		if (util_prov->alter_defaults) {
+			ret = util_prov->alter_defaults(api_version, user_info,
+							prov_info, fi);
+			assert(ret == FI_SUCCESS);
+		}
+
 		if (!*info)
 			*info = fi;
 		else
diff --git a/prov/util/src/util_av.c b/prov/util/src/util_av.c
index ea929dd..9401189 100644
--- a/prov/util/src/util_av.c
+++ b/prov/util/src/util_av.c
@@ -247,6 +247,14 @@ void *ofi_av_get_addr(struct util_av *av, fi_addr_t fi_addr)
 	return entry->data;
 }
 
+void *ofi_av_addr_context(struct util_av *av, fi_addr_t fi_addr)
+{
+	void *addr;
+
+	addr = ofi_av_get_addr(av, fi_addr);
+	return (char *) addr + av->context_offset;
+}
+
 int ofi_verify_av_insert(struct util_av *av, uint64_t flags, void *context)
 {
 	if (av->flags & FI_EVENT) {
@@ -274,13 +282,11 @@ int ofi_verify_av_insert(struct util_av *av, uint64_t flags, void *context)
 	return 0;
 }
 
-/*
- * Must hold AV lock
- */
 int ofi_av_insert_addr(struct util_av *av, const void *addr, fi_addr_t *fi_addr)
 {
 	struct util_av_entry *entry = NULL;
 
+	assert(fastlock_held(&av->lock));
 	HASH_FIND(hh, av->hash, addr, av->addrlen, entry);
 	if (entry) {
 		if (fi_addr)
@@ -318,13 +324,11 @@ int ofi_av_elements_iter(struct util_av *av, ofi_av_apply_func apply, void *arg)
 	return 0;
 }
 
-/*
- * Must hold AV lock
- */
 int ofi_av_remove_addr(struct util_av *av, fi_addr_t fi_addr)
 {
 	struct util_av_entry *av_entry;
 
+	assert(fastlock_held(&av->lock));
 	av_entry = ofi_bufpool_get_ibuf(av->av_entry_pool, fi_addr);
 	if (!av_entry)
 		return -FI_ENOENT;
diff --git a/prov/util/src/util_buf.c b/prov/util/src/util_buf.c
index 9e7a4fb..7834265 100644
--- a/prov/util/src/util_buf.c
+++ b/prov/util/src/util_buf.c
@@ -170,7 +170,9 @@ int ofi_bufpool_create_attr(struct ofi_bufpool_attr *attr,
 
 	entry_sz = (attr->size + sizeof(struct ofi_bufpool_hdr));
 	OFI_DBG_ADD(entry_sz, sizeof(struct ofi_bufpool_ftr));
-	pool->entry_size = ofi_get_aligned_size(entry_sz, attr->alignment);
+	if (!attr->alignment)
+		pool->attr.alignment = entry_sz;
+	pool->entry_size = ofi_get_aligned_size(entry_sz, pool->attr.alignment);
 
 	if (!attr->chunk_cnt) {
 		pool->attr.chunk_cnt =
diff --git a/prov/util/src/util_cq.c b/prov/util/src/util_cq.c
index a0be452..9ee3024 100644
--- a/prov/util/src/util_cq.c
+++ b/prov/util/src/util_cq.c
@@ -53,13 +53,13 @@ static void ofi_cq_insert_aux(struct util_cq *cq,
 	slist_insert_tail(&entry->list_entry, &cq->aux_queue);
 }
 
-/* Caller must hold 'cq lock' */
 int ofi_cq_write_overflow(struct util_cq *cq, void *context, uint64_t flags,
 			  size_t len, void *buf, uint64_t data, uint64_t tag,
 			  fi_addr_t src)
 {
 	struct util_cq_aux_entry *entry;
 
+	assert(fastlock_held(&cq->cq_lock));
 	FI_DBG(cq->domain->prov, FI_LOG_CQ, "writing to CQ overflow list\n");
 	assert(ofi_cirque_freecnt(cq->cirq) <= 1);
 
@@ -79,12 +79,12 @@ int ofi_cq_write_overflow(struct util_cq *cq, void *context, uint64_t flags,
 	return 0;
 }
 
-/* Caller must hold 'cq lock' */
 int ofi_cq_insert_error(struct util_cq *cq,
 			const struct fi_cq_err_entry *err_entry)
 {
 	struct util_cq_aux_entry *entry;
 
+	assert(fastlock_held(&cq->cq_lock));
 	assert(err_entry->err);
 	if (!(entry = calloc(1, sizeof(*entry))))
 		return -FI_ENOMEM;
@@ -638,7 +638,7 @@ cleanup:
 }
 
 uint64_t ofi_rx_flags[] = {
-	[ofi_op_msg] = FI_RECV,
+	[ofi_op_msg] = FI_MSG | FI_RECV,
 	[ofi_op_tagged] = FI_RECV | FI_TAGGED,
 	[ofi_op_read_req] = FI_RMA | FI_REMOTE_READ,
 	[ofi_op_read_rsp] = FI_RMA | FI_REMOTE_READ,
@@ -647,7 +647,7 @@ uint64_t ofi_rx_flags[] = {
 	[ofi_op_atomic] = FI_ATOMIC | FI_REMOTE_WRITE,
 	[ofi_op_atomic_fetch] = FI_ATOMIC | FI_REMOTE_READ,
 	[ofi_op_atomic_compare] = FI_ATOMIC | FI_REMOTE_READ,
-	[ofi_op_read_async] = FI_RMA | FI_READ,
+	[ofi_op_read_async] = FI_RMA | FI_REMOTE_READ,
 };
 
 uint64_t ofi_tx_flags[] = {
@@ -660,6 +660,6 @@ uint64_t ofi_tx_flags[] = {
 	[ofi_op_atomic] = FI_ATOMIC | FI_WRITE,
 	[ofi_op_atomic_fetch] = FI_ATOMIC | FI_READ,
 	[ofi_op_atomic_compare] = FI_ATOMIC | FI_READ,
-	[ofi_op_read_async] = FI_RMA | FI_RMA,
+	[ofi_op_read_async] = FI_RMA | FI_READ,
 };
 
diff --git a/prov/util/src/util_mem_monitor.c b/prov/util/src/util_mem_monitor.c
index 126aa23..57b599e 100644
--- a/prov/util/src/util_mem_monitor.c
+++ b/prov/util/src/util_mem_monitor.c
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2017 Cray Inc. All rights reserved.
- * Copyright (c) 2017-2019 Intel Inc. All rights reserved.
+ * Copyright (c) 2017-2021 Intel Inc. All rights reserved.
  * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
  *                         All rights reserved.
  * (C) Copyright 2020 Hewlett Packard Enterprise Development LP
@@ -34,9 +34,14 @@
  * SOFTWARE.
  */
 
-#include <ofi_mr.h>
 #include <unistd.h>
 
+#include <ofi_mr.h>
+#include <ofi_hmem.h>
+#include <ofi_enosys.h>
+#include <rdma/fi_ext.h>
+
+
 pthread_mutex_t mm_lock = PTHREAD_MUTEX_INITIALIZER;
 pthread_rwlock_t mm_list_rwlock = PTHREAD_RWLOCK_INITIALIZER;
 
@@ -55,6 +60,7 @@ struct ofi_mem_monitor *uffd_monitor = &uffd.monitor;
 struct ofi_mem_monitor *default_monitor;
 struct ofi_mem_monitor *default_cuda_monitor;
 struct ofi_mem_monitor *default_rocr_monitor;
+struct ofi_mem_monitor *default_ze_monitor;
 
 static size_t ofi_default_cache_size(void)
 {
@@ -92,14 +98,8 @@ void ofi_monitors_init(void)
 	memhooks_monitor->init(memhooks_monitor);
 	cuda_monitor->init(cuda_monitor);
 	rocr_monitor->init(rocr_monitor);
-
-#if HAVE_MEMHOOKS_MONITOR
-        default_monitor = memhooks_monitor;
-#elif HAVE_UFFD_MONITOR
-        default_monitor = uffd_monitor;
-#else
-        default_monitor = NULL;
-#endif
+	ze_monitor->init(ze_monitor);
+	import_monitor->init(import_monitor);
 
 	fi_param_define(NULL, "mr_cache_max_size", FI_PARAM_SIZE_T,
 			"Defines the total number of bytes for all memory"
@@ -128,6 +128,9 @@ void ofi_monitors_init(void)
 	fi_param_define(NULL, "mr_rocr_cache_monitor_enabled", FI_PARAM_BOOL,
 			"Enable or disable the ROCR cache memory monitor. "
 			"Monitor is enabled by default.");
+	fi_param_define(NULL, "mr_ze_cache_monitor_enabled", FI_PARAM_BOOL,
+			"Enable or disable the ZE cache memory monitor. "
+			"Monitor is enabled by default.");
 
 	fi_param_get_size_t(NULL, "mr_cache_max_size", &cache_params.max_size);
 	fi_param_get_size_t(NULL, "mr_cache_max_count", &cache_params.max_cnt);
@@ -136,10 +139,26 @@ void ofi_monitors_init(void)
 			  &cache_params.cuda_monitor_enabled);
 	fi_param_get_bool(NULL, "mr_rocr_cache_monitor_enabled",
 			  &cache_params.rocr_monitor_enabled);
+	fi_param_get_bool(NULL, "mr_ze_cache_monitor_enabled",
+			  &cache_params.ze_monitor_enabled);
 
 	if (!cache_params.max_size)
 		cache_params.max_size = ofi_default_cache_size();
 
+	/*
+	 * At this time, the import monitor could have set the default monitor,
+	 * do not override
+	 */
+	if (!default_monitor) {
+#if HAVE_MEMHOOKS_MONITOR
+		default_monitor = memhooks_monitor;
+#elif HAVE_UFFD_MONITOR
+		default_monitor = uffd_monitor;
+#else
+		default_monitor = NULL;
+#endif
+	}
+
 	if (cache_params.monitor != NULL) {
 		if (!strcmp(cache_params.monitor, "userfaultfd")) {
 #if HAVE_UFFD_MONITOR
@@ -169,6 +188,11 @@ void ofi_monitors_init(void)
 		default_rocr_monitor = rocr_monitor;
 	else
 		default_rocr_monitor = NULL;
+
+	if (cache_params.ze_monitor_enabled)
+		default_ze_monitor = ze_monitor;
+	else
+		default_ze_monitor = NULL;
 }
 
 void ofi_monitors_cleanup(void)
@@ -177,6 +201,8 @@ void ofi_monitors_cleanup(void)
 	memhooks_monitor->cleanup(memhooks_monitor);
 	cuda_monitor->cleanup(cuda_monitor);
 	rocr_monitor->cleanup(rocr_monitor);
+	ze_monitor->cleanup(ze_monitor);
+	import_monitor->cleanup(import_monitor);
 }
 
 /* Monitors array must be of size OFI_HMEM_MAX. */
@@ -208,6 +234,9 @@ int ofi_monitors_add_cache(struct ofi_mem_monitor **monitors,
 	for (iface = FI_HMEM_SYSTEM; iface < OFI_HMEM_MAX; iface++) {
 		cache->monitors[iface] = NULL;
 
+		if (!ofi_hmem_is_initialized(iface))
+			continue;
+
 		monitor = monitors[iface];
 		if (!monitor) {
 			FI_DBG(&core_prov, FI_LOG_MR,
@@ -218,9 +247,7 @@ int ofi_monitors_add_cache(struct ofi_mem_monitor **monitors,
 
 		if (dlist_empty(&monitor->list)) {
 			ret = monitor->start(monitor);
-			if (ret == -FI_ENOSYS)
-				continue;
-			else if (ret)
+			if (ret)
 				goto err;
 		}
 
@@ -552,3 +579,151 @@ static void ofi_uffd_stop(struct ofi_mem_monitor *monitor)
 }
 
 #endif /* HAVE_UFFD_MONITOR */
+
+
+static void ofi_import_monitor_init(struct ofi_mem_monitor *monitor);
+static void ofi_import_monitor_cleanup(struct ofi_mem_monitor *monitor);
+static int ofi_import_monitor_start(struct ofi_mem_monitor *monitor);
+static void ofi_import_monitor_stop(struct ofi_mem_monitor *monitor);
+static int ofi_import_monitor_subscribe(struct ofi_mem_monitor *notifier,
+					const void *addr, size_t len,
+					union ofi_mr_hmem_info *hmem_info);
+static void ofi_import_monitor_unsubscribe(struct ofi_mem_monitor *notifier,
+					   const void *addr, size_t len,
+					   union ofi_mr_hmem_info *hmem_info);
+static bool ofi_import_monitor_valid(struct ofi_mem_monitor *notifier,
+				     const void *addr, size_t len,
+				     union ofi_mr_hmem_info *hmem_info);
+
+struct ofi_import_monitor {
+	struct ofi_mem_monitor monitor;
+	struct fid_mem_monitor *impfid;
+};
+
+static struct ofi_import_monitor impmon = {
+	.monitor.iface = FI_HMEM_SYSTEM,
+	.monitor.init = ofi_import_monitor_init,
+	.monitor.cleanup = ofi_import_monitor_cleanup,
+	.monitor.start = ofi_import_monitor_start,
+	.monitor.stop = ofi_import_monitor_stop,
+	.monitor.subscribe = ofi_import_monitor_subscribe,
+	.monitor.unsubscribe = ofi_import_monitor_unsubscribe,
+	.monitor.valid = ofi_import_monitor_valid,
+};
+
+struct ofi_mem_monitor *import_monitor = &impmon.monitor;
+
+static void ofi_import_monitor_init(struct ofi_mem_monitor *monitor)
+{
+	ofi_monitor_init(monitor);
+}
+
+static void ofi_import_monitor_cleanup(struct ofi_mem_monitor *monitor)
+{
+	assert(!impmon.impfid);
+	ofi_monitor_cleanup(monitor);
+}
+
+static int ofi_import_monitor_start(struct ofi_mem_monitor *monitor)
+{
+	if (!impmon.impfid)
+		return -FI_ENOSYS;
+
+	return impmon.impfid->export_ops->start(impmon.impfid);
+}
+
+static void ofi_import_monitor_stop(struct ofi_mem_monitor *monitor)
+{
+	assert(impmon.impfid);
+	impmon.impfid->export_ops->stop(impmon.impfid);
+}
+
+static int ofi_import_monitor_subscribe(struct ofi_mem_monitor *notifier,
+					const void *addr, size_t len,
+					union ofi_mr_hmem_info *hmem_info)
+{
+	assert(impmon.impfid);
+	return impmon.impfid->export_ops->subscribe(impmon.impfid, addr, len);
+}
+
+static void ofi_import_monitor_unsubscribe(struct ofi_mem_monitor *notifier,
+					   const void *addr, size_t len,
+					   union ofi_mr_hmem_info *hmem_info)
+{
+	assert(impmon.impfid);
+	return impmon.impfid->export_ops->unsubscribe(impmon.impfid, addr, len);
+}
+
+static bool ofi_import_monitor_valid(struct ofi_mem_monitor *notifier,
+				     const void *addr, size_t len,
+				     union ofi_mr_hmem_info *hmem_info)
+{
+	assert(impmon.impfid);
+	return impmon.impfid->export_ops->valid(impmon.impfid, addr, len);
+}
+
+static void ofi_import_monitor_notify(struct fid_mem_monitor *monitor,
+				      const void *addr, size_t len)
+{
+	assert(monitor->fid.context == &impmon);
+	pthread_rwlock_rdlock(&mm_list_rwlock);
+	pthread_mutex_lock(&mm_lock);
+	ofi_monitor_notify(&impmon.monitor, addr, len);
+	pthread_mutex_unlock(&mm_lock);
+	pthread_rwlock_unlock(&mm_list_rwlock);
+}
+
+static int ofi_close_import(struct fid *fid)
+{
+	impmon.impfid = NULL;
+	return 0;
+}
+
+static struct fi_ops_mem_notify import_ops = {
+	.size = sizeof(struct fi_ops_mem_notify),
+	.notify = ofi_import_monitor_notify,
+};
+
+static struct fi_ops impfid_ops = {
+	.size = sizeof(struct fi_ops),
+	.close = ofi_close_import,
+	.bind = fi_no_bind,
+	.control = fi_no_control,
+	.ops_open = fi_no_ops_open,
+	.tostr = fi_no_tostr,
+	.ops_set = fi_no_ops_set,
+};
+
+int ofi_monitor_import(struct fid *fid)
+{
+	struct fid_mem_monitor *impfid;
+
+	if (fid->fclass != FI_CLASS_MEM_MONITOR)
+		return -FI_ENOSYS;
+
+	if (impmon.impfid) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+			"imported monitor already exists\n");
+		return -FI_EBUSY;
+	}
+
+	if (default_monitor && !dlist_empty(&default_monitor->list)) {
+		FI_WARN(&core_prov, FI_LOG_MR,
+			"cannot replace active monitor\n");
+		return -FI_EBUSY;
+	}
+
+	impfid = container_of(fid, struct fid_mem_monitor, fid);
+	if (impfid->export_ops->size < sizeof(struct fi_ops_mem_monitor))
+		return -FI_EINVAL;
+
+	impmon.impfid = impfid;
+	impfid->fid.context = &impmon;
+	impfid->fid.ops = &impfid_ops;
+	impfid->import_ops = &import_ops;
+
+	FI_INFO(&core_prov, FI_LOG_MR,
+		"setting imported memory monitor as default\n");
+	default_monitor = &impmon.monitor;
+	return 0;
+}
diff --git a/prov/util/src/util_mr_cache.c b/prov/util/src/util_mr_cache.c
index 9c05136..0334753 100644
--- a/prov/util/src/util_mr_cache.c
+++ b/prov/util/src/util_mr_cache.c
@@ -41,12 +41,14 @@
 #include <ofi_mr.h>
 #include <ofi_list.h>
 #include <ofi_tree.h>
+#include <ofi_enosys.h>
 
 
 struct ofi_mr_cache_params cache_params = {
 	.max_cnt = 1024,
 	.cuda_monitor_enabled = true,
 	.rocr_monitor_enabled = true,
+	.ze_monitor_enabled = true,
 };
 
 static int util_mr_find_within(struct ofi_rbmap *map, void *key, void *data)
@@ -133,7 +135,7 @@ static void util_mr_uncache_entry(struct ofi_mr_cache *cache,
 
 	if (entry->use_cnt == 0) {
 		dlist_remove(&entry->list_entry);
-		dlist_insert_tail(&entry->list_entry, &cache->flush_list);
+		dlist_insert_tail(&entry->list_entry, &cache->dead_region_list);
 	} else {
 		cache->uncached_cnt++;
 		cache->uncached_size += entry->info.iov.iov_len;
@@ -180,46 +182,44 @@ void ofi_mr_cache_notify(struct ofi_mr_cache *cache, const void *addr, size_t le
 		util_mr_uncache_entry(cache, entry);
 }
 
+/* Function to remove dead regions and prune MR cache size.
+ * Returns true if any entries were flushed from the cache.
+ */
 bool ofi_mr_cache_flush(struct ofi_mr_cache *cache, bool flush_lru)
 {
+	struct dlist_entry free_list;
 	struct ofi_mr_entry *entry;
+	bool entries_freed;
 
-	pthread_mutex_lock(&mm_lock);
-	while (!dlist_empty(&cache->flush_list)) {
-		dlist_pop_front(&cache->flush_list, struct ofi_mr_entry,
-				entry, list_entry);
-		FI_DBG(cache->domain->prov, FI_LOG_MR, "flush %p (len: %zu)\n",
-		       entry->info.iov.iov_base, entry->info.iov.iov_len);
-		pthread_mutex_unlock(&mm_lock);
+	dlist_init(&free_list);
 
-		util_mr_free_entry(cache, entry);
-		pthread_mutex_lock(&mm_lock);
-	}
+	pthread_mutex_lock(&mm_lock);
 
-	if (!flush_lru || dlist_empty(&cache->lru_list)) {
-		pthread_mutex_unlock(&mm_lock);
-		return false;
-	}
+	dlist_splice_tail(&free_list, &cache->dead_region_list);
 
-	do {
+	while (flush_lru && !dlist_empty(&cache->lru_list)) {
 		dlist_pop_front(&cache->lru_list, struct ofi_mr_entry,
 				entry, list_entry);
 		dlist_init(&entry->list_entry);
-		FI_DBG(cache->domain->prov, FI_LOG_MR, "flush %p (len: %zu)\n",
-		       entry->info.iov.iov_base, entry->info.iov.iov_len);
-
 		util_mr_uncache_entry_storage(cache, entry);
-		pthread_mutex_unlock(&mm_lock);
+		dlist_insert_tail(&entry->list_entry, &free_list);
 
-		util_mr_free_entry(cache, entry);
-		pthread_mutex_lock(&mm_lock);
+		flush_lru = ofi_mr_cache_full(cache);
+	}
 
-	} while (!dlist_empty(&cache->lru_list) &&
-		 ((cache->cached_cnt >= cache_params.max_cnt) ||
-		  (cache->cached_size >= cache_params.max_size)));
 	pthread_mutex_unlock(&mm_lock);
 
-	return true;
+	entries_freed = !dlist_empty(&free_list);
+
+	while(!dlist_empty(&free_list)) {
+		dlist_pop_front(&free_list, struct ofi_mr_entry,
+				entry, list_entry);
+		FI_DBG(cache->domain->prov, FI_LOG_MR, "flush %p (len: %zu)\n",
+			entry->info.iov.iov_base, entry->info.iov.iov_len);
+		util_mr_free_entry(cache, entry);
+	}
+
+	return entries_freed;
 }
 
 void ofi_mr_cache_delete(struct ofi_mr_cache *cache, struct ofi_mr_entry *entry)
@@ -285,8 +285,7 @@ util_mr_cache_create(struct ofi_mr_cache *cache, const struct ofi_mr_info *info,
 		goto unlock;
 	}
 
-	if ((cache->cached_cnt >= cache_params.max_cnt) ||
-	    (cache->cached_size >= cache_params.max_size)) {
+	if (ofi_mr_cache_full(cache)) {
 		cache->uncached_cnt++;
 		cache->uncached_size += info->iov.iov_len;
 	} else {
@@ -321,9 +320,11 @@ int ofi_mr_cache_search(struct ofi_mr_cache *cache, const struct fi_mr_attr *att
 			struct ofi_mr_entry **entry)
 {
 	struct ofi_mr_info info;
+	struct ofi_mem_monitor *monitor;
+	bool flush_lru;
 	int ret;
-	struct ofi_mem_monitor *monitor = cache->monitors[attr->iface];
 
+	monitor = cache->monitors[attr->iface];
 	if (!monitor) {
 		FI_WARN(&core_prov, FI_LOG_MR,
 			"MR cache disabled for %s memory\n",
@@ -341,11 +342,10 @@ int ofi_mr_cache_search(struct ofi_mr_cache *cache, const struct fi_mr_attr *att
 
 	do {
 		pthread_mutex_lock(&mm_lock);
-
-		if ((cache->cached_cnt >= cache_params.max_cnt) ||
-		    (cache->cached_size >= cache_params.max_size)) {
+		flush_lru = ofi_mr_cache_full(cache);
+		if (flush_lru || !dlist_empty(&cache->dead_region_list)) {
 			pthread_mutex_unlock(&mm_lock);
-			ofi_mr_cache_flush(cache, true);
+			ofi_mr_cache_flush(cache, flush_lru);
 			pthread_mutex_lock(&mm_lock);
 		}
 
@@ -492,7 +492,7 @@ int ofi_mr_cache_init(struct util_domain *domain,
 
 	pthread_mutex_init(&cache->lock, NULL);
 	dlist_init(&cache->lru_list);
-	dlist_init(&cache->flush_list);
+	dlist_init(&cache->dead_region_list);
 	cache->cached_cnt = 0;
 	cache->cached_size = 0;
 	cache->uncached_cnt = 0;
@@ -526,3 +526,52 @@ destroy:
 	cache->domain = NULL;
 	return ret;
 }
+
+
+
+static int ofi_close_cache_fid(struct fid *fid)
+{
+	free(fid);
+	return 0;
+}
+
+static int ofi_bind_cache_fid(struct fid *fid, struct fid *bfid,
+			      uint64_t flags)
+{
+	if (flags || bfid->fclass != FI_CLASS_MEM_MONITOR)
+		return -FI_EINVAL;
+
+	return ofi_monitor_import(bfid);
+}
+
+static struct fi_ops ofi_mr_cache_ops = {
+	.size = sizeof(struct fi_ops),
+	.close = ofi_close_cache_fid,
+	.bind = ofi_bind_cache_fid,
+	.control = fi_no_control,
+	.ops_open = fi_no_ops_open,
+	.tostr = fi_no_tostr,
+	.ops_set = fi_no_ops_set,
+};
+
+int ofi_open_mr_cache(uint32_t version, void *attr, size_t attr_len,
+		      uint64_t flags, struct fid **fid, void *context)
+{
+	struct fid *cache_fid;
+
+	if (FI_VERSION_LT(version, FI_VERSION(1, 13)) || attr_len)
+		return -FI_EINVAL;
+
+	if (flags)
+		return -FI_EBADFLAGS;
+
+	cache_fid = calloc(1, sizeof(*cache_fid));
+	if (!cache_fid)
+		return -FI_ENOMEM;
+
+	cache_fid->fclass = FI_CLASS_MR_CACHE;
+	cache_fid->context = context;
+	cache_fid->ops = &ofi_mr_cache_ops;
+	*fid = cache_fid;
+	return 0;
+}
diff --git a/prov/util/src/util_mr_map.c b/prov/util/src/util_mr_map.c
index 78e6459..46213a6 100644
--- a/prov/util/src/util_mr_map.c
+++ b/prov/util/src/util_mr_map.c
@@ -266,6 +266,13 @@ int ofi_mr_regattr(struct fid *fid, const struct fi_mr_attr *attr,
 
 	ofi_mr_update_attr(domain->fabric->fabric_fid.api_version,
 			   domain->info_domain_caps, attr, &cur_abi_attr);
+	if (!hmem_ops[cur_abi_attr.iface].initialized) {
+		FI_WARN(domain->mr_map.prov, FI_LOG_MR,
+			"MR registration failed - hmem iface not initialized\n");
+		free(mr);
+		return -FI_ENOSYS;
+	}
+
 	fastlock_acquire(&domain->lock);
 
 	mr->mr_fid.fid.fclass = FI_CLASS_MR;
diff --git a/prov/util/src/util_shm.c b/prov/util/src/util_shm.c
index 9426d76..06aa391 100644
--- a/prov/util/src/util_shm.c
+++ b/prov/util/src/util_shm.c
@@ -255,6 +255,7 @@ int smr_create(const struct fi_provider *prov, struct smr_map *map,
 
 	*smr = mapped_addr;
 	fastlock_init(&(*smr)->lock);
+	ofi_atomic_initialize32(&(*smr)->signal, 0);
 
 	(*smr)->map = map;
 	(*smr)->version = SMR_VERSION;
@@ -350,11 +351,13 @@ int smr_map_to_region(const struct fi_provider *prov, struct smr_peer *peer_buf)
 	struct smr_region *peer;
 	size_t size;
 	int fd, ret = 0;
+	struct stat sts;
 	struct dlist_entry *entry;
+	const char *name = smr_no_prefix(peer_buf->peer.name);
+	char tmp[NAME_MAX];
 
 	pthread_mutex_lock(&ep_list_lock);
-	entry = dlist_find_first_match(&ep_name_list, smr_match_name,
-				       peer_buf->peer.name);
+	entry = dlist_find_first_match(&ep_name_list, smr_match_name, name);
 	if (entry) {
 		peer_buf->region = container_of(entry, struct smr_ep_name,
 						entry)->region;
@@ -363,12 +366,24 @@ int smr_map_to_region(const struct fi_provider *prov, struct smr_peer *peer_buf)
 	}
 	pthread_mutex_unlock(&ep_list_lock);
 
-	fd = shm_open(peer_buf->peer.name, O_RDWR, S_IRUSR | S_IWUSR);
+	fd = shm_open(name, O_RDWR, S_IRUSR | S_IWUSR);
 	if (fd < 0) {
 		FI_WARN_ONCE(prov, FI_LOG_AV, "shm_open error\n");
 		return -errno;
 	}
 
+	memset(tmp, 0, sizeof(tmp));
+	snprintf(tmp, sizeof(tmp), "/dev/shm/%s", name);
+	if (stat(tmp, &sts) == -1) {
+		ret = -errno;
+		goto out;
+	}
+
+	if (sts.st_size < sizeof(*peer)) {
+		ret = -FI_ENOENT;
+		goto out;
+	}
+
 	peer = mmap(NULL, sizeof(*peer), PROT_READ | PROT_WRITE,
 		    MAP_SHARED, fd, 0);
 	if (peer == MAP_FAILED) {
@@ -380,7 +395,7 @@ int smr_map_to_region(const struct fi_provider *prov, struct smr_peer *peer_buf)
 	if (!peer->pid) {
 		FI_WARN(prov, FI_LOG_AV, "peer not initialized\n");
 		munmap(peer, sizeof(*peer));
-		ret = -FI_EAGAIN;
+		ret = -FI_ENOENT;
 		goto out;
 	}
 
@@ -488,7 +503,7 @@ void smr_map_del(struct smr_map *map, int64_t id)
 
 	pthread_mutex_lock(&ep_list_lock);
 	entry = dlist_find_first_match(&ep_name_list, smr_match_name,
-				       map->peers[id].peer.name);
+				       smr_no_prefix(map->peers[id].peer.name));
 	pthread_mutex_unlock(&ep_list_lock);
 
 	fastlock_acquire(&map->lock);
diff --git a/prov/util/src/util_wait.c b/prov/util/src/util_wait.c
index 07d6a7a..f20745e 100644
--- a/prov/util/src/util_wait.c
+++ b/prov/util/src/util_wait.c
@@ -383,9 +383,9 @@ release:
 
 static int util_wait_fd_run(struct fid_wait *wait_fid, int timeout)
 {
+	struct ofi_epollfds_event event;
 	struct util_wait_fd *wait;
 	uint64_t endtime;
-	void *ep_context[1];
 	int ret;
 
 	wait = container_of(wait_fid, struct util_wait_fd, util_wait.wait_fid);
@@ -400,12 +400,17 @@ static int util_wait_fd_run(struct fid_wait *wait_fid, int timeout)
 			return -FI_ETIMEDOUT;
 
 		ret = (wait->util_wait.wait_obj == FI_WAIT_FD) ?
-		      ofi_epoll_wait(wait->epoll_fd, ep_context, 1, timeout) :
-		      ofi_pollfds_wait(wait->pollfds, ep_context, 1, timeout);
+		      ofi_epoll_wait(wait->epoll_fd, &event, 1, timeout) :
+		      ofi_pollfds_wait(wait->pollfds, &event, 1, timeout);
 		if (ret > 0)
 			return FI_SUCCESS;
 
 		if (ret < 0) {
+#if ENABLE_DEBUG
+			/* ignore interrupts in order to enable debugging */
+			if (ret == -FI_EINTR)
+				continue;
+#endif
 			FI_WARN(wait->util_wait.prov, FI_LOG_FABRIC,
 				"poll failed\n");
 			return ret;
diff --git a/prov/util/src/ze_mem_monitor.c b/prov/util/src/ze_mem_monitor.c
new file mode 100644
index 0000000..866885b
--- /dev/null
+++ b/prov/util/src/ze_mem_monitor.c
@@ -0,0 +1,117 @@
+/*
+ * (C) Copyright 2021 Intel Corporation
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "ofi_mr.h"
+
+#if HAVE_LIBZE
+
+#include "ofi_hmem.h"
+
+static int ze_mm_subscribe(struct ofi_mem_monitor *monitor, const void *addr,
+			   size_t len, union ofi_mr_hmem_info *hmem_info)
+{
+	return ze_hmem_get_id(addr, &hmem_info->ze_id);
+}
+
+static void ze_mm_unsubscribe(struct ofi_mem_monitor *monitor,
+			      const void *addr, size_t len,
+			      union ofi_mr_hmem_info *hmem_info)
+{
+	/* no-op */
+}
+
+static bool ze_mm_valid(struct ofi_mem_monitor *monitor,
+			const void *addr, size_t len,
+			union ofi_mr_hmem_info *hmem_info)
+{
+	uint64_t id;
+	int ret;
+
+	ret = ze_hmem_get_id(addr, &id);
+	if (ret)
+		return false;
+
+
+	return id == hmem_info->ze_id;
+}
+
+static int ze_monitor_start(struct ofi_mem_monitor *monitor)
+{
+	/* no-op */
+	return FI_SUCCESS;
+}
+
+#else
+
+static int ze_mm_subscribe(struct ofi_mem_monitor *monitor, const void *addr,
+			   size_t len, union ofi_mr_hmem_info *hmem_info)
+{
+	return -FI_ENOSYS;
+}
+
+static void ze_mm_unsubscribe(struct ofi_mem_monitor *monitor,
+			      const void *addr, size_t len,
+			      union ofi_mr_hmem_info *hmem_info)
+{
+}
+
+static bool ze_mm_valid(struct ofi_mem_monitor *monitor,
+			const void *addr, size_t len,
+			union ofi_mr_hmem_info *hmem_info)
+{
+	return false;
+}
+
+static int ze_monitor_start(struct ofi_mem_monitor *monitor)
+{
+	return -FI_ENOSYS;
+}
+
+#endif /* HAVE_LIBZE */
+
+void ze_monitor_stop(struct ofi_mem_monitor *monitor)
+{
+	/* no-op */
+}
+
+static struct ofi_mem_monitor ze_mm = {
+	.iface = FI_HMEM_ZE,
+	.init = ofi_monitor_init,
+	.cleanup = ofi_monitor_cleanup,
+	.start = ze_monitor_start,
+	.stop = ze_monitor_stop,
+	.subscribe = ze_mm_subscribe,
+	.unsubscribe = ze_mm_unsubscribe,
+	.valid = ze_mm_valid,
+};
+
+struct ofi_mem_monitor *ze_monitor = &ze_mm;
diff --git a/prov/verbs/configure.m4 b/prov/verbs/configure.m4
index 2d51072..aa793e0 100644
--- a/prov/verbs/configure.m4
+++ b/prov/verbs/configure.m4
@@ -47,6 +47,10 @@ AC_DEFUN([FI_VERBS_CONFIGURE],[
 	AS_IF([test $verbs_ibverbs_happy -eq 1 && \
 	       test $verbs_rdmacm_happy -eq 1], [$1], [$2])
 
+	#Set CPPFLAGS to allow correct include path to be used by AC_CHECK_DECL()
+	fi_verbs_configure_save_CPPFLAGS=$CPPFLAGS
+	CPPFLAGS=$verbs_ibverbs_CPPFLAGS
+
 	#See if we have extended verbs calls
 	VERBS_HAVE_QUERY_EX=0
 	AS_IF([test $verbs_ibverbs_happy -eq 1],[
@@ -79,6 +83,18 @@ AC_DEFUN([FI_VERBS_CONFIGURE],[
 	AC_DEFINE_UNQUOTED([VERBS_HAVE_RDMA_ESTABLISH],[$VERBS_HAVE_RDMA_ESTABLISH],
 		[Whether rdma/rdma_cma.h has rdma_establish() support or not])
 
+	#See if we have rdma-core dmabuf mr support
+	VERBS_HAVE_DMABUF_MR=0
+	AS_IF([test $verbs_ibverbs_happy -eq 1],[
+		AC_CHECK_DECL([ibv_reg_dmabuf_mr],
+			[VERBS_HAVE_DMABUF_MR=1],[],
+			[#include <infiniband/verbs.h>])
+		])
+	AC_DEFINE_UNQUOTED([VERBS_HAVE_DMABUF_MR],[$VERBS_HAVE_DMABUF_MR],
+		[Whether infiniband/verbs.h has ibv_reg_dmabuf_mr() support or not])
+
+	CPPFLAGS=$fi_verbs_configure_save_CPPFLAGS
+
 	# Technically, verbs_ibverbs_CPPFLAGS and
 	# verbs_rdmacm_CPPFLAGS could be different, but it is highly
 	# unlikely that they ever will be.  So only list
diff --git a/prov/verbs/src/fi_verbs.c b/prov/verbs/src/fi_verbs.c
index 665b22d..155e048 100644
--- a/prov/verbs/src/fi_verbs.c
+++ b/prov/verbs/src/fi_verbs.c
@@ -424,7 +424,7 @@ fn:
 }
 
 #if ENABLE_DEBUG
-static int vrb_dbg_query_qp_attr(struct ibv_qp *qp)
+static void vrb_dbg_query_qp_attr(struct ibv_qp *qp)
 {
 	struct ibv_qp_init_attr attr = { 0 };
 	struct ibv_qp_attr qp_attr = { 0 };
@@ -434,8 +434,9 @@ static int vrb_dbg_query_qp_attr(struct ibv_qp *qp)
 			   IBV_QP_RNR_RETRY | IBV_QP_MIN_RNR_TIMER, &attr);
 	if (ret) {
 		VERBS_WARN(FI_LOG_EP_CTRL, "Unable to query QP\n");
-		return ret;
+		return;
 	}
+
 	FI_DBG(&vrb_prov, FI_LOG_EP_CTRL, "QP attributes: "
 	       "min_rnr_timer"	": %" PRIu8 ", "
 	       "timeout"	": %" PRIu8 ", "
@@ -443,16 +444,14 @@ static int vrb_dbg_query_qp_attr(struct ibv_qp *qp)
 	       "rnr_retry"	": %" PRIu8 "\n",
 	       qp_attr.min_rnr_timer, qp_attr.timeout, qp_attr.retry_cnt,
 	       qp_attr.rnr_retry);
-	return 0;
 }
 #else
-static int vrb_dbg_query_qp_attr(struct ibv_qp *qp)
+static void vrb_dbg_query_qp_attr(struct ibv_qp *qp)
 {
-	return 0;
 }
 #endif
 
-int vrb_set_rnr_timer(struct ibv_qp *qp)
+void vrb_set_rnr_timer(struct ibv_qp *qp)
 {
 	struct ibv_qp_attr attr = { 0 };
 	int ret;
@@ -468,17 +467,13 @@ int vrb_set_rnr_timer(struct ibv_qp *qp)
 
 	/* XRC initiator QP do not have responder logic */
 	if (qp->qp_type == IBV_QPT_XRC_SEND)
-		return 0;
+		return;
 
 	ret = ibv_modify_qp(qp, &attr, IBV_QP_MIN_RNR_TIMER);
-	if (ret) {
-		VERBS_WARN(FI_LOG_EQ, "Unable to modify QP attribute\n");
-		return ret;
-	}
-	ret = vrb_dbg_query_qp_attr(qp);
 	if (ret)
-		return ret;
-	return 0;
+		VERBS_WARN(FI_LOG_EQ, "Unable to modify QP attribute\n");
+
+	vrb_dbg_query_qp_attr(qp);
 }
 
 int vrb_find_max_inline(struct ibv_pd *pd, struct ibv_context *context,
@@ -781,6 +776,28 @@ static void vrb_set_peer_mem_support(void)
 	fclose(kallsyms_fd);
 }
 
+static void vrb_set_dmabuf_support(void)
+{
+	char *line = NULL;
+	size_t line_size = 0;
+	ssize_t bytes;
+	FILE *kallsyms_fd;
+
+	kallsyms_fd = fopen("/proc/kallsyms", "r");
+	if (!kallsyms_fd)
+		return;
+
+	while ((bytes = getline(&line, &line_size, kallsyms_fd)) != -1) {
+		if (strstr(line, "ib_umem_dmabuf_get")) {
+			vrb_gl_data.dmabuf_support = true;
+			break;
+		}
+	}
+
+	free(line);
+	fclose(kallsyms_fd);
+}
+
 static void vrb_fini(void)
 {
 #if HAVE_VERBS_DL
@@ -801,6 +818,7 @@ VERBS_INI
 	ofi_monitors_init();
 #endif
 	vrb_set_peer_mem_support();
+	vrb_set_dmabuf_support();
 
 	if (vrb_read_params()|| vrb_init_info(&vrb_util_prov.info))
 		return NULL;
diff --git a/prov/verbs/src/fi_verbs.h b/prov/verbs/src/fi_verbs.h
index 9310639..adef026 100644
--- a/prov/verbs/src/fi_verbs.h
+++ b/prov/verbs/src/fi_verbs.h
@@ -179,6 +179,7 @@ extern struct vrb_gl_data {
 	} msg;
 
 	bool	peer_mem_support;
+	bool	dmabuf_support;
 } vrb_gl_data;
 
 struct verbs_addr {
@@ -365,9 +366,8 @@ struct vrb_domain {
 
 	ssize_t		(*send_credits)(struct fid_ep *ep, uint64_t credits);
 
-	/* Indicates that MSG endpoints should use the XRC transport.
-	 * TODO: Move selection of XRC/RC to endpoint info from domain */
-	int				flags;
+	/* Domain use of specific extended H/W capabilities, e.g. XRC, ODP */
+	uint64_t			ext_flags;
 	struct {
 		int			xrcd_fd;
 		struct ibv_xrcd		*xrcd;
@@ -847,7 +847,7 @@ ssize_t vrb_eq_write_event(struct vrb_eq *eq, uint32_t event,
 int vrb_query_atomic(struct fid_domain *domain_fid, enum fi_datatype datatype,
 			enum fi_op op, struct fi_atomic_attr *attr,
 			uint64_t flags);
-int vrb_set_rnr_timer(struct ibv_qp *qp);
+void vrb_set_rnr_timer(struct ibv_qp *qp);
 void vrb_cleanup_cq(struct vrb_ep *cur_ep);
 int vrb_find_max_inline(struct ibv_pd *pd, struct ibv_context *context,
 			   enum ibv_qp_type qp_type);
diff --git a/prov/verbs/src/verbs_cm.c b/prov/verbs/src/verbs_cm.c
index 534f5e0..de02a43 100644
--- a/prov/verbs/src/verbs_cm.c
+++ b/prov/verbs/src/verbs_cm.c
@@ -306,12 +306,15 @@ vrb_msg_ep_reject(struct fid_pep *pep, fid_t handle,
 	vrb_msg_ep_prepare_cm_data(param, paramlen, cm_hdr);
 
 	fastlock_acquire(&_pep->eq->lock);
-	if (connreq->is_xrc)
+	if (connreq->is_xrc) {
 		ret = vrb_msg_xrc_ep_reject(connreq, cm_hdr,
 				(uint8_t)(sizeof(*cm_hdr) + paramlen));
-	else
+	} else if (connreq->id) {
 		ret = rdma_reject(connreq->id, cm_hdr,
 			(uint8_t)(sizeof(*cm_hdr) + paramlen)) ? -errno : 0;
+	} else {
+		ret = -FI_EBUSY;
+	}
 	fastlock_release(&_pep->eq->lock);
 
 	free(connreq);
diff --git a/prov/verbs/src/verbs_cm_xrc.c b/prov/verbs/src/verbs_cm_xrc.c
index d831307..083cb27 100644
--- a/prov/verbs/src/verbs_cm_xrc.c
+++ b/prov/verbs/src/verbs_cm_xrc.c
@@ -165,9 +165,9 @@ static void vrb_log_ep_conn(struct vrb_xrc_ep *ep, char *desc)
 			   (void *) ep, ep->tgt_ibv_qp->qp_num);
 }
 
-/* Caller must hold eq:lock */
 void vrb_free_xrc_conn_setup(struct vrb_xrc_ep *ep, int disconnect)
 {
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(ep->conn_setup);
 
 	/* If a disconnect is requested then the XRC bidirectional connection
@@ -201,13 +201,13 @@ void vrb_free_xrc_conn_setup(struct vrb_xrc_ep *ep, int disconnect)
 	}
 }
 
-/* Caller must hold the eq:lock */
 int vrb_connect_xrc(struct vrb_xrc_ep *ep, struct sockaddr *addr,
 		       int reciprocal, void *param, size_t paramlen)
 {
 	struct vrb_domain *domain = vrb_ep_to_domain(&ep->base_ep);
 	int ret;
 
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(!ep->base_ep.id && !ep->base_ep.ibv_qp && !ep->ini_conn);
 
 	domain->xrc.lock_acquire(&domain->xrc.ini_lock);
@@ -231,11 +231,11 @@ int vrb_connect_xrc(struct vrb_xrc_ep *ep, struct sockaddr *addr,
 	return FI_SUCCESS;
 }
 
-/* Caller must hold the eq:lock */
 void vrb_ep_ini_conn_done(struct vrb_xrc_ep *ep, uint32_t tgt_qpn)
 {
 	struct vrb_domain *domain = vrb_ep_to_domain(&ep->base_ep);
 
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(ep->base_ep.id && ep->ini_conn);
 
 	domain->xrc.lock_acquire(&domain->xrc.ini_lock);
@@ -261,9 +261,9 @@ void vrb_ep_ini_conn_done(struct vrb_xrc_ep *ep, uint32_t tgt_qpn)
 	domain->xrc.lock_release(&domain->xrc.ini_lock);
 }
 
-/* Caller must hold the eq:lock */
 void vrb_ep_ini_conn_rejected(struct vrb_xrc_ep *ep)
 {
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(ep->base_ep.id && ep->ini_conn);
 
 	vrb_log_ep_conn(ep, "INI Connection Rejected");
@@ -281,14 +281,14 @@ void vrb_ep_tgt_conn_done(struct vrb_xrc_ep *ep)
 	}
 }
 
-/* Caller must hold the eq:lock */
 int vrb_resend_shared_accept_xrc(struct vrb_xrc_ep *ep,
-				    struct vrb_connreq *connreq,
-				    struct rdma_cm_id *id)
+				 struct vrb_connreq *connreq,
+				 struct rdma_cm_id *id)
 {
 	struct rdma_conn_param conn_param = { 0 };
 	struct vrb_xrc_cm_data *cm_data = ep->accept_param_data;
 
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(cm_data && ep->tgt_ibv_qp);
 	assert(ep->tgt_ibv_qp->qp_num == connreq->xrc.tgt_qpn);
 	assert(ep->peer_srqn == connreq->xrc.peer_srqn);
@@ -310,9 +310,8 @@ int vrb_resend_shared_accept_xrc(struct vrb_xrc_ep *ep,
 	return rdma_accept(id, &conn_param);
 }
 
-/* Caller must hold the eq:lock */
 int vrb_accept_xrc(struct vrb_xrc_ep *ep, int reciprocal,
-		      void *param, size_t paramlen)
+		   void *param, size_t paramlen)
 {
 	struct sockaddr *addr;
 	struct vrb_connreq *connreq;
@@ -321,6 +320,7 @@ int vrb_accept_xrc(struct vrb_xrc_ep *ep, int reciprocal,
 	struct vrb_xrc_cm_data connect_cm_data;
 	int ret;
 
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	addr = rdma_get_local_addr(ep->tgt_id);
 	if (addr)
 		ofi_straddr_dbg(&vrb_prov, FI_LOG_CORE, "src_addr", addr);
diff --git a/prov/verbs/src/verbs_cq.c b/prov/verbs/src/verbs_cq.c
index 14d91ba..8ba57a4 100644
--- a/prov/verbs/src/verbs_cq.c
+++ b/prov/verbs/src/verbs_cq.c
@@ -226,12 +226,12 @@ vrb_cq_sread(struct fid_cq *cq, void *buf, size_t count, const void *cond,
 	return cur ? cur : ret;
 }
 
-/* Must be called with CQ lock held. */
 int vrb_poll_cq(struct vrb_cq *cq, struct ibv_wc *wc)
 {
 	struct vrb_context *ctx;
 	int ret;
 
+	assert(fastlock_held(&cq->util_cq.cq_lock));
 	do {
 		ret = ibv_poll_cq(cq->cq, 1, wc);
 		if (ret <= 0)
@@ -263,11 +263,11 @@ int vrb_poll_cq(struct vrb_cq *cq, struct ibv_wc *wc)
 	return ret;
 }
 
-/* Must be called with CQ lock held. */
 int vrb_save_wc(struct vrb_cq *cq, struct ibv_wc *wc)
 {
 	struct vrb_wc_entry *wce;
 
+	assert(fastlock_held(&cq->util_cq.cq_lock));
 	wce = ofi_buf_alloc(cq->wce_pool);
 	if (!wce) {
 		FI_WARN(&vrb_prov, FI_LOG_CQ,
@@ -568,6 +568,7 @@ int vrb_cq_open(struct fid_domain *domain_fid, struct fi_cq_attr *attr,
 	size_t size;
 	int ret;
 	struct fi_cq_attr tmp_attr = *attr;
+	int comp_vector = 0;
 
 	cq = calloc(1, sizeof(*cq));
 	if (!cq)
@@ -594,6 +595,19 @@ int vrb_cq_open(struct fid_domain *domain_fid, struct fi_cq_attr *attr,
 		goto err4;
 	}
 
+	if (attr->flags & FI_AFFINITY) {
+		if (attr->signaling_vector < 0 ||
+		    attr->signaling_vector > domain->verbs->num_comp_vectors)  {
+
+			VERBS_WARN(FI_LOG_CQ,
+				   "Invalid value for the CQ attribute signaling_vector: %d\n",
+				   attr->signaling_vector);
+			ret = -FI_EINVAL;
+			goto err4;
+		}
+		comp_vector = attr->signaling_vector;
+	}
+
 	if (cq->wait_obj != FI_WAIT_NONE) {
 		cq->channel = ibv_create_comp_channel(domain->verbs);
 		if (!cq->channel) {
@@ -629,7 +643,7 @@ int vrb_cq_open(struct fid_domain *domain_fid, struct fi_cq_attr *attr,
 	 * num_qp_per_cq = ibv_device_attr->max_cqe / (qp_send_wr + qp_recv_wr)
 	 */
 	cq->cq = ibv_create_cq(domain->verbs, size, cq, cq->channel,
-			       attr->signaling_vector);
+			       comp_vector);
 	if (!cq->cq) {
 		ret = -errno;
 		VERBS_WARN(FI_LOG_CQ, "Unable to create verbs CQ\n");
diff --git a/prov/verbs/src/verbs_dgram_ep_msg.c b/prov/verbs/src/verbs_dgram_ep_msg.c
index 9a9cb62..e018d93 100644
--- a/prov/verbs/src/verbs_dgram_ep_msg.c
+++ b/prov/verbs/src/verbs_dgram_ep_msg.c
@@ -162,10 +162,11 @@ vrb_dgram_ep_senddata(struct fid_ep *ep_fid, const void *buf,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_COMP(ep, (uintptr_t)context),
 		.opcode = IBV_WR_SEND_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.send_flags = VERBS_INJECT(ep, len, desc),
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	if (vrb_dgram_ep_set_addr(ep, dest_addr, &wr))
 		return -FI_ENOENT;
 
@@ -181,10 +182,11 @@ vrb_dgram_ep_injectdata(struct fid_ep *ep_fid, const void *buf, size_t len,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_NO_COMP_FLAG,
 		.opcode = IBV_WR_SEND_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.send_flags = IBV_SEND_INLINE,
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	if (vrb_dgram_ep_set_addr(ep, dest_addr, &wr))
 		return -FI_ENOENT;
 
diff --git a/prov/verbs/src/verbs_domain.c b/prov/verbs/src/verbs_domain.c
index e62d868..d2125a4 100644
--- a/prov/verbs/src/verbs_domain.c
+++ b/prov/verbs/src/verbs_domain.c
@@ -169,7 +169,7 @@ static int vrb_domain_close(fid_t fid)
 			ofi_ns_stop_server(&fab->name_server);
 		break;
 	case FI_EP_MSG:
-		if (domain->flags & VRB_USE_XRC) {
+		if (domain->ext_flags & VRB_USE_XRC) {
 			ret = vrb_domain_xrc_cleanup(domain);
 			if (ret)
 				return ret;
@@ -215,7 +215,7 @@ static int vrb_open_device_by_name(struct vrb_domain *domain, const char *name)
 		const char *rdma_name = ibv_get_device_name(dev_list[i]->device);
 		switch (domain->ep_type) {
 		case FI_EP_MSG:
-			ret = domain->flags & VRB_USE_XRC ?
+			ret = domain->ext_flags & VRB_USE_XRC ?
 				vrb_cmp_xrc_domain_name(name, rdma_name) :
 				strcmp(name, rdma_name);
 			break;
@@ -283,6 +283,7 @@ vrb_domain(struct fid_fabric *fabric, struct fi_info *info,
 		[FI_HMEM_SYSTEM] = default_monitor,
 		[FI_HMEM_CUDA] = default_cuda_monitor,
 		[FI_HMEM_ROCR] = default_rocr_monitor,
+		[FI_HMEM_ZE] = default_ze_monitor,
 	};
 	enum fi_hmem_iface iface;
 	struct vrb_domain *_domain;
@@ -313,7 +314,7 @@ vrb_domain(struct fid_fabric *fabric, struct fi_info *info,
 		goto err2;
 
 	_domain->ep_type = VRB_EP_TYPE(info);
-	_domain->flags |= vrb_is_xrc_info(info) ? VRB_USE_XRC : 0;
+	_domain->ext_flags |= vrb_is_xrc_info(info) ? VRB_USE_XRC : 0;
 
 	ret = vrb_open_device_by_name(_domain, info->domain_attr->name);
 	if (ret)
@@ -325,7 +326,7 @@ vrb_domain(struct fid_fabric *fabric, struct fi_info *info,
 		goto err3;
 	}
 
-	_domain->flags |= vrb_odp_flag(_domain->verbs);
+	_domain->ext_flags |= vrb_odp_flag(_domain->verbs);
 	_domain->util_domain.domain_fid.fid.fclass = FI_CLASS_DOMAIN;
 	_domain->util_domain.domain_fid.fid.context = context;
 	_domain->util_domain.domain_fid.fid.ops = &vrb_fid_ops;
@@ -369,7 +370,7 @@ vrb_domain(struct fid_fabric *fabric, struct fi_info *info,
 		_domain->util_domain.domain_fid.ops = &vrb_dgram_domain_ops;
 		break;
 	case FI_EP_MSG:
-		if (_domain->flags & VRB_USE_XRC) {
+		if (_domain->ext_flags & VRB_USE_XRC) {
 			ret = vrb_domain_xrc_init(_domain);
 			if (ret)
 				goto err4;
diff --git a/prov/verbs/src/verbs_domain_xrc.c b/prov/verbs/src/verbs_domain_xrc.c
index 553dde1..4a43dab 100644
--- a/prov/verbs/src/verbs_domain_xrc.c
+++ b/prov/verbs/src/verbs_domain_xrc.c
@@ -58,6 +58,7 @@ static int vrb_create_ini_qp(struct vrb_xrc_ep *ep)
 	attr_ex.comp_mask = IBV_QP_INIT_ATTR_PD;
 	attr_ex.pd = domain->pd;
 	attr_ex.qp_context = domain;
+	attr_ex.srq = NULL;
 
 	ret = rdma_create_qp_ex(ep->base_ep.id, &attr_ex);
 	if (ret) {
@@ -80,15 +81,15 @@ static inline void vrb_set_ini_conn_key(struct vrb_xrc_ep *ep,
 				  struct vrb_cq, util_cq);
 }
 
-/* Caller must hold domain:xrc.ini_lock */
 int vrb_get_shared_ini_conn(struct vrb_xrc_ep *ep,
-			       struct vrb_ini_shared_conn **ini_conn) {
+			    struct vrb_ini_shared_conn **ini_conn) {
 	struct vrb_domain *domain = vrb_ep_to_domain(&ep->base_ep);
 	struct vrb_ini_conn_key key;
 	struct vrb_ini_shared_conn *conn;
 	struct ofi_rbnode *node;
 	int ret;
 
+	assert(fastlock_held(&domain->xrc.ini_lock));
 	vrb_set_ini_conn_key(ep, &key);
 	node = ofi_rbmap_find(domain->xrc.ini_conn_rbmap, &key);
 	if (node) {
@@ -136,13 +137,13 @@ insert_err:
 	return ret;
 }
 
-/* Caller must hold domain:xrc.ini_lock */
 void _vrb_put_shared_ini_conn(struct vrb_xrc_ep *ep)
 {
 	struct vrb_domain *domain = vrb_ep_to_domain(&ep->base_ep);
 	struct vrb_ini_shared_conn *ini_conn;
 	struct vrb_ini_conn_key key;
 
+	assert(fastlock_held(&domain->xrc.ini_lock));
 	if (!ep->ini_conn)
 		return;
 
@@ -190,10 +191,11 @@ void vrb_put_shared_ini_conn(struct vrb_xrc_ep *ep)
 	domain->xrc.lock_release(&domain->xrc.ini_lock);
 }
 
-/* Caller must hold domain:xrc.ini_lock */
 void vrb_add_pending_ini_conn(struct vrb_xrc_ep *ep, int reciprocal,
-				 void *conn_param, size_t conn_paramlen)
+			      void *conn_param, size_t conn_paramlen)
 {
+	assert(fastlock_held(&vrb_ep_to_domain(&ep->base_ep)->xrc.ini_lock));
+
 	ep->conn_setup->pending_recip = reciprocal;
 	ep->conn_setup->pending_paramlen = MIN(conn_paramlen,
 				sizeof(ep->conn_setup->pending_param));
@@ -428,9 +430,9 @@ static int vrb_put_tgt_qp(struct vrb_xrc_ep *ep)
 	return FI_SUCCESS;
 }
 
-/* Caller must hold eq:lock */
 int vrb_ep_destroy_xrc_qp(struct vrb_xrc_ep *ep)
 {
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	vrb_put_shared_ini_conn(ep);
 
 	if (ep->base_ep.id) {
@@ -540,7 +542,7 @@ int vrb_domain_xrc_init(struct vrb_domain *domain)
 		domain->xrc.lock_acquire = ofi_fastlock_acquire;
 		domain->xrc.lock_release = ofi_fastlock_release;
 	}
-	domain->flags |= VRB_USE_XRC;
+	domain->ext_flags |= VRB_USE_XRC;
 	return FI_SUCCESS;
 
 rbmap_err:
diff --git a/prov/verbs/src/verbs_ep.c b/prov/verbs/src/verbs_ep.c
index 2898d76..4bd2e6a 100644
--- a/prov/verbs/src/verbs_ep.c
+++ b/prov/verbs/src/verbs_ep.c
@@ -432,12 +432,12 @@ static int vrb_close_free_ep(struct vrb_ep *ep)
 	return 0;
 }
 
-/* Caller must hold eq:lock */
-static inline void vrb_ep_xrc_close(struct vrb_ep *ep)
+static void vrb_ep_xrc_close(struct vrb_ep *ep)
 {
 	struct vrb_xrc_ep *xrc_ep = container_of(ep, struct vrb_xrc_ep,
-						    base_ep);
+						 base_ep);
 
+	assert(fastlock_held(&ep->eq->lock));
 	if (xrc_ep->conn_setup)
 		vrb_free_xrc_conn_setup(xrc_ep, 0);
 
@@ -687,7 +687,6 @@ static int vrb_create_dgram_ep(struct vrb_domain *domain, struct vrb_ep *ep,
 	return 0;
 }
 
-/* vrb_srq_ep::xrc.prepost_lock must be held */
 FI_VERBS_XRC_ONLY
 static int vrb_process_xrc_preposted(struct vrb_srq_ep *srq_ep)
 {
@@ -695,6 +694,7 @@ static int vrb_process_xrc_preposted(struct vrb_srq_ep *srq_ep)
 	struct slist_entry *entry;
 	int ret;
 
+	assert(fastlock_held(&srq_ep->xrc.prepost_lock));
 	/* The pre-post SRQ function ops have been replaced so the
 	 * posting here results in adding the RX entries to the SRQ */
 	while (!slist_empty(&srq_ep->xrc.prepost_list)) {
@@ -875,7 +875,7 @@ static int vrb_ep_enable(struct fid_ep *ep_fid)
 		if (ep->srq_ep) {
 			/* Override receive function pointers to prevent the user from
 			 * posting Receive WRs to a QP where a SRQ is attached to it */
-			if (domain->flags & VRB_USE_XRC) {
+			if (domain->ext_flags & VRB_USE_XRC) {
 				*ep->util_ep.ep_fid.msg = vrb_msg_srq_xrc_ep_msg_ops;
 				return vrb_ep_enable_xrc(ep);
 			} else {
@@ -883,7 +883,7 @@ static int vrb_ep_enable(struct fid_ep *ep_fid)
 				ep->util_ep.ep_fid.msg->recvv = fi_no_msg_recvv;
 				ep->util_ep.ep_fid.msg->recvmsg = fi_no_msg_recvmsg;
 			}
-		} else if (domain->flags & VRB_USE_XRC) {
+		} else if (domain->ext_flags & VRB_USE_XRC) {
 			VERBS_WARN(FI_LOG_EP_CTRL, "XRC EP_MSG not bound "
 				   "to srx_context\n");
 			return -FI_EINVAL;
@@ -1115,7 +1115,7 @@ int vrb_open_ep(struct fid_domain *domain, struct fi_info *info,
 
 	switch (info->ep_attr->type) {
 	case FI_EP_MSG:
-		if (dom->flags & VRB_USE_XRC) {
+		if (dom->ext_flags & VRB_USE_XRC) {
 			if (dom->util_domain.threading == FI_THREAD_SAFE) {
 				*ep->util_ep.ep_fid.msg = vrb_msg_xrc_ep_msg_ops_ts;
 				ep->util_ep.ep_fid.rma = &vrb_msg_xrc_ep_rma_ops_ts;
@@ -1139,7 +1139,7 @@ int vrb_open_ep(struct fid_domain *domain, struct fi_info *info,
 
 		if (!info->handle) {
 			/* Only RC, XRC active RDMA CM ID is created at connect */
-			if (!(dom->flags & VRB_USE_XRC)) {
+			if (!(dom->ext_flags & VRB_USE_XRC)) {
 				ret = vrb_create_ep(ep,
 					vrb_get_port_space(info->addr_format), &ep->id);
 				if (ret)
@@ -1149,7 +1149,7 @@ int vrb_open_ep(struct fid_domain *domain, struct fi_info *info,
 		} else if (info->handle->fclass == FI_CLASS_CONNREQ) {
 			connreq = container_of(info->handle,
 					       struct vrb_connreq, handle);
-			if (dom->flags & VRB_USE_XRC) {
+			if (dom->ext_flags & VRB_USE_XRC) {
 				assert(connreq->is_xrc);
 
 				if (!connreq->xrc.is_reciprocal) {
@@ -1159,7 +1159,12 @@ int vrb_open_ep(struct fid_domain *domain, struct fi_info *info,
 						goto err1;
 				}
 			} else {
+				/* ep now owns this rdma cm id, prevent trying to access
+				 * it outside of ep operations to avoid possible use-after-
+				 * free bugs in case the ep is closed
+				 */
 				ep->id = connreq->id;
+				connreq->id = NULL;
 				ep->ibv_qp = ep->id->qp;
 				ep->id->context = &ep->util_ep.ep_fid.fid;
 			}
@@ -1626,12 +1631,12 @@ static void vrb_cleanup_prepost_bufs(struct vrb_srq_ep *srq_ep)
 	}
 }
 
-/* Must hold the associated CQ lock cq::xrc.srq_list_lock */
 int vrb_xrc_close_srq(struct vrb_srq_ep *srq_ep)
 {
 	int ret;
 
-	assert(srq_ep->domain->flags & VRB_USE_XRC);
+	assert(fastlock_held(&srq_ep->xrc.cq->xrc.srq_list_lock));
+	assert(srq_ep->domain->ext_flags & VRB_USE_XRC);
 	if (!srq_ep->xrc.cq || !srq_ep->srq)
 		return FI_SUCCESS;
 
@@ -1656,7 +1661,7 @@ static int vrb_srq_close(fid_t fid)
 	struct vrb_cq *cq = srq_ep->xrc.cq;
 	int ret;
 
-	if (srq_ep->domain->flags & VRB_USE_XRC) {
+	if (srq_ep->domain->ext_flags & VRB_USE_XRC) {
 		if (cq) {
 			fastlock_acquire(&cq->xrc.srq_list_lock);
 			ret = vrb_xrc_close_srq(srq_ep);
@@ -1724,7 +1729,7 @@ int vrb_srq_context(struct fid_domain *domain, struct fi_rx_attr *attr,
 
 	/* XRC SRQ creation is delayed until the first endpoint it is bound
 	 * to is enabled.*/
-	if (dom->flags & VRB_USE_XRC) {
+	if (dom->ext_flags & VRB_USE_XRC) {
 		fastlock_init(&srq_ep->xrc.prepost_lock);
 		slist_init(&srq_ep->xrc.prepost_list);
 		dlist_init(&srq_ep->xrc.srq_entry);
diff --git a/prov/verbs/src/verbs_eq.c b/prov/verbs/src/verbs_eq.c
index 33473b6..1b861ca 100644
--- a/prov/verbs/src/verbs_eq.c
+++ b/prov/verbs/src/verbs_eq.c
@@ -76,11 +76,11 @@ unlock:
 	return rd;
 }
 
-/* Caller must hold eq:lock */
 void vrb_eq_set_xrc_conn_tag(struct vrb_xrc_ep *ep)
 {
 	struct vrb_eq *eq = ep->base_ep.eq;
 
+	assert(fastlock_held(&eq->lock));
 	assert(ep->conn_setup);
 	assert(ep->conn_setup->conn_tag == VERBS_CONN_TAG_INVALID);
 	ep->conn_setup->conn_tag =
@@ -88,12 +88,12 @@ void vrb_eq_set_xrc_conn_tag(struct vrb_xrc_ep *ep)
 				ofi_idx_insert(eq->xrc.conn_key_map, ep));
 }
 
-/* Caller must hold eq:lock */
 void vrb_eq_clear_xrc_conn_tag(struct vrb_xrc_ep *ep)
 {
 	struct vrb_eq *eq = ep->base_ep.eq;
 	int index;
 
+	assert(fastlock_held(&eq->lock));
 	assert(ep->conn_setup);
 	if (ep->conn_setup->conn_tag == VERBS_CONN_TAG_INVALID)
 		return;
@@ -108,13 +108,13 @@ void vrb_eq_clear_xrc_conn_tag(struct vrb_xrc_ep *ep)
 	ep->conn_setup->conn_tag = VERBS_CONN_TAG_INVALID;
 }
 
-/* Caller must hold eq:lock */
 struct vrb_xrc_ep *vrb_eq_xrc_conn_tag2ep(struct vrb_eq *eq,
-						uint32_t conn_tag)
+					  uint32_t conn_tag)
 {
 	struct vrb_xrc_ep *ep;
 	int index;
 
+	assert(fastlock_held(&eq->lock));
 	index = ofi_key2idx(&eq->xrc.conn_key_idx, (uint64_t)conn_tag);
 	ep = ofi_idx_lookup(eq->xrc.conn_key_map, index);
 	if (!ep || ep->magic != VERBS_XRC_EP_MAGIC) {
@@ -354,14 +354,14 @@ static int vrb_sidr_conn_compare(struct ofi_rbmap *map,
 		-1 : _key->recip > ep->recip_accept;
 }
 
-/* Caller must hold eq:lock */
 struct vrb_xrc_ep *vrb_eq_get_sidr_conn(struct vrb_eq *eq,
-					      struct sockaddr *peer,
-					      uint16_t pep_port, bool recip)
+					struct sockaddr *peer,
+					uint16_t pep_port, bool recip)
 {
 	struct ofi_rbnode *node;
 	struct vrb_sidr_conn_key key;
 
+	assert(fastlock_held(&eq->lock));
 	vrb_set_sidr_conn_key(peer, pep_port, recip, &key);
 	node = ofi_rbmap_find(&eq->xrc.sidr_conn_rbmap, &key);
 	if (OFI_LIKELY(!node))
@@ -370,13 +370,13 @@ struct vrb_xrc_ep *vrb_eq_get_sidr_conn(struct vrb_eq *eq,
 	return (struct vrb_xrc_ep *) node->data;
 }
 
-/* Caller must hold eq:lock */
 int vrb_eq_add_sidr_conn(struct vrb_xrc_ep *ep,
-			    void *param_data, size_t param_len)
+			 void *param_data, size_t param_len)
 {
 	int ret;
 	struct vrb_sidr_conn_key key;
 
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(!ep->accept_param_data);
 	assert(param_len);
 	assert(ep->tgt_id && ep->tgt_id->ps == RDMA_PS_UDP);
@@ -406,9 +406,9 @@ int vrb_eq_add_sidr_conn(struct vrb_xrc_ep *ep,
 	return FI_SUCCESS;
 }
 
-/* Caller must hold eq:lock */
 void vrb_eq_remove_sidr_conn(struct vrb_xrc_ep *ep)
 {
+	assert(fastlock_held(&ep->base_ep.eq->lock));
 	assert(ep->conn_map_node);
 
 	ofi_rbmap_delete(&ep->base_ep.eq->xrc.sidr_conn_rbmap,
@@ -654,7 +654,6 @@ vrb_eq_xrc_recip_conn_event(struct vrb_eq *eq,
 	return sizeof(*entry) + len;
 }
 
-/* Caller must hold eq:lock */
 static int
 vrb_eq_xrc_rej_event(struct vrb_eq *eq, struct rdma_cm_event *cma_event)
 {
@@ -663,6 +662,7 @@ vrb_eq_xrc_rej_event(struct vrb_eq *eq, struct rdma_cm_event *cma_event)
 	struct vrb_xrc_conn_info xrc_info;
 	enum vrb_xrc_ep_conn_state state;
 
+	assert(fastlock_held(&eq->lock));
 	ep = container_of(fid, struct vrb_xrc_ep, base_ep.util_ep.ep_fid);
 	if (ep->magic != VERBS_XRC_EP_MAGIC) {
 		VERBS_WARN(FI_LOG_EP_CTRL,
@@ -703,11 +703,12 @@ vrb_eq_xrc_rej_event(struct vrb_eq *eq, struct rdma_cm_event *cma_event)
 	return state == VRB_XRC_ORIG_CONNECTING ? FI_SUCCESS : -FI_EAGAIN;
 }
 
-/* Caller must hold eq:lock */
-static inline int
+static int
 vrb_eq_xrc_connect_retry(struct vrb_xrc_ep *ep,
 			 struct rdma_cm_event *cma_event, int *acked)
 {
+	assert(fastlock_held(&ep->base_ep.eq->lock));
+
 	if (ep->base_ep.info_attr.src_addr)
 		ofi_straddr_dbg(&vrb_prov, FI_LOG_EP_CTRL,
 				"Connect retry src ",
@@ -728,15 +729,15 @@ vrb_eq_xrc_connect_retry(struct vrb_xrc_ep *ep,
 			       ep->conn_setup->pending_paramlen);
 }
 
-/* Caller must hold eq:lock */
-static inline int
+static int
 vrb_eq_xrc_cm_err_event(struct vrb_eq *eq,
-                           struct rdma_cm_event *cma_event, int *acked)
+                        struct rdma_cm_event *cma_event, int *acked)
 {
 	struct vrb_xrc_ep *ep;
 	fid_t fid = cma_event->id->context;
 	int ret;
 
+	assert(fastlock_held(&eq->lock));
 	ep = container_of(fid, struct vrb_xrc_ep, base_ep.util_ep.ep_fid);
 	if (ep->magic != VERBS_XRC_EP_MAGIC) {
 		VERBS_WARN(FI_LOG_EP_CTRL, "CM ID context invalid\n");
@@ -776,12 +777,11 @@ vrb_eq_xrc_cm_err_event(struct vrb_eq *eq,
         return FI_SUCCESS;
 }
 
-/* Caller must hold eq:lock */
-static inline int
+static int
 vrb_eq_xrc_connected_event(struct vrb_eq *eq,
-			      struct rdma_cm_event *cma_event, int *acked,
-			      struct fi_eq_cm_entry *entry, size_t len,
-			      uint32_t *event)
+			   struct rdma_cm_event *cma_event, int *acked,
+			   struct fi_eq_cm_entry *entry, size_t len,
+			   uint32_t *event)
 {
 	struct vrb_xrc_ep *ep;
 	fid_t fid = cma_event->id->context;
@@ -789,6 +789,7 @@ vrb_eq_xrc_connected_event(struct vrb_eq *eq,
 
 	ep = container_of(fid, struct vrb_xrc_ep, base_ep.util_ep.ep_fid);
 
+	assert(fastlock_held(&eq->lock));
 	assert(ep->conn_state == VRB_XRC_ORIG_CONNECTING ||
 	       ep->conn_state == VRB_XRC_RECIP_CONNECTING);
 
@@ -807,14 +808,15 @@ vrb_eq_xrc_connected_event(struct vrb_eq *eq,
 	return ret;
 }
 
-/* Caller must hold eq:lock */
-static inline void
+static void
 vrb_eq_xrc_timewait_event(struct vrb_eq *eq,
-			     struct rdma_cm_event *cma_event, int *acked)
+			  struct rdma_cm_event *cma_event, int *acked)
 {
 	fid_t fid = cma_event->id->context;
 	struct vrb_xrc_ep *ep = container_of(fid, struct vrb_xrc_ep,
 						base_ep.util_ep.ep_fid);
+
+	assert(fastlock_held(&eq->lock));
 	assert(ep->magic == VERBS_XRC_EP_MAGIC);
 	assert(ep->conn_setup);
 
@@ -833,14 +835,15 @@ vrb_eq_xrc_timewait_event(struct vrb_eq *eq,
 		vrb_free_xrc_conn_setup(ep, 0);
 }
 
-/* Caller must hold eq:lock */
 static inline void
 vrb_eq_xrc_disconnect_event(struct vrb_eq *eq,
 			       struct rdma_cm_event *cma_event, int *acked)
 {
 	fid_t fid = cma_event->id->context;
 	struct vrb_xrc_ep *ep = container_of(fid, struct vrb_xrc_ep,
-						base_ep.util_ep.ep_fid);
+					     base_ep.util_ep.ep_fid);
+
+	assert(fastlock_held(&eq->lock));
 	assert(ep->magic == VERBS_XRC_EP_MAGIC);
 
 	if (ep->conn_setup && cma_event->id == ep->base_ep.id) {
@@ -918,9 +921,7 @@ vrb_eq_cm_process_event(struct vrb_eq *eq,
 		if (cma_event->id->qp &&
 		    cma_event->id->qp->context->device->transport_type !=
 		    IBV_TRANSPORT_IWARP) {
-			ret = vrb_set_rnr_timer(cma_event->id->qp);
-			if (ret)
-				goto ack;
+			vrb_set_rnr_timer(cma_event->id->qp);
 		}
 		ep = container_of(fid, struct vrb_ep, util_ep.ep_fid);
 		if (vrb_is_xrc_ep(ep)) {
@@ -1061,12 +1062,12 @@ int vrb_eq_match_event(struct dlist_entry *item, const void *arg)
 	}
 }
 
-/* Caller must hold eq->lock */
 void vrb_eq_remove_events(struct vrb_eq *eq, struct fid *fid)
 {
 	struct dlist_entry *item;
 	struct vrb_eq_entry *entry;
 
+	assert(fastlock_held(&eq->lock));
 	while ((item =
 		dlistfd_remove_first_match(&eq->list_head,
 					   vrb_eq_match_event, fid))) {
@@ -1205,7 +1206,7 @@ vrb_eq_sread(struct fid_eq *eq_fid, uint32_t *event,
 		void *buf, size_t len, int timeout, uint64_t flags)
 {
 	struct vrb_eq *eq;
-	void *contexts;
+	struct ofi_epollfds_event fdevent;
 	ssize_t ret;
 
 	eq = container_of(eq_fid, struct vrb_eq, eq_fid.fid);
@@ -1215,12 +1216,12 @@ vrb_eq_sread(struct fid_eq *eq_fid, uint32_t *event,
 		if (ret && (ret != -FI_EAGAIN))
 			return ret;
 
-		ret = ofi_epoll_wait(eq->epollfd, &contexts, 1, timeout);
+		ret = ofi_epoll_wait(eq->epollfd, &fdevent, 1, timeout);
 		if (ret == 0)
 			return -FI_EAGAIN;
 		else if (ret < 0)
 			return -errno;
-	};
+	}
 }
 
 static const char *
diff --git a/prov/verbs/src/verbs_info.c b/prov/verbs/src/verbs_info.c
index 616d3b7..b2ea962 100644
--- a/prov/verbs/src/verbs_info.c
+++ b/prov/verbs/src/verbs_info.c
@@ -734,6 +734,9 @@ static bool vrb_hmem_supported(const char *dev_name)
 	if (vrb_gl_data.peer_mem_support && strstr(dev_name, "mlx"))
 		return true;
 
+	if (vrb_gl_data.dmabuf_support && strstr(dev_name, "mlx5"))
+		return true;
+
 	return false;
 }
 
diff --git a/prov/verbs/src/verbs_mr.c b/prov/verbs/src/verbs_mr.c
index 215d412..8a6def2 100644
--- a/prov/verbs/src/verbs_mr.c
+++ b/prov/verbs/src/verbs_mr.c
@@ -57,6 +57,31 @@ static struct fi_ops vrb_mr_fi_ops = {
 	.ops_open = fi_no_ops_open,
 };
 
+#if VERBS_HAVE_DMABUF_MR
+static inline
+struct ibv_mr *vrb_mr_ibv_reg_dmabuf_mr(struct ibv_pd *pd, const void *buf,
+				        size_t len, int vrb_access)
+{
+	void *handle;
+	void *base;
+	uint64_t offset;
+	int err;
+
+	err = ze_hmem_get_handle((void *)buf, &handle);
+	if (err)
+		return NULL;
+
+	err = ze_hmem_get_base_addr((void *)buf, &base);
+	if (err)
+		return NULL;
+
+	offset = (uintptr_t)buf - (uintptr_t)base;
+	return ibv_reg_dmabuf_mr(pd, offset, len, (uint64_t)buf/* iova */,
+				 (int)(uintptr_t)handle/* dmabuf fd */,
+				 vrb_access);
+}
+#endif
+
 static inline
 int vrb_mr_reg_common(struct vrb_mem_desc *md, int vrb_access, const void *buf,
 		      size_t len, void *context, enum fi_hmem_iface iface,
@@ -70,10 +95,17 @@ int vrb_mr_reg_common(struct vrb_mem_desc *md, int vrb_access, const void *buf,
 	md->info.iov.iov_base = (void *) buf;
 	md->info.iov.iov_len = len;
 
-	if (md->domain->flags & VRB_USE_ODP && iface == FI_HMEM_SYSTEM)
+	if (md->domain->ext_flags & VRB_USE_ODP && iface == FI_HMEM_SYSTEM)
 		vrb_access |= VRB_ACCESS_ON_DEMAND;
 
-	md->mr = ibv_reg_mr(md->domain->pd, (void *) buf, len, vrb_access);
+#if VERBS_HAVE_DMABUF_MR
+	if (iface == FI_HMEM_ZE)
+		md->mr = vrb_mr_ibv_reg_dmabuf_mr(md->domain->pd, buf, len,
+					          vrb_access);
+	else
+#endif
+		md->mr = ibv_reg_mr(md->domain->pd, (void *) buf, len,
+				    vrb_access);
 	if (!md->mr) {
 		if (len)
 			return -errno;
diff --git a/prov/verbs/src/verbs_msg.c b/prov/verbs/src/verbs_msg.c
index 17fc9a5..31948c8 100644
--- a/prov/verbs/src/verbs_msg.c
+++ b/prov/verbs/src/verbs_msg.c
@@ -127,10 +127,11 @@ vrb_msg_ep_senddata(struct fid_ep *ep_fid, const void *buf, size_t len,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_COMP(ep, (uintptr_t)context),
 		.opcode = IBV_WR_SEND_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.send_flags = VERBS_INJECT(ep, len, desc),
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	return vrb_send_buf(ep, &wr, buf, len, desc);
 }
 
@@ -171,10 +172,11 @@ static ssize_t vrb_msg_ep_injectdata(struct fid_ep *ep_fid, const void *buf, siz
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_NO_COMP_FLAG,
 		.opcode = IBV_WR_SEND_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.send_flags = IBV_SEND_INLINE,
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	return vrb_send_buf(ep, &wr, buf, len, NULL);
 }
 
@@ -283,10 +285,11 @@ vrb_msg_xrc_ep_senddata(struct fid_ep *ep_fid, const void *buf, size_t len,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_COMP(&ep->base_ep, (uintptr_t)context),
 		.opcode = IBV_WR_SEND_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.send_flags = VERBS_INJECT(&ep->base_ep, len, desc),
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	VRB_SET_REMOTE_SRQN(wr, ep->peer_srqn);
 
 	return vrb_send_buf(&ep->base_ep, &wr, buf, len, desc);
@@ -333,10 +336,11 @@ static ssize_t vrb_msg_xrc_ep_injectdata(struct fid_ep *ep_fid, const void *buf,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_NO_COMP_FLAG,
 		.opcode = IBV_WR_SEND_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.send_flags = IBV_SEND_INLINE,
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	VRB_SET_REMOTE_SRQN(wr, ep->peer_srqn);
 
 	return vrb_send_buf(&ep->base_ep, &wr, buf, len, NULL);
diff --git a/prov/verbs/src/verbs_rma.c b/prov/verbs/src/verbs_rma.c
index 074baff..188dcc4 100644
--- a/prov/verbs/src/verbs_rma.c
+++ b/prov/verbs/src/verbs_rma.c
@@ -167,12 +167,13 @@ vrb_msg_ep_rma_writedata(struct fid_ep *ep_fid, const void *buf, size_t len,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_COMP(ep, (uintptr_t)context),
 		.opcode = IBV_WR_RDMA_WRITE_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.wr.rdma.remote_addr = addr,
 		.wr.rdma.rkey = (uint32_t)key,
 		.send_flags = VERBS_INJECT(ep, len, desc),
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	return vrb_send_buf(ep, &wr, buf, len, desc);
 }
 
@@ -220,12 +221,13 @@ vrb_msg_ep_rma_inject_writedata(struct fid_ep *ep_fid, const void *buf, size_t l
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_NO_COMP_FLAG,
 		.opcode = IBV_WR_RDMA_WRITE_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.wr.rdma.remote_addr = addr,
 		.wr.rdma.rkey = (uint32_t)key,
 		.send_flags = IBV_SEND_INLINE,
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	return vrb_send_buf(ep, &wr, buf, len, NULL);
 }
 
@@ -413,12 +415,13 @@ vrb_msg_xrc_ep_rma_writedata(struct fid_ep *ep_fid, const void *buf,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_COMP(&ep->base_ep, (uintptr_t)context),
 		.opcode = IBV_WR_RDMA_WRITE_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.wr.rdma.remote_addr = addr,
 		.wr.rdma.rkey = (uint32_t)key,
 		.send_flags = VERBS_INJECT(&ep->base_ep, len, desc),
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	VRB_SET_REMOTE_SRQN(wr, ep->peer_srqn);
 
 	return vrb_send_buf(&ep->base_ep, &wr, buf, len, desc);
@@ -471,12 +474,13 @@ vrb_msg_xrc_ep_rma_inject_writedata(struct fid_ep *ep_fid,
 	struct ibv_send_wr wr = {
 		.wr_id = VERBS_NO_COMP_FLAG,
 		.opcode = IBV_WR_RDMA_WRITE_WITH_IMM,
-		.imm_data = htonl((uint32_t)data),
 		.wr.rdma.remote_addr = addr,
 		.wr.rdma.rkey = (uint32_t)key,
 		.send_flags = IBV_SEND_INLINE,
 	};
 
+	wr.imm_data = htonl((uint32_t)data);
+
 	VRB_SET_REMOTE_SRQN(wr, ep->peer_srqn);
 
 	return vrb_send_buf(&ep->base_ep, &wr, buf, len, NULL);
diff --git a/src/common.c b/src/common.c
index e196d1f..fdaece4 100644
--- a/src/common.c
+++ b/src/common.c
@@ -49,6 +49,7 @@
 
 #include <inttypes.h>
 #include <netinet/in.h>
+#include <netinet/ip.h>
 #include <arpa/inet.h>
 #include <sys/types.h>
 #include <sys/socket.h>
@@ -67,6 +68,7 @@
 #include <ofi_epoll.h>
 #include <ofi_list.h>
 #include <ofi_osd.h>
+#include <ofi_iov.h>
 #include <shared/ofi_str.h>
 
 struct fi_provider core_prov = {
@@ -1003,6 +1005,338 @@ int ofi_discard_socket(SOCKET sock, size_t len)
 	return ret;
 }
 
+size_t ofi_byteq_readv(struct ofi_byteq *byteq, struct iovec *iov,
+		       size_t cnt, size_t offset)
+{
+	size_t avail, len;
+
+	if (cnt == 1 && !offset)
+		return ofi_byteq_read(byteq, iov[0].iov_base, iov[0].iov_len);
+
+	avail = ofi_byteq_readable(byteq);
+	if (!avail)
+		return 0;
+
+	len = ofi_copy_iov_buf(iov, cnt, offset, &byteq->data[byteq->head],
+			       avail, OFI_COPY_BUF_TO_IOV);
+	if (len < avail) {
+		byteq->head += len;
+	} else {
+		byteq->head = 0;
+		byteq->tail = 0;
+	}
+	return len;
+}
+
+void ofi_byteq_writev(struct ofi_byteq *byteq, const struct iovec *iov,
+		      size_t cnt)
+{
+	size_t i;
+
+	assert(ofi_total_iov_len(iov, cnt) <= ofi_byteq_writeable(byteq));
+
+	if (cnt == 1) {
+		ofi_byteq_write(byteq, iov[0].iov_base, iov[0].iov_len);
+		return;
+	}
+
+	for (i = 0; i < cnt; i++) {
+		memcpy(&byteq->data[byteq->tail], iov[i].iov_base,
+		       iov[i].iov_len);
+		byteq->tail += iov[i].iov_len;
+	}
+}
+
+
+ssize_t ofi_bsock_flush(struct ofi_bsock *bsock)
+{
+	ssize_t ret;
+
+	if (!ofi_bsock_tosend(bsock))
+		return 0;
+
+	ret = ofi_byteq_send(&bsock->sq, bsock->sock);
+	if (ret < 0) {
+		return ofi_sockerr() == EPIPE ?
+			-FI_ENOTCONN : -ofi_sockerr();
+	}
+
+	return ofi_bsock_tosend(bsock) ? -FI_EAGAIN : 0;
+}
+
+ssize_t ofi_bsock_send(struct ofi_bsock *bsock, const void *buf, size_t *len)
+{
+	size_t avail;
+	ssize_t ret;
+
+	avail = ofi_bsock_tosend(bsock);
+	if (avail) {
+		if (*len < ofi_byteq_writeable(&bsock->sq)) {
+			ofi_byteq_write(&bsock->sq, buf, *len);
+			ret = ofi_bsock_flush(bsock);
+			return !ret || ret == -FI_EAGAIN ? *len : ret;
+		}
+
+		ret = ofi_bsock_flush(bsock);
+		if (ret)
+			return ret;
+	}
+
+	assert(!ofi_bsock_tosend(bsock));
+	if (*len > bsock->zerocopy_size) {
+		ret = ofi_send_socket(bsock->sock, buf, *len,
+				      MSG_NOSIGNAL | OFI_ZEROCOPY);
+		if (ret >= 0) {
+			bsock->async_index++;
+			*len = ret;
+			return -FI_EINPROGRESS;
+		}
+	} else {
+		ret = ofi_send_socket(bsock->sock, buf, *len, MSG_NOSIGNAL);
+	}
+	if (ret < 0) {
+		if (OFI_SOCK_TRY_SND_RCV_AGAIN(ofi_sockerr()) &&
+		    *len < ofi_byteq_writeable(&bsock->sq)) {
+			ofi_byteq_write(&bsock->sq, buf, *len);
+			return *len;
+		}
+		return ofi_sockerr() == EPIPE ? -FI_ENOTCONN : -ofi_sockerr();
+	}
+	*len = ret;
+	return ret;
+}
+
+ssize_t ofi_bsock_sendv(struct ofi_bsock *bsock, const struct iovec *iov,
+			size_t cnt, size_t *len)
+{
+	struct msghdr msg;
+	size_t avail;
+	ssize_t ret;
+
+	if (cnt == 1) {
+		*len = iov[0].iov_len;
+		return ofi_bsock_send(bsock, iov[0].iov_base, len);
+	}
+
+	*len = ofi_total_iov_len(iov, cnt);
+	avail = ofi_bsock_tosend(bsock);
+	if (avail) {
+		if (*len < ofi_byteq_writeable(&bsock->sq)) {
+			ofi_byteq_writev(&bsock->sq, iov, cnt);
+			ret = ofi_bsock_flush(bsock);
+			return !ret || ret == -FI_EAGAIN ? *len : ret;
+		}
+
+		ret = ofi_bsock_flush(bsock);
+		if (ret)
+			return ret;
+	}
+
+	assert(!ofi_bsock_tosend(bsock));
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	msg.msg_flags = 0;
+	msg.msg_name = NULL;
+	msg.msg_namelen = 0;
+	msg.msg_iov = (struct iovec *) iov;
+	msg.msg_iovlen = cnt;
+
+	if (*len > bsock->zerocopy_size) {
+		ret = ofi_sendmsg_tcp(bsock->sock, &msg,
+				      MSG_NOSIGNAL | OFI_ZEROCOPY);
+		if (ret >= 0) {
+			bsock->async_index++;
+			*len = ret;
+			return -FI_EINPROGRESS;
+		}
+	} else {
+		ret = ofi_sendmsg_tcp(bsock->sock, &msg, MSG_NOSIGNAL);
+	}
+	if (ret < 0) {
+		if (OFI_SOCK_TRY_SND_RCV_AGAIN(ofi_sockerr()) &&
+		    *len < ofi_byteq_writeable(&bsock->sq)) {
+			ofi_byteq_writev(&bsock->sq, iov, cnt);
+			return *len;
+		}
+		return ofi_sockerr() == EPIPE ? -FI_ENOTCONN : -ofi_sockerr();
+	}
+	*len = ret;
+	return ret;
+}
+
+ssize_t ofi_bsock_recv(struct ofi_bsock *bsock, void *buf, size_t len)
+{
+	size_t bytes;
+	ssize_t ret;
+
+	bytes = ofi_byteq_read(&bsock->rq, buf, len);
+	if (bytes) {
+		if (bytes == len)
+			return len;
+		buf = (char *) buf + bytes;
+		len -= bytes;
+	}
+
+	assert(!ofi_bsock_readable(bsock));
+	if (len < (bsock->rq.size >> 1)) {
+		ret = ofi_byteq_recv(&bsock->rq, bsock->sock);
+		if (ret <= 0)
+			goto out;
+
+		assert(ofi_bsock_readable(bsock));
+		bytes += ofi_byteq_read(&bsock->rq, buf, len);
+		return bytes;
+	}
+
+	ret = ofi_recv_socket(bsock->sock, buf, len, MSG_NOSIGNAL);
+	if (ret > 0)
+		return bytes + ret;
+
+out:
+	if (bytes)
+		return bytes;
+	return ret ? -ofi_sockerr(): -FI_ENOTCONN;
+}
+
+ssize_t ofi_bsock_recvv(struct ofi_bsock *bsock, struct iovec *iov, size_t cnt)
+{
+	struct msghdr msg;
+	size_t len, bytes;
+	ssize_t ret;
+
+	if (cnt == 1)
+		return ofi_bsock_recv(bsock, iov[0].iov_base, iov[0].iov_len);
+
+	len = ofi_total_iov_len(iov, cnt);
+	if (ofi_byteq_readable(&bsock->rq)) {
+		bytes = ofi_byteq_readv(&bsock->rq, iov, cnt, 0);
+		if (bytes == len)
+			return len;
+
+		len -= bytes;
+	} else {
+		bytes = 0;
+	}
+
+	assert(!ofi_bsock_readable(bsock));
+	if (len < (bsock->rq.size >> 1)) {
+		ret = ofi_byteq_recv(&bsock->rq, bsock->sock);
+		if (ret <= 0)
+			goto out;
+
+		assert(ofi_bsock_readable(bsock));
+		bytes += ofi_byteq_readv(&bsock->rq, iov, cnt, bytes);
+		return bytes;
+	}
+
+	/* It's too difficult to adjust the iov without copying it, so return
+	 * what data we have.  The caller will consume the iov and retry.
+	 */
+	if (bytes)
+		return bytes;
+
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	msg.msg_flags = 0;
+	msg.msg_name = NULL;
+	msg.msg_namelen = 0;
+	msg.msg_iov = iov;
+	msg.msg_iovlen = cnt;
+
+	ret = ofi_recvmsg_tcp(bsock->sock, &msg, MSG_NOSIGNAL);
+	if (ret > 0)
+		return ret;
+out:
+	if (bytes)
+		return bytes;
+	return ret ? -ofi_sockerr(): -FI_ENOTCONN;
+}
+
+#ifdef MSG_ZEROCOPY
+uint32_t ofi_bsock_async_done(const struct fi_provider *prov,
+			      struct ofi_bsock *bsock)
+{
+	struct msghdr msg = {};
+	struct sock_extended_err *serr;
+	struct cmsghdr *cmsg;
+	/* x2 is arbitrary but avoids truncation */
+	uint8_t ctrl[CMSG_SPACE(sizeof(*serr) * 2)];
+	int ret;
+
+	msg.msg_control = &ctrl;
+	msg.msg_controllen = sizeof(ctrl);
+	ret = recvmsg(bsock->sock, &msg, MSG_ERRQUEUE);
+	if (ret < 0) {
+		FI_WARN(prov, FI_LOG_EP_DATA,
+			"Error reading MSG_ERRQUEUE (%s)\n", strerror(errno));
+		goto disable;
+	}
+
+	assert(!(msg.msg_flags & MSG_CTRUNC));
+	cmsg = CMSG_FIRSTHDR(&msg);
+	if ((cmsg->cmsg_level != SOL_IP && cmsg->cmsg_type != IP_RECVERR) &&
+	    (cmsg->cmsg_level != SOL_IPV6 && cmsg->cmsg_type != IPV6_RECVERR)) {
+		FI_WARN(prov, FI_LOG_EP_DATA,
+			"Unexpected cmsg level (!IP) or type (!RECVERR)\n");
+		goto disable;
+	}
+
+	serr = (void *) CMSG_DATA(cmsg);
+	if ((serr->ee_origin != SO_EE_ORIGIN_ZEROCOPY) || serr->ee_errno) {
+		FI_WARN(prov, FI_LOG_EP_DATA,
+			"Unexpected sock err origin or errno\n");
+		goto disable;
+	}
+
+	bsock->done_index = serr->ee_data;
+	if (serr->ee_code & SO_EE_CODE_ZEROCOPY_COPIED) {
+		FI_WARN(prov, FI_LOG_EP_DATA,
+			"Zerocopy data was copied\n");
+disable:
+		FI_WARN(prov, FI_LOG_EP_DATA, "disabling zerocopy\n");
+		bsock->zerocopy_size = SIZE_MAX;
+	}
+	return bsock->done_index;
+}
+#else
+uint32_t ofi_bsock_async_done(const struct fi_provider *prov,
+			      struct ofi_bsock *bsock)
+{
+	return 0;
+}
+#endif
+
+int ofi_pollfds_grow(struct ofi_pollfds *pfds, int max_size)
+{
+	struct pollfd *fds;
+	void *contexts;
+	size_t size;
+
+	if (max_size < pfds->size)
+		return FI_SUCCESS;
+
+	size = max_size + 1;
+	if (size < pfds->size + 64)
+		size = pfds->size + 64;
+
+	fds = calloc(size, sizeof(*pfds->fds) + sizeof(*pfds->context));
+	if (!fds)
+		return -FI_ENOMEM;
+
+	contexts = fds + size;
+	if (pfds->size) {
+		memcpy(fds, pfds->fds, pfds->size * sizeof(*pfds->fds));
+		memcpy(contexts, pfds->context, pfds->size * sizeof(*pfds->context));
+		free(pfds->fds);
+	}
+
+	while (pfds->size < size)
+		fds[pfds->size++].fd = INVALID_SOCKET;
+
+	pfds->fds = fds;
+	pfds->context = contexts;
+	return FI_SUCCESS;
+}
 
 int ofi_pollfds_create(struct ofi_pollfds **pfds)
 {
@@ -1012,14 +1346,9 @@ int ofi_pollfds_create(struct ofi_pollfds **pfds)
 	if (!*pfds)
 		return -FI_ENOMEM;
 
-	(*pfds)->size = 64;
-	(*pfds)->fds = calloc((*pfds)->size, sizeof(*(*pfds)->fds) +
-			    sizeof(*(*pfds)->context));
-	if (!(*pfds)->fds) {
-		ret = -FI_ENOMEM;
+	ret = ofi_pollfds_grow(*pfds, 63);
+	if (ret)
 		goto err1;
-	}
-	(*pfds)->context = (void *)((*pfds)->fds + (*pfds)->size);
 
 	ret = fd_signal_init(&(*pfds)->signal);
 	if (ret)
@@ -1064,113 +1393,93 @@ int ofi_pollfds_add(struct ofi_pollfds *pfds, int fd, uint32_t events,
 	return ofi_pollfds_ctl(pfds, POLLFDS_CTL_ADD, fd, events, context);
 }
 
-int ofi_pollfds_mod(struct ofi_pollfds *pfds, int fd, uint32_t events,
-		    void *context)
+static int ofi_pollfds_find(struct slist_entry *entry, const void *arg)
 {
-	return ofi_pollfds_ctl(pfds, POLLFDS_CTL_MOD, fd, events, context);
-}
+	struct ofi_pollfds_work_item *item;
+	int fd = (int) (uintptr_t) arg;
 
-int ofi_pollfds_del(struct ofi_pollfds *pfds, int fd)
-{
-	return ofi_pollfds_ctl(pfds, POLLFDS_CTL_DEL, fd, 0, NULL);
+	item = container_of(entry, struct ofi_pollfds_work_item, entry);
+	return item->fd == fd;
 }
 
-static int ofi_pollfds_array(struct ofi_pollfds *pfds)
+/* We're not changing the fds, just fields.  This is always 'racy' if
+ * the app modifies the events being monitored for an fd while another
+ * thread waits on the fds.  The other thread can always return before
+ * the modifications have been made.  The caller must be prepared to
+ * handle this, same as if epoll were used directly.
+ *
+ * Updating the events is a common case, so handle this immediately
+ * without the overhead of queuing a work item.
+ */
+int ofi_pollfds_mod(struct ofi_pollfds *pfds, int fd, uint32_t events,
+		    void *context)
 {
-	struct pollfd *fds;
-	void *contexts;
-
-	fds = calloc(pfds->size + 64,
-		     sizeof(*pfds->fds) + sizeof(*pfds->context));
-	if (!fds)
-		return -FI_ENOMEM;
+	struct slist_entry *entry;
+	struct ofi_pollfds_work_item *item;
+	int ret;
 
-	pfds->size += 64;
-	contexts = fds + pfds->size;
+	fastlock_acquire(&pfds->lock);
+	ret = ofi_pollfds_do_mod(pfds, fd, events, context);
+	if (!ret)
+		goto signal;
+
+	/* fd may be queued for insertion */
+	entry = slist_find_first_match(&pfds->work_item_list, ofi_pollfds_find,
+				       (void *) (uintptr_t) fd);
+	if (entry) {
+		item = container_of(entry, struct ofi_pollfds_work_item, entry);
+		item->events = events;
+		item->context = context;
+	}
 
-	memcpy(fds, pfds->fds, pfds->nfds * sizeof(*pfds->fds));
-	memcpy(contexts, pfds->context, pfds->nfds * sizeof(*pfds->context));
-	free(pfds->fds);
-	pfds->fds = fds;
-	pfds->context = contexts;
-	return FI_SUCCESS;
+signal:
+	fd_signal_set(&pfds->signal);
+	fastlock_release(&pfds->lock);
+	return 0;
 }
 
-static void ofi_pollfds_cleanup(struct ofi_pollfds *pfds)
+int ofi_pollfds_del(struct ofi_pollfds *pfds, int fd)
 {
-	int i;
-
-	for (i = 0; i < pfds->nfds; i++) {
-		while (pfds->fds[i].fd == INVALID_SOCKET) {
-			pfds->nfds--;
-			if (i == pfds->nfds)
-				break;
-
-			pfds->fds[i].fd = pfds->fds[pfds->nfds].fd;
-			pfds->fds[i].events = pfds->fds[pfds->nfds].events;
-			pfds->fds[i].revents = pfds->fds[pfds->nfds].revents;
-			pfds->context[i] = pfds->context[pfds->nfds];
-		}
-	}
+	return ofi_pollfds_ctl(pfds, POLLFDS_CTL_DEL, fd, 0, NULL);
 }
 
 static void ofi_pollfds_process_work(struct ofi_pollfds *pfds)
 {
 	struct slist_entry *entry;
 	struct ofi_pollfds_work_item *item;
-	int i;
 
 	while (!slist_empty(&pfds->work_item_list)) {
-		if ((pfds->nfds == pfds->size) &&
-		    ofi_pollfds_array(pfds))
-			continue;
-
 		entry = slist_remove_head(&pfds->work_item_list);
 		item = container_of(entry, struct ofi_pollfds_work_item, entry);
 
 		switch (item->type) {
 		case POLLFDS_CTL_ADD:
-			pfds->fds[pfds->nfds].fd = item->fd;
-			pfds->fds[pfds->nfds].events = item->events;
-			pfds->fds[pfds->nfds].revents = 0;
-			pfds->context[pfds->nfds] = item->context;
-			pfds->nfds++;
+			ofi_pollfds_do_add(pfds, item);
 			break;
 		case POLLFDS_CTL_DEL:
-			for (i = 0; i < pfds->nfds; i++) {
-				if (pfds->fds[i].fd == item->fd) {
-					pfds->fds[i].fd = INVALID_SOCKET;
-					break;
-				}
-			}
-			break;
-		case POLLFDS_CTL_MOD:
-			for (i = 0; i < pfds->nfds; i++) {
-				if (pfds->fds[i].fd == item->fd) {
-					pfds->fds[i].events = item->events;
-					pfds->fds[i].revents &= item->events;
-					pfds->context[i] = item->context;
-					break;
-				}
-			}
+			ofi_pollfds_do_del(pfds, item);
 			break;
 		default:
 			assert(0);
-			goto out;
+			break;
 		}
 		free(item);
 	}
-out:
-	ofi_pollfds_cleanup(pfds);
 }
 
-int ofi_pollfds_wait(struct ofi_pollfds *pfds, void **contexts,
-		     int max_contexts, int timeout)
+int ofi_pollfds_wait(struct ofi_pollfds *pfds,
+		     struct ofi_epollfds_event *events,
+		     int maxevents, int timeout)
 {
 	int i, ret;
 	int found = 0;
 	uint64_t start = (timeout >= 0) ? ofi_gettime_ms() : 0;
 
+	fastlock_acquire(&pfds->lock);
+	if (!slist_empty(&pfds->work_item_list))
+		ofi_pollfds_process_work(pfds);
+	fastlock_release(&pfds->lock);
+
 	do {
 		ret = poll(pfds->fds, pfds->nfds, timeout);
 		if (ret == SOCKET_ERROR)
@@ -1178,19 +1487,23 @@ int ofi_pollfds_wait(struct ofi_pollfds *pfds, void **contexts,
 		else if (ret == 0)
 			return 0;
 
-		if (pfds->fds[0].revents)
-			fd_signal_reset(&pfds->signal);
-
 		fastlock_acquire(&pfds->lock);
 		if (!slist_empty(&pfds->work_item_list))
 			ofi_pollfds_process_work(pfds);
-
 		fastlock_release(&pfds->lock);
 
+		if (pfds->fds[0].revents) {
+			fd_signal_reset(&pfds->signal);
+			ret--;
+		}
+
+		ret = MIN(maxevents, ret);
+
 		/* Index 0 is the internal signaling fd, skip it */
-		for (i = 1; i < pfds->nfds && found < max_contexts; i++) {
+		for (i = 1; i < pfds->nfds && found < ret; i++) {
 			if (pfds->fds[i].revents) {
-				contexts[found++] = pfds->context[i];
+				events[found].events = pfds->fds[i].revents;
+				events[found++].data.ptr = pfds->context[i];
 			}
 		}
 
diff --git a/src/enosys.c b/src/enosys.c
index 32f1bcc..13ace0c 100644
--- a/src/enosys.c
+++ b/src/enosys.c
@@ -53,6 +53,15 @@ int fi_no_ops_open(struct fid *fid, const char *name,
 {
 	return -FI_ENOSYS;
 }
+int fi_no_tostr(const struct fid *fid, char *buf, size_t len)
+{
+	return -FI_ENOSYS;
+}
+int fi_no_ops_set(struct fid *fid, const char *name, uint64_t flags,
+		  void *ops, void *context)
+{
+	return -FI_ENOSYS;
+}
 
 /*
  * struct fi_ops_fabric
diff --git a/src/fabric.c b/src/fabric.c
index 86734d7..34ec75e 100644
--- a/src/fabric.c
+++ b/src/fabric.c
@@ -49,11 +49,13 @@
 #include "ofi_prov.h"
 #include "ofi_perf.h"
 #include "ofi_hmem.h"
+#include "rdma/fi_ext.h"
 
 #ifdef HAVE_LIBDL
 #include <dlfcn.h>
 #endif
 
+
 struct ofi_prov {
 	struct ofi_prov		*next;
 	char			*prov_name;
@@ -62,12 +64,102 @@ struct ofi_prov {
 	bool			hidden;
 };
 
+enum ofi_prov_order {
+	OFI_PROV_ORDER_VERSION,
+	OFI_PROV_ORDER_REGISTER,
+};
+
 static struct ofi_prov *prov_head, *prov_tail;
+static enum ofi_prov_order prov_order = OFI_PROV_ORDER_VERSION;
 int ofi_init = 0;
 extern struct ofi_common_locks common_locks;
 
 static struct fi_filter prov_filter;
 
+
+static struct ofi_prov *
+ofi_alloc_prov(const char *prov_name)
+{
+	struct ofi_prov *prov;
+
+	prov = calloc(sizeof *prov, 1);
+	if (!prov)
+		return NULL;
+
+	prov->prov_name = strdup(prov_name);
+	if (!prov->prov_name) {
+		free(prov);
+		return NULL;
+	}
+
+	return prov;
+}
+
+static void
+ofi_init_prov(struct ofi_prov *prov, struct fi_provider *provider,
+	      void *dlhandle)
+{
+	prov->provider = provider;
+	prov->dlhandle = dlhandle;
+}
+
+static void ofi_cleanup_prov(struct fi_provider *provider, void *dlhandle)
+{
+	if (provider) {
+		fi_param_undefine(provider);
+		if (provider->cleanup)
+			provider->cleanup();
+	}
+
+#ifdef HAVE_LIBDL
+	if (dlhandle)
+		dlclose(dlhandle);
+#else
+	OFI_UNUSED(dlhandle);
+#endif
+}
+
+static void ofi_free_prov(struct ofi_prov *prov)
+{
+	ofi_cleanup_prov(prov->provider, prov->dlhandle);
+	free(prov->prov_name);
+	free(prov);
+}
+
+static void ofi_insert_prov(struct ofi_prov *prov)
+{
+	struct ofi_prov *cur, *prev;
+
+	for (prev = NULL, cur = prov_head; cur; prev = cur, cur = cur->next) {
+		if ((strlen(prov->prov_name) == strlen(cur->prov_name)) &&
+		    !strcasecmp(prov->prov_name, cur->prov_name)) {
+			if ((prov_order == OFI_PROV_ORDER_VERSION) &&
+			    FI_VERSION_LT(cur->provider->version,
+					  prov->provider->version)) {
+				cur->hidden = true;
+				prov->next = cur;
+				if (prev)
+					prev->next = prov;
+				else
+					prov_head = prov;
+			} else {
+				prov->hidden = true;
+				prov->next = cur->next;
+				cur->next = prov;
+				if (prov_tail == cur)
+					prov_tail = prov;
+			}
+			return;
+		}
+	}
+
+	if (prov_tail)
+		prov_tail->next = prov;
+	else
+		prov_head = prov;
+	prov_tail = prov;
+}
+
 static int ofi_find_name(char **names, const char *name)
 {
 	int i;
@@ -310,51 +402,6 @@ struct fi_provider *ofi_get_hook(const char *name)
 	return provider;
 }
 
-static void cleanup_provider(struct fi_provider *provider, void *dlhandle)
-{
-	OFI_UNUSED(dlhandle);
-
-	if (provider) {
-		fi_param_undefine(provider);
-
-		if (provider->cleanup)
-			provider->cleanup();
-	}
-
-#ifdef HAVE_LIBDL
-	if (dlhandle)
-		dlclose(dlhandle);
-#endif
-}
-
-static struct ofi_prov *ofi_create_prov_entry(const char *prov_name)
-{
-	struct ofi_prov *prov = NULL;
-	prov = calloc(sizeof *prov, 1);
-	if (!prov) {
-		FI_WARN(&core_prov, FI_LOG_CORE,
-			"Not enough memory to allocate provider registry\n");
-		return NULL;
-	}
-
-	prov->prov_name = strdup(prov_name);
-	if (!prov->prov_name) {
-		FI_WARN(&core_prov, FI_LOG_CORE,
-			"Failed to init pre-registered provider name\n");
-		free(prov);
-		return NULL;
-	}
-	if (prov_tail)
-		prov_tail->next = prov;
-	else
-		prov_head = prov;
-	prov_tail = prov;
-
-	prov->hidden = false;
-
-	return prov;
-}
-
 /* This is the default order that providers will be reported when a provider
  * is available.  Initialize the socket(s) provider last.  This will result in
  * it being the least preferred provider.
@@ -362,8 +409,8 @@ static struct ofi_prov *ofi_create_prov_entry(const char *prov_name)
 static void ofi_ordered_provs_init(void)
 {
 	char *ordered_prov_names[] = {
-		"psm3", "psm2", "psm", "efa", "usnic", "gni", "bgq", "verbs",
-		"netdir", "ofi_rxm", "ofi_rxd", "shm",
+		"efa", "psm2", "psm", "usnic", "gni", "bgq", "verbs",
+		"netdir", "psm3", "ofi_rxm", "ofi_rxd", "shm",
 		/* Initialize the socket based providers last of the
 		 * standard providers.  This will result in them being
 		 * the least preferred providers.
@@ -378,10 +425,16 @@ static void ofi_ordered_provs_init(void)
 		 */
 		"ofi_hook_perf", "ofi_hook_debug", "ofi_hook_noop",
 	};
-	int num_provs = sizeof(ordered_prov_names)/sizeof(ordered_prov_names[0]), i;
+	struct ofi_prov *prov;
+	int num_provs, i;
+
+	num_provs = sizeof(ordered_prov_names) / sizeof(ordered_prov_names[0]);
 
-	for (i = 0; i < num_provs; i++)
-		ofi_create_prov_entry(ordered_prov_names[i]);
+	for (i = 0; i < num_provs; i++) {
+		prov = ofi_alloc_prov(ordered_prov_names[i]);
+		if (prov)
+			ofi_insert_prov(prov);
+	}
 }
 
 static void ofi_set_prov_type(struct fi_prov_context *ctx,
@@ -455,48 +508,23 @@ static void ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 		ctx->disable_layering = 1;
 
 	prov = ofi_getprov(provider->name, strlen(provider->name));
-	if (prov) {
-		/* If this provider has not been init yet, then we add the
-		 * provider and dlhandle to the struct and exit.
-		 */
-		if (prov->provider == NULL)
-			goto update_prov_registry;
-
-		/* If this provider is older than an already-loaded
-		 * provider of the same name, then discard this one.
-		 */
-		if (FI_VERSION_GE(prov->provider->version, provider->version)) {
-			FI_INFO(&core_prov, FI_LOG_CORE,
-				"a newer %s provider was already loaded; "
-				"ignoring this one\n", provider->name);
-			goto cleanup;
-		}
-
-		/* This provider is newer than an already-loaded
-		 * provider of the same name, so discard the
-		 * already-loaded one.
-		 */
-		FI_INFO(&core_prov, FI_LOG_CORE,
-			"an older %s provider was already loaded; "
-			"keeping this one and ignoring the older one\n",
-			provider->name);
-		cleanup_provider(prov->provider, prov->dlhandle);
+	if (prov && !prov->provider) {
+		ofi_init_prov(prov, provider, dlhandle);
 	} else {
-		prov = ofi_create_prov_entry(provider->name);
+		prov = ofi_alloc_prov(provider->name);
 		if (!prov)
 			goto cleanup;
+
+		ofi_init_prov(prov, provider, dlhandle);
+		ofi_insert_prov(prov);
 	}
 
 	if (hidden)
 		prov->hidden = true;
-
-update_prov_registry:
-	prov->dlhandle = dlhandle;
-	prov->provider = provider;
 	return;
 
 cleanup:
-	cleanup_provider(provider, dlhandle);
+	ofi_cleanup_prov(provider, dlhandle);
 }
 
 #ifdef HAVE_LIBDL
@@ -592,11 +620,11 @@ static void ofi_reg_dl_prov(const char *lib)
 
 static void ofi_ini_dir(const char *dir)
 {
-	int n = 0;
+	int n;
 	char *lib;
 	struct dirent **liblist = NULL;
 
-	n = scandir(dir, &liblist, lib_filter, NULL);
+	n = scandir(dir, &liblist, lib_filter, alphasort);
 	if (n < 0)
 		goto libdl_done;
 
@@ -618,8 +646,8 @@ libdl_done:
 	free(liblist);
 }
 
-/* Search standard system library paths (i.e. LD_LIBRARY_PATH) for known DL provider
- * libraries.
+/* Search standard system library paths (i.e. LD_LIBRARY_PATH) for DLLs for
+ * known providers.
  */
 static void ofi_find_prov_libs(void)
 {
@@ -629,7 +657,6 @@ static void ofi_find_prov_libs(void)
 	char* short_prov_name;
 
 	for (prov = prov_head; prov; prov = prov->next) {
-
 		if (!prov->prov_name)
 			continue;
 
@@ -650,6 +677,55 @@ static void ofi_find_prov_libs(void)
 		free(lib);
 	}
 }
+
+static void ofi_load_dl_prov(void)
+{
+	char **dirs;
+	char *provdir = NULL;
+	void *dlhandle;
+	int i;
+
+	/* If dlopen fails, assume static linking and return */
+	dlhandle = dlopen(NULL, RTLD_NOW);
+	if (!dlhandle)
+		return;
+	dlclose(dlhandle);
+
+	fi_param_define(NULL, "provider_path", FI_PARAM_STRING,
+			"Search for providers in specific path.  Path is "
+			"specified similar to dir1:dir2:dir3.  If the path "
+			"starts with @, loaded providers are given preference "
+			"based on discovery order, rather than version. "
+			"(default: " PROVDLDIR ")");
+
+	fi_param_get_str(NULL, "provider_path", &provdir);
+	if (!provdir || !strlen(provdir)) {
+		ofi_find_prov_libs();
+		dirs = ofi_split_and_alloc(PROVDLDIR, ":", NULL);
+	} else if (provdir[0] == '@') {
+		prov_order = OFI_PROV_ORDER_REGISTER;
+		if (strlen(provdir) == 1)
+			dirs = ofi_split_and_alloc(PROVDLDIR, ":", NULL);
+		else
+			dirs = ofi_split_and_alloc(&provdir[1], ":", NULL);
+	} else {
+		dirs = ofi_split_and_alloc(provdir, ":", NULL);
+	}
+
+	if (dirs) {
+		for (i = 0; dirs[i]; i++)
+			ofi_ini_dir(dirs[i]);
+
+		ofi_free_string_array(dirs);
+	}
+}
+
+#else /* HAVE_LIBDL */
+
+static void ofi_load_dl_prov(void)
+{
+}
+
 #endif
 
 void fi_ini(void)
@@ -688,37 +764,7 @@ void fi_ini(void)
 	fi_param_get_str(NULL, "provider", &param_val);
 	ofi_create_filter(&prov_filter, param_val);
 
-#ifdef HAVE_LIBDL
-	int n = 0;
-	char **dirs;
-	char *provdir = NULL;
-	void *dlhandle;
-
-	/* If dlopen fails, assume static linking and just return
-	   without error */
-	dlhandle = dlopen(NULL, RTLD_NOW);
-	if (dlhandle == NULL) {
-		goto libdl_done;
-	}
-	dlclose(dlhandle);
-
-	fi_param_define(NULL, "provider_path", FI_PARAM_STRING,
-			"Search for providers in specific path (default: "
-			PROVDLDIR ")");
-	fi_param_get_str(NULL, "provider_path", &provdir);
-	if (!provdir) {
-		provdir = PROVDLDIR;
-		ofi_find_prov_libs();
-	}
-	dirs = ofi_split_and_alloc(provdir, ":", NULL);
-	if (dirs) {
-		for (n = 0; dirs[n]; ++n) {
-			ofi_ini_dir(dirs[n]);
-		}
-		ofi_free_string_array(dirs);
-	}
-libdl_done:
-#endif
+	ofi_load_dl_prov();
 
 	ofi_register_provider(PSM3_INIT, NULL);
 	ofi_register_provider(PSM2_INIT, NULL);
@@ -758,9 +804,7 @@ FI_DESTRUCTOR(fi_fini(void))
 	while (prov_head) {
 		prov = prov_head;
 		prov_head = prov->next;
-		cleanup_provider(prov->provider, prov->dlhandle);
-		free(prov->prov_name);
-		free(prov);
+		ofi_free_prov(prov);
 	}
 
 	ofi_free_filter(&prov_filter);
@@ -1233,6 +1277,19 @@ uint32_t DEFAULT_SYMVER_PRE(fi_version)(void)
 }
 DEFAULT_SYMVER(fi_version_, fi_version, FABRIC_1.0);
 
+__attribute__((visibility ("default"),EXTERNALLY_VISIBLE))
+int DEFAULT_SYMVER_PRE(fi_open)(uint32_t version, const char *name,
+		void *attr, size_t attr_len, uint64_t flags,
+		struct fid **fid, void *context)
+{
+	if (!strcasecmp("mr_cache", name))
+		return ofi_open_mr_cache(version, attr, attr_len,
+					 flags, fid, context);
+
+	return -FI_ENOSYS;
+}
+DEFAULT_SYMVER(fi_open_, fi_open, FABRIC_1.5);
+
 static const char *const errstr[] = {
 	[FI_EOTHER - FI_ERRNO_OFFSET] = "Unspecified error",
 	[FI_ETOOSMALL - FI_ERRNO_OFFSET] = "Provided buffer is too small",
@@ -1247,6 +1304,7 @@ static const char *const errstr[] = {
 	[FI_ENOKEY - FI_ERRNO_OFFSET] = "Required key not available",
 	[FI_ENOAV - FI_ERRNO_OFFSET] = "Missing or unavailable address vector",
 	[FI_EOVERRUN - FI_ERRNO_OFFSET] = "Queue has been overrun",
+	[FI_ENORX - FI_ERRNO_OFFSET] = "Receiver not ready, no receive buffers available",
 };
 
 __attribute__((visibility ("default"),EXTERNALLY_VISIBLE))
diff --git a/src/fi_tostr.c b/src/fi_tostr.c
index 9a91856..658a13e 100644
--- a/src/fi_tostr.c
+++ b/src/fi_tostr.c
@@ -625,6 +625,8 @@ static void ofi_tostr_atomic_type(char *buf, size_t len, enum fi_datatype type)
 	CASEENUMSTRN(FI_UINT32, len);
 	CASEENUMSTRN(FI_INT64, len);
 	CASEENUMSTRN(FI_UINT64, len);
+	CASEENUMSTRN(FI_INT128, len);
+	CASEENUMSTRN(FI_UINT128, len);
 	CASEENUMSTRN(FI_FLOAT, len);
 	CASEENUMSTRN(FI_DOUBLE, len);
 	CASEENUMSTRN(FI_FLOAT_COMPLEX, len);
@@ -842,7 +844,7 @@ char *DEFAULT_SYMVER_PRE(fi_tostr_r)(char *buf, size_t len,
 	}
 	return buf;
 }
-CURRENT_SYMVER(fi_tostr_r_, fi_tostr_r);
+DEFAULT_SYMVER(fi_tostr_r_, fi_tostr_r, FABRIC_1.4);
 
 __attribute__((visibility ("default"),EXTERNALLY_VISIBLE))
 char *DEFAULT_SYMVER_PRE(fi_tostr)(const void *data, enum fi_type datatype)
diff --git a/src/hmem.c b/src/hmem.c
index 33288e8..6797c50 100644
--- a/src/hmem.c
+++ b/src/hmem.c
@@ -1,6 +1,7 @@
 /*
  * (C) Copyright 2020 Hewlett Packard Enterprise Development LP
  * (C) Copyright 2020-2021 Intel Corporation. All rights reserved.
+ * (C) Copyright 2021 Amazon.com, Inc. or its affiliates.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -39,26 +40,9 @@
 #include "ofi.h"
 #include "ofi_iov.h"
 
-struct ofi_hmem_ops {
-	bool initialized;
-	int (*init)(void);
-	int (*cleanup)(void);
-	int (*copy_to_hmem)(uint64_t device, void *dest, const void *src,
-			    size_t size);
-	int (*copy_from_hmem)(uint64_t device, void *dest, const void *src,
-			      size_t size);
-	bool (*is_addr_valid)(const void *addr);
-	int (*get_handle)(void *dev_buf, void **handle);
-	int (*open_handle)(void **handle, uint64_t device, void **ipc_ptr);
-	int (*close_handle)(void *ipc_ptr);
-	int (*host_register)(void *ptr, size_t size);
-	int (*host_unregister)(void *ptr);
-	int (*get_base_addr)(const void *ptr, void **base);
-};
-
-static struct ofi_hmem_ops hmem_ops[] = {
+struct ofi_hmem_ops hmem_ops[] = {
 	[FI_HMEM_SYSTEM] = {
-		.initialized = false,
+		.initialized = true,
 		.init = ofi_hmem_init_noop,
 		.cleanup = ofi_hmem_cleanup_noop,
 		.copy_to_hmem = ofi_memcpy,
@@ -69,6 +53,7 @@ static struct ofi_hmem_ops hmem_ops[] = {
 		.host_register = ofi_hmem_register_noop,
 		.host_unregister = ofi_hmem_host_unregister_noop,
 		.get_base_addr = ofi_hmem_no_base_addr,
+		.is_ipc_enabled = ofi_hmem_no_is_ipc_enabled,
 	},
 	[FI_HMEM_CUDA] = {
 		.initialized = false,
@@ -77,12 +62,13 @@ static struct ofi_hmem_ops hmem_ops[] = {
 		.copy_to_hmem = cuda_copy_to_dev,
 		.copy_from_hmem = cuda_copy_from_dev,
 		.is_addr_valid = cuda_is_addr_valid,
-		.get_handle = ofi_hmem_no_get_handle,
-		.open_handle = ofi_hmem_no_open_handle,
-		.close_handle = ofi_hmem_no_close_handle,
+		.get_handle = cuda_get_handle,
+		.open_handle = cuda_open_handle,
+		.close_handle = cuda_close_handle,
 		.host_register = cuda_host_register,
 		.host_unregister = cuda_host_unregister,
 		.get_base_addr = ofi_hmem_no_base_addr,
+		.is_ipc_enabled = cuda_is_ipc_enabled,
 	},
 	[FI_HMEM_ROCR] = {
 		.initialized = false,
@@ -97,6 +83,7 @@ static struct ofi_hmem_ops hmem_ops[] = {
 		.host_register = rocr_host_register,
 		.host_unregister = rocr_host_unregister,
 		.get_base_addr = ofi_hmem_no_base_addr,
+		.is_ipc_enabled = ofi_hmem_no_is_ipc_enabled,
 	},
 	[FI_HMEM_ZE] = {
 		.initialized = false,
@@ -111,6 +98,7 @@ static struct ofi_hmem_ops hmem_ops[] = {
 		.host_register = ofi_hmem_register_noop,
 		.host_unregister = ofi_hmem_host_unregister_noop,
 		.get_base_addr = ze_hmem_get_base_addr,
+		.is_ipc_enabled = ze_hmem_p2p_enabled,
 	},
 };
 
@@ -213,6 +201,11 @@ int ofi_hmem_get_base_addr(enum fi_hmem_iface iface, const void *ptr,
 	return hmem_ops[iface].get_base_addr(ptr, base);
 }
 
+bool ofi_hmem_is_initialized(enum fi_hmem_iface iface)
+{
+	return hmem_ops[iface].initialized;
+}
+
 void ofi_hmem_init(void)
 {
 	int iface, ret;
@@ -240,7 +233,7 @@ void ofi_hmem_cleanup(void)
 	enum fi_hmem_iface iface;
 
 	for (iface = 0; iface < ARRAY_SIZE(hmem_ops); iface++) {
-		if (hmem_ops[iface].initialized)
+		if (ofi_hmem_is_initialized(iface))
 			hmem_ops[iface].cleanup();
 	}
 }
@@ -255,7 +248,7 @@ enum fi_hmem_iface ofi_get_hmem_iface(const void *addr)
 	 */
 	for (iface = ARRAY_SIZE(hmem_ops) - 1; iface > FI_HMEM_SYSTEM;
 	     iface--) {
-		if (hmem_ops[iface].initialized &&
+		if (ofi_hmem_is_initialized(iface) &&
 		    hmem_ops[iface].is_addr_valid(addr))
 			return iface;
 	}
@@ -268,7 +261,7 @@ int ofi_hmem_host_register(void *ptr, size_t size)
 	int iface, ret;
 
 	for (iface = 0; iface < ARRAY_SIZE(hmem_ops); iface++) {
-		if (!hmem_ops[iface].initialized)
+		if (!ofi_hmem_is_initialized(iface))
 			continue;
 
 		ret = hmem_ops[iface].host_register(ptr, size);
@@ -285,7 +278,7 @@ err:
 		fi_strerror(-ret));
 
 	for (iface--; iface >= 0; iface--) {
-		if (!hmem_ops[iface].initialized)
+		if (!ofi_hmem_is_initialized(iface))
 			continue;
 
 		hmem_ops[iface].host_unregister(ptr);
@@ -299,7 +292,7 @@ int ofi_hmem_host_unregister(void *ptr)
 	int iface, ret;
 
 	for (iface = 0; iface < ARRAY_SIZE(hmem_ops); iface++) {
-		if (!hmem_ops[iface].initialized)
+		if (!ofi_hmem_is_initialized(iface))
 			continue;
 
 		ret = hmem_ops[iface].host_unregister(ptr);
@@ -317,3 +310,8 @@ err:
 
 	return ret;
 }
+
+bool ofi_hmem_is_ipc_enabled(enum fi_hmem_iface iface)
+{
+	return hmem_ops[iface].is_ipc_enabled();
+}
diff --git a/src/hmem_cuda.c b/src/hmem_cuda.c
index 9581c72..4c73fe5 100644
--- a/src/hmem_cuda.c
+++ b/src/hmem_cuda.c
@@ -1,5 +1,6 @@
 /*
  * (C) Copyright 2020 Hewlett Packard Enterprise Development LP
+ * (C) Copyright 2021 Amazon.com, Inc. or its affiliates.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -43,8 +44,10 @@
 #include <cuda_runtime.h>
 
 struct cuda_ops {
-	cudaError_t (*cudaMemcpy)(void *dst, const void *src, size_t count,
+	cudaError_t (*cudaMemcpy)(void *dst, const void *src, size_t size,
 				  enum cudaMemcpyKind kind);
+	cudaError_t (*cudaFree)(void* ptr);
+	cudaError_t (*cudaMalloc)(void** ptr, size_t size);
 	const char *(*cudaGetErrorName)(cudaError_t error);
 	const char *(*cudaGetErrorString)(cudaError_t error);
 	CUresult (*cuPointerGetAttribute)(void *data,
@@ -54,9 +57,21 @@ struct cuda_ops {
 					unsigned int flags);
 	cudaError_t (*cudaHostUnregister)(void *ptr);
 	cudaError_t (*cudaGetDeviceCount)(int *count);
+	cudaError_t (*cudaGetDevice)(int *device);
+	cudaError_t (*cudaSetDevice)(int device);
+	cudaError_t (*cudaIpcOpenMemHandle)(void **devptr,
+					    cudaIpcMemHandle_t handle,
+					    unsigned int flags);
+	cudaError_t (*cudaIpcGetMemHandle)(cudaIpcMemHandle_t *handle,
+					   void *devptr);
+	cudaError_t (*cudaIpcCloseMemHandle)(void *devptr);
 };
 
-static int hmem_cuda_use_gdrcopy;
+static bool hmem_cuda_use_gdrcopy;
+static bool cuda_ipc_enabled;
+
+static cudaError_t cuda_disabled_cudaMemcpy(void *dst, const void *src,
+					    size_t size, enum cudaMemcpyKind kind);
 
 #ifdef ENABLE_CUDA_DLOPEN
 
@@ -70,20 +85,27 @@ static struct cuda_ops cuda_ops;
 
 static struct cuda_ops cuda_ops = {
 	.cudaMemcpy = cudaMemcpy,
+	.cudaFree = cudaFree,
+	.cudaMalloc = cudaMalloc,
 	.cudaGetErrorName = cudaGetErrorName,
 	.cudaGetErrorString = cudaGetErrorString,
 	.cuPointerGetAttribute = cuPointerGetAttribute,
 	.cudaHostRegister = cudaHostRegister,
 	.cudaHostUnregister = cudaHostUnregister,
 	.cudaGetDeviceCount = cudaGetDeviceCount,
+	.cudaGetDevice = cudaGetDevice,
+	.cudaSetDevice = cudaSetDevice,
+	.cudaIpcOpenMemHandle = cudaIpcOpenMemHandle,
+	.cudaIpcGetMemHandle = cudaIpcGetMemHandle,
+	.cudaIpcCloseMemHandle = cudaIpcCloseMemHandle
 };
 
 #endif /* ENABLE_CUDA_DLOPEN */
 
-cudaError_t ofi_cudaMemcpy(void *dst, const void *src, size_t count,
+cudaError_t ofi_cudaMemcpy(void *dst, const void *src, size_t size,
 			   enum cudaMemcpyKind kind)
 {
-	return cuda_ops.cudaMemcpy(dst, src, count, kind);
+	return cuda_ops.cudaMemcpy(dst, src, size, kind);
 }
 
 const char *ofi_cudaGetErrorName(cudaError_t error)
@@ -117,16 +139,16 @@ static cudaError_t ofi_cudaGetDeviceCount(int *count)
 	return cuda_ops.cudaGetDeviceCount(count);
 }
 
-int cuda_copy_to_dev(uint64_t device, void *dev, const void *host, size_t size)
+int cuda_copy_to_dev(uint64_t device, void *dst, const void *src, size_t size)
 {
 	if (hmem_cuda_use_gdrcopy) {
-		cuda_gdrcopy_to_dev(device, dev, host, size);
+		cuda_gdrcopy_to_dev(device, dst, src, size);
 		return FI_SUCCESS;
 	}
 
 	cudaError_t cuda_ret;
 
-	cuda_ret = ofi_cudaMemcpy(dev, host, size, cudaMemcpyHostToDevice);
+	cuda_ret = ofi_cudaMemcpy(dst, src, size, cudaMemcpyDefault);
 	if (cuda_ret == cudaSuccess)
 		return 0;
 
@@ -138,16 +160,16 @@ int cuda_copy_to_dev(uint64_t device, void *dev, const void *host, size_t size)
 	return -FI_EIO;
 }
 
-int cuda_copy_from_dev(uint64_t device, void *host, const void *dev, size_t size)
+int cuda_copy_from_dev(uint64_t device, void *dst, const void *src, size_t size)
 {
 	if (hmem_cuda_use_gdrcopy) {
-		cuda_gdrcopy_from_dev(device, host, dev, size);
+		cuda_gdrcopy_from_dev(device, dst, src, size);
 		return FI_SUCCESS;
 	}
 
 	cudaError_t cuda_ret;
 
-	cuda_ret = ofi_cudaMemcpy(host, dev, size, cudaMemcpyDeviceToHost);
+	cuda_ret = ofi_cudaMemcpy(dst, src, size, cudaMemcpyDefault);
 	if (cuda_ret == cudaSuccess)
 		return 0;
 
@@ -176,6 +198,70 @@ int cuda_dev_unregister(uint64_t handle)
 	return FI_SUCCESS;
 }
 
+int cuda_get_handle(void *dev_buf, void **handle)
+{
+	cudaError_t cuda_ret;
+
+	cuda_ret = cuda_ops.cudaIpcGetMemHandle((cudaIpcMemHandle_t *)handle,
+						dev_buf);
+
+	if (cuda_ret == cudaSuccess)
+		return FI_SUCCESS;
+
+	FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to perform cudaIpcGetMemHandle: %s:%s\n",
+			ofi_cudaGetErrorName(cuda_ret),
+			ofi_cudaGetErrorString(cuda_ret));
+
+	return -FI_EINVAL;
+}
+
+int cuda_open_handle(void **handle, uint64_t device, void **ipc_ptr)
+{
+	cudaError_t cuda_ret;
+
+	cuda_ret = cuda_ops.cudaIpcOpenMemHandle(ipc_ptr,
+						 *(cudaIpcMemHandle_t *)handle,
+						 cudaIpcMemLazyEnablePeerAccess);
+
+	if (cuda_ret == cudaSuccess)
+		return FI_SUCCESS;
+
+	FI_WARN(&core_prov, FI_LOG_CORE,
+		"Failed to perform cudaIpcOpenMemHandle: %s:%s\n",
+		ofi_cudaGetErrorName(cuda_ret),
+		ofi_cudaGetErrorString(cuda_ret));
+
+	return -FI_EINVAL;
+}
+
+int cuda_close_handle(void *ipc_ptr)
+{
+	cudaError_t cuda_ret;
+
+	cuda_ret = cuda_ops.cudaIpcCloseMemHandle(ipc_ptr);
+
+	if (cuda_ret == cudaSuccess)
+		return FI_SUCCESS;
+
+	FI_WARN(&core_prov, FI_LOG_CORE,
+		"Failed to perform cudaIpcCloseMemHandle: %s:%s\n",
+		ofi_cudaGetErrorName(cuda_ret),
+		ofi_cudaGetErrorString(cuda_ret));
+
+	return -FI_EINVAL;
+}
+
+static cudaError_t cuda_disabled_cudaMemcpy(void *dst, const void *src,
+					    size_t size, enum cudaMemcpyKind kind)
+{
+	FI_WARN(&core_prov, FI_LOG_CORE,
+		"cudaMemcpy was called but FI_HMEM_CUDA_ENABLE_XFER = 0, "
+		"no copy will occur to prevent deadlock.");
+
+	return cudaErrorInvalidValue;
+}
+
 static int cuda_hmem_dl_init(void)
 {
 #ifdef ENABLE_CUDA_DLOPEN
@@ -202,6 +288,18 @@ static int cuda_hmem_dl_init(void)
 		goto err_dlclose_cuda;
 	}
 
+	cuda_ops.cudaFree = dlsym(cudart_handle, "cudaFree");
+	if (!cuda_ops.cudaFree) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find cudaFree\n");
+		goto err_dlclose_cuda;
+	}
+
+	cuda_ops.cudaMalloc = dlsym(cudart_handle, "cudaMalloc");
+	if (!cuda_ops.cudaMalloc) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find cudaMalloc\n");
+		goto err_dlclose_cuda;
+	}
+
 	cuda_ops.cudaGetErrorName = dlsym(cudart_handle, "cudaGetErrorName");
 	if (!cuda_ops.cudaGetErrorName) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
@@ -248,6 +346,46 @@ static int cuda_hmem_dl_init(void)
 		goto err_dlclose_cuda;
 	}
 
+	cuda_ops.cudaGetDevice = dlsym(cudart_handle,
+					    "cudaGetDevice");
+	if (!cuda_ops.cudaGetDevice) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to find cudaGetDevice\n");
+		goto err_dlclose_cuda;
+	}
+
+	cuda_ops.cudaSetDevice = dlsym(cudart_handle,
+					    "cudaSetDevice");
+	if (!cuda_ops.cudaSetDevice) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to find cudaSetDevice\n");
+		goto err_dlclose_cuda;
+	}
+
+	cuda_ops.cudaIpcOpenMemHandle = dlsym(cudart_handle,
+					    "cudaIpcOpenMemHandle");
+	if (!cuda_ops.cudaIpcOpenMemHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to find cudaIpcOpenMemHandle\n");
+		goto err_dlclose_cuda;
+	}
+
+	cuda_ops.cudaIpcGetMemHandle = dlsym(cudart_handle,
+					    "cudaIpcGetMemHandle");
+	if (!cuda_ops.cudaIpcGetMemHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to find cudaIpcGetMemHandle\n");
+		goto err_dlclose_cuda;
+	}
+
+	cuda_ops.cudaIpcCloseMemHandle = dlsym(cudart_handle,
+					    "cudaIpcCloseMemHandle");
+	if (!cuda_ops.cudaIpcCloseMemHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to find cudaIpcCloseMemHandle\n");
+		goto err_dlclose_cuda;
+	}
+
 	return FI_SUCCESS;
 
 err_dlclose_cuda:
@@ -300,6 +438,22 @@ static int cuda_hmem_verify_devices(void)
 int cuda_hmem_init(void)
 {
 	int ret;
+	int gdrcopy_ret;
+	bool cuda_enable_xfer;
+	int devices, dev_id, orig_dev_id;
+	void *ptr;
+	cudaIpcMemHandle_t handle;
+	cudaError_t err;
+
+	fi_param_define(NULL, "hmem_cuda_use_gdrcopy", FI_PARAM_BOOL,
+			"Use gdrcopy to copy data to/from CUDA GPU memory. "
+			"If libfabric is not compiled with gdrcopy support, "
+			"this variable is not checked. (default: true)");
+	fi_param_define(NULL, "hmem_cuda_enable_xfer", FI_PARAM_BOOL,
+			"Enable use of CUDA APIs for copying data to/from CUDA "
+			"GPU memory. This should be disabled if CUDA "
+			"operations on the default stream would result in a "
+			"deadlock in the application. (default: true)");
 
 	ret = cuda_hmem_dl_init();
 	if (ret != FI_SUCCESS)
@@ -309,18 +463,82 @@ int cuda_hmem_init(void)
 	if (ret != FI_SUCCESS)
 		goto dl_cleanup;
 
-	ret = cuda_gdrcopy_hmem_init();
-	if (ret == FI_SUCCESS) {
-		hmem_cuda_use_gdrcopy = 1;
-		fi_param_define(NULL, "hmem_cuda_use_gdrcopy", FI_PARAM_BOOL,
-				"Use gdrcopy to copy data to/from GPU memory");
-		fi_param_get_bool(NULL, "hmem_cuda_use_gdrcopy",
-				  &hmem_cuda_use_gdrcopy);
-	} else {
-		hmem_cuda_use_gdrcopy = 0;
-		if (ret != -FI_ENOSYS)
-			FI_WARN(&core_prov, FI_LOG_CORE,
-				"gdrcopy initialization failed! gdrcopy will not be used.\n");
+	fi_param_get_bool(NULL, "hmem_cuda_use_gdrcopy",
+			  &ret);
+	hmem_cuda_use_gdrcopy = (ret != 0);
+	if (hmem_cuda_use_gdrcopy) {
+		gdrcopy_ret = cuda_gdrcopy_hmem_init();
+		if (gdrcopy_ret != FI_SUCCESS) {
+			hmem_cuda_use_gdrcopy = false;
+			if (gdrcopy_ret != -FI_ENOSYS)
+				FI_WARN(&core_prov, FI_LOG_CORE,
+					"gdrcopy initialization failed! "
+					"gdrcopy will not be used.\n");
+		}
+	}
+
+	ret = 1;
+	fi_param_get_bool(NULL, "hmem_cuda_enable_xfer", &ret);
+	cuda_enable_xfer = (ret != 0);
+
+	if (!cuda_enable_xfer)
+		cuda_ops.cudaMemcpy = cuda_disabled_cudaMemcpy;
+
+	/*
+	 * CUDA IPC is only enabled if gdrcopy is not in use and
+	 * cudaMemcpy can be used.
+	 */
+	cuda_ipc_enabled = !hmem_cuda_use_gdrcopy && cuda_enable_xfer;
+
+	ret = FI_SUCCESS;
+
+	/*
+	 * If IPC may be enabled, loop over each device and ensure it
+	 * supports IPC. If any device lacks IPC support, set
+	 * cuda_ipc_enabled to false.
+	 */
+	if (cuda_ipc_enabled) {
+		cuda_ops.cudaGetDevice(&orig_dev_id);
+		cuda_ops.cudaGetDeviceCount(&devices);
+		for (dev_id = 0; dev_id < devices; ++dev_id) {
+
+			cuda_ops.cudaSetDevice(dev_id);
+
+			/* CUDA runtime and driver APIs do not expose IPC support
+			 * directly in the device properties, so try using IPC and
+			 * see if it errors out. Checking if unified memory is
+			 * supported is not sufficient as Tegra devices support
+			 * unified memory, but not IPC.
+			 */
+
+			err = cuda_ops.cudaMalloc(&ptr, 1);
+			if (err != cudaSuccess) {
+				FI_INFO(&core_prov, FI_LOG_CORE,
+					"Could not allocate memory on CUDA device %d\n",
+					dev_id);
+				cuda_ipc_enabled = false;
+				ret = -FI_EINVAL;
+				break;
+			}
+
+			err = cuda_ops.cudaIpcGetMemHandle(&handle, ptr);
+			cuda_ops.cudaFree(&ptr);
+
+			if (err != cudaSuccess) {
+				FI_INFO(&core_prov, FI_LOG_CORE,
+					"Could not get IPC handle on CUDA device %d,"
+					"disabling CUDA IPC\n",
+					dev_id);
+				cuda_ipc_enabled = false;
+				ret = -FI_EINVAL;
+				break;
+			}
+		}
+
+		cuda_ops.cudaSetDevice(orig_dev_id);
+
+		if (ret != FI_SUCCESS)
+			goto dl_cleanup;
 	}
 
 	return ret;
@@ -334,7 +552,8 @@ dl_cleanup:
 int cuda_hmem_cleanup(void)
 {
 	cuda_hmem_dl_cleanup();
-	cuda_gdrcopy_hmem_cleanup();
+	if (hmem_cuda_use_gdrcopy)
+		cuda_gdrcopy_hmem_cleanup();
 	return FI_SUCCESS;
 }
 
@@ -393,7 +612,7 @@ int cuda_host_register(void *ptr, size_t size)
 		return FI_SUCCESS;
 
 	FI_WARN(&core_prov, FI_LOG_CORE,
-		"Failed to perform cudaMemcpy: %s:%s\n",
+		"Failed to perform cudaHostRegister: %s:%s\n",
 		ofi_cudaGetErrorName(cuda_ret),
 		ofi_cudaGetErrorString(cuda_ret));
 
@@ -409,13 +628,18 @@ int cuda_host_unregister(void *ptr)
 		return FI_SUCCESS;
 
 	FI_WARN(&core_prov, FI_LOG_CORE,
-		"Failed to perform cudaMemcpy: %s:%s\n",
+		"Failed to perform cudaHostUnregister: %s:%s\n",
 		ofi_cudaGetErrorName(cuda_ret),
 		ofi_cudaGetErrorString(cuda_ret));
 
 	return -FI_EIO;
 }
 
+bool cuda_is_ipc_enabled(void)
+{
+	return cuda_ipc_enabled;
+}
+
 #else
 
 int cuda_copy_to_dev(uint64_t device, void *dev, const void *host, size_t size)
@@ -463,4 +687,24 @@ int cuda_dev_unregister(uint64_t handle)
 	return FI_SUCCESS;
 }
 
+int cuda_get_handle(void *dev_buf, void **handle)
+{
+	return -FI_ENOSYS;
+}
+
+int cuda_open_handle(void **handle, uint64_t device, void **ipc_ptr)
+{
+	return -FI_ENOSYS;
+}
+
+int cuda_close_handle(void *ipc_ptr)
+{
+	return -FI_ENOSYS;
+}
+
+bool cuda_is_ipc_enabled(void)
+{
+	return false;
+}
+
 #endif /* HAVE_LIBCUDA */
diff --git a/src/hmem_ze.c b/src/hmem_ze.c
index 57d2a7f..ee0d2a3 100644
--- a/src/hmem_ze.c
+++ b/src/hmem_ze.c
@@ -37,21 +37,20 @@
 #include "ofi_hmem.h"
 #include "ofi.h"
 
-#ifdef HAVE_LIBZE
+#if HAVE_LIBZE
 
 #include <dirent.h>
-#include <drm/i915_drm.h>
-#include <sys/ioctl.h>
 #include <level_zero/ze_api.h>
 
 static ze_context_handle_t context;
 static ze_device_handle_t devices[ZE_MAX_DEVICES];
 static ze_command_queue_handle_t cmd_queue[ZE_MAX_DEVICES];
 static int num_devices = 0;
+static int ordinals[ZE_MAX_DEVICES];
 static int dev_fds[ZE_MAX_DEVICES];
 static bool p2p_enabled = false;
 
-static const ze_command_queue_desc_t cq_desc = {
+static ze_command_queue_desc_t cq_desc = {
 	.stype		= ZE_STRUCTURE_TYPE_COMMAND_QUEUE_DESC,
 	.pNext		= NULL,
 	.ordinal	= 0,
@@ -61,13 +60,247 @@ static const ze_command_queue_desc_t cq_desc = {
 	.priority	= ZE_COMMAND_QUEUE_PRIORITY_NORMAL,
 };
 
-static const ze_command_list_desc_t cl_desc = {
+static ze_command_list_desc_t cl_desc = {
 	.stype				= ZE_STRUCTURE_TYPE_COMMAND_LIST_DESC,
 	.pNext				= NULL,
 	.commandQueueGroupOrdinal	= 0,
 	.flags				= 0,
 };
 
+struct libze_ops {
+	ze_result_t (*zeInit)(ze_init_flags_t flags);
+	ze_result_t (*zeDriverGet)(uint32_t *pCount,
+				   ze_driver_handle_t *phDrivers);
+	ze_result_t (*zeDeviceGet)(ze_driver_handle_t hDriver,
+				   uint32_t *pCount,
+				   ze_device_handle_t *phDevices);
+	ze_result_t (*zeDeviceGetCommandQueueGroupProperties)(ze_device_handle_t hDevice,
+			uint32_t *pCount,
+			ze_command_queue_group_properties_t *pCommandQueueGroupProperties);
+	ze_result_t (*zeDeviceCanAccessPeer)(ze_device_handle_t hDevice,
+					     ze_device_handle_t hPeerDevice,
+					     ze_bool_t *value);
+	ze_result_t (*zeContextCreate)(ze_driver_handle_t hDriver,
+				       const ze_context_desc_t *desc,
+				       ze_context_handle_t *phContext);
+	ze_result_t (*zeContextDestroy)(ze_context_handle_t hContext);
+	ze_result_t (*zeCommandQueueCreate)(ze_context_handle_t hContext,
+					    ze_device_handle_t hDevice,
+					    const ze_command_queue_desc_t *desc,
+					    ze_command_queue_handle_t *phCommandQueue);
+	ze_result_t (*zeCommandQueueDestroy)(ze_command_queue_handle_t hCommandQueue);
+	ze_result_t (*zeCommandQueueExecuteCommandLists)(
+					ze_command_queue_handle_t hCommandQueue,
+					uint32_t numCommandLists,
+					ze_command_list_handle_t *phCommandLists,
+					ze_fence_handle_t hFence);
+	ze_result_t (*zeCommandListCreate)(ze_context_handle_t hContext,
+					   ze_device_handle_t hDevice,
+					   const ze_command_list_desc_t *desc,
+					   ze_command_list_handle_t *phCommandList);
+	ze_result_t (*zeCommandListDestroy)(ze_command_list_handle_t hCommandList);
+	ze_result_t (*zeCommandListClose)(ze_command_list_handle_t hCommandList);
+	ze_result_t (*zeCommandListAppendMemoryCopy)(
+				ze_command_list_handle_t hCommandList,
+				void *dstptr, const void *srcptr, size_t size,
+				ze_event_handle_t hSignalEvent,
+				uint32_t numWaitEvents,
+				ze_event_handle_t *phWaitEvents);
+	ze_result_t (*zeMemGetAllocProperties)(
+				ze_context_handle_t hContext, const void *ptr,
+				ze_memory_allocation_properties_t *pMemAllocProperties,
+				ze_device_handle_t *phDevice);
+	ze_result_t (*zeMemGetAddressRange)(
+				ze_context_handle_t hContext, const void *ptr,
+				void **pBase, size_t *pSize);
+	ze_result_t (*zeMemGetIpcHandle)(ze_context_handle_t hContext,
+					 const void *ptr,
+					 ze_ipc_mem_handle_t *pIpcHandle);
+	ze_result_t (*zeMemOpenIpcHandle)(ze_context_handle_t hContext,
+					  ze_device_handle_t hDevice,
+					  ze_ipc_mem_handle_t handle,
+					  ze_ipc_memory_flags_t flags,
+					  void **pptr);
+	ze_result_t (*zeMemCloseIpcHandle)(ze_context_handle_t hContext,
+					   const void *ptr);
+};
+
+#ifdef ENABLE_ZE_DLOPEN
+
+#include <dlfcn.h>
+
+static void *libze_handle;
+static struct libze_ops libze_ops;
+
+#else
+
+static struct libze_ops libze_ops = {
+	.zeInit = zeInit,
+	.zeDriverGet = zeDriverGet,
+	.zeDeviceGet = zeDeviceGet,
+	.zeDeviceGetCommandQueueGroupProperties = zeDeviceGetCommandQueueGroupProperties,
+	.zeDeviceCanAccessPeer = zeDeviceCanAccessPeer,
+	.zeContextCreate = zeContextCreate,
+	.zeContextDestroy = zeContextDestroy,
+	.zeCommandQueueCreate = zeCommandQueueCreate,
+	.zeCommandQueueDestroy = zeCommandQueueDestroy,
+	.zeCommandQueueExecuteCommandLists = zeCommandQueueExecuteCommandLists,
+	.zeCommandListCreate = zeCommandListCreate,
+	.zeCommandListDestroy = zeCommandListDestroy,
+	.zeCommandListClose = zeCommandListClose,
+	.zeCommandListAppendMemoryCopy = zeCommandListAppendMemoryCopy,
+	.zeMemGetAllocProperties = zeMemGetAllocProperties,
+	.zeMemGetAddressRange = zeMemGetAddressRange,
+	.zeMemGetIpcHandle = zeMemGetIpcHandle,
+	.zeMemOpenIpcHandle = zeMemOpenIpcHandle,
+	.zeMemCloseIpcHandle = zeMemCloseIpcHandle,
+};
+
+#endif /* ENABLE_ZE_DLOPEN */
+
+ze_result_t ofi_zeInit(ze_init_flags_t flags)
+{
+	return (*libze_ops.zeInit)(flags);
+}
+
+ze_result_t ofi_zeDriverGet(uint32_t *pCount, ze_driver_handle_t *phDrivers)
+{
+	return (*libze_ops.zeDriverGet)(pCount, phDrivers);
+}
+
+ze_result_t ofi_zeDeviceGet(ze_driver_handle_t hDriver, uint32_t *pCount,
+			    ze_device_handle_t *phDevices)
+{
+	return (*libze_ops.zeDeviceGet)(hDriver, pCount, phDevices);
+}
+
+ze_result_t ofi_zeDeviceGetCommandQueueGroupProperties(ze_device_handle_t hDevice,
+	       uint32_t *pCount,
+	       ze_command_queue_group_properties_t *pCommandQueueGroupProperties)
+{
+	return (*libze_ops.zeDeviceGetCommandQueueGroupProperties)(hDevice,
+					pCount, pCommandQueueGroupProperties);
+}
+
+ze_result_t ofi_zeDeviceCanAccessPeer(ze_device_handle_t hDevice,
+				      ze_device_handle_t hPeerDevice,
+				      ze_bool_t *value)
+{
+	return (*libze_ops.zeDeviceCanAccessPeer)(hDevice, hPeerDevice, value);
+}
+
+ze_result_t ofi_zeContextCreate(ze_driver_handle_t hDriver,
+				const ze_context_desc_t *desc,
+				ze_context_handle_t *phContext)
+{
+	return (*libze_ops.zeContextCreate)(hDriver, desc, phContext);
+}
+
+ze_result_t ofi_zeContextDestroy(ze_context_handle_t hContext)
+{
+	return (*libze_ops.zeContextDestroy)(hContext);
+}
+
+ze_result_t ofi_zeCommandQueueCreate(ze_context_handle_t hContext,
+				     ze_device_handle_t hDevice,
+				     const ze_command_queue_desc_t *desc,
+				     ze_command_queue_handle_t *phCommandQueue)
+{
+	return (*libze_ops.zeCommandQueueCreate)(hContext, hDevice, desc,
+						 phCommandQueue);
+}
+
+ze_result_t ofi_zeCommandQueueDestroy(ze_command_queue_handle_t hCommandQueue)
+{
+	return (*libze_ops.zeCommandQueueDestroy)(hCommandQueue);
+}
+
+ze_result_t ofi_zeCommandQueueExecuteCommandLists(
+				ze_command_queue_handle_t hCommandQueue,
+				uint32_t numCommandLists,
+				ze_command_list_handle_t *phCommandLists,
+				ze_fence_handle_t hFence)
+{
+	return (*libze_ops.zeCommandQueueExecuteCommandLists)(
+				hCommandQueue, numCommandLists, phCommandLists,
+				hFence);
+}
+
+ze_result_t ofi_zeCommandListCreate(ze_context_handle_t hContext,
+				    ze_device_handle_t hDevice,
+				    const ze_command_list_desc_t *desc,
+				    ze_command_list_handle_t *phCommandList)
+{
+	return (*libze_ops.zeCommandListCreate)(hContext, hDevice, desc,
+						phCommandList);
+}
+
+ze_result_t ofi_zeCommandListDestroy(ze_command_list_handle_t hCommandList)
+{
+	return (*libze_ops.zeCommandListDestroy)(hCommandList);
+}
+
+ze_result_t ofi_zeCommandListClose(ze_command_list_handle_t hCommandList)
+{
+	return (*libze_ops.zeCommandListClose)(hCommandList);
+}
+
+ze_result_t ofi_zeCommandListAppendMemoryCopy(
+				ze_command_list_handle_t hCommandList,
+				void *dstptr, const void *srcptr, size_t size,
+				ze_event_handle_t hSignalEvent,
+				uint32_t numWaitEvents,
+				ze_event_handle_t *phWaitEvents)
+{
+	return (*libze_ops.zeCommandListAppendMemoryCopy)(
+				hCommandList, dstptr, srcptr, size, hSignalEvent,
+				numWaitEvents, phWaitEvents);
+}
+
+ze_result_t ofi_zeMemGetAllocProperties(ze_context_handle_t hContext,
+					const void *ptr,
+					ze_memory_allocation_properties_t
+						*pMemAllocProperties,
+					ze_device_handle_t *phDevice)
+{
+	return (*libze_ops.zeMemGetAllocProperties)(
+					hContext, ptr, pMemAllocProperties,
+					phDevice);
+}
+
+ze_result_t ofi_zeMemGetAddressRange(ze_context_handle_t hContext,
+				     const void *ptr, void **pBase,
+				     size_t *pSize)
+{
+	return (*libze_ops.zeMemGetAddressRange)(hContext, ptr, pBase, pSize);
+}
+
+ze_result_t ofi_zeMemGetIpcHandle(ze_context_handle_t hContext, const void *ptr,
+				  ze_ipc_mem_handle_t *pIpcHandle)
+{
+	return (*libze_ops.zeMemGetIpcHandle)(hContext, ptr, pIpcHandle);
+}
+
+ze_result_t ofi_zeMemOpenIpcHandle(ze_context_handle_t hContext,
+				   ze_device_handle_t hDevice,
+				   ze_ipc_mem_handle_t handle,
+				   ze_ipc_memory_flags_t flags,
+				   void **pptr)
+{
+	return (*libze_ops.zeMemOpenIpcHandle)(hContext, hDevice, handle, flags,
+					       pptr);
+}
+
+ze_result_t ofi_zeMemCloseIpcHandle(ze_context_handle_t hContext,
+				    const void *ptr)
+{
+	return (*libze_ops.zeMemCloseIpcHandle)(hContext, ptr);
+}
+
+#if HAVE_DRM
+#include <drm/i915_drm.h>
+#include <sys/ioctl.h>
+
 static int ze_hmem_init_fds(void)
 {
 	const char *dev_dir = "/dev/dri/by-path";
@@ -103,6 +336,270 @@ static int ze_hmem_init_fds(void)
 	return FI_SUCCESS;
 }
 
+int ze_hmem_get_shared_handle(int dev_fd, void *dev_buf, int *ze_fd,
+			      void **handle)
+{
+	struct drm_prime_handle open_fd = {0, 0, 0};
+	ze_ipc_mem_handle_t ze_handle;
+	int ret;
+
+	ret = ze_hmem_get_handle(dev_buf, (void **) &ze_handle);
+	if (ret)
+		return ret;
+
+	memcpy(ze_fd, &ze_handle, sizeof(*ze_fd));
+	memcpy(&open_fd.fd, &ze_handle, sizeof(open_fd.fd));
+	ret = ioctl(dev_fd, DRM_IOCTL_PRIME_FD_TO_HANDLE, &open_fd);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"ioctl call failed on get, err %d\n", errno);
+		return -FI_EINVAL;
+	}
+
+	*(int *) handle = open_fd.handle;
+	return FI_SUCCESS;
+}
+
+int ze_hmem_open_shared_handle(int dev_fd, void **handle, int *ze_fd,
+			       uint64_t device, void **ipc_ptr)
+{
+	struct drm_prime_handle open_fd = {0, 0, 0};
+	ze_ipc_mem_handle_t ze_handle;
+	int ret;
+
+	open_fd.flags = DRM_CLOEXEC | DRM_RDWR;
+	open_fd.handle = *(int *) handle;
+
+	ret = ioctl(dev_fd, DRM_IOCTL_PRIME_HANDLE_TO_FD, &open_fd);
+	if (ret) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"ioctl call failed on open, err %d\n", errno);
+		return -FI_EINVAL;
+	}
+
+	*ze_fd = open_fd.fd;
+	memset(&ze_handle, 0, sizeof(ze_handle));
+	memcpy(&ze_handle, &open_fd.fd, sizeof(open_fd.fd));
+	return ze_hmem_open_handle((void **) &ze_handle, device, ipc_ptr);
+}
+
+bool ze_hmem_p2p_enabled(void)
+{
+	return p2p_enabled;
+}
+
+#else
+
+static int ze_hmem_init_fds(void)
+{
+	return FI_SUCCESS;
+}
+
+int ze_hmem_get_shared_handle(int dev_fd, void *dev_buf, int *ze_fd,
+			      void **handle)
+{
+	return -FI_ENOSYS;
+}
+int ze_hmem_open_shared_handle(int dev_fd, void **handle, int *ze_fd,
+			       uint64_t device, void **ipc_ptr)
+{
+	return -FI_ENOSYS;
+}
+
+bool ze_hmem_p2p_enabled(void)
+{
+	return false;
+}
+
+#endif //HAVE_DRM
+
+static int ze_hmem_dl_init(void)
+{
+#ifdef ENABLE_ZE_DLOPEN
+	libze_handle = dlopen("libze_loader.so", RTLD_NOW);
+	if (!libze_handle) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to dlopen libze_loader.so\n");
+		goto err_out;
+	}
+
+	libze_ops.zeInit = dlsym(libze_handle, "zeInit");
+	if (!libze_ops.zeInit) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeInit\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDriverGet = dlsym(libze_handle, "zeDriverGet");
+	if (!libze_ops.zeDriverGet) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeDriverGet\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceGet = dlsym(libze_handle, "zeDeviceGet");
+	if (!libze_ops.zeDeviceGet) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeDeviceGet\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceGetCommandQueueGroupProperties = dlsym(libze_handle,
+				"zeDeviceGetCommandQueueGroupProperties");
+	if (!libze_ops.zeDeviceGetCommandQueueGroupProperties) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to find zeDeviceGetCommandQueueGroupProperties\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceCanAccessPeer = dlsym(libze_handle, "zeDeviceCanAccessPeer");
+	if (!libze_ops.zeDeviceCanAccessPeer) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeDeviceCanAccessPeer\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextCreate = dlsym(libze_handle, "zeContextCreate");
+	if (!libze_ops.zeContextCreate) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeContextCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextDestroy = dlsym(libze_handle, "zeContextDestroy");
+	if (!libze_ops.zeContextDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeContextDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextDestroy = dlsym(libze_handle, "zeContextDestroy");
+	if (!libze_ops.zeContextDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeContextDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueCreate = dlsym(libze_handle, "zeCommandQueueCreate");
+	if (!libze_ops.zeCommandQueueCreate) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandQueueCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueDestroy = dlsym(libze_handle, "zeCommandQueueDestroy");
+	if (!libze_ops.zeCommandQueueDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandQueueDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueExecuteCommandLists = dlsym(libze_handle, "zeCommandQueueExecuteCommandLists");
+	if (!libze_ops.zeCommandQueueExecuteCommandLists) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandQueueExecuteCommandLists\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListCreate = dlsym(libze_handle, "zeCommandListCreate");
+	if (!libze_ops.zeCommandListCreate) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListDestroy = dlsym(libze_handle, "zeCommandListDestroy");
+	if (!libze_ops.zeCommandListDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListClose = dlsym(libze_handle, "zeCommandListClose");
+	if (!libze_ops.zeCommandListClose) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListClose\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListAppendMemoryCopy = dlsym(libze_handle, "zeCommandListAppendMemoryCopy");
+	if (!libze_ops.zeCommandListAppendMemoryCopy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListAppendMemoryCopy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemGetAllocProperties = dlsym(libze_handle, "zeMemGetAllocProperties");
+	if (!libze_ops.zeMemGetAllocProperties) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemGetAllocProperties\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemGetAddressRange = dlsym(libze_handle, "zeMemGetAddressRange");
+	if (!libze_ops.zeMemGetAddressRange) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemGetAddressRange\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemGetIpcHandle = dlsym(libze_handle, "zeMemGetIpcHandle");
+	if (!libze_ops.zeMemGetIpcHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemGetIpcHandle\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemOpenIpcHandle = dlsym(libze_handle, "zeMemOpenIpcHandle");
+	if (!libze_ops.zeMemOpenIpcHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemOpenIpcHandle\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemCloseIpcHandle = dlsym(libze_handle, "zeMemCloseIpcHandle");
+	if (!libze_ops.zeMemCloseIpcHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemCloseIpcHandle\n");
+		goto err_dlclose;
+	}
+
+	return FI_SUCCESS;
+
+err_dlclose:
+	dlclose(libze_handle);
+
+err_out:
+	return -FI_ENODATA;
+
+#else
+	return FI_SUCCESS;
+#endif /* ENABLE_ZE_DLOPEN */
+}
+
+static void ze_hmem_dl_cleanup(void)
+{
+#ifdef ENABLE_ZE_DLOPEN
+	dlclose(libze_handle);
+#endif
+}
+
+static int ze_hmem_find_copy_only_engine(int device_num, int *ordinal)
+{
+	ze_result_t ze_ret;
+	uint32_t cq_grp_count = 0;
+	ze_command_queue_group_properties_t *cq_grp_props;
+	int i;
+
+	ze_ret = ofi_zeDeviceGetCommandQueueGroupProperties(devices[device_num],
+							    &cq_grp_count, NULL);
+	if (ze_ret)
+		goto out;
+
+	cq_grp_props = calloc(cq_grp_count, sizeof(*cq_grp_props));
+
+	ze_ret = ofi_zeDeviceGetCommandQueueGroupProperties(devices[device_num],
+							    &cq_grp_count,
+							    cq_grp_props);
+	if (ze_ret)
+		goto out;
+
+	for (i = 0; i < cq_grp_count; i++) {
+		if (cq_grp_props[i].flags &
+		    ZE_COMMAND_QUEUE_GROUP_PROPERTY_FLAG_COPY &&
+		    !(cq_grp_props[i].flags &
+		      ZE_COMMAND_QUEUE_GROUP_PROPERTY_FLAG_COMPUTE)) {
+			break;
+		}
+	}
+
+out:
+	free(cq_grp_props);
+	*ordinal = i == cq_grp_count ? 0 : i;
+	return ze_ret;
+}
+
 int ze_hmem_init(void)
 {
 	ze_driver_handle_t driver;
@@ -113,16 +610,20 @@ int ze_hmem_init(void)
 	bool p2p = true;
 	int ret;
 
-	ze_ret = zeInit(ZE_INIT_FLAG_GPU_ONLY);
+	ret = ze_hmem_dl_init();
+	if (ret)
+		return ret;
+
+	ze_ret = ofi_zeInit(ZE_INIT_FLAG_GPU_ONLY);
 	if (ze_ret)
 		return -FI_EIO;
 
 	count = 1;
-	ze_ret = zeDriverGet(&count, &driver);
+	ze_ret = ofi_zeDriverGet(&count, &driver);
 	if (ze_ret)
 		return -FI_EIO;
 
-	ze_ret = zeContextCreate(driver, &context_desc, &context);
+	ze_ret = ofi_zeContextCreate(driver, &context_desc, &context);
 	if (ze_ret)
 		return -FI_EIO;
 
@@ -130,11 +631,11 @@ int ze_hmem_init(void)
 		;
 
 	count = 0;
-	ze_ret = zeDeviceGet(driver, &count, NULL);
+	ze_ret = ofi_zeDeviceGet(driver, &count, NULL);
 	if (ze_ret || count > ZE_MAX_DEVICES)
 		goto err;
 
-	ze_ret = zeDeviceGet(driver, &count, devices);
+	ze_ret = ofi_zeDeviceGet(driver, &count, devices);
 	if (ze_ret)
 		goto err;
 
@@ -143,13 +644,21 @@ int ze_hmem_init(void)
 		goto err;
 
 	for (num_devices = 0; num_devices < count; num_devices++) {
-		ze_ret = zeCommandQueueCreate(context, devices[num_devices], &cq_desc,
-					      &cmd_queue[num_devices]);
+		ze_ret = ze_hmem_find_copy_only_engine(num_devices,
+						       &ordinals[num_devices]);
+		if (ze_ret)
+			goto err;
+
+		cq_desc.ordinal = ordinals[num_devices];
+		ze_ret = ofi_zeCommandQueueCreate(context,
+						  devices[num_devices],
+						  &cq_desc,
+						  &cmd_queue[num_devices]);
 		if (ze_ret)
 			goto err;
 
 		for (i = 0; i < count; i++) {
-			if (zeDeviceCanAccessPeer(devices[num_devices],
+			if (ofi_zeDeviceCanAccessPeer(devices[num_devices],
 					devices[i], &access) || !access)
 				p2p = false;
 		}
@@ -171,7 +680,7 @@ int ze_hmem_cleanup(void)
 	int i, ret = FI_SUCCESS;
 
 	for (i = 0; i < num_devices; i++) {
-		if (cmd_queue[i] && zeCommandQueueDestroy(cmd_queue[i])) {
+		if (cmd_queue[i] && ofi_zeCommandQueueDestroy(cmd_queue[i])) {
 			FI_WARN(&core_prov, FI_LOG_CORE,
 				"Failed to destroy ZE cmd_queue\n");
 			ret = -FI_EINVAL;
@@ -182,9 +691,10 @@ int ze_hmem_cleanup(void)
 		}
 	}
 
-	if (zeContextDestroy(context))
-		return -FI_EINVAL;
+	if (ofi_zeContextDestroy(context))
+		ret = -FI_EINVAL;
 
+	ze_hmem_dl_cleanup();
 	return ret;
 }
 
@@ -194,23 +704,26 @@ int ze_hmem_copy(uint64_t device, void *dst, const void *src, size_t size)
 	ze_result_t ze_ret;
 	int dev_id = (int) device;
 
-	ze_ret = zeCommandListCreate(context, devices[dev_id], &cl_desc, &cmd_list);
+	cl_desc.commandQueueGroupOrdinal = ordinals[dev_id];
+	ze_ret = ofi_zeCommandListCreate(context, devices[dev_id], &cl_desc,
+					 &cmd_list);
 	if (ze_ret)
 		goto err;
 
-	ze_ret = zeCommandListAppendMemoryCopy(cmd_list, dst, src, size, NULL, 0, NULL);
+	ze_ret = ofi_zeCommandListAppendMemoryCopy(cmd_list, dst, src, size,
+						   NULL, 0, NULL);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandListClose(cmd_list);
+	ze_ret = ofi_zeCommandListClose(cmd_list);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandQueueExecuteCommandLists(cmd_queue[dev_id], 1,
-						   &cmd_list, NULL);
+	ze_ret = ofi_zeCommandQueueExecuteCommandLists(cmd_queue[dev_id], 1,
+						       &cmd_list, NULL);
 
 free:
-	if (!zeCommandListDestroy(cmd_list) && !ze_ret)
+	if (!ofi_zeCommandListDestroy(cmd_list) && !ze_ret)
 		return FI_SUCCESS;
 err:
 	FI_WARN(&core_prov, FI_LOG_CORE,
@@ -222,25 +735,23 @@ err:
 bool ze_is_addr_valid(const void *addr)
 {
 	ze_result_t ze_ret;
-	ze_memory_allocation_properties_t mem_prop;
+	ze_memory_allocation_properties_t mem_props;
 	ze_device_handle_t device;
-	int i;
 
-	for (i = 0; i < num_devices; i++) {
-		ze_ret = zeMemGetAllocProperties(context, addr, &mem_prop,
-						 &device);
-		if (!ze_ret && mem_prop.type == ZE_MEMORY_TYPE_DEVICE)
-			return true;
-	}
-	return false;
+	mem_props.stype = ZE_STRUCTURE_TYPE_MEMORY_ALLOCATION_PROPERTIES;
+	mem_props.pNext = NULL;
+	ze_ret = ofi_zeMemGetAllocProperties(context, addr, &mem_props,
+					     &device);
+
+	return (!ze_ret && mem_props.type == ZE_MEMORY_TYPE_DEVICE);
 }
 
 int ze_hmem_get_handle(void *dev_buf, void **handle)
 {
 	ze_result_t ze_ret;
 
-	ze_ret = zeMemGetIpcHandle(context, dev_buf,
-				   (ze_ipc_mem_handle_t *) handle);
+	ze_ret = ofi_zeMemGetIpcHandle(context, dev_buf,
+				       (ze_ipc_mem_handle_t *) handle);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE, "Unable to get handle\n");
 		return -FI_EINVAL;
@@ -253,9 +764,9 @@ int ze_hmem_open_handle(void **handle, uint64_t device, void **ipc_ptr)
 {
 	ze_result_t ze_ret;
 
-	ze_ret = zeMemOpenIpcHandle(context, devices[device],
-				    *((ze_ipc_mem_handle_t *) handle),
-				    0, ipc_ptr);
+	ze_ret = ofi_zeMemOpenIpcHandle(context, devices[device],
+					*((ze_ipc_mem_handle_t *) handle),
+					0, ipc_ptr);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
 			"Unable to open memory handle\n");
@@ -265,83 +776,50 @@ int ze_hmem_open_handle(void **handle, uint64_t device, void **ipc_ptr)
 	return FI_SUCCESS;
 }
 
-int ze_hmem_get_shared_handle(int dev_fd, void *dev_buf, int *ze_fd,
-			      void **handle)
+int ze_hmem_close_handle(void *ipc_ptr)
 {
-	struct drm_prime_handle open_fd = {0, 0, 0};
-	ze_ipc_mem_handle_t ze_handle;
-	int ret;
-
-	ret = ze_hmem_get_handle(dev_buf, (void **) &ze_handle);
-	if (ret)
-		return ret;
+	ze_result_t ze_ret;
 
-	memcpy(ze_fd, &ze_handle, sizeof(*ze_fd));
-	memcpy(&open_fd.fd, &ze_handle, sizeof(open_fd.fd));
-	ret = ioctl(dev_fd, DRM_IOCTL_PRIME_FD_TO_HANDLE, &open_fd);
-	if (ret) {
+	ze_ret = ofi_zeMemCloseIpcHandle(context, ipc_ptr);
+	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
-			"ioctl call failed on get, err %d\n", errno);
+			"Unable to close memory handle\n");
 		return -FI_EINVAL;
 	}
 
-	*(int *) handle = open_fd.handle;
 	return FI_SUCCESS;
 }
 
-int ze_hmem_open_shared_handle(int dev_fd, void **handle, int *ze_fd,
-			       uint64_t device, void **ipc_ptr)
-{
-	struct drm_prime_handle open_fd = {0, 0, 0};
-	ze_ipc_mem_handle_t ze_handle;
-	int ret;
-
-	open_fd.flags = DRM_CLOEXEC | DRM_RDWR;
-	open_fd.handle = *(int *) handle;
-
-	ret = ioctl(dev_fd, DRM_IOCTL_PRIME_HANDLE_TO_FD, &open_fd);
-	if (ret) {
-		FI_WARN(&core_prov, FI_LOG_CORE,
-			"ioctl call failed on open, err %d\n", errno);
-		return -FI_EINVAL;
-	}
-
-	*ze_fd = open_fd.fd;
-	memset(&ze_handle, 0, sizeof(ze_handle));
-	memcpy(&ze_handle, &open_fd.fd, sizeof(open_fd.fd));
-	return ze_hmem_open_handle((void **) &ze_handle, device, ipc_ptr);
-}
-
-int ze_hmem_close_handle(void *ipc_ptr)
+int ze_hmem_get_base_addr(const void *ptr, void **base)
 {
 	ze_result_t ze_ret;
+	size_t size;
 
-	ze_ret = zeMemCloseIpcHandle(context, ipc_ptr);
+	ze_ret = ofi_zeMemGetAddressRange(context, ptr, base, &size);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
-			"Unable to close memory handle\n");
+			"Could not get base addr\n");
 		return -FI_EINVAL;
 	}
-
 	return FI_SUCCESS;
 }
 
-bool ze_hmem_p2p_enabled(void)
-{
-	return p2p_enabled;
-}
-
-int ze_hmem_get_base_addr(const void *ptr, void **base)
+int ze_hmem_get_id(const void *ptr, uint64_t *id)
 {
 	ze_result_t ze_ret;
-	size_t size;
+	ze_memory_allocation_properties_t mem_props;
+	ze_device_handle_t device;
 
-	ze_ret = zeMemGetAddressRange(context, ptr, base, &size);
-	if (ze_ret) {
+	mem_props.stype = ZE_STRUCTURE_TYPE_MEMORY_ALLOCATION_PROPERTIES;
+	mem_props.pNext = NULL;
+	ze_ret = ofi_zeMemGetAllocProperties(context, ptr, &mem_props, &device);
+	if (ze_ret || mem_props.type == ZE_MEMORY_TYPE_UNKNOWN) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
-			"Could not get base addr\n");
+			"Could not get memory id\n");
 		return -FI_EINVAL;
 	}
+
+	*id = mem_props.id;
 	return FI_SUCCESS;
 }
 
@@ -410,6 +888,11 @@ int ze_hmem_get_base_addr(const void *ptr, void **base)
 	return -FI_ENOSYS;
 }
 
+int ze_hmem_get_id(const void *ptr, uint64_t *id)
+{
+	return -FI_ENOSYS;
+}
+
 int *ze_hmem_get_dev_fds(int *nfds)
 {
 	*nfds = 0;
diff --git a/src/log.c b/src/log.c
index 868f035..b9789c4 100644
--- a/src/log.c
+++ b/src/log.c
@@ -78,6 +78,7 @@ enum {
 	 ((uint64_t) (1 << (subsys + FI_LOG_SUBSYS_OFFSET))) | \
 	 ((uint64_t) (1 << level)))
 
+static int log_interval = 2000;
 uint64_t log_mask;
 struct fi_filter prov_log_filter;
 
@@ -103,6 +104,11 @@ void fi_log_init(void)
 	int level, i;
 	char *levelstr = NULL, *provstr = NULL, *subsysstr = NULL;
 
+	fi_param_define(NULL, "log_interval", FI_PARAM_INT,
+			"Delay in ms between rate limited log messages "
+			"(default 2000)");
+	fi_param_get_int(NULL, "log_interval", &log_interval);
+
 	fi_param_define(NULL, "log_level", FI_PARAM_STRING,
 			"Specify logging level: warn, trace, info, debug (default: warn)");
 	fi_param_get_str(NULL, "log_level", &levelstr);
@@ -146,6 +152,24 @@ int DEFAULT_SYMVER_PRE(fi_log_enabled)(const struct fi_provider *prov,
 DEFAULT_SYMVER(fi_log_enabled_, fi_log_enabled, FABRIC_1.0);
 
 __attribute__((visibility ("default"),EXTERNALLY_VISIBLE))
+int DEFAULT_SYMVER_PRE(fi_log_ready)(const struct fi_provider *prov,
+		enum fi_log_level level, enum fi_log_subsys subsys,
+		uint64_t *showtime)
+{
+	uint64_t cur;
+
+	if (fi_log_enabled(prov, level, subsys)) {
+		cur = ofi_gettime_ms();
+		if (cur >= *showtime) {
+			*showtime = cur + (uint64_t) log_interval;
+			return true;
+		}
+	}
+	return false;
+}
+CURRENT_SYMVER(fi_log_ready_, fi_log_ready);
+
+__attribute__((visibility ("default"),EXTERNALLY_VISIBLE))
 void DEFAULT_SYMVER_PRE(fi_log)(const struct fi_provider *prov, enum fi_log_level level,
 		enum fi_log_subsys subsys, const char *func, int line,
 		const char *fmt, ...)
diff --git a/src/unix/osd.c b/src/unix/osd.c
index ac11573..e32b05a 100644
--- a/src/unix/osd.c
+++ b/src/unix/osd.c
@@ -296,3 +296,45 @@ int ofi_set_thread_affinity(const char *s)
 	return -FI_ENOSYS;
 #endif
 }
+
+
+void ofi_pollfds_do_add(struct ofi_pollfds *pfds,
+			struct ofi_pollfds_work_item *item)
+{
+	if (item->fd >= pfds->size) {
+		if (ofi_pollfds_grow(pfds, item->fd))
+			return;
+	}
+
+	pfds->fds[item->fd].fd = item->fd;
+	pfds->fds[item->fd].events = item->events;
+	pfds->fds[item->fd].revents = 0;
+	pfds->context[item->fd] = item->context;
+	if (item->fd >= pfds->nfds)
+		pfds->nfds = item->fd + 1;
+}
+
+int ofi_pollfds_do_mod(struct ofi_pollfds *pfds, int fd, uint32_t events,
+		       void *context)
+{
+	if ((fd < pfds->nfds) && (pfds->fds[fd].fd == fd)) {
+		pfds->fds[fd].events = events;
+		pfds->context[fd] = context;
+		return FI_SUCCESS;
+	}
+
+	return -FI_ENOENT;
+}
+
+void ofi_pollfds_do_del(struct ofi_pollfds *pfds,
+			struct ofi_pollfds_work_item *item)
+{
+	if (item->fd >= pfds->nfds)
+		return;
+
+	pfds->fds[item->fd].fd = INVALID_SOCKET;
+	pfds->fds[item->fd].events = 0;
+	pfds->fds[item->fd].revents = 0;
+	while (pfds->nfds && pfds->fds[pfds->nfds - 1].fd == INVALID_SOCKET)
+		pfds->nfds--;
+}
diff --git a/src/var.c b/src/var.c
index 6103db1..dd5f116 100644
--- a/src/var.c
+++ b/src/var.c
@@ -258,6 +258,7 @@ int DEFAULT_SYMVER_PRE(fi_param_get)(struct fi_provider *provider,
 {
 	struct fi_param_entry *param;
 	char *str_value;
+	int parsed_boolean;
 	int ret = FI_SUCCESS;
 
 	if (!provider)
@@ -294,11 +295,17 @@ int DEFAULT_SYMVER_PRE(fi_param_get)(struct fi_provider *provider,
 			"read int var %s=%d\n", param_name, *(int *) value);
 		break;
 	case FI_PARAM_BOOL:
-		* ((int *) value) = fi_parse_bool(str_value);
+		parsed_boolean = fi_parse_bool(str_value);
+		if (parsed_boolean == -1) {
+			ret = -FI_EINVAL;
+			FI_WARN(provider, FI_LOG_CORE,
+					"failed to parse bool var %s=%s\n", param_name, str_value);
+			break;
+		}
+
+		* ((int *) value) = parsed_boolean;
 		FI_INFO(provider, FI_LOG_CORE,
 			"read bool var %s=%d\n", param_name, *(int *) value);
-		if (*(int *) value == -1)
-			ret = -FI_EINVAL;
 		break;
 	case FI_PARAM_SIZE_T:
 		* ((size_t *) value) = strtol(str_value, NULL, 0);
diff --git a/src/windows/osd.c b/src/windows/osd.c
index 7c0005d..83ce12a 100644
--- a/src/windows/osd.c
+++ b/src/windows/osd.c
@@ -600,3 +600,55 @@ ssize_t ofi_recvmsg_udp(SOCKET fd, struct msghdr *msg, int flags)
 	ret = WSARecvMsg(fd, msg, &bytes, NULL, NULL);
 	return ret ? ret : bytes;
 }
+
+
+void ofi_pollfds_do_add(struct ofi_pollfds *pfds,
+			struct ofi_pollfds_work_item *item)
+{
+	if (pfds->nfds == pfds->size) {
+		if (ofi_pollfds_grow(pfds, pfds->size + 1))
+			return;
+	}
+
+	pfds->fds[pfds->nfds].fd = item->fd;
+	pfds->fds[pfds->nfds].events = item->events;
+	pfds->fds[pfds->nfds].revents = 0;
+	pfds->context[pfds->nfds] = item->context;
+	pfds->nfds++;
+}
+
+int ofi_pollfds_do_mod(struct ofi_pollfds *pfds, int fd, uint32_t events,
+		       void *context)
+{
+	int i;
+
+	/* 0 is signaling fd */
+	for (i = 1; i < pfds->nfds; i++) {
+		if (pfds->fds[i].fd == fd) {
+			pfds->fds[i].events = events;
+			pfds->context[i] = context;
+			return FI_SUCCESS;
+		}
+	}
+
+	return -FI_ENOENT;
+}
+
+void ofi_pollfds_do_del(struct ofi_pollfds *pfds,
+			struct ofi_pollfds_work_item *item)
+{
+	int i;
+
+	for (i = 0; i < pfds->nfds; i++) {
+		if (pfds->fds[i].fd == item->fd) {
+			pfds->fds[i].fd = INVALID_SOCKET;
+
+			pfds->nfds--;
+			pfds->fds[i].fd = pfds->fds[pfds->nfds].fd;
+			pfds->fds[i].events = pfds->fds[pfds->nfds].events;
+			pfds->fds[i].revents = pfds->fds[pfds->nfds].revents;
+			pfds->context[i] = pfds->context[pfds->nfds];
+			break;
+		}
+	}
+}
diff --git a/util/pingpong.c b/util/pingpong.c
index f554189..8b3355f 100644
--- a/util/pingpong.c
+++ b/util/pingpong.c
@@ -120,6 +120,7 @@ struct pp_opts {
 		__LINE__, ##__VA_ARGS__)
 
 int pp_debug;
+int pp_ipv6;
 
 #define PP_DEBUG(fmt, ...)                                                     \
 	do {                                                                   \
@@ -299,7 +300,7 @@ static int pp_getaddrinfo(char *name, uint16_t port, struct addrinfo **results)
 	char port_s[6];
 
 	struct addrinfo hints = {
-	    .ai_family = AF_INET,       /* IPv4 */
+	    .ai_family = pp_ipv6 ? AF_INET6 : AF_INET,
 	    .ai_socktype = SOCK_STREAM, /* TCP socket */
 	    .ai_protocol = IPPROTO_TCP, /* Any protocol */
 	    .ai_flags = AI_NUMERICSERV /* numeric port is used */
@@ -320,9 +321,22 @@ out:
 	return ret;
 }
 
+static void pp_print_addrinfo(struct addrinfo *ai, char *msg)
+{
+	char s[80] = {0};
+	void *addr;
+
+	if (ai->ai_family == AF_INET6)
+		addr = &((struct sockaddr_in6 *)ai->ai_addr)->sin6_addr;
+	else
+		addr = &((struct sockaddr_in *)ai->ai_addr)->sin_addr;
+
+	inet_ntop(ai->ai_family, addr, s, 80);
+	PP_DEBUG("%s %s\n", msg, s);
+}
+
 static int pp_ctrl_init_client(struct ct_pingpong *ct)
 {
-	struct sockaddr_in in_addr = {0};
 	struct addrinfo *results;
 	struct addrinfo *rp;
 	int errno_save = 0;
@@ -346,13 +360,27 @@ static int pp_ctrl_init_client(struct ct_pingpong *ct)
 		}
 
 		if (ct->opts.src_port != 0) {
-			in_addr.sin_family = AF_INET;
-			in_addr.sin_port = htons(ct->opts.src_port);
-			in_addr.sin_addr.s_addr = htonl(INADDR_ANY);
+			if (pp_ipv6) {
+				struct sockaddr_in6 in6_addr = {0};
 
-			ret =
-			    bind(ct->ctrl_connfd, (struct sockaddr *)&in_addr,
-				 sizeof(in_addr));
+				in6_addr.sin6_family = AF_INET6;
+				in6_addr.sin6_port = htons(ct->opts.src_port);
+				in6_addr.sin6_addr = in6addr_any;
+
+				ret =
+				    bind(ct->ctrl_connfd, (struct sockaddr *)&in6_addr,
+					 sizeof(in6_addr));
+			} else {
+				struct sockaddr_in in_addr = {0};
+
+				in_addr.sin_family = AF_INET;
+				in_addr.sin_port = htons(ct->opts.src_port);
+				in_addr.sin_addr.s_addr = htonl(INADDR_ANY);
+
+				ret =
+				    bind(ct->ctrl_connfd, (struct sockaddr *)&in_addr,
+					 sizeof(in_addr));
+			}
 			if (ret == -1) {
 				errno_save = ofi_sockerr();
 				ofi_close_socket(ct->ctrl_connfd);
@@ -360,6 +388,8 @@ static int pp_ctrl_init_client(struct ct_pingpong *ct)
 			}
 		}
 
+		pp_print_addrinfo(rp, "CLIENT: connecting to");
+
 		ret = connect(ct->ctrl_connfd, rp->ai_addr, rp->ai_addrlen);
 		if (ret != -1)
 			break;
@@ -383,12 +413,11 @@ static int pp_ctrl_init_client(struct ct_pingpong *ct)
 
 static int pp_ctrl_init_server(struct ct_pingpong *ct)
 {
-	struct sockaddr_in ctrl_addr = {0};
 	int optval = 1;
 	SOCKET listenfd;
 	int ret;
 
-	listenfd = ofi_socket(AF_INET, SOCK_STREAM, 0);
+	listenfd = ofi_socket(pp_ipv6 ? AF_INET6 : AF_INET, SOCK_STREAM, 0);
 	if (listenfd == INVALID_SOCKET) {
 		ret = -ofi_sockerr();
 		PP_PRINTERR("socket", ret);
@@ -403,12 +432,25 @@ static int pp_ctrl_init_server(struct ct_pingpong *ct)
 		goto fail_close_socket;
 	}
 
-	ctrl_addr.sin_family = AF_INET;
-	ctrl_addr.sin_port = htons(ct->opts.src_port);
-	ctrl_addr.sin_addr.s_addr = htonl(INADDR_ANY);
+	if (pp_ipv6) {
+		struct sockaddr_in6 ctrl6_addr = {0};
+
+		ctrl6_addr.sin6_family = AF_INET6;
+		ctrl6_addr.sin6_port = htons(ct->opts.src_port);
+		ctrl6_addr.sin6_addr = in6addr_any;
+
+		ret = bind(listenfd, (struct sockaddr *)&ctrl6_addr,
+			   sizeof(ctrl6_addr));
+	} else {
+		struct sockaddr_in ctrl_addr = {0};
+
+		ctrl_addr.sin_family = AF_INET;
+		ctrl_addr.sin_port = htons(ct->opts.src_port);
+		ctrl_addr.sin_addr.s_addr = htonl(INADDR_ANY);
 
-	ret = bind(listenfd, (struct sockaddr *)&ctrl_addr,
-		   sizeof(ctrl_addr));
+		ret = bind(listenfd, (struct sockaddr *)&ctrl_addr,
+			   sizeof(ctrl_addr));
+	}
 	if (ret == -1) {
 		ret = -ofi_sockerr();
 		PP_PRINTERR("bind", ret);
@@ -1379,6 +1421,11 @@ static int pp_alloc_active_res(struct ct_pingpong *ct, struct fi_info *fi)
 {
 	int ret;
 
+	if (fi->tx_attr->mode & FI_MSG_PREFIX)
+		ct->tx_prefix_size = fi->ep_attr->msg_prefix_size;
+	if (fi->rx_attr->mode & FI_MSG_PREFIX)
+		ct->rx_prefix_size = fi->ep_attr->msg_prefix_size;
+
 	ret = pp_alloc_msgs(ct);
 	if (ret)
 		return ret;
@@ -1414,11 +1461,6 @@ static int pp_alloc_active_res(struct ct_pingpong *ct, struct fi_info *fi)
 		}
 	}
 
-	if (fi->tx_attr->mode & FI_MSG_PREFIX)
-		ct->tx_prefix_size = fi->ep_attr->msg_prefix_size;
-	if (fi->rx_attr->mode & FI_MSG_PREFIX)
-		ct->rx_prefix_size = fi->ep_attr->msg_prefix_size;
-
 	ret = fi_endpoint(ct->domain, fi, &(ct->ep), NULL);
 	if (ret) {
 		PP_PRINTERR("fi_endpoint", ret);
@@ -1970,6 +2012,7 @@ static void pp_pingpong_usage(struct ct_pingpong *ct, char *name, char *desc)
 
 	fprintf(stderr, " %-20s %s\n", "-h", "display this help output");
 	fprintf(stderr, " %-20s %s\n", "-v", "enable debugging output");
+	fprintf(stderr, " %-20s %s\n", "-6", "use IPv6 address");
 }
 
 static void pp_parse_opts(struct ct_pingpong *ct, int op, char *optarg)
@@ -2050,6 +2093,12 @@ static void pp_parse_opts(struct ct_pingpong *ct, int op, char *optarg)
 	case 'v':
 		pp_debug = 1;
 		break;
+
+	/* IPV6 */
+	case '6':
+		pp_ipv6 = 1;
+		break;
+
 	default:
 		/* let getopt handle unknown opts*/
 		break;
@@ -2153,7 +2202,9 @@ static int run_pingpong_dgram(struct ct_pingpong *ct)
 	/* Post an extra receive to avoid lacking a posted receive in the
 	 * finalize.
 	 */
-	ret = fi_recv(ct->ep, ct->rx_buf, ct->rx_size, fi_mr_desc(ct->mr), 0,
+	ret = fi_recv(ct->ep, ct->rx_buf,
+		      MAX(ct->rx_size, PP_MAX_CTRL_MSG) +  ct->rx_prefix_size,
+		      fi_mr_desc(ct->mr), 0,
 		      ct->rx_ctx_ptr);
 	if (ret)
 		return ret;
@@ -2243,7 +2294,7 @@ int main(int argc, char **argv)
 
 	ofi_osd_init();
 
-	while ((op = getopt(argc, argv, "hvd:p:e:I:S:B:P:cm:")) != -1) {
+	while ((op = getopt(argc, argv, "hvd:p:e:I:S:B:P:cm:6")) != -1) {
 		switch (op) {
 		default:
 			pp_parse_opts(&ct, op, optarg);
