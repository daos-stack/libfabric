diff --git a/configure.ac b/configure.ac
index 97f14bb..0b25e04 100644
--- a/configure.ac
+++ b/configure.ac
@@ -184,10 +184,19 @@ fi
 AC_DEFINE_UNQUOTED([PT_LOCK_SPIN], [$have_spinlock],
 	[Define to 1 if pthread_spin_init is available.])
 
-AC_CHECK_FUNCS([epoll_create])
-if test "$ac_cv_func_epoll_create" = yes; then
-  AC_DEFINE([HAVE_EPOLL], [1], [Define if you have epoll support.])
-fi
+AC_ARG_ENABLE([epoll],
+    [AS_HELP_STRING([--disable-epoll],
+        [Disable epoll if available@<:@default=no@:>@])],
+    [],
+    [enable_epoll=auto]
+)
+
+AS_IF([test x"$enable_epoll" != x"no"],
+    [AC_CHECK_FUNCS([epoll_create])
+     if test "$ac_cv_func_epoll_create" = yes; then
+        AC_DEFINE([HAVE_EPOLL], [1], [Define if you have epoll support.])
+     fi]
+)
 
 AC_CHECK_HEADER([linux/perf_event.h],
     [AC_CHECK_DECL([__builtin_ia32_rdpmc],
diff --git a/fabtests/common/shared.c b/fabtests/common/shared.c
index 0e8e76b..6edb7f7 100644
--- a/fabtests/common/shared.c
+++ b/fabtests/common/shared.c
@@ -379,7 +379,7 @@ static int ft_alloc_ctx_array(struct ft_context **mr_array, char ***mr_bufs,
 	for (i = 0; i < opts.window_size; i++) {
 		context = &(*mr_array)[i];
 		if (!(opts.options & FT_OPT_ALLOC_MULT_MR)) {
-			context->buf = default_buf;
+			context->buf = default_buf + mr_size * i;
 			continue;
 		}
 		(*mr_bufs)[i] = calloc(1, mr_size);
@@ -435,7 +435,8 @@ static int ft_alloc_msgs(void)
 		ft_set_tx_rx_sizes(&tx_size, &rx_size);
 		tx_mr_size = 0;
 		rx_mr_size = 0;
-		buf_size = MAX(tx_size, FT_MAX_CTRL_MSG) + MAX(rx_size, FT_MAX_CTRL_MSG);
+		buf_size = MAX(tx_size, FT_MAX_CTRL_MSG) * opts.window_size + 
+			   MAX(rx_size, FT_MAX_CTRL_MSG) * opts.window_size;
 	}
 
 	if (opts.options & FT_OPT_ALIGN) {
@@ -459,9 +460,11 @@ static int ft_alloc_msgs(void)
 	}
 	memset(buf, 0, buf_size);
 	rx_buf = buf;
-	tx_buf = (char *) buf + MAX(rx_size, FT_MAX_CTRL_MSG);
-	tx_buf = (void *) (((uintptr_t) tx_buf + alignment - 1) &
-			   ~(alignment - 1));
+
+	if (opts.options & FT_OPT_ALLOC_MULT_MR)
+		tx_buf = (char *) buf + MAX(rx_size, FT_MAX_CTRL_MSG);
+	else
+		tx_buf = (char *) buf + MAX(rx_size, FT_MAX_CTRL_MSG) * opts.window_size;
 
 	remote_cq_data = ft_init_cq_data(fi);
 
diff --git a/fabtests/functional/bw.c b/fabtests/functional/bw.c
index 4da6540..4ba6769 100644
--- a/fabtests/functional/bw.c
+++ b/fabtests/functional/bw.c
@@ -218,6 +218,11 @@ int main(int argc, char **argv)
 		case '?':
 		case 'h':
 			ft_usage(argv[0], "A bandwidth test with data verification.");
+			FT_PRINT_OPTS_USAGE("-T sleep_time",
+				"Receive side delay before starting");
+			FT_PRINT_OPTS_USAGE("-v", "Enable data verification");
+			FT_PRINT_OPTS_USAGE("-W window_size",
+				"Set transmit window size before waiting for completion");
 			return EXIT_FAILURE;
 		}
 	}
diff --git a/fabtests/functional/multi_ep.c b/fabtests/functional/multi_ep.c
index ccbabc0..3e122ca 100644
--- a/fabtests/functional/multi_ep.c
+++ b/fabtests/functional/multi_ep.c
@@ -300,7 +300,7 @@ int main(int argc, char **argv)
 			ft_usage(argv[0], "Multi endpoint test");
 			FT_PRINT_OPTS_USAGE("-c <int>",
 				"number of endpoints to create and test (def 3)");
-			FT_PRINT_OPTS_USAGE("-v", "Enable DataCheck testing");
+			FT_PRINT_OPTS_USAGE("-v", "Enable data verification");
 			return EXIT_FAILURE;
 		}
 	}
diff --git a/fabtests/functional/multi_mr.c b/fabtests/functional/multi_mr.c
index 23aebc8..6a814fc 100644
--- a/fabtests/functional/multi_mr.c
+++ b/fabtests/functional/multi_mr.c
@@ -303,9 +303,10 @@ int main(int argc, char **argv)
 		case '?':
 		case 'h':
 			ft_usage(argv[0], "Ping-pong multi memory region test");
-			FT_PRINT_OPTS_USAGE("-c <int>", "number of memory regions to create and test");
+			FT_PRINT_OPTS_USAGE("-c <int>",
+				"number of memory regions to create and test");
 			FT_PRINT_OPTS_USAGE("-V", "Enable verbose printing");
-			FT_PRINT_OPTS_USAGE("-v", "Enable DataCheck testing");
+			FT_PRINT_OPTS_USAGE("-v", "Enable data verification");
 			return EXIT_FAILURE;
 		}
 	}
diff --git a/fabtests/functional/multi_recv.c b/fabtests/functional/multi_recv.c
index e3b006b..a17fa92 100644
--- a/fabtests/functional/multi_recv.c
+++ b/fabtests/functional/multi_recv.c
@@ -474,8 +474,10 @@ int main(int argc, char **argv)
 			break;
 		case '?':
 		case 'h':
-			ft_csusage(argv[0], "Streaming RDM client-server using multi recv buffer.");
+			ft_csusage(argv[0],
+				"Streaming RDM client-server using multi recv buffer.");
 			FT_PRINT_OPTS_USAGE("-M", "enable testing with fi_recvmsg");
+			FT_PRINT_OPTS_USAGE("-v", "Enable data verification");
 			return EXIT_FAILURE;
 		}
 	}
diff --git a/fabtests/functional/rdm_atomic.c b/fabtests/functional/rdm_atomic.c
index c2b3655..86e46d2 100644
--- a/fabtests/functional/rdm_atomic.c
+++ b/fabtests/functional/rdm_atomic.c
@@ -435,6 +435,10 @@ static int init_fabric(void)
 {
 	int ret;
 
+	ret  = ft_init_oob();
+	if (ret)
+		return ret;
+
 	ret = ft_getinfo(hints, &fi);
 	if (ret)
 		return ret;
diff --git a/fabtests/include/shared.h b/fabtests/include/shared.h
index ec148a8..82b1782 100644
--- a/fabtests/include/shared.h
+++ b/fabtests/include/shared.h
@@ -65,9 +65,6 @@ static inline int ft_exit_code(int ret)
 	return absret > 255 ? EXIT_FAILURE : absret;
 }
 
-#define ft_foreach_info(fi, info) \
-	for (fi = info; fi; fi = fi->next)
-
 #define ft_sa_family(addr) (((struct sockaddr *)(addr))->sa_family)
 
 struct test_size_param {
diff --git a/fabtests/man/fabtests.7.md b/fabtests/man/fabtests.7.md
index ed16de3..6d7f00f 100644
--- a/fabtests/man/fabtests.7.md
+++ b/fabtests/man/fabtests.7.md
@@ -428,6 +428,9 @@ the list available for that test.
 *-M <mcast_addr>*
 : For multicast tests, specifies the address of the multicast group to join.
 
+*-v*
+: Add data verification check to data transfers.
+
 # USAGE EXAMPLES
 
 ## A simple example
@@ -451,11 +454,12 @@ This will run "fi_rdm_atomic" for all atomic operations with
 
 ## Run multinode tests
 
-	server and clients are invoked with the same command: 
-		fi_multinode -n <number of processes> -s <server_addr> 
+	Server and clients are invoked with the same command: 
+		fi_multinode -n <number of processes> -s <server_addr> -C <mode>
 	
-	a process on the server must be started before any of the clients can be started 
-	succesfully. 
+	A process on the server must be started before any of the clients can be started 
+	succesfully. -C lists the mode that the tests will run in. Currently the options are
+  for rma and msg. If not provided, the test will default to msg. 
 
 ## Run fi_ubertest
 
diff --git a/fabtests/man/man7/fabtests.7 b/fabtests/man/man7/fabtests.7
index 81fef18..59c296d 100644
--- a/fabtests/man/man7/fabtests.7
+++ b/fabtests/man/man7/fabtests.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fabtests" "7" "2019\-10\-25" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fabtests" "7" "2020\-03\-02" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -623,6 +623,11 @@ For multicast tests, specifies the address of the multicast group to
 join.
 .RS
 .RE
+.TP
+.B \f[I]\-v\f[]
+Add data verification check to data transfers.
+.RS
+.RE
 .SH USAGE EXAMPLES
 .SS A simple example
 .IP
@@ -657,13 +662,16 @@ This will run "fi_rdm_atomic" for all atomic operations with
 .IP
 .nf
 \f[C]
-server\ and\ clients\ are\ invoked\ with\ the\ same\ command:\ 
-\ \ \ \ fi_multinode\ \-n\ <number\ of\ processes>\ \-s\ <server_addr>\ 
+Server\ and\ clients\ are\ invoked\ with\ the\ same\ command:\ 
+\ \ \ \ fi_multinode\ \-n\ <number\ of\ processes>\ \-s\ <server_addr>\ \-C\ <mode>
 
-a\ process\ on\ the\ server\ must\ be\ started\ before\ any\ of\ the\ clients\ can\ be\ started\ 
-succesfully.\ 
+A\ process\ on\ the\ server\ must\ be\ started\ before\ any\ of\ the\ clients\ can\ be\ started\ 
+succesfully.\ \-C\ lists\ the\ mode\ that\ the\ tests\ will\ run\ in.\ Currently\ the\ options\ are
 \f[]
 .fi
+.PP
+for rma and msg.
+If not provided, the test will default to msg.
 .SS Run fi_ubertest
 .IP
 .nf
diff --git a/fabtests/multinode/include/core.h b/fabtests/multinode/include/core.h
index d1bb11c..fc9d64a 100644
--- a/fabtests/multinode/include/core.h
+++ b/fabtests/multinode/include/core.h
@@ -44,20 +44,33 @@
 
 #define PM_DEFAULT_OOB_PORT (8228)
 
+enum multi_xfer{
+	multi_msg,
+	multi_rma,
+};
+
+struct multi_xfer_method {
+	char* name;
+	int (*send)();
+	int (*recv)();
+	int (*wait)();
+};
+
 struct pm_job_info {
 	size_t		my_rank;
 	size_t		num_ranks;
 	int		sock;
 	int		*clients; //only valid for server
+	struct fi_rma_iov 	*multi_iovs;
 
 	struct sockaddr_storage oob_server_addr;
 	size_t 		server_addr_len;
 	void		*names;
 	size_t		name_len;
 	fi_addr_t	*fi_addrs;
+	enum multi_xfer transfer_method;
 };
 
-
 struct multinode_xfer_state {
 	int 			iteration;
 	size_t			recvs_posted;
@@ -82,3 +95,9 @@ extern struct pm_job_info pm_job;
 int multinode_run_tests(int argc, char **argv);
 int pm_allgather(void *my_item, void *items, int item_size);
 void pm_barrier();
+int multi_msg_send();
+int multi_msg_recv();
+int multi_msg_wait();
+int multi_rma_write();
+int multi_rma_recv();
+int multi_rma_wait();
diff --git a/fabtests/multinode/src/core.c b/fabtests/multinode/src/core.c
index 17730c5..8a1511d 100644
--- a/fabtests/multinode/src/core.c
+++ b/fabtests/multinode/src/core.c
@@ -53,20 +53,51 @@
 #include <arpa/inet.h>
 #include <assert.h>
 
+char *tx_barrier;
+char *rx_barrier;
+struct fid_mr *mr_barrier;
+struct fi_context2 *barrier_tx_ctx, *barrier_rx_ctx;
+
 struct pattern_ops *pattern;
 struct multinode_xfer_state state;
+struct multi_xfer_method method;
+struct multi_xfer_method multi_xfer_methods[] = {
+	{
+		.name = "send/recv",
+		.send = multi_msg_send,
+		.recv = multi_msg_recv,
+		.wait = multi_msg_wait,
+	},
+	{
+		.name = "rma",
+		.send = multi_rma_write,
+		.recv = multi_rma_recv,
+		.wait = multi_rma_wait,
+	}
+};
 
-static int multinode_setup_fabric(int argc, char **argv)
+static int multi_setup_fabric(int argc, char **argv)
 {
 	char my_name[FT_MAX_CTRL_MSG];
 	size_t len;
-	int ret;
+	int i, ret;
+	struct fi_rma_iov *remote = malloc(sizeof(*remote));
 
 	hints->ep_attr->type = FI_EP_RDM;
-	hints->caps = FI_MSG;
 	hints->mode = FI_CONTEXT;
 	hints->domain_attr->mr_mode = opts.mr_mode;
 
+	if (pm_job.transfer_method == multi_msg) {
+		hints->caps = FI_MSG;
+	} else if (pm_job.transfer_method == multi_rma) {
+		hints->caps = FI_MSG | FI_RMA;
+	} else {
+		printf("Not a valid cabability\n");
+		return -FI_ENODATA;
+	}
+
+	method = multi_xfer_methods[pm_job.transfer_method];
+
 	tx_seq = 0;
 	rx_seq = 0;
 	tx_cq_cntr = 0;
@@ -99,8 +130,8 @@ static int multinode_setup_fabric(int argc, char **argv)
 		goto err;
 	}
 
-	pm_job.name_len = len;
-	pm_job.names = malloc(len * pm_job.num_ranks);
+	pm_job.name_len = 256;
+	pm_job.names = malloc(pm_job.name_len * pm_job.num_ranks);
 	if (!pm_job.names) {
 		FT_ERR("error allocating memory for address exchange\n");
 		ret = -FI_ENOMEM;
@@ -123,28 +154,72 @@ static int multinode_setup_fabric(int argc, char **argv)
 		goto err;
 	}
 
-	ret = fi_av_insert(av, pm_job.names, pm_job.num_ranks,
-			   pm_job.fi_addrs, 0, NULL);
-	if (ret != pm_job.num_ranks) {
-		FT_ERR("unable to insert all addresses into AV table\n");
-		ret = -1;
+	for (i = 0; i < pm_job.num_ranks; i++) {
+		ret = fi_av_insert(av, (char*)pm_job.names + i * pm_job.name_len, 1,
+			   &pm_job.fi_addrs[i], 0, NULL);
+		if (ret != 1) {
+			FT_ERR("unable to insert all addresses into AV table\n");
+			ret = -1;
+			goto err;
+		}
+	}
+
+	pm_job.multi_iovs = malloc(sizeof(*(pm_job.multi_iovs)) * pm_job.num_ranks);
+	if (!pm_job.multi_iovs) {
+		FT_ERR("error allocation memory for rma_iovs\n");
+		goto err;
+	}
+
+	if (fi->domain_attr->mr_mode & FI_MR_VIRT_ADDR) 
+		remote->addr = (uintptr_t) rx_buf;
+	else
+		remote->addr = 0;
+
+	remote->key = fi_mr_key(mr);
+	remote->len = rx_size;
+
+	ret = pm_allgather(remote, pm_job.multi_iovs, sizeof(*remote));
+	if (ret) {
+		FT_ERR("error exchanging rma_iovs\n");
 		goto err;
 	}
+	for (i = 0; i < pm_job.num_ranks; i++) {
+		pm_job.multi_iovs[i].addr += (tx_size * pm_job.my_rank);
+	}
+
 	return 0;
 err:
 	ft_free_res();
 	return ft_exit_code(ret);
 }
 
-static int multinode_post_rx()
+static int ft_progress(struct fid_cq *cq, uint64_t total, uint64_t *cq_cntr)
+{
+	struct fi_cq_err_entry comp;
+	int ret;
+
+	ret = fi_cq_read(cq, &comp, 1);
+	if (ret > 0)
+		(*cq_cntr)++;
+
+	if (ret >= 0 || ret == -FI_EAGAIN)
+		return 0;
+
+	if (ret == -FI_EAVAIL) {
+		ret = ft_cq_readerr(cq);
+		(*cq_cntr)++;
+	} else {
+		FT_PRINTERR("fi_cq_read/sread", ret);
+	}
+	return ret;
+}
+
+int multi_msg_recv()
 {
 	int ret, offset;
 
 	/* post receives */
-	while (!state.all_recvs_posted) {
-
-		if (state.rx_window == 0)
-			break;
+	while (!state.all_recvs_posted && state.rx_window) {
 
 		ret = pattern->next_source(&state.cur_source);
 		if (ret == -FI_ENODATA) {
@@ -167,19 +242,16 @@ static int multinode_post_rx()
 		rx_ctx_arr[offset].state = OP_PENDING;
 		state.recvs_posted++;
 		state.rx_window--;
-	};
+	}
 	return 0;
 }
 
-static int multinode_post_tx()
+int multi_msg_send()
 {
 	int ret, offset;
 	fi_addr_t dest;
 
-	while (!state.all_sends_posted) {
-
-		if (state.tx_window == 0)
-			break;
+	while (!state.all_sends_posted && state.tx_window) {
 
 		ret = pattern->next_target(&state.cur_target);
 		if (ret == -FI_ENODATA) {
@@ -209,7 +281,7 @@ static int multinode_post_tx()
 	return 0;
 }
 
-static int multinode_wait_for_comp()
+int multi_msg_wait()
 {
 	int ret, i;
 
@@ -235,7 +307,109 @@ static int multinode_wait_for_comp()
 	return 0;
 }
 
-static inline void multinode_init_state()
+int multi_rma_write()
+{
+	int ret, rc;
+
+	while (!state.all_sends_posted && state.tx_window) {
+
+		ret = pattern->next_target(&state.cur_target);
+		if (ret == -FI_ENODATA) {
+			state.all_sends_posted = true;
+			break;
+		} else if (ret < 0) {
+			return ret;
+		}
+
+		snprintf((char*) tx_buf + tx_size * state.cur_target, tx_size,
+		        "Hello World! from %zu to %i on the %zuth iteration, %s test",
+		        pm_job.my_rank, state.cur_target, 
+		        (size_t) tx_seq, pattern->name);
+
+		while (1) {
+			ret = fi_write(ep, 
+				tx_buf + tx_size * state.cur_target,
+				opts.transfer_size, mr_desc, 
+				pm_job.fi_addrs[state.cur_target], 
+				pm_job.multi_iovs[state.cur_target].addr,
+				pm_job.multi_iovs[state.cur_target].key, 
+				&tx_ctx_arr[state.tx_window].context);
+			if (!ret)
+				break;
+		
+			if (ret != -FI_EAGAIN) {
+				printf("RMA write failed");
+				return ret;
+			}
+
+			rc = ft_progress(txcq, tx_seq, &tx_cq_cntr);
+			if (rc && rc != -FI_EAGAIN) {
+				printf("Failed to get rma completion");
+				return rc;
+			}
+		}
+		tx_seq++;
+	
+		state.sends_posted++;
+		state.tx_window--;
+	}
+	return 0;
+}
+
+int multi_rma_recv()
+{
+	state.all_recvs_posted = true;
+	return 0;
+}
+
+int multi_rma_wait()
+{
+	int ret;
+
+	ret = ft_get_tx_comp(tx_seq);
+	if (ret)
+		return ret;
+
+	state.rx_window = opts.window_size;
+	state.tx_window = opts.window_size;
+
+	if (state.all_recvs_posted && state.all_sends_posted)
+		state.all_completions_done = true;
+
+	return 0;
+}
+
+int send_recv_barrier(int sync)
+{
+	int ret, i;
+
+	for(i = 0; i < pm_job.num_ranks; i++) {
+
+		ret = ft_post_rx_buf(ep, opts.transfer_size,
+			     &barrier_rx_ctx[i],
+			     rx_buf, mr_desc, 0);
+		if (ret)
+			return ret;
+	}
+
+	for (i = 0; i < pm_job.num_ranks; i++) {
+		ret = ft_post_tx_buf(ep, pm_job.fi_addrs[i], 0, 
+				     NO_CQ_DATA, &barrier_tx_ctx[i],
+		                     tx_buf, mr_desc, 0);
+		if (ret)
+			return ret;
+	}
+
+	ret = ft_get_tx_comp(tx_seq);
+	if (ret)
+		return ret;
+
+	ret = ft_get_rx_comp(rx_seq);	
+
+	return ret;
+}
+
+static inline void multi_init_state()
 {
 	state.cur_source = PATTERN_NO_CURRENT;
 	state.cur_target = PATTERN_NO_CURRENT;
@@ -248,41 +422,47 @@ static inline void multinode_init_state()
 	state.tx_window = opts.window_size;
 }
 
-static int multinode_run_test()
+static int multi_run_test()
 {
 	int ret;
 	int iter;
 
 	for (iter = 0; iter < opts.iterations; iter++) {
 
-		multinode_init_state();
+		multi_init_state();
 		while (!state.all_completions_done ||
 				!state.all_recvs_posted ||
 				!state.all_sends_posted) {
-			ret = multinode_post_rx();
+			ret = method.recv();
 			if (ret)
 				return ret;
 
-			ret = multinode_post_tx();
+			ret = method.send();
 			if (ret)
 				return ret;
 
-			ret = multinode_wait_for_comp();
+			ret = method.wait();
 			if (ret)
 				return ret;
-
-			pm_barrier();
 		}
+
+		ret = send_recv_barrier(iter);
+		if (ret)
+			return ret;
 	}
 	return 0;
 }
 
 static void pm_job_free_res()
 {
-
 	free(pm_job.names);
-
 	free(pm_job.fi_addrs);
+	free(pm_job.multi_iovs);
+
+	free(barrier_tx_ctx);
+	free(barrier_rx_ctx);
+
+	FT_CLOSE_FID(mr_barrier);
 }
 
 int multinode_run_tests(int argc, char **argv)
@@ -290,21 +470,34 @@ int multinode_run_tests(int argc, char **argv)
 	int ret = FI_SUCCESS;
 	int i;
 
-	ret = multinode_setup_fabric(argc, argv);
+
+	barrier_tx_ctx = malloc(sizeof(*barrier_tx_ctx) * pm_job.num_ranks);
+	if (!barrier_tx_ctx)
+		return -FI_ENOMEM;
+
+	barrier_rx_ctx = malloc(sizeof(*barrier_rx_ctx) * pm_job.num_ranks);
+	if (!barrier_rx_ctx)
+		return -FI_ENOMEM;
+
+	ret = multi_setup_fabric(argc, argv);
 	if (ret)
 		return ret;
+	
 
 	for (i = 0; i < NUM_TESTS && !ret; i++) {
 		printf("starting %s... ", patterns[i].name);
 		pattern = &patterns[i];
-		ret = multinode_run_test();
+		ret = multi_run_test();
 		if (ret)
 			printf("failed\n");
 		else
 			printf("passed\n");
+
+		fflush(stdout);
 	}
 
 	pm_job_free_res();
 	ft_free_res();
 	return ft_exit_code(ret);
 }
+
diff --git a/fabtests/multinode/src/harness.c b/fabtests/multinode/src/harness.c
index b1b9551..29fab85 100644
--- a/fabtests/multinode/src/harness.c
+++ b/fabtests/multinode/src/harness.c
@@ -45,6 +45,18 @@
 #include <core.h>
 struct pm_job_info pm_job;
 
+static int parse_caps(char* caps)
+{
+	if (strcmp(caps, "msg") == 0) {
+		return multi_msg;
+	} else if (strcmp(caps, "rma") == 0) {
+		return multi_rma;
+	} else {
+		printf("Warn: Invalid capability, defaulting to msg\n");
+		return multi_msg;
+	}
+}
+
 static inline ssize_t socket_send(int sock, void *buf, size_t len, int flags)
 {
 	ssize_t ret;
@@ -273,7 +285,7 @@ int main(int argc, char **argv)
 	int c, ret;
 
 	opts = INIT_OPTS;
-	opts.options |= (FT_OPT_SIZE | FT_OPT_ALLOC_MULT_MR);
+	opts.options |= FT_OPT_SIZE;
 
 	pm_job.clients = NULL;
 
@@ -281,7 +293,7 @@ int main(int argc, char **argv)
 	if (!hints)
 		return EXIT_FAILURE;
 
-	while ((c = getopt(argc, argv, "n:h" CS_OPTS INFO_OPTS)) != -1) {
+	while ((c = getopt(argc, argv, "n:C:h" CS_OPTS INFO_OPTS)) != -1) {
 		switch (c) {
 		default:
 			ft_parse_addr_opts(c, optarg, &opts);
@@ -291,6 +303,9 @@ int main(int argc, char **argv)
 		case 'n':
 			pm_job.num_ranks = atoi(optarg);
 			break;
+		case 'C':
+			pm_job.transfer_method = parse_caps(optarg);
+			break;
 		case '?':
 		case 'h':
 			ft_usage(argv[0], "A simple multinode test");
diff --git a/fabtests/multinode/src/pattern.c b/fabtests/multinode/src/pattern.c
index 31ca177..3cf7059 100644
--- a/fabtests/multinode/src/pattern.c
+++ b/fabtests/multinode/src/pattern.c
@@ -102,11 +102,6 @@ static int mesh_next(int *cur)
 
 struct pattern_ops patterns[] = {
 	{
-		.name = "full_mesh",
-		.next_source = mesh_next,
-		.next_target = mesh_next,
-	},
-	{
 		.name = "ring",
 		.next_source = ring_next,
 		.next_target = ring_current,
@@ -121,6 +116,11 @@ struct pattern_ops patterns[] = {
 		.next_source = broadcast_gather_current,
 		.next_target = broadcast_gather_next,
 	},
+	{
+		.name = "full_mesh",
+		.next_source = mesh_next,
+		.next_target = mesh_next,
+	},
 };
 
 const int NUM_TESTS = ARRAY_SIZE(patterns);
diff --git a/fabtests/scripts/runfabtests.sh b/fabtests/scripts/runfabtests.sh
index 2587c94..f468c9b 100755
--- a/fabtests/scripts/runfabtests.sh
+++ b/fabtests/scripts/runfabtests.sh
@@ -201,7 +201,8 @@ complex_tests=(
 )
 
 multinode_tests=(
-	"fi_multinode"
+	"fi_multinode -C msg"
+	"fi_multinode -C rma"
 	"fi_multinode_coll"
 )
 
@@ -567,20 +568,21 @@ function complex_test {
 }
 
 function multinode_test {
-	local test=$1
+	local test="$1"
 	local s_ret=0
 	local c_ret=0
+	local c_out_arr=()
 	local num_procs=$2
 	local test_exe="${test} -n $num_procs -p \"${PROV}\"" 	
+	local c_out
 	local start_time
 	local end_time
 	local test_time
 
-
 	is_excluded "$test" && return
 
 	start_time=$(date '+%s')
-
+	
 	s_cmd="${BIN_PATH}${test_exe} ${S_ARGS} -s ${S_INTERFACE}"
 	${SERVER_CMD} "${EXPORT_ENV} $s_cmd" &> $s_outp &
 	s_pid=$!
@@ -589,36 +591,58 @@ function multinode_test {
 	c_pid_arr=()	
 	for ((i=1; i<num_procs; i++))
 	do
+		local c_out=$(mktemp fabtests.c_outp${i}.XXXXXX)
 		c_cmd="${BIN_PATH}${test_exe} ${S_ARGS} -s ${S_INTERFACE}"
-		${CLIENT_CMD} "${EXPORT_ENV} $c_cmd" &> $c_outp & 
+		${CLIENT_CMD} "${EXPORT_ENV} $c_cmd" &> $c_out & 
 		c_pid_arr+=($!)
+		c_out_arr+=($c_out)
 	done
 
 	for pid in ${c_pid_arr[*]}; do
 		wait $pid
+		c_ret=($?)||$c_ret
 	done
 	
-
 	[[ c_ret -ne 0 ]] && kill -9 $s_pid 2> /dev/null
 
 	wait $s_pid
 	s_ret=$?
+	echo "server finished"
 	
 	end_time=$(date '+%s')
 	test_time=$(compute_duration "$start_time" "$end_time")
-
+	
+	pe=1
 	if [[ $STRICT_MODE -eq 0 && $s_ret -eq $FI_ENODATA && $c_ret -eq $FI_ENODATA ]] ||
 	   [[ $STRICT_MODE -eq 0 && $s_ret -eq $FI_ENOSYS && $c_ret -eq $FI_ENOSYS ]]; then
-		print_results "$test_exe" "Notrun" "$test_time" "$s_outp" "$s_cmd" "$c_outp" "$c_cmd"
+		print_results "$test_exe" "Notrun" "$test_time" "$s_outp" "$s_cmd" "" "$c_cmd"
+		for c_out in "${c_out_arr[@]}" 
+		do
+			printf -- "  client_stdout $pe: |\n"
+			sed -e 's/^/    /' < $c_out
+			pe=$((pe+1))
+		done
 		skip_count+=1
 	elif [ $s_ret -ne 0 -o $c_ret -ne 0 ]; then
-		print_results "$test_exe" "Fail" "$test_time" "$s_outp" "$s_cmd" "$c_outp" "$c_cmd"
+		print_results "$test_exe" "Fail" "$test_time" "$s_outp" "$s_cmd" "" "$c_cmd"
+		for c_out in "${c_out_arr[@]}" 
+		do
+			printf -- "  client_stdout $pe: |\n"
+			sed -e 's/^/    /' < $c_out
+			pe=$((pe+1))
+		done
 		if [ $s_ret -eq 124 -o $c_ret -eq 124 ]; then
 			cleanup
 		fi
 		fail_count+=1
 	else
-		print_results "$test_exe" "Pass" "$test_time" "$s_outp" "$s_cmd" "$c_outp" "$c_cmd"
+		print_results "$test_exe" "Pass" "$test_time" "$s_outp" "$s_cmd" "" "$c_cmd"
+		for c_out in "${c_out_arr[@]}" 
+		do
+			printf -- "  client_stdout $pe: |\n"
+			sed -e 's/^/    /' < $c_out
+			pe=$((pe+1))
+		done
 		pass_count+=1
 	fi
 }
@@ -644,6 +668,7 @@ function main {
 
 	set_core_util
 	set_excludes
+	
 
 	if [[ $1 == "quick" ]]; then
 		local -r tests="unit functional short"
@@ -651,7 +676,7 @@ function main {
 		local -r tests="complex"
 		complex_type=$1
 	else
-		local -r tests=$(echo $1 | sed 's/all/unit,functional,standard,complex/g' | tr ',' ' ')
+		local -r tests=$(echo $1 | sed 's/all/unit,functional,standard,complex,multinode/g' | tr ',' ' ')
 		if [[ $1 == "all" || $1 == "complex" ]]; then
 			complex_type="all"
 		fi
@@ -693,12 +718,11 @@ function main {
 		complex)
 			for test in "${complex_tests[@]}"; do
 				complex_test $test $complex_type
-
 			done
 		;;
 		multinode)
 			for test in "${multinode_tests[@]}"; do
-					multinode_test $test 3
+					multinode_test "$test" 3
 			done
 		;;
 		*)
diff --git a/fabtests/test_configs/sockets/sockets.exclude b/fabtests/test_configs/sockets/sockets.exclude
index ea80e22..b25e942 100644
--- a/fabtests/test_configs/sockets/sockets.exclude
+++ b/fabtests/test_configs/sockets/sockets.exclude
@@ -2,4 +2,3 @@
 
 -e dgram
 dgram
-multinode
diff --git a/fabtests/ubertest/uber.c b/fabtests/ubertest/uber.c
index 3c8476f..eb6ef3b 100644
--- a/fabtests/ubertest/uber.c
+++ b/fabtests/ubertest/uber.c
@@ -291,6 +291,7 @@ static void ft_fw_update_info(struct ft_info *test_info, struct fi_info *info)
 	if (info->domain_attr) {
 		test_info->progress = info->domain_attr->data_progress;
 		test_info->threading = info->domain_attr->threading;
+		test_info->mr_mode = info->domain_attr->mr_mode;
 	}
 
 	test_info->mode = info->mode;
diff --git a/fabtests/unit/getinfo_test.c b/fabtests/unit/getinfo_test.c
index bb4fa51..869470e 100644
--- a/fabtests/unit/getinfo_test.c
+++ b/fabtests/unit/getinfo_test.c
@@ -47,6 +47,8 @@
 typedef int (*ft_getinfo_init)(struct fi_info *);
 typedef int (*ft_getinfo_test)(char *, char *, uint64_t, struct fi_info *, struct fi_info **);
 typedef int (*ft_getinfo_check)(struct fi_info *);
+typedef int (*ft_getinfo_init_val)(struct fi_info *, uint64_t);
+typedef int (*ft_getinfo_check_val)(struct fi_info *, uint64_t);
 
 static char err_buf[512];
 static char new_prov_var[128];
@@ -110,91 +112,199 @@ static int invalid_dom(struct fi_info *hints)
 	return 0;
 }
 
-static int validate_msg_ordering_bits(char *node, char *service, uint64_t flags,
-		struct fi_info *hints, struct fi_info **info)
+static int validate_bit_combos(char *node, char *service, uint64_t flags,
+		struct fi_info *hints, struct fi_info **info, uint64_t bits,
+		ft_getinfo_init_val init, ft_getinfo_check_val check)
 {
 	int i, ret;
-	uint64_t ordering_bits = (FI_ORDER_STRICT | FI_ORDER_DATA);
-	uint64_t *msg_order_combinations;
-	int cnt;
+	uint64_t *combinations;
+	int cnt, fail, skipped;
 
-	ret = ft_alloc_bit_combo(0, ordering_bits, &msg_order_combinations, &cnt);
+	ret = ft_alloc_bit_combo(0, bits, &combinations, &cnt);
 	if (ret) {
 		FT_UNIT_STRERR(err_buf, "ft_alloc_bit_combo failed", ret);
 		return ret;
 	}
 
-	/* test for what ordering support exists on this provider */
-	/* test ordering support in TX ATTRIBUTE */
-	for (i = 0; i < cnt; i++) {
-		hints->tx_attr->msg_order = msg_order_combinations[i];
+	for (i = 0, fail = skipped = 0; i < cnt; i++) {
+		init(hints, combinations[i]);
 		ret = fi_getinfo(FT_FIVERSION, node, service, flags, hints, info);
 		if (ret) {
-			if (ret == -FI_ENODATA)
+			if (ret == -FI_ENODATA) {
+				skipped++;
 				continue;
+			}
 			FT_UNIT_STRERR(err_buf, "fi_getinfo failed", ret);
-			goto failed_getinfo;
+			goto out;
 		}
 
-		ft_foreach_info(fi, *info) {
-			FT_DEBUG("\nTesting for fabric: %s, domain: %s, endpoint type: %d",
-					fi->fabric_attr->name, fi->domain_attr->name,
-					fi->ep_attr->type);
-			if (hints->tx_attr->msg_order) {
-				if ((fi->tx_attr->msg_order & hints->tx_attr->msg_order) !=
-				    hints->tx_attr->msg_order) {
-					FT_DEBUG("tx msg_order not matching - hints: %"
-						 PRIx64 " prov: %" PRIx64 "\n",
-						 hints->tx_attr->msg_order,
-						 fi->tx_attr->msg_order);
-					ret = -FI_EOTHER;
-					fi_freeinfo(*info);
-					goto failed_getinfo;
-				}
+		for (fi = *info; fi; fi = fi->next) {
+			if (check && check(fi, combinations[i])) {
+				FT_DEBUG("%s:failed check for caps [%s]\n",
+					 fi->fabric_attr->prov_name,
+					 fi_tostr(&combinations[i], FI_TYPE_CAPS));
+				ret = -FI_EIO;
 			}
 		}
-		fi_freeinfo(*info);
+		if (ret)
+			fail++;
 	}
+	ret = 0;
+	printf("(passed)(skipped) (%d)(%d)/%d combinations\n",
+		cnt - (fail + skipped), skipped, cnt);
+out:
+	fi = NULL;
+	ft_free_bit_combo(combinations);
+	return fail ? -FI_EIO : ret;
+}
 
-	/* test ordering support in RX ATTRIBUTE */
-	for (i = 0; i < cnt; i++) {
-		hints->tx_attr->msg_order = 0;
-		hints->rx_attr->msg_order = msg_order_combinations[i];
-		ret = fi_getinfo(FT_FIVERSION, node, service, flags, hints, info);
-		if (ret) {
-			if (ret == -FI_ENODATA)
-				continue;
-			FT_UNIT_STRERR(err_buf, "fi_getinfo failed", ret);
-			goto failed_getinfo;
-		}
-		ft_foreach_info(fi, *info) {
-			FT_DEBUG("\nTesting for fabric: %s, domain: %s, endpoint type: %d",
-					fi->fabric_attr->name, fi->domain_attr->name,
-					fi->ep_attr->type);
-			if (hints->rx_attr->msg_order) {
-				if ((fi->rx_attr->msg_order & hints->rx_attr->msg_order) !=
-				    hints->rx_attr->msg_order) {
-					FT_DEBUG("rx msg_order not matching - hints: %"
-						 PRIx64 " prov: %" PRIx64 "\n",
-						 hints->rx_attr->msg_order,
-						 fi->rx_attr->msg_order);
-					ret = -FI_EOTHER;
-					fi_freeinfo(*info);
-					goto failed_getinfo;
-				}
-			}
-		}
-		fi_freeinfo(*info);
+#define check_has_bits(val, bits)	(((val) & (bits)) != (bits))
+#define check_only_has_bits(val, bits)	((val) & ~(bits))
+
+static int init_tx_order(struct fi_info *hints, uint64_t order)
+{
+	hints->tx_attr->msg_order = order;
+	return 0;
+}
+
+static int check_tx_order(struct fi_info *info, uint64_t order)
+{
+	return check_has_bits(info->tx_attr->msg_order, order);
+}
+
+static int validate_tx_ordering_bits(char *node, char *service, uint64_t flags,
+		struct fi_info *hints, struct fi_info **info)
+{
+	return validate_bit_combos(node, service, flags, hints, info,
+				   FI_ORDER_STRICT | FI_ORDER_DATA,
+				   init_tx_order, check_tx_order);
+}
+
+static int init_rx_order(struct fi_info *hints, uint64_t order)
+{
+	hints->rx_attr->msg_order = order;
+	return 0;
+}
+
+static int check_rx_order(struct fi_info *info, uint64_t order)
+{
+	return check_has_bits(info->rx_attr->msg_order, order);
+}
+
+static int validate_rx_ordering_bits(char *node, char *service, uint64_t flags,
+		struct fi_info *hints, struct fi_info **info)
+{
+	return validate_bit_combos(node, service, flags, hints, info,
+				   FI_ORDER_STRICT | FI_ORDER_DATA,
+				   init_rx_order, check_rx_order);
+}
+
+static int init_caps(struct fi_info *hints, uint64_t bits)
+{
+	hints->caps = bits;
+	return 0;
+}
+
+#define PRIMARY_TX_CAPS	(FI_MSG | FI_RMA | FI_TAGGED | FI_ATOMIC | \
+			 FI_MULTICAST | FI_NAMED_RX_CTX | FI_HMEM)
+#define PRIMARY_RX_CAPS (FI_MSG | FI_RMA | FI_TAGGED | FI_ATOMIC | \
+			 FI_DIRECTED_RECV | FI_VARIABLE_MSG | \
+			 FI_HMEM)
+
+#define PRIMARY_CAPS (PRIMARY_TX_CAPS | PRIMARY_RX_CAPS)
+#define DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM | FI_SHARED_AV)
+#define SEC_TX_CAPS (FI_TRIGGER | FI_FENCE | FI_RMA_PMEM)
+#define SEC_RX_CAPS (FI_RMA_PMEM | FI_SOURCE | FI_SOURCE_ERR | \
+		     FI_RMA_EVENT | FI_MULTI_RECV | FI_TRIGGER)
+#define MOD_TX_CAPS (FI_SEND | FI_READ | FI_WRITE)
+#define MOD_RX_CAPS (FI_RECV | FI_REMOTE_READ | FI_REMOTE_WRITE)
+#define OPT_TX_CAPS (MOD_TX_CAPS | SEC_TX_CAPS)
+#define OPT_RX_CAPS (MOD_RX_CAPS | SEC_RX_CAPS)
+#define OPT_CAPS (DOMAIN_CAPS | OPT_TX_CAPS | OPT_RX_CAPS)
+
+static void print_incorrect_caps(char *prov, char *attr,
+				 uint64_t expected, uint64_t actual)
+{
+	FT_DEBUG("%s: %s->caps has unexpected caps -\n", prov, attr);
+	FT_DEBUG("expected\t[%s]\n", fi_tostr(&expected, FI_TYPE_CAPS));
+	FT_DEBUG("actual\t[%s]\n", fi_tostr(&actual, FI_TYPE_CAPS));
+}
+
+static int check_no_extra_caps(struct fi_info *info, uint64_t caps)
+{
+	if (caps & check_only_has_bits(info->caps, caps | OPT_CAPS)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "info",
+				caps & PRIMARY_CAPS, info->caps & ~OPT_CAPS);
+		return 1;
+	}
+	if (check_only_has_bits(info->tx_attr->caps,
+				PRIMARY_TX_CAPS | OPT_TX_CAPS)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "tx_attr",
+				     caps & PRIMARY_TX_CAPS,
+				     info->tx_attr->caps & ~OPT_TX_CAPS);
+		return 1;
+	}
+	if (check_only_has_bits(info->tx_attr->caps, info->caps)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "tx_attr",
+				     info->caps & (PRIMARY_TX_CAPS | OPT_TX_CAPS),
+				     info->tx_attr->caps);
+	}
+	if (check_only_has_bits(info->rx_attr->caps,
+				PRIMARY_RX_CAPS | OPT_RX_CAPS)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "rx_attr",
+				     caps & PRIMARY_RX_CAPS,
+				     info->rx_attr->caps & ~OPT_RX_CAPS);
+		return 1;
+	}
+	if (check_only_has_bits(info->rx_attr->caps, info->caps)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "rx_attr",
+				     info->caps & (PRIMARY_RX_CAPS | OPT_RX_CAPS),
+				     info->rx_attr->caps);
+		return 1;
+	}
+	return 0;
+}
+
+static int check_caps(struct fi_info *info, uint64_t caps)
+{
+	int ret;
+
+	ret = check_no_extra_caps(info, caps);
+	if (!caps)
+		return ret;
+
+	if (check_has_bits(info->caps, caps)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "info",
+				caps & PRIMARY_CAPS, info->caps & ~OPT_CAPS);
+		return 1;
+	}
+	if (check_has_bits(info->tx_attr->caps, caps & PRIMARY_TX_CAPS)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "tx_attr",
+				     caps & PRIMARY_TX_CAPS,
+				     info->tx_attr->caps & ~OPT_TX_CAPS);
+		return 1;
+	}
+	if (check_has_bits(info->rx_attr->caps, caps & PRIMARY_RX_CAPS)) {
+		print_incorrect_caps(info->fabric_attr->prov_name, "rx_attr",
+				     caps & PRIMARY_RX_CAPS,
+				     info->rx_attr->caps & ~OPT_RX_CAPS);
+		return 1;
 	}
 
-	*info = NULL;
-	ft_free_bit_combo(msg_order_combinations);
 	return 0;
+}
 
-failed_getinfo:
-	*info = NULL;
-	ft_free_bit_combo(msg_order_combinations);
-	return ret;
+static int validate_primary_caps(char *node, char *service, uint64_t flags,
+		struct fi_info *hints, struct fi_info **info)
+{
+	return validate_bit_combos(node, service, flags, hints, info,
+				   PRIMARY_TX_CAPS | PRIMARY_RX_CAPS,
+				   init_caps, check_caps);
+}
+
+static int test_null_hints_caps(struct fi_info *info)
+{
+	return check_no_extra_caps(info, 0);
 }
 
 static int init_valid_rma_RAW_ordering_no_set_size(struct fi_info *hints)
@@ -218,7 +328,8 @@ static int init_valid_rma_RAW_ordering_set_size(struct fi_info *hints)
 
 	ret = fi_getinfo(FT_FIVERSION, NULL, NULL, 0, hints, &fi);
 	if (ret) {
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		return ret;
 	}
 	if (fi->ep_attr->max_order_raw_size > 0)
@@ -250,7 +361,8 @@ static int init_valid_rma_WAR_ordering_set_size(struct fi_info *hints)
 
 	ret = fi_getinfo(FT_FIVERSION, NULL, NULL, 0, hints, &fi);
 	if (ret) {
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		return ret;
 	}
 	if (fi->ep_attr->max_order_war_size > 0)
@@ -281,7 +393,8 @@ static int init_valid_rma_WAW_ordering_set_size(struct fi_info *hints)
 	hints->rx_attr->msg_order = FI_ORDER_WAW;
 	ret = fi_getinfo(FT_FIVERSION, NULL, NULL, 0, hints, &fi);
 	if (ret) {
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		return ret;
 	}
 	if (fi->ep_attr->max_order_waw_size > 0)
@@ -338,7 +451,8 @@ static int init_invalid_rma_RAW_ordering_size(struct fi_info *hints)
 
 	ret = fi_getinfo(FT_FIVERSION, NULL, NULL, 0, hints, &fi);
 	if (ret) {
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		return ret;
 	}
 
@@ -363,7 +477,8 @@ static int init_invalid_rma_WAR_ordering_size(struct fi_info *hints)
 
 	ret = fi_getinfo(FT_FIVERSION, NULL, NULL, 0, hints, &fi);
 	if (ret) {
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		return ret;
 	}
 
@@ -388,7 +503,8 @@ static int init_invalid_rma_WAW_ordering_size(struct fi_info *hints)
 
 	ret = fi_getinfo(FT_FIVERSION, NULL, NULL, 0, hints, &fi);
 	if (ret) {
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		return ret;
 	}
 
@@ -450,46 +566,28 @@ static int check_mr_unspec(struct fi_info *info)
 		EXIT_FAILURE : 0;
 }
 
-static int test_mr_modes(char *node, char *service, uint64_t flags,
-			 struct fi_info *hints, struct fi_info **info)
+static int init_mr_mode(struct fi_info *hints, uint64_t mode)
 {
-	struct fi_info *fi;
-	uint64_t *mr_modes;
-	int i, cnt, ret;
-
-	ret = ft_alloc_bit_combo(0, FI_MR_LOCAL | FI_MR_RAW | FI_MR_VIRT_ADDR |
-			FI_MR_ALLOCATED | FI_MR_PROV_KEY | FI_MR_MMU_NOTIFY |
-			FI_MR_RMA_EVENT | FI_MR_ENDPOINT, &mr_modes, &cnt);
-	if (ret)
-		return ret;
+	hints->domain_attr->mr_mode = (uint32_t) mode;
+	return 0;
+}
 
-	for (i = 0; i < cnt; i++) {
-		hints->domain_attr->mr_mode = (uint32_t) mr_modes[i];
-		ret = fi_getinfo(FT_FIVERSION, node, service, flags, hints, info);
-		if (ret) {
-			if (ret == -FI_ENODATA)
-				continue;
-			FT_UNIT_STRERR(err_buf, "fi_getinfo failed", ret);
-			goto out;
-		}
+static int check_mr_mode(struct fi_info *info, uint64_t mode)
+{
+	return check_only_has_bits(info->domain_attr->mr_mode, mode);
+}
 
-		ft_foreach_info(fi, *info) {
-			if (fi->domain_attr->mr_mode & ~hints->domain_attr->mr_mode) {
-				ret = -FI_EOTHER;
-				fi_freeinfo(*info);
-				goto out;
-			}
-		}
-		fi_freeinfo(*info);
-	}
+static int validate_mr_modes(char *node, char *service, uint64_t flags,
+		struct fi_info *hints, struct fi_info **info)
+{
+	uint64_t mode_bits = FI_MR_LOCAL | FI_MR_RAW | FI_MR_VIRT_ADDR |
+			FI_MR_ALLOCATED | FI_MR_PROV_KEY | FI_MR_MMU_NOTIFY |
+			FI_MR_RMA_EVENT | FI_MR_ENDPOINT;
 
-out:
-	*info = NULL;
-	ft_free_bit_combo(mr_modes);
-	return ret;
+	return validate_bit_combos(node, service, flags, hints, info, mode_bits,
+				   init_mr_mode, check_mr_mode);
 }
 
-
 /*
  * Progress checks
  */
@@ -542,23 +640,24 @@ static int check_ctrl_auto(struct fi_info *info)
 }
 
 
-/*
- * Local and remote comm checks
- */
-static int init_comm_both(struct fi_info *hints)
+static int init_domain_caps(struct fi_info *hints, uint64_t caps)
 {
-	hints->caps |= FI_LOCAL_COMM | FI_REMOTE_COMM;
+	hints->domain_attr->caps = caps;
 	return 0;
 }
 
-static int check_comm_both(struct fi_info *info)
+static int check_domain_caps(struct fi_info *info, uint64_t caps)
 {
-	return (info->caps & FI_LOCAL_COMM) && (info->caps & FI_REMOTE_COMM) &&
-	       (info->domain_attr->caps & FI_LOCAL_COMM) &&
-	       (info->domain_attr->caps & FI_REMOTE_COMM) ?
-		0 : EXIT_FAILURE;
+	return check_has_bits(info->domain_attr->caps, caps);
 }
 
+static int validate_domain_caps(char *node, char *service, uint64_t flags,
+		struct fi_info *hints, struct fi_info **info)
+{
+	return validate_bit_combos(node, service, flags, hints, info,
+				   FI_LOCAL_COMM | FI_REMOTE_COMM | FI_SHARED_AV,
+				   init_domain_caps, check_domain_caps);
+}
 
 /*
  * getinfo test
@@ -591,18 +690,19 @@ static int getinfo_unit_test(char *node, char *service, uint64_t flags,
 			ret = 0;
 			goto out;
 		}
-		sprintf(err_buf, "fi_getinfo failed %s(%d)", fi_strerror(-ret), -ret);
+		sprintf(err_buf, "fi_getinfo returned %d - %s",
+			-ret, fi_strerror(-ret));
 		goto out;
 	}
 
 	if (!info || !check)
 		goto out;
 
-	ft_foreach_info(fi, info) {
+	for (fi = info; fi; fi = fi->next) {
 		FT_DEBUG("\nTesting for fabric: %s, domain: %s, endpoint type: %d",
-				fi->fabric_attr->name, fi->domain_attr->name,
+				fi->fabric_attr->prov_name, fi->domain_attr->name,
 				fi->ep_attr->type);
-		ret = check(info);
+		ret = check(fi);
 		if (ret)
 			break;
 	}
@@ -687,8 +787,11 @@ getinfo_test(util, 1, "Test if we get utility provider when requested",
 		NULL, NULL, 0, hints, NULL, NULL, check_util_prov, 0)
 
 /* Message Ordering Tests */
-getinfo_test(msg_ordering, 1, "Test msg ordering bits supported are set",
-		NULL, NULL, 0, hints, NULL, validate_msg_ordering_bits, NULL, 0)
+getinfo_test(msg_ordering, 1, "Test tx ordering bits supported are set",
+		NULL, NULL, 0, hints, NULL, validate_tx_ordering_bits, NULL, 0)
+getinfo_test(msg_ordering, 2, "Test rx ordering bits supported are set",
+		NULL, NULL, 0, hints, NULL, validate_rx_ordering_bits, NULL, 0)
+
 getinfo_test(raw_ordering, 1, "Test rma RAW ordering size is set",
 		NULL, NULL, 0, hints, init_valid_rma_RAW_ordering_no_set_size,
 		NULL, check_valid_rma_ordering_sizes, 0)
@@ -729,7 +832,7 @@ getinfo_test(mr_mode, 4, "Test FI_MR_BASIC (v1.0)", NULL, NULL, 0,
 getinfo_test(mr_mode, 5, "Test FI_MR_SCALABLE (v1.0)", NULL, NULL, 0,
      	     hints, init_mr_scalable, test_mr_v1_0, check_mr_scalable, -FI_ENODATA)
 getinfo_test(mr_mode, 6, "Test mr_mode bits", NULL, NULL, 0,
-	     hints, NULL, test_mr_modes, NULL, 0)
+	     hints, NULL, validate_mr_modes, NULL, 0)
 
 /* Progress tests */
 getinfo_test(progress, 1, "Test data manual progress", NULL, NULL, 0,
@@ -741,10 +844,13 @@ getinfo_test(progress, 3, "Test ctrl manual progress", NULL, NULL, 0,
 getinfo_test(progress, 4, "Test ctrl auto progress", NULL, NULL, 0,
 	     hints, init_ctrl_auto, NULL, check_ctrl_auto, 0)
 
-
-/* Cap local and remote comm tests */
-getinfo_test(comm, 1, "Test local and remote comm support", NULL, NULL, 0,
-	     hints, init_comm_both, NULL, check_comm_both, 0)
+/* Capability test */
+getinfo_test(caps, 1, "Test capability bits supported are set",
+		NULL, NULL, 0, hints, NULL, validate_primary_caps, NULL, 0)
+getinfo_test(caps, 2, "Test capability with no hints",
+		NULL, NULL, 0, NULL, NULL, NULL, test_null_hints_caps, 0)
+getinfo_test(caps, 3, "Test domain capabilities", NULL, NULL, 0,
+	     hints, NULL, validate_domain_caps, NULL, 0)
 
 
 static void usage(void)
@@ -803,6 +909,7 @@ int main(int argc, char **argv)
 		TEST_ENTRY_GETINFO(src_dest1),
 		TEST_ENTRY_GETINFO(src_dest2),
 		TEST_ENTRY_GETINFO(msg_ordering1),
+		TEST_ENTRY_GETINFO(msg_ordering2),
 		TEST_ENTRY_GETINFO(raw_ordering1),
 		TEST_ENTRY_GETINFO(raw_ordering2),
 		TEST_ENTRY_GETINFO(war_ordering1),
@@ -823,7 +930,9 @@ int main(int argc, char **argv)
 		TEST_ENTRY_GETINFO(progress2),
 		TEST_ENTRY_GETINFO(progress3),
 		TEST_ENTRY_GETINFO(progress4),
-		TEST_ENTRY_GETINFO(comm1),
+		TEST_ENTRY_GETINFO(caps1),
+		TEST_ENTRY_GETINFO(caps2),
+		TEST_ENTRY_GETINFO(caps3),
 		{ NULL, "" }
 	};
 
@@ -861,6 +970,7 @@ int main(int argc, char **argv)
 		opts.src_port = "9228";
 
 	hints->mode = ~0;
+	hints->domain_attr->mr_mode = opts.mr_mode;
 
 	if (hints->fabric_attr->prov_name) {
 		if (set_prov(hints->fabric_attr->prov_name))
diff --git a/include/ofi_epoll.h b/include/ofi_epoll.h
index def823e..cd46405 100644
--- a/include/ofi_epoll.h
+++ b/include/ofi_epoll.h
@@ -46,18 +46,18 @@
 #ifdef HAVE_EPOLL
 #include <sys/epoll.h>
 
-#define FI_EPOLL_IN  EPOLLIN
-#define FI_EPOLL_OUT EPOLLOUT
+#define OFI_EPOLL_IN  EPOLLIN
+#define OFI_EPOLL_OUT EPOLLOUT
 
-typedef int fi_epoll_t;
+typedef int ofi_epoll_t;
 
-static inline int fi_epoll_create(int *ep)
+static inline int ofi_epoll_create(int *ep)
 {
 	*ep = epoll_create(4);
 	return *ep < 0 ? -ofi_syserr() : 0;
 }
 
-static inline int fi_epoll_add(int ep, int fd, uint32_t events, void *context)
+static inline int ofi_epoll_add(int ep, int fd, uint32_t events, void *context)
 {
 	struct epoll_event event;
 	int ret;
@@ -70,7 +70,7 @@ static inline int fi_epoll_add(int ep, int fd, uint32_t events, void *context)
 	return 0;
 }
 
-static inline int fi_epoll_mod(int ep, int fd, uint32_t events, void *context)
+static inline int ofi_epoll_mod(int ep, int fd, uint32_t events, void *context)
 {
 	struct epoll_event event;
 
@@ -79,12 +79,12 @@ static inline int fi_epoll_mod(int ep, int fd, uint32_t events, void *context)
 	return epoll_ctl(ep, EPOLL_CTL_MOD, fd, &event) ? -ofi_syserr() : 0;
 }
 
-static inline int fi_epoll_del(int ep, int fd)
+static inline int ofi_epoll_del(int ep, int fd)
 {
 	return epoll_ctl(ep, EPOLL_CTL_DEL, fd, NULL) ? -ofi_syserr() : 0;
 }
 
-static inline int fi_epoll_wait(int ep, void **contexts, int max_contexts,
+static inline int ofi_epoll_wait(int ep, void **contexts, int max_contexts,
                                 int timeout)
 {
 	struct epoll_event events[max_contexts];
@@ -100,7 +100,7 @@ static inline int fi_epoll_wait(int ep, void **contexts, int max_contexts,
 	return ret;
 }
 
-static inline void fi_epoll_close(int ep)
+static inline void ofi_epoll_close(int ep)
 {
 	close(ep);
 }
@@ -108,20 +108,20 @@ static inline void fi_epoll_close(int ep)
 #else
 #include <poll.h>
 
-#define FI_EPOLL_IN  POLLIN
-#define FI_EPOLL_OUT POLLOUT
+#define OFI_EPOLL_IN  POLLIN
+#define OFI_EPOLL_OUT POLLOUT
 
-enum fi_epoll_ctl {
+enum ofi_epoll_ctl {
 	EPOLL_CTL_ADD,
 	EPOLL_CTL_DEL,
 	EPOLL_CTL_MOD,
 };
 
-struct fi_epoll_work_item {
+struct ofi_epoll_work_item {
 	int		fd;
 	uint32_t	events;
 	void		*context;
-	enum fi_epoll_ctl type;
+	enum ofi_epoll_ctl type;
 	struct slist_entry entry;
 };
 
@@ -134,15 +134,15 @@ typedef struct fi_epoll {
 	struct fd_signal signal;
 	struct slist	work_item_list;
 	fastlock_t	lock;
-} *fi_epoll_t;
+} *ofi_epoll_t;
 
-int fi_epoll_create(struct fi_epoll **ep);
-int fi_epoll_add(struct fi_epoll *ep, int fd, uint32_t events, void *context);
-int fi_epoll_mod(struct fi_epoll *ep, int fd, uint32_t events, void *context);
-int fi_epoll_del(struct fi_epoll *ep, int fd);
-int fi_epoll_wait(struct fi_epoll *ep, void **contexts, int max_contexts,
+int ofi_epoll_create(struct fi_epoll **ep);
+int ofi_epoll_add(struct fi_epoll *ep, int fd, uint32_t events, void *context);
+int ofi_epoll_mod(struct fi_epoll *ep, int fd, uint32_t events, void *context);
+int ofi_epoll_del(struct fi_epoll *ep, int fd);
+int ofi_epoll_wait(struct fi_epoll *ep, void **contexts, int max_contexts,
                   int timeout);
-void fi_epoll_close(struct fi_epoll *ep);
+void ofi_epoll_close(struct fi_epoll *ep);
 
 #endif /* HAVE_EPOLL */
 
diff --git a/include/ofi_net.h b/include/ofi_net.h
index 59ff11d..7a924df 100644
--- a/include/ofi_net.h
+++ b/include/ofi_net.h
@@ -125,6 +125,8 @@ int ofi_discard_socket(SOCKET sock, size_t len);
 #define AF_IB 27
 #endif
 
+#define OFI_ADDRSTRLEN (INET6_ADDRSTRLEN + 50)
+
 union ofi_sock_ip {
 	struct sockaddr		sa;
 	struct sockaddr_in	sin;
@@ -133,16 +135,21 @@ union ofi_sock_ip {
 };
 
 struct ofi_addr_list_entry {
-	char ipstr[INET6_ADDRSTRLEN];
-	union ofi_sock_ip ipaddr;
-	size_t speed;
-	struct slist_entry entry;
+	struct slist_entry	entry;
+	char			ipstr[INET6_ADDRSTRLEN];
+	union ofi_sock_ip	ipaddr;
+	size_t			speed;
+	char			net_name[OFI_ADDRSTRLEN];
+	char			ifa_name[OFI_ADDRSTRLEN];
 };
 
 int ofi_addr_cmp(const struct fi_provider *prov, const struct sockaddr *sa1,
 		const struct sockaddr *sa2);
 int ofi_getifaddrs(struct ifaddrs **ifap);
-void ofi_get_list_of_addr(struct fi_provider *prov, const char *env_name,
+
+void ofi_set_netmask_str(char *netstr, size_t len, struct ifaddrs *ifa);
+
+void ofi_get_list_of_addr(const struct fi_provider *prov, const char *env_name,
 			  struct slist *addr_list);
 void ofi_free_list_of_addr(struct slist *addr_list);
 
@@ -153,7 +160,6 @@ void ofi_free_list_of_addr(struct slist *addr_list);
 #define ofi_sin6_addr(addr) (((struct sockaddr_in6 *)(addr))->sin6_addr)
 #define ofi_sin6_port(addr) (((struct sockaddr_in6 *)(addr))->sin6_port)
 
-#define OFI_ADDRSTRLEN (INET6_ADDRSTRLEN + 50)
 
 static inline size_t ofi_sizeofaddr(const struct sockaddr *addr)
 {
diff --git a/include/ofi_util.h b/include/ofi_util.h
index fde7447..02ddcc4 100644
--- a/include/ofi_util.h
+++ b/include/ofi_util.h
@@ -415,7 +415,7 @@ int fi_wait_cleanup(struct util_wait *wait);
 struct util_wait_fd {
 	struct util_wait	util_wait;
 	struct fd_signal	signal;
-	fi_epoll_t		epoll_fd;
+	ofi_epoll_t		epoll_fd;
 	struct dlist_entry	fd_list;
 	fastlock_t		lock;
 };
@@ -829,7 +829,8 @@ const char *ofi_eq_strerror(struct fid_eq *eq_fid, int prov_errno,
 #define FI_PRIMARY_CAPS	(FI_MSG | FI_RMA | FI_TAGGED | FI_ATOMICS | FI_MULTICAST | \
 			 FI_NAMED_RX_CTX | FI_DIRECTED_RECV | \
 			 FI_READ | FI_WRITE | FI_RECV | FI_SEND | \
-			 FI_REMOTE_READ | FI_REMOTE_WRITE | FI_COLLECTIVE)
+			 FI_REMOTE_READ | FI_REMOTE_WRITE | FI_COLLECTIVE | \
+			 FI_HMEM)
 
 #define FI_SECONDARY_CAPS (FI_MULTI_RECV | FI_SOURCE | FI_RMA_EVENT | \
 			   FI_SHARED_AV | FI_TRIGGER | FI_FENCE | \
@@ -891,6 +892,9 @@ struct fi_info *ofi_allocinfo_internal(void);
 int util_getinfo(const struct util_prov *util_prov, uint32_t version,
 		 const char *node, const char *service, uint64_t flags,
 		 const struct fi_info *hints, struct fi_info **info);
+int ofi_ip_getinfo(const struct util_prov *prov, uint32_t version,
+		   const char *node, const char *service, uint64_t flags,
+		   const struct fi_info *hints, struct fi_info **info);
 
 
 struct fid_list_entry {
diff --git a/include/rdma/providers/fi_log.h b/include/rdma/providers/fi_log.h
index a326df7..a42d725 100644
--- a/include/rdma/providers/fi_log.h
+++ b/include/rdma/providers/fi_log.h
@@ -100,6 +100,15 @@ void fi_log(const struct fi_provider *prov, enum fi_log_level level,
 	do {} while (0)
 #endif
 
+#define FI_WARN_ONCE(prov, subsystem, ...) ({				\
+	static int warned;						\
+	if (!warned && fi_log_enabled(prov, FI_LOG_WARN, subsystem)) {	\
+		fi_log(prov, FI_LOG_WARN, subsystem,			\
+			__func__, __LINE__, __VA_ARGS__);		\
+		warned = 1;						\
+	}								\
+})
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/libfabric.vcxproj b/libfabric.vcxproj
index c0380af..495702e 100644
--- a/libfabric.vcxproj
+++ b/libfabric.vcxproj
@@ -338,6 +338,14 @@
     <ClCompile Include="prov\netdir\src\netdir_init.c" />
     <ClCompile Include="prov\netdir\src\netdir_ndinit.c" />
     <ClCompile Include="prov\netdir\src\netdir_pep.c" />
+    <ClCompile Include="prov\sockets\src\sock_attr.c">
+      <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Debug-v140|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
+      <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Debug-v141|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
+      <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Debug-ICC|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
+      <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Release-v140|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
+      <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Release-v141|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
+      <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Release-ICC|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
+    </ClCompile>
     <ClCompile Include="prov\sockets\src\sock_atomic.c">
       <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Debug-v140|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
       <AdditionalIncludeDirectories Condition="'$(Configuration)|$(Platform)'=='Debug-v141|x64'">$(ProjectDir)prov\sockets\include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
diff --git a/man/fi_domain.3.md b/man/fi_domain.3.md
index 2f134de..3703aa8 100644
--- a/man/fi_domain.3.md
+++ b/man/fi_domain.3.md
@@ -163,17 +163,31 @@ accessed by multiple threads.  Applications which can guarantee
 serialization in their access of provider allocated resources and
 interfaces enables a provider to eliminate lower-level locks.
 
-*FI_THREAD_UNSPEC*
-: This value indicates that no threading model has been defined.  It
-  may be used on input hints to the fi_getinfo call.  When specified,
-  providers will return a threading model that allows for the greatest
-  level of parallelism.
+*FI_THREAD_COMPLETION*
+: The completion threading model is intended for providers that make use
+  of manual progress.  Applications must serialize access to all objects
+  that are associated through the use of having a shared completion
+  structure.  This includes endpoint, transmit context, receive context,
+  completion queue, counter, wait set, and poll set objects.
 
-*FI_THREAD_SAFE*
-: A thread safe serialization model allows a multi-threaded
-  application to access any allocated resources through any interface
-  without restriction.  All providers are required to support
-  FI_THREAD_SAFE.
+  For example, threads must serialize access to an endpoint and its
+  bound completion queue(s) and/or counters.  Access to endpoints that
+  share the same completion queue must also be serialized.
+
+  The use of FI_THREAD_COMPLETION can increase parallelism over
+  FI_THREAD_SAFE, but requires the use of isolated resources.
+
+*FI_THREAD_DOMAIN*
+: A domain serialization model requires applications to serialize
+  access to all objects belonging to a domain.
+
+*FI_THREAD_ENDPOINT*
+: The endpoint threading model is similar to FI_THREAD_FID, but with
+  the added restriction that serialization is required when accessing
+  the same endpoint, even if multiple transmit and receive contexts are
+  used.  Conceptually, FI_THREAD_ENDPOINT maps well to providers that
+  implement fabric services in hardware but use a single command
+  queue to access different data flows.
 
 *FI_THREAD_FID*
 : A fabric descriptor (FID) serialization model requires applications
@@ -199,31 +213,17 @@ interfaces enables a provider to eliminate lower-level locks.
   fabric services in hardware and provide separate command queues to
   different data flows.
 
-*FI_THREAD_ENDPOINT*
-: The endpoint threading model is similar to FI_THREAD_FID, but with
-  the added restriction that serialization is required when accessing
-  the same endpoint, even if multiple transmit and receive contexts are
-  used.  Conceptually, FI_THREAD_ENDPOINT maps well to providers that
-  implement fabric services in hardware but use a single command
-  queue to access different data flows.
-
-*FI_THREAD_COMPLETION*
-: The completion threading model is intended for providers that make use
-  of manual progress.  Applications must serialize access to all objects
-  that are associated through the use of having a shared completion
-  structure.  This includes endpoint, transmit context, receive context,
-  completion queue, counter, wait set, and poll set objects.
-
-  For example, threads must serialize access to an endpoint and its
-  bound completion queue(s) and/or counters.  Access to endpoints that
-  share the same completion queue must also be serialized.
-
-  The use of FI_THREAD_COMPLETION can increase parallelism over
-  FI_THREAD_SAFE, but requires the use of isolated resources.
+*FI_THREAD_SAFE*
+: A thread safe serialization model allows a multi-threaded
+  application to access any allocated resources through any interface
+  without restriction.  All providers are required to support
+  FI_THREAD_SAFE.
 
-*FI_THREAD_DOMAIN*
-: A domain serialization model requires applications to serialize
-  access to all objects belonging to a domain.
+*FI_THREAD_UNSPEC*
+: This value indicates that no threading model has been defined.  It
+  may be used on input hints to the fi_getinfo call.  When specified,
+  providers will return a threading model that allows for the greatest
+  level of parallelism.
 
 ## Progress Models (control_progress / data_progress)
 
@@ -254,10 +254,6 @@ reliable transfers, as a result of retry and acknowledgement processing.
 To balance between performance and ease of use, two progress models
 are defined.
 
-*FI_PROGRESS_UNSPEC*
-: This value indicates that no progress model has been defined.  It
-  may be used on input hints to the fi_getinfo call.
-
 *FI_PROGRESS_AUTO*
 : This progress model indicates that the provider will make forward
   progress on an asynchronous operation without further intervention
@@ -293,6 +289,10 @@ are defined.
   manual progress may still need application assistance to process
   received operations.
 
+*FI_PROGRESS_UNSPEC*
+: This value indicates that no progress model has been defined.  It
+  may be used on input hints to the fi_getinfo call.
+
 ## Resource Management (resource_mgmt)
 
 Resource management (RM) is provider and protocol support to protect
@@ -314,10 +314,6 @@ provider implementation and protocol may still provide some level of
 protection against overruns.  However, such protection is not guaranteed.
 The following values for resource management are defined.
 
-*FI_RM_UNSPEC*
-: This value indicates that no resource management model has been defined.
-  It may be used on input hints to the fi_getinfo call.
-
 *FI_RM_DISABLED*
 : The provider is free to select an implementation and protocol that does
   not protect against resource overruns.  The application is responsible
@@ -326,6 +322,10 @@ The following values for resource management are defined.
 *FI_RM_ENABLED*
 : Resource management is enabled for this provider domain.
 
+*FI_RM_UNSPEC*
+: This value indicates that no resource management model has been defined.
+  It may be used on input hints to the fi_getinfo call.
+
 The behavior of the various resource management options depends on whether
 the endpoint is reliable or unreliable, as well as provider and protocol
 specific implementation details, as shown in the following table.  The
@@ -427,15 +427,15 @@ Specifies the type of address vectors that are usable with this domain.
 For additional details on AV type, see [`fi_av`(3)](fi_av.3.html).
 The following values may be specified.
 
-*FI_AV_UNSPEC*
-: Any address vector format is requested and supported.
-
 *FI_AV_MAP*
 : Only address vectors of type AV map are requested or supported.
 
 *FI_AV_TABLE*
 : Only address vectors of type AV index are requested or supported.
 
+*FI_AV_UNSPEC*
+: Any address vector format is requested and supported.
+
 Address vectors are only used by connectionless endpoints.  Applications
 that require the use of a specific type of address vector should set the
 domain attribute av_type to the necessary value when calling fi_getinfo.
@@ -450,6 +450,13 @@ Defines memory registration specific mode bits used with this domain.
 Full details on MR mode options are available in [`fi_mr`(3)](fi_mr.3.html).
 The following values may be specified.
 
+*FI_MR_ALLOCATED*
+: Indicates that memory registration occurs on allocated data buffers, and
+  physical pages must back all virtual addresses being registered.
+
+*FI_MR_ENDPOINT*
+: Memory registration occurs at the endpoint level, rather than domain.
+
 *FI_MR_LOCAL*
 : The provider is optimized around having applications register memory
   for locally accessed data buffers.  Data buffers used in send and
@@ -457,39 +464,32 @@ The following values may be specified.
   operations must be registered by the application for access domains
   opened with this capability.
 
-*FI_MR_RAW*
-: The provider requires additional setup as part of their memory registration
-  process.  This mode is required by providers that use a memory key
-  that is larger than 64-bits.
-
-*FI_MR_VIRT_ADDR*
-: Registered memory regions are referenced by peers using the virtual address
-  of the registered memory region, rather than a 0-based offset.
-
-*FI_MR_ALLOCATED*
-: Indicates that memory registration occurs on allocated data buffers, and
-  physical pages must back all virtual addresses being registered.
-
-*FI_MR_PROV_KEY*
-: Memory registration keys are selected and returned by the provider.
-
 *FI_MR_MMU_NOTIFY*
 : Indicates that the application is responsible for notifying the provider
   when the page tables referencing a registered memory region may have been
   updated.
 
+*FI_MR_PROV_KEY*
+: Memory registration keys are selected and returned by the provider.
+
+*FI_MR_RAW*
+: The provider requires additional setup as part of their memory registration
+  process.  This mode is required by providers that use a memory key
+  that is larger than 64-bits.
+
 *FI_MR_RMA_EVENT*
 : Indicates that the memory regions associated with completion counters
   must be explicitly enabled after being bound to any counter.
 
-*FI_MR_ENDPOINT*
-: Memory registration occurs at the endpoint level, rather than domain.
-
 *FI_MR_UNSPEC*
 : Defined for compatibility -- library versions 1.4 and earlier.  Setting
   mr_mode to 0 indicates that FI_MR_BASIC or FI_MR_SCALABLE are requested
   and supported.
 
+*FI_MR_VIRT_ADDR*
+: Registered memory regions are referenced by peers using the virtual address
+  of the registered memory region, rather than a 0-based offset.
+
 *FI_MR_BASIC*
 : Defined for compatibility -- library versions 1.4 and earlier.  Only
   basic memory registration operations are requested or supported.
diff --git a/man/fi_efa.7.md b/man/fi_efa.7.md
index 3c6c849..ef871df 100644
--- a/man/fi_efa.7.md
+++ b/man/fi_efa.7.md
@@ -118,7 +118,7 @@ These OFI runtime parameters apply only to the RDM endpoint.
 
 *FI_EFA_MR_CACHE_ENABLE*
 : Enables using the mr cache and in-line registration instead of a bounce
-  buffer for iov's larger than max_memcpy_size. Defaults to false. When
+  buffer for iov's larger than max_memcpy_size. Defaults to true. When
   disabled, only uses a bounce buffer
 
 *FI_EFA_MR_CACHE_MERGE_REGIONS*
diff --git a/man/fi_endpoint.3.md b/man/fi_endpoint.3.md
index a3f7827..34a1cc4 100644
--- a/man/fi_endpoint.3.md
+++ b/man/fi_endpoint.3.md
@@ -286,11 +286,6 @@ CQs, based on the type of operation.  This is specified using
 fi_ep_bind flags.  The following flags may be OR'ed together when
 binding an endpoint to a completion domain CQ.
 
-*FI_TRANSMIT*
-: Directs the completion of outbound data transfer requests to the
-  specified completion queue.  This includes send message, RMA, and
-  atomic operations.
-
 *FI_RECV*
 : Directs the notification of inbound data transfers to the specified
   completion queue.  This includes received messages.  This binding
@@ -317,28 +312,24 @@ binding an endpoint to a completion domain CQ.
   See Notes section below for additional information on how this flag
   interacts with the FI_CONTEXT and FI_CONTEXT2 mode bits.
 
+*FI_TRANSMIT*
+: Directs the completion of outbound data transfer requests to the
+  specified completion queue.  This includes send message, RMA, and
+  atomic operations.
+
 An endpoint may optionally be bound to a completion counter.  Associating
 an endpoint with a counter is in addition to binding the EP with a CQ.  When
 binding an endpoint to a counter, the following flags may be specified.
 
-*FI_SEND*
-: Increments the specified counter whenever a message transfer initiated
-  over the endpoint has completed successfully or in error.  Sent messages
-  include both tagged and normal message operations.
-
-*FI_RECV*
-: Increments the specified counter whenever a message is
-  received over the endpoint.  Received messages include both tagged
-  and normal message operations.
-
 *FI_READ*
 : Increments the specified counter whenever an RMA read, atomic fetch,
   or atomic compare operation initiated from the endpoint has completed
   successfully or in error.
 
-*FI_WRITE*
-: Increments the specified counter whenever an RMA write or base atomic
-  operation initiated from the endpoint has completed successfully or in error.
+*FI_RECV*
+: Increments the specified counter whenever a message is
+  received over the endpoint.  Received messages include both tagged
+  and normal message operations.
 
 *FI_REMOTE_READ*
 : Increments the specified counter whenever an RMA read, atomic fetch, or
@@ -352,6 +343,15 @@ binding an endpoint to a counter, the following flags may be specified.
   the given endpoint.  Use of this flag requires that the
   endpoint be created using FI_RMA_EVENT.
 
+*FI_SEND*
+: Increments the specified counter whenever a message transfer initiated
+  over the endpoint has completed successfully or in error.  Sent messages
+  include both tagged and normal message operations.
+
+*FI_WRITE*
+: Increments the specified counter whenever an RMA write or base atomic
+  operation initiated from the endpoint has completed successfully or in error.
+
 An endpoint may only be bound to a single CQ or counter for a given
 type of operation.  For example, a EP may not bind to two counters
 both using FI_WRITE.  Furthermore, providers may limit CQ and counter
@@ -436,6 +436,10 @@ The base operation of an endpoint is selected during creation using
 struct fi_info.  The following control commands and arguments may be
 assigned to an endpoint.
 
+**FI_BACKLOG - int *value**
+: This option only applies to passive endpoints.  It is used to set the
+  connection request backlog for listening endpoints.
+
 **FI_GETOPSFLAG -- uint64_t *flags**
 : Used to retrieve the current value of flags associated with the data
   transfer operations initiated on the endpoint. The control argument must
@@ -443,6 +447,14 @@ assigned to an endpoint.
   data transfer flags to be returned.
   See below for a list of control flags.
 
+**FI_GETWAIT -- void \*\***
+: This command allows the user to retrieve the file descriptor associated
+  with a socket endpoint.  The fi_control arg parameter should be an address
+  where a pointer to the returned file descriptor will be written.  See fi_eq.3
+  for addition details using fi_control with FI_GETWAIT.  The file descriptor
+  may be used for notification that the endpoint is ready to send or receive
+  data.
+
 **FI_SETOPSFLAG -- uint64_t *flags**
 : Used to change the data transfer operation flags associated with an
   endpoint. The control argument must include FI_TRANSMIT or FI_RECV (not both)
@@ -451,18 +463,6 @@ assigned to an endpoint.
   attributes that were set when the endpoint was created.
   Valid control flags are defined below.
 
-**FI_BACKLOG - int *value**
-: This option only applies to passive endpoints.  It is used to set the
-  connection request backlog for listening endpoints.
-
-*FI_GETWAIT (void \*\*)*
-: This command allows the user to retrieve the file descriptor associated
-  with a socket endpoint.  The fi_control arg parameter should be an address
-  where a pointer to the returned file descriptor will be written.  See fi_eq.3
-  for addition details using fi_control with FI_GETWAIT.  The file descriptor
-  may be used for notification that the endpoint is ready to send or receive
-  data.
-
 ## fi_getopt / fi_setopt
 
 Endpoint protocol operations may be retrieved using fi_getopt or set
@@ -475,23 +475,6 @@ The following option levels and option names and parameters are defined.
 
 *FI_OPT_ENDPOINT*
 
-- *FI_OPT_MIN_MULTI_RECV - size_t*
-: Defines the minimum receive buffer space available when the receive
-  buffer is released by the provider (see FI_MULTI_RECV).  Modifying this
-  value is only guaranteed to set the minimum buffer space needed on
-  receives posted after the value has been changed.  It is recommended
-  that applications that want to override the default MIN_MULTI_RECV
-  value set this option before enabling the corresponding endpoint.
-
-- *FI_OPT_CM_DATA_SIZE - size_t*
-: Defines the size of available space in CM messages for user-defined
-  data.  This value limits the amount of data that applications can exchange
-  between peer endpoints using the fi_connect, fi_accept, and fi_reject
-  operations.  The size returned is dependent upon the properties of the
-  endpoint, except in the case of passive endpoints, in which the size reflects
-  the maximum size of the data that may be present as part of a connection
-  request event. This option is read only.
-
 - *FI_OPT_BUFFERED_LIMIT - size_t*
 : Defines the maximum size of a buffered message that will be reported
   to users as part of a receive completion when the FI_BUFFERED_RECV mode
@@ -520,6 +503,23 @@ The following option levels and option names and parameters are defined.
   sized renedezvous protocol message usually results in better latency for the
   overall transfer of a large message.
 
+- *FI_OPT_CM_DATA_SIZE - size_t*
+: Defines the size of available space in CM messages for user-defined
+  data.  This value limits the amount of data that applications can exchange
+  between peer endpoints using the fi_connect, fi_accept, and fi_reject
+  operations.  The size returned is dependent upon the properties of the
+  endpoint, except in the case of passive endpoints, in which the size reflects
+  the maximum size of the data that may be present as part of a connection
+  request event. This option is read only.
+
+- *FI_OPT_MIN_MULTI_RECV - size_t*
+: Defines the minimum receive buffer space available when the receive
+  buffer is released by the provider (see FI_MULTI_RECV).  Modifying this
+  value is only guaranteed to set the minimum buffer space needed on
+  receives posted after the value has been changed.  It is recommended
+  that applications that want to override the default MIN_MULTI_RECV
+  value set this option before enabling the corresponding endpoint.
+
 ## fi_tc_dscp_set
 
 This call converts a DSCP defined value into a libfabric traffic class value.
@@ -584,25 +584,26 @@ struct fi_ep_attr {
 If specified, indicates the type of fabric interface communication
 desired.  Supported types are:
 
-*FI_EP_UNSPEC*
-: The type of endpoint is not specified.  This is usually provided as
-  input, with other attributes of the endpoint or the provider
-  selecting the type.
-
-*FI_EP_MSG*
-: Provides a reliable, connection-oriented data transfer service with
-  flow control that maintains message boundaries.
-
 *FI_EP_DGRAM*
 : Supports a connectionless, unreliable datagram communication.
   Message boundaries are maintained, but the maximum message size may
   be limited to the fabric MTU.  Flow control is not guaranteed.
 
+*FI_EP_MSG*
+: Provides a reliable, connection-oriented data transfer service with
+  flow control that maintains message boundaries.
+
 *FI_EP_RDM*
 : Reliable datagram message.  Provides a reliable, unconnected data
   transfer service with flow control that maintains message
   boundaries.
 
+*FI_EP_SOCK_DGRAM*
+: A connectionless, unreliable datagram endpoint with UDP socket-like
+  semantics.  FI_EP_SOCK_DGRAM is most useful for applications designed
+  around using UDP sockets.  See the SOCKET ENDPOINT section for additional
+  details and restrictions that apply to datagram socket endpoints.
+
 *FI_EP_SOCK_STREAM*
 : Data streaming endpoint with TCP socket-like semantics.  Provides
   a reliable, connection-oriented data transfer service that does
@@ -611,11 +612,10 @@ desired.  Supported types are:
   ENDPOINT section for additional details and restrictions that apply
   to stream endpoints.
 
-*FI_EP_SOCK_DGRAM*
-: A connectionless, unreliable datagram endpoint with UDP socket-like
-  semantics.  FI_EP_SOCK_DGRAM is most useful for applications designed
-  around using UDP sockets.  See the SOCKET ENDPOINT section for additional
-  details and restrictions that apply to datagram socket endpoints.
+*FI_EP_UNSPEC*
+: The type of endpoint is not specified.  This is usually provided as
+  input, with other attributes of the endpoint or the provider
+  selecting the type.
 
 ## Protocol
 
@@ -626,65 +626,65 @@ Provider specific protocols are also allowed.  Provider specific
 protocols will be indicated by having the upper bit of the
 protocol value set to one.
 
-*FI_PROTO_UNSPEC*
-: The protocol is not specified.  This is usually provided as input,
-  with other attributes of the socket or the provider selecting the
-  actual protocol.
+*FI_PROTO_GNI*
+: Protocol runs over Cray GNI low-level interface.
 
-*FI_PROTO_RDMA_CM_IB_RC*
-: The protocol runs over Infiniband reliable-connected queue pairs,
-  using the RDMA CM protocol for connection establishment.
+*FI_PROTO_IB_RDM*
+: Reliable-datagram protocol implemented over InfiniBand reliable-connected
+  queue pairs.
+
+*FI_PROTO_IB_UD*
+: The protocol runs over Infiniband unreliable datagram queue pairs.
 
 *FI_PROTO_IWARP*
 : The protocol runs over the Internet wide area RDMA protocol transport.
 
-*FI_PROTO_IB_UD*
-: The protocol runs over Infiniband unreliable datagram queue pairs.
+*FI_PROTO_IWARP_RDM*
+: Reliable-datagram protocol implemented over iWarp reliable-connected
+  queue pairs.
+
+*FI_PROTO_NETWORKDIRECT*
+: Protocol runs over Microsoft NetworkDirect service provider interface.
+  This adds reliable-datagram semantics over the NetworkDirect connection-
+  oriented endpoint semantics.
 
 *FI_PROTO_PSMX*
 : The protocol is based on an Intel proprietary protocol known as PSM,
   performance scaled messaging.  PSMX is an extended version of the
   PSM protocol to support the libfabric interfaces.
 
-*FI_PROTO_UDP*
-: The protocol sends and receives UDP datagrams.  For example, an
-  endpoint using *FI_PROTO_UDP* will be able to communicate with a
-  remote peer that is using Berkeley *SOCK_DGRAM* sockets using
-  *IPPROTO_UDP*.
-
-*FI_PROTO_SOCK_TCP*
-: The protocol is layered over TCP packets.
-
-*FI_PROTO_IWARP_RDM*
-: Reliable-datagram protocol implemented over iWarp reliable-connected
-  queue pairs.
+*FI_PROTO_PSMX2*
+: The protocol is based on an Intel proprietary protocol known as PSM2,
+  performance scaled messaging version 2.  PSMX2 is an extended version of the
+  PSM2 protocol to support the libfabric interfaces.
 
-*FI_PROTO_IB_RDM*
-: Reliable-datagram protocol implemented over InfiniBand reliable-connected
-  queue pairs.
+*FI_PROTO_RDMA_CM_IB_RC*
+: The protocol runs over Infiniband reliable-connected queue pairs,
+  using the RDMA CM protocol for connection establishment.
 
-*FI_PROTO_GNI*
-: Protocol runs over Cray GNI low-level interface.
+*FI_PROTO_RXD*
+: Reliable-datagram protocol implemented over datagram endpoints.  RXD is
+  a libfabric utility component that adds RDM endpoint semantics over
+  DGRAM endpoint semantics.
 
 *FI_PROTO_RXM*
 : Reliable-datagram protocol implemented over message endpoints.  RXM is
   a libfabric utility component that adds RDM endpoint semantics over
   MSG endpoint semantics.
 
-*FI_PROTO_RXD*
-: Reliable-datagram protocol implemented over datagram endpoints.  RXD is
-  a libfabric utility component that adds RDM endpoint semantics over
-  DGRAM endpoint semantics.
+*FI_PROTO_SOCK_TCP*
+: The protocol is layered over TCP packets.
 
-*FI_PROTO_NETWORKDIRECT*
-: Protocol runs over Microsoft NetworkDirect service provider interface.
-  This adds reliable-datagram semantics over the NetworkDirect connection-
-  oriented endpoint semantics.
+*FI_PROTO_UDP*
+: The protocol sends and receives UDP datagrams.  For example, an
+  endpoint using *FI_PROTO_UDP* will be able to communicate with a
+  remote peer that is using Berkeley *SOCK_DGRAM* sockets using
+  *IPPROTO_UDP*.
 
-*FI_PROTO_PSMX2*
-: The protocol is based on an Intel proprietary protocol known as PSM2,
-  performance scaled messaging version 2.  PSMX2 is an extended version of the
-  PSM2 protocol to support the libfabric interfaces.
+*FI_PROTO_UNSPEC*
+: The protocol is not specified.  This is usually provided as input,
+  with other attributes of the socket or the provider selecting the
+  actual protocol.
 
 ## protocol_version - Protocol Version
 
@@ -870,8 +870,20 @@ struct fi_tx_attr {
 The requested capabilities of the context.  The capabilities must be
 a subset of those requested of the associated endpoint.  See the
 CAPABILITIES section of fi_getinfo(3) for capability details.  If
-the caps field is 0 on input to fi_getinfo(3), the caps value from the
-fi_info structure will be used.
+the caps field is 0 on input to fi_getinfo(3), the applicable
+capability bits from the fi_info structure will be used.
+
+The following capabilities apply to the transmit attributes: FI_MSG,
+FI_RMA, FI_TAGGED, FI_ATOMIC, FI_READ, FI_WRITE, FI_SEND, FI_HMEM,
+FI_TRIGGER, FI_FENCE, FI_MULTICAST, FI_RMA_PMEM, and FI_NAMED_RX_CTX.
+
+Many applications will be able to ignore this field and rely solely
+on the fi_info::caps field.  Use of this field provides fine grained
+control over the transmit capabilities associated with an endpoint.
+It is useful when handling scalable endpoints, with multiple transmit
+contexts, for example, and allows configuring a specific transmit
+context with fewer capabilities than that supported by the endpoint
+or other transmit contexts.
 
 ## mode
 
@@ -910,6 +922,30 @@ which message data is sent or received by the transport layer.  Message
 ordering requires matching ordering semantics on the receiving side of a data
 transfer operation in order to guarantee that ordering is met.
 
+*FI_ORDER_ATOMIC_RAR*
+: Atomic read after read.  If set, atomic fetch operations are
+  transmitted in the order submitted relative to other
+  atomic fetch operations.  If not set, atomic fetches
+  may be transmitted out of order from their submission.
+
+*FI_ORDER_ATOMIC_RAW*
+: Atomic read after write.  If set, atomic fetch operations are
+  transmitted in the order submitted relative to atomic update
+  operations.  If not set, atomic fetches may be transmitted ahead
+  of atomic updates.
+
+*FI_ORDER_ATOMIC_WAR*
+: RMA write after read.  If set, atomic update operations are
+  transmitted in the order submitted relative to atomic fetch
+  operations.  If not set, atomic updates may be transmitted
+  ahead of atomic fetches.
+
+*FI_ORDER_ATOMIC_WAW*
+: RMA write after write.  If set, atomic update operations are
+  transmitted in the order submitted relative to other atomic
+  update operations.  If not atomic updates may be
+  transmitted out of order from their submission.
+
 *FI_ORDER_NONE*
 : No ordering is specified.  This value may be used as input in order
   to obtain the default message order supported by the provider. FI_ORDER_NONE
@@ -921,54 +957,18 @@ transfer operation in order to guarantee that ordering is met.
   RMA and atomic read operations.  If not set, RMA and atomic reads
   may be transmitted out of order from their submission.
 
-*FI_ORDER_RAW*
-: Read after write.  If set, RMA and atomic read operations are
-  transmitted in the order submitted relative to RMA and atomic write
-  operations.  If not set, RMA and atomic reads may be transmitted ahead
-  of RMA and atomic writes.
-
 *FI_ORDER_RAS*
 : Read after send.  If set, RMA and atomic read operations are
   transmitted in the order submitted relative to message send
   operations, including tagged sends.  If not set, RMA and atomic
   reads may be transmitted ahead of sends.
 
-*FI_ORDER_WAR*
-: Write after read.  If set, RMA and atomic write operations are
-  transmitted in the order submitted relative to RMA and atomic read
-  operations.  If not set, RMA and atomic writes may be transmitted
-  ahead of RMA and atomic reads.
-
-*FI_ORDER_WAW*
-: Write after write.  If set, RMA and atomic write operations are
-  transmitted in the order submitted relative to other RMA and atomic
-  write operations.  If not set, RMA and atomic writes may be
-  transmitted out of order from their submission.
-
-*FI_ORDER_WAS*
-: Write after send.  If set, RMA and atomic write operations are
-  transmitted in the order submitted relative to message send
-  operations, including tagged sends.  If not set, RMA and atomic
-  writes may be transmitted ahead of sends.
-
-*FI_ORDER_SAR*
-: Send after read.  If set, message send operations, including tagged
-  sends, are transmitted in order submitted relative to RMA and atomic
-  read operations.  If not set, message sends may be transmitted ahead
-  of RMA and atomic reads.
-
-*FI_ORDER_SAW*
-: Send after write.  If set, message send operations, including tagged
-  sends, are transmitted in order submitted relative to RMA and atomic
-  write operations.  If not set, message sends may be transmitted ahead
+*FI_ORDER_RAW*
+: Read after write.  If set, RMA and atomic read operations are
+  transmitted in the order submitted relative to RMA and atomic write
+  operations.  If not set, RMA and atomic reads may be transmitted ahead
   of RMA and atomic writes.
 
-*FI_ORDER_SAS*
-: Send after send.  If set, message send operations, including tagged
-  sends, are transmitted in the order submitted relative to other
-  message send.  If not set, message sends may be transmitted out of
-  order from their submission.
-
 *FI_ORDER_RMA_RAR*
 : RMA read after read.  If set, RMA read operations are
   transmitted in the order submitted relative to other
@@ -993,28 +993,40 @@ transfer operation in order to guarantee that ordering is met.
   write operations.  If not set, RMA writes may be
   transmitted out of order from their submission.
 
-*FI_ORDER_ATOMIC_RAR*
-: Atomic read after read.  If set, atomic fetch operations are
-  transmitted in the order submitted relative to other
-  atomic fetch operations.  If not set, atomic fetches
-  may be transmitted out of order from their submission.
+*FI_ORDER_SAR*
+: Send after read.  If set, message send operations, including tagged
+  sends, are transmitted in order submitted relative to RMA and atomic
+  read operations.  If not set, message sends may be transmitted ahead
+  of RMA and atomic reads.
 
-*FI_ORDER_ATOMIC_RAW*
-: Atomic read after write.  If set, atomic fetch operations are
-  transmitted in the order submitted relative to atomic update
-  operations.  If not set, atomic fetches may be transmitted ahead
-  of atomic updates.
+*FI_ORDER_SAS*
+: Send after send.  If set, message send operations, including tagged
+  sends, are transmitted in the order submitted relative to other
+  message send.  If not set, message sends may be transmitted out of
+  order from their submission.
 
-*FI_ORDER_ATOMIC_WAR*
-: RMA write after read.  If set, atomic update operations are
-  transmitted in the order submitted relative to atomic fetch
-  operations.  If not set, atomic updates may be transmitted
-  ahead of atomic fetches.
+*FI_ORDER_SAW*
+: Send after write.  If set, message send operations, including tagged
+  sends, are transmitted in order submitted relative to RMA and atomic
+  write operations.  If not set, message sends may be transmitted ahead
+  of RMA and atomic writes.
 
-*FI_ORDER_ATOMIC_WAW*
-: RMA write after write.  If set, atomic update operations are
-  transmitted in the order submitted relative to other atomic
-  update operations.  If not atomic updates may be
+*FI_ORDER_WAR*
+: Write after read.  If set, RMA and atomic write operations are
+  transmitted in the order submitted relative to RMA and atomic read
+  operations.  If not set, RMA and atomic writes may be transmitted
+  ahead of RMA and atomic reads.
+
+*FI_ORDER_WAS*
+: Write after send.  If set, RMA and atomic write operations are
+  transmitted in the order submitted relative to message send
+  operations, including tagged sends.  If not set, RMA and atomic
+  writes may be transmitted ahead of sends.
+
+*FI_ORDER_WAW*
+: Write after write.  If set, RMA and atomic write operations are
+  transmitted in the order submitted relative to other RMA and atomic
+  write operations.  If not set, RMA and atomic writes may be
   transmitted out of order from their submission.
 
 ## comp_order - Completion Ordering
@@ -1094,6 +1106,16 @@ domain.
   network capacity and resource allocation are distributed fairly across the
   applications.
 
+*FI_TC_BULK_DATA*
+: This class is intended for large data transfers associated with I/O and
+  is present to separate sustained I/O transfers from other application
+  inter-process communications.
+
+*FI_TC_DEDICATED_ACCESS*
+: This class operates at the highest priority, except the management class.
+  It carries a high bandwidth allocation, minimum latency targets, and the
+  highest scheduling and arbitration priority.
+
 *FI_TC_LOW_LATENCY*
 : This class supports low latency, low jitter data patterns typically caused by
   transactional data exchanges, barrier synchronizations, and collective
@@ -1103,15 +1125,10 @@ domain.
   typically require accompanying bandwidth and message size limitations so
   as not to consume excessive bandwidth at high priority.
 
-*FI_TC_DEDICATED_ACCESS*
-: This class operates at the highest priority, except the management class.
-  It carries a high bandwidth allocation, minimum latency targets, and the
-  highest scheduling and arbitration priority.
-
-*FI_TC_BULK_DATA*
-: This class is intended for large data transfers associated with I/O and
-  is present to separate sustained I/O transfers from other application
-  inter-process communications.
+*FI_TC_NETWORK_CTRL*
+: This class is intended for traffic directly related to fabric (network)
+  management, which is critical to the correct operation of the network.
+  Its use is typically restricted to privileged network management applications.
 
 *FI_TC_SCAVENGER*
 : This class is used for data that is desired but does not have strict delivery
@@ -1119,11 +1136,6 @@ domain.
   Use of this class indicates that the traffic is considered lower priority
   and should not interfere with higher priority workflows.
 
-*FI_TC_NETWORK_CTRL*
-: This class is intended for traffic directly related to fabric (network)
-  management, which is critical to the correct operation of the network.
-  Its use is typically restricted to privileged network management applications.
-
 *fi_tc_dscp_set / fi_tc_dscp_get*
 : DSCP values are supported via the DSCP get and set functions.  The
   definitions for DSCP values are outside the scope of libfabric.  See
@@ -1153,8 +1165,21 @@ struct fi_rx_attr {
 The requested capabilities of the context.  The capabilities must be
 a subset of those requested of the associated endpoint.  See the
 CAPABILITIES section if fi_getinfo(3) for capability details.  If
-the caps field is 0 on input to fi_getinfo(3), the caps value from the
-fi_info structure will be used.
+the caps field is 0 on input to fi_getinfo(3), the applicable
+capability bits from the fi_info structure will be used.
+
+The following capabilities apply to the receive attributes: FI_MSG,
+FI_RMA, FI_TAGGED, FI_ATOMIC, FI_REMOTE_READ, FI_REMOTE_WRITE, FI_RECV,
+FI_HMEM, FI_TRIGGER, FI_RMA_PMEM, FI_DIRECTED_RECV, FI_VARIABLE_MSG,
+FI_MULTI_RECV, FI_SOURCE, FI_RMA_EVENT, and FI_SOURCE_ERROR.
+
+Many applications will be able to ignore this field and rely solely
+on the fi_info::caps field.  Use of this field provides fine grained
+control over the receive capabilities associated with an endpoint.
+It is useful when handling scalable endpoints, with multiple receive
+contexts, for example, and allows configuring a specific receive
+context with fewer capabilities than that supported by the endpoint
+or other receive contexts.
 
 ## mode
 
@@ -1194,6 +1219,11 @@ FI_ORDER_ATOMIC_RAW, FI_ORDER_ATOMIC_WAR, and FI_ORDER_ATOMIC_WAW.
 For a description of completion ordering, see the comp_order field in
 the _Transmit Context Attribute_ section.
 
+*FI_ORDER_DATA*
+: When set, this bit indicates that received data is written into memory
+  in order.  Data ordering applies to memory accessed as part of a single
+  operation and between operations if message ordering is guaranteed.
+
 *FI_ORDER_NONE*
 : No ordering is defined for completed operations.  Receive operations may
   complete in any order, regardless of their submission order.
@@ -1202,11 +1232,6 @@ the _Transmit Context Attribute_ section.
 : Receive operations complete in the order in which they are processed by
   the receive context, based on the receive side msg_order attribute.
 
-*FI_ORDER_DATA*
-: When set, this bit indicates that received data is written into memory
-  in order.  Data ordering applies to memory accessed as part of a single
-  operation and between operations if message ordering is guaranteed.
-
 ## total_buffered_recv
 
 This field is supported for backwards compatibility purposes.
@@ -1407,6 +1432,24 @@ transfer operations, where a flags parameter is not available.  Data
 transfer operations that take flags as input override the op_flags
 value of transmit or receive context attributes of an endpoint.
 
+*FI_COMMIT_COMPLETE*
+: Indicates that a completion should not be generated (locally or at the
+  peer) until the result of an operation have been made persistent.
+  See [`fi_cq`(3)](fi_cq.3.html) for additional details on completion
+  semantics.
+
+*FI_COMPLETION*
+: Indicates that a completion queue entry should be written for data
+  transfer operations. This flag only applies to operations issued on an
+  endpoint that was bound to a completion queue with the
+  FI_SELECTIVE_COMPLETION flag set, otherwise, it is ignored.  See the
+  fi_ep_bind section above for more detail.
+
+*FI_DELIVERY_COMPLETE*
+: Indicates that a completion should be generated when the operation has been
+  processed by the destination endpoint(s).  See [`fi_cq`(3)](fi_cq.3.html)
+  for additional details on completion semantics.
+
 *FI_INJECT*
 : Indicates that all outbound data buffers should be returned to the
   user's control immediately after a data transfer call returns, even
@@ -1417,6 +1460,16 @@ value of transmit or receive context attributes of an endpoint.
   this flag. This limit is indicated using inject_size (see inject_size
   above).
 
+*FI_INJECT_COMPLETE*
+: Indicates that a completion should be generated when the
+  source buffer(s) may be reused.  See [`fi_cq`(3)](fi_cq.3.html) for
+  additional details on completion semantics.
+
+*FI_MULTICAST*
+: Indicates that data transfers will target multicast addresses by default.
+  Any fi_addr_t passed into a data transfer operation will be treated as a
+  multicast address.
+
 *FI_MULTI_RECV*
 : Applies to posted receive operations.  This flag allows the user to
   post a single buffer that will receive multiple incoming messages.
@@ -1429,39 +1482,11 @@ value of transmit or receive context attributes of an endpoint.
   available buffer space falls below the specified minimum (see
   FI_OPT_MIN_MULTI_RECV).
 
-*FI_COMPLETION*
-: Indicates that a completion queue entry should be written for data
-  transfer operations. This flag only applies to operations issued on an
-  endpoint that was bound to a completion queue with the
-  FI_SELECTIVE_COMPLETION flag set, otherwise, it is ignored.  See the
-  fi_ep_bind section above for more detail.
-
-*FI_INJECT_COMPLETE*
-: Indicates that a completion should be generated when the
-  source buffer(s) may be reused.  See [`fi_cq`(3)](fi_cq.3.html) for
-  additional details on completion semantics.
-
 *FI_TRANSMIT_COMPLETE*
 : Indicates that a completion should be generated when the transmit
   operation has completed relative to the local provider.  See
   [`fi_cq`(3)](fi_cq.3.html) for additional details on completion semantics.
 
-*FI_DELIVERY_COMPLETE*
-: Indicates that a completion should be generated when the operation has been
-  processed by the destination endpoint(s).  See [`fi_cq`(3)](fi_cq.3.html)
-  for additional details on completion semantics.
-
-*FI_COMMIT_COMPLETE*
-: Indicates that a completion should not be generated (locally or at the
-  peer) until the result of an operation have been made persistent.
-  See [`fi_cq`(3)](fi_cq.3.html) for additional details on completion
-  semantics.
-
-*FI_MULTICAST*
-: Indicates that data transfers will target multicast addresses by default.
-  Any fi_addr_t passed into a data transfer operation will be treated as a
-  multicast address.
-
 # NOTES
 
 Users should call fi_close to release all resources allocated to the
diff --git a/man/fi_getinfo.3.md b/man/fi_getinfo.3.md
index e6dd173..55822bd 100644
--- a/man/fi_getinfo.3.md
+++ b/man/fi_getinfo.3.md
@@ -254,37 +254,6 @@ Applications may use this feature to request a minimal set of
 requirements, then check the returned capabilities to enable
 additional optimizations.
 
-*FI_MSG*
-: Specifies that an endpoint should support sending and receiving
-  messages or datagrams.  Message capabilities imply support for send
-  and/or receive queues.  Endpoints supporting this capability support
-  operations defined by struct fi_ops_msg.
-
-  The caps may be used to specify or restrict the type of messaging
-  operations that are supported.  In the absence of any relevant
-  flags, FI_MSG implies the ability to send and receive messages.
-  Applications can use the FI_SEND and FI_RECV flags to optimize an
-  endpoint as send-only or receive-only.
-
-*FI_RMA*
-: Specifies that the endpoint should support RMA read and write
-  operations.  Endpoints supporting this capability support operations
-  defined by struct fi_ops_rma.  In the absence of any relevant flags,
-  FI_RMA implies the ability to initiate and be the target of remote
-  memory reads and writes.  Applications can use the FI_READ,
-  FI_WRITE, FI_REMOTE_READ, and FI_REMOTE_WRITE flags to restrict the
-  types of RMA operations supported by an endpoint.
-
-*FI_TAGGED*
-: Specifies that the endpoint should handle tagged message transfers.
-  Tagged message transfers associate a user-specified key or tag with
-  each message that is used for matching purposes at the remote side.
-  Endpoints supporting this capability support operations defined by
-  struct fi_ops_tagged.  In the absence of any relevant flags,
-  FI_TAGGED implies the ability to send and receive tagged messages.
-  Applications can use the FI_SEND and FI_RECV flags to optimize an
-  endpoint as send-only or receive-only.
-
 *FI_ATOMIC*
 : Specifies that the endpoint supports some set of atomic operations.
   Endpoints supporting this capability support operations defined by
@@ -294,56 +263,77 @@ additional optimizations.
   FI_WRITE, FI_REMOTE_READ, and FI_REMOTE_WRITE flags to restrict the
   types of atomic operations supported by an endpoint.
 
-*FI_MULTICAST*
-: Indicates that the endpoint support multicast data transfers.  This
-  capability must be paired with at least one other data transfer capability,
-  (e.g. FI_MSG, FI_SEND, FI_RECV, ...).
-
-*FI_NAMED_RX_CTX*
-: Requests that endpoints which support multiple receive contexts
-  allow an initiator to target (or name) a specific receive context as
-  part of a data transfer operation.
-
 *FI_DIRECTED_RECV*
 : Requests that the communication endpoint use the source address of
   an incoming message when matching it with a receive buffer.  If this
   capability is not set, then the src_addr parameter for msg and tagged
   receive operations is ignored.
 
+*FI_FENCE*
+: Indicates that the endpoint support the FI_FENCE flag on data
+  transfer operations.  Support requires tracking that all previous
+  transmit requests to a specified remote endpoint complete prior
+  to initiating the fenced operation.  Fenced operations are often
+  used to enforce ordering between operations that are not otherwise
+  guaranteed by the underlying provider or protocol.
+
+*FI_HMEM*
+: Specifies that the endpoint should support transfers to and from
+  device memory. 
+
+*FI_LOCAL_COMM*
+: Indicates that the endpoint support host local communication.  This
+  flag may be used in conjunction with FI_REMOTE_COMM to indicate that
+  local and remote communication are required.  If neither FI_LOCAL_COMM
+  or FI_REMOTE_COMM are specified, then the provider will indicate
+  support for the configuration that minimally affects performance.
+  Providers that set FI_LOCAL_COMM but not FI_REMOTE_COMM, for example
+  a shared memory provider, may only be used to communication between
+  processes on the same system.
+
+*FI_MSG*
+: Specifies that an endpoint should support sending and receiving
+  messages or datagrams.  Message capabilities imply support for send
+  and/or receive queues.  Endpoints supporting this capability support
+  operations defined by struct fi_ops_msg.
+
+  The caps may be used to specify or restrict the type of messaging
+  operations that are supported.  In the absence of any relevant
+  flags, FI_MSG implies the ability to send and receive messages.
+  Applications can use the FI_SEND and FI_RECV flags to optimize an
+  endpoint as send-only or receive-only.
+
+*FI_MULTICAST*
+: Indicates that the endpoint support multicast data transfers.  This
+  capability must be paired with FI_MSG.  Aplications can use FI_SEND
+  and FI_RECV to optimize multicast as send-only or receive-only.
+
 *FI_MULTI_RECV*
 : Specifies that the endpoint must support the FI_MULTI_RECV flag when
   posting receive buffers.
 
-*FI_SOURCE*
-: Requests that the endpoint return source addressing data as part of
-  its completion data.  This capability only applies to connectionless
-  endpoints.  Note that returning source address information may
-  require that the provider perform address translation and/or look-up
-  based on data available in the underlying protocol in order to
-  provide the requested data, which may adversely affect performance.
-  The performance impact may be greater for address vectors of type
-  FI_AV_TABLE.
+*FI_NAMED_RX_CTX*
+: Requests that endpoints which support multiple receive contexts
+  allow an initiator to target (or name) a specific receive context as
+  part of a data transfer operation.
 
 *FI_READ*
 : Indicates that the user requires an endpoint capable of initiating
   reads against remote memory regions.  This flag requires that FI_RMA
   and/or FI_ATOMIC be set.
 
-*FI_WRITE*
-: Indicates that the user requires an endpoint capable of initiating
-  writes against remote memory regions.  This flag requires that FI_RMA
-  and/or FI_ATOMIC be set.
-
-*FI_SEND*
-: Indicates that the user requires an endpoint capable of sending
-  message data transfers.  Message transfers include base message
-  operations as well as tagged message functionality.
-
 *FI_RECV*
 : Indicates that the user requires an endpoint capable of receiving
   message data transfers.  Message transfers include base message
   operations as well as tagged message functionality.
 
+*FI_REMOTE_COMM*
+: Indicates that the endpoint support communication with endpoints
+  located at remote nodes (across the fabric).  See FI_LOCAL_COMM for
+  additional details.  Providers that set FI_REMOTE_COMM but not
+  FI_LOCAL_COMM, for example NICs that lack loopback support, cannot
+  be used to communicate with processes on the same system.
+
 *FI_REMOTE_READ*
 : Indicates that the user requires an endpoint capable of receiving
   read memory operations from remote endpoints.  This flag requires
@@ -354,45 +344,47 @@ additional optimizations.
   write memory operations from remote endpoints.  This flag requires
   that FI_RMA and/or FI_ATOMIC be set.
 
+*FI_RMA*
+: Specifies that the endpoint should support RMA read and write
+  operations.  Endpoints supporting this capability support operations
+  defined by struct fi_ops_rma.  In the absence of any relevant flags,
+  FI_RMA implies the ability to initiate and be the target of remote
+  memory reads and writes.  Applications can use the FI_READ,
+  FI_WRITE, FI_REMOTE_READ, and FI_REMOTE_WRITE flags to restrict the
+  types of RMA operations supported by an endpoint.
+
 *FI_RMA_EVENT*
 : Requests that an endpoint support the generation of completion events
   when it is the target of an RMA and/or atomic operation.  This
   flag requires that FI_REMOTE_READ and/or FI_REMOTE_WRITE be enabled on
   the endpoint.
 
+*FI_RMA_PMEM*
+: Indicates that the provider is 'persistent memory aware' and supports
+  RMA operations to and from persistent memory.  Persistent memory aware
+  providers must support registration of memory that is backed by non-
+  volatile memory, RMA transfers to/from persistent memory, and enhanced
+  completion semantics.  This flag requires that FI_RMA be set.
+  This capability is experimental.
+
+*FI_SEND*
+: Indicates that the user requires an endpoint capable of sending
+  message data transfers.  Message transfers include base message
+  operations as well as tagged message functionality.
+
 *FI_SHARED_AV*
 : Requests or indicates support for address vectors which may be shared
   among multiple processes.
 
-*FI_TRIGGER*
-: Indicates that the endpoint should support triggered operations.
-  Endpoints support this capability must meet the usage model as
-  described by fi_trigger.3.
-
-*FI_FENCE*
-: Indicates that the endpoint support the FI_FENCE flag on data
-  transfer operations.  Support requires tracking that all previous
-  transmit requests to a specified remote endpoint complete prior
-  to initiating the fenced operation.  Fenced operations are often
-  used to enforce ordering between operations that are not otherwise
-  guaranteed by the underlying provider or protocol.
-
-*FI_LOCAL_COMM*
-: Indicates that the endpoint support host local communication.  This
-  flag may be used in conjunction with FI_REMOTE_COMM to indicate that
-  local and remote communication are required.  If neither FI_LOCAL_COMM
-  or FI_REMOTE_COMM are specified, then the provider will indicate
-  support for the configuration that minimally affects performance.
-  Providers that set FI_LOCAL_COMM but not FI_REMOTE_COMM, for example
-  a shared memory provider, may only be used to communication between
-  processes on the same system.
-
-*FI_REMOTE_COMM*
-: Indicates that the endpoint support communication with endpoints
-  located at remote nodes (across the fabric).  See FI_LOCAL_COMM for
-  additional details.  Providers that set FI_REMOTE_COMM but not
-  FI_LOCAL_COMM, for example NICs that lack loopback support, cannot
-  be used to communicate with processes on the same system.
+*FI_SOURCE*
+: Requests that the endpoint return source addressing data as part of
+  its completion data.  This capability only applies to connectionless
+  endpoints.  Note that returning source address information may
+  require that the provider perform address translation and/or look-up
+  based on data available in the underlying protocol in order to
+  provide the requested data, which may adversely affect performance.
+  The performance impact may be greater for address vectors of type
+  FI_AV_TABLE.
 
 *FI_SOURCE_ERR*
 : Must be paired with FI_SOURCE.  When specified, this requests that
@@ -402,13 +394,20 @@ additional optimizations.
   validate incoming source address data against addresses stored in
   the local address vector, which may adversely affect performance.
 
-*FI_RMA_PMEM*
-: Indicates that the provider is 'persistent memory aware' and supports
-  RMA operations to and from persistent memory.  Persistent memory aware
-  providers must support registration of memory that is backed by non-
-  volatile memory, RMA transfers to/from persistent memory, and enhanced
-  completion semantics.  This flag requires that FI_RMA be set.
-  This capability is experimental.
+*FI_TAGGED*
+: Specifies that the endpoint should handle tagged message transfers.
+  Tagged message transfers associate a user-specified key or tag with
+  each message that is used for matching purposes at the remote side.
+  Endpoints supporting this capability support operations defined by
+  struct fi_ops_tagged.  In the absence of any relevant flags,
+  FI_TAGGED implies the ability to send and receive tagged messages.
+  Applications can use the FI_SEND and FI_RECV flags to optimize an
+  endpoint as send-only or receive-only.
+
+*FI_TRIGGER*
+: Indicates that the endpoint should support triggered operations.
+  Endpoints support this capability must meet the usage model as
+  described by fi_trigger.3.
 
 *FI_VARIABLE_MSG*
 
@@ -420,22 +419,31 @@ additional optimizations.
   are any messages larger than an endpoint configurable size.  This
   flag requires that FI_MSG and/or FI_TAGGED be set.
 
-*FI_HMEM*
-: Specifies that the endpoint should support transfers to and from
-  device memory. 
+*FI_WRITE*
+: Indicates that the user requires an endpoint capable of initiating
+  writes against remote memory regions.  This flag requires that FI_RMA
+  and/or FI_ATOMIC be set.
 
-Capabilities may be grouped into two general categories: primary and
-secondary.  Primary capabilities must explicitly be requested by an
-application, and a provider must enable support for only those primary
-capabilities which were selected.  Secondary capabilities may optionally
-be requested by an application.  If requested, a provider must support
-the capability or fail the fi_getinfo request (FI_ENODATA).  A provider
+Capabilities may be grouped into three general categories: primary,
+secondary, and primary modifiers.  Primary capabilities must explicitly
+be requested by an application, and a provider must enable support for
+only those primary capabilities which were selected.  Primary modifiers
+are used to limit a primary capability, such as restricting an endpoint
+to being send-only.  If no modifiers are specified for an applicable
+capability, all relevant modifiers are assumed.  See above definitions
+for details.
+
+Secondary capabilities may optionally be requested by an application.
+If requested, a provider must support the capability or fail the
+fi_getinfo request (FI_ENODATA).  A provider
 may optionally report non-selected secondary capabilities if doing so
 would not compromise performance or security.
 
 Primary capabilities: FI_MSG, FI_RMA, FI_TAGGED, FI_ATOMIC, FI_MULTICAST,
-FI_NAMED_RX_CTX, FI_DIRECTED_RECV, FI_READ, FI_WRITE, FI_RECV, FI_SEND,
-FI_REMOTE_READ, FI_REMOTE_WRITE, FI_VARIABLE_MSG, FI_HMEM.
+FI_NAMED_RX_CTX, FI_DIRECTED_RECV, FI_VARIABLE_MSG, FI_HMEM
+
+Primary modifiers: FI_READ, FI_WRITE, FI_RECV, FI_SEND,
+FI_REMOTE_READ, FI_REMOTE_WRITE
 
 Secondary capabilities: FI_MULTI_RECV, FI_SOURCE, FI_RMA_EVENT, FI_SHARED_AV,
 FI_TRIGGER, FI_FENCE, FI_LOCAL_COMM, FI_REMOTE_COMM, FI_SOURCE_ERR, FI_RMA_PMEM.
@@ -458,6 +466,30 @@ created using the returned fi_info.  The set of modes are listed
 below.  If a NULL hints structure is provided, then the provider's
 supported set of modes will be returned in the info structure(s).
 
+*FI_ASYNC_IOV*
+: Applications can reference multiple data buffers as part of a single
+  operation through the use of IO vectors (SGEs).  Typically,
+  the contents of an IO vector are copied by the provider into an
+  internal buffer area, or directly to the underlying hardware.
+  However, when a large number of IOV entries are supported,
+  IOV buffering may have a negative impact on performance and memory
+  consumption.  The FI_ASYNC_IOV mode indicates that the application
+  must provide the buffering needed for the IO vectors.  When set,
+  an application must not modify an IO vector of length > 1, including any
+  related memory descriptor array, until the associated
+  operation has completed.
+
+*FI_BUFFERED_RECV*
+: The buffered receive mode bit indicates that the provider owns the
+  data buffer(s) that are accessed by the networking layer for received
+  messages.  Typically, this implies that data must be copied from the
+  provider buffer into the application buffer.  Applications that can
+  handle message processing from network allocated data buffers can set
+  this mode bit to avoid copies.  For full details on application
+  requirements to support this mode, see the 'Buffered Receives' section
+  in [`fi_msg`(3)](fi_msg.3.html).  This mode bit applies to FI_MSG and
+  FI_TAGGED receive operations.
+
 *FI_CONTEXT*
 : Specifies that the provider requires that applications use struct
   fi_context as their per operation context parameter for operations
@@ -534,25 +566,6 @@ supported set of modes will be returned in the info structure(s).
   must be a contiguous region, though it may or may not be directly
   adjacent to the payload portion of the buffer.
 
-*FI_ASYNC_IOV*
-: Applications can reference multiple data buffers as part of a single
-  operation through the use of IO vectors (SGEs).  Typically,
-  the contents of an IO vector are copied by the provider into an
-  internal buffer area, or directly to the underlying hardware.
-  However, when a large number of IOV entries are supported,
-  IOV buffering may have a negative impact on performance and memory
-  consumption.  The FI_ASYNC_IOV mode indicates that the application
-  must provide the buffering needed for the IO vectors.  When set,
-  an application must not modify an IO vector of length > 1, including any
-  related memory descriptor array, until the associated
-  operation has completed.
-
-*FI_RX_CQ_DATA*
-: This mode bit only applies to data transfers that set FI_REMOTE_CQ_DATA.
-  When set, a data transfer that carries remote CQ data will consume a
-  receive buffer at the target.  This is true even for operations that would
-  normally not consume posted receive buffers, such as RMA write operations.
-
 *FI_NOTIFY_FLAGS_ONLY*
 : This bit indicates that general completion flags may not be set by
   the provider, and are not needed by the application.  If specified,
@@ -567,16 +580,11 @@ supported set of modes will be returned in the info structure(s).
   and counters among endpoints, transmit contexts, and receive contexts that
   have the same set of capability flags.
 
-*FI_BUFFERED_RECV*
-: The buffered receive mode bit indicates that the provider owns the
-  data buffer(s) that are accessed by the networking layer for received
-  messages.  Typically, this implies that data must be copied from the
-  provider buffer into the application buffer.  Applications that can
-  handle message processing from network allocated data buffers can set
-  this mode bit to avoid copies.  For full details on application
-  requirements to support this mode, see the 'Buffered Receives' section
-  in [`fi_msg`(3)](fi_msg.3.html).  This mode bit applies to FI_MSG and
-  FI_TAGGED receive operations.
+*FI_RX_CQ_DATA*
+: This mode bit only applies to data transfers that set FI_REMOTE_CQ_DATA.
+  When set, a data transfer that carries remote CQ data will consume a
+  receive buffer at the target.  This is true even for operations that would
+  normally not consume posted receive buffers, such as RMA write operations.
 
 # ADDRESSING FORMATS
 
@@ -591,6 +599,43 @@ formats.  In some cases, a selected addressing format may need to be
 translated or mapped into an address which is native to the
 fabric.  See [`fi_av`(3)](fi_av.3.html).
 
+*FI_ADDR_BGQ*
+: Address is an IBM proprietary format that is used with their Blue Gene Q
+  systems.
+
+*FI_ADDR_EFA*
+: Address is an Amazon Elastic Fabric Adapter (EFA) proprietary format.
+
+*FI_ADDR_GNI*
+: Address is a Cray proprietary format that is used with their GNI
+  protocol.
+
+*FI_ADDR_PSMX*
+: Address is an Intel proprietary format used with their Performance Scaled
+  Messaging protocol.
+
+*FI_ADDR_PSMX2*
+: Address is an Intel proprietary format used with their Performance Scaled
+  Messaging protocol version 2.
+
+*FI_ADDR_STR*
+: Address is a formatted character string.  The length and content of
+  the string is address and/or provider specific, but in general follows
+  a URI model:
+
+```
+address_format[://[node][:[service][/[field3]...][?[key=value][&k2=v2]...]]]
+```
+
+  Examples:
+  - fi_sockaddr://10.31.6.12:7471
+  - fi_sockaddr_in6://[fe80::6:12]:7471
+  - fi_sockaddr://10.31.6.12:7471?qos=3
+
+  Since the string formatted address does not contain any provider
+  information, the prov_name field of the fabric attribute structure should
+  be used to filter by provider if necessary.
+
 *FI_FORMAT_UNSPEC*
 : FI_FORMAT_UNSPEC indicates that a provider specific address format
   should be selected.  Provider specific addresses may be protocol
@@ -606,41 +651,19 @@ fabric.  See [`fi_av`(3)](fi_av.3.html).
   will be determined at run time by interfaces examining the sa_family
   field.
 
+*FI_SOCKADDR_IB*
+: Address is of type sockaddr_ib (defined in Linux kernel source)
+
 *FI_SOCKADDR_IN*
 : Address is of type sockaddr_in (IPv4).
 
 *FI_SOCKADDR_IN6*
 : Address is of type sockaddr_in6 (IPv6).
 
-*FI_SOCKADDR_IB*
-: Address is of type sockaddr_ib (defined in Linux kernel source)
-
 *FI_ADDR_PSMX*
 : Address is an Intel proprietary format that is used with their PSMX
   (extended performance scaled messaging) protocol.
 
-*FI_ADDR_GNI*
-: Address is a Cray proprietary format that is used with their GNI
-  protocol.
-
-*FI_ADDR_STR*
-: Address is a formatted character string.  The length and content of
-  the string is address and/or provider specific, but in general follows
-  a URI model:
-
-```
-address_format[://[node][:[service][/[field3]...][?[key=value][&k2=v2]...]]]
-```
-
-  Examples:
-  - fi_sockaddr://10.31.6.12:7471
-  - fi_sockaddr_in6://[fe80::6:12]:7471
-  - fi_sockaddr://10.31.6.12:7471?qos=3
-
-  Since the string formatted address does not contain any provider
-  information, the prov_name field of the fabric attribute structure should
-  be used to filter by provider if necessary.
-
 # FLAGS
 
 The operation of the fi_getinfo call may be controlled through the use of
@@ -651,12 +674,6 @@ input flags.  Valid flags include the following.
   of a fabric address, such as a dotted decimal IP address.  Use of
   this flag will suppress any lengthy name resolution protocol.
 
-*FI_SOURCE*
-: Indicates that the node and service parameters specify the local
-  source address to associate with an endpoint.  If specified, either
-  the node and/or service parameter must be non-NULL.  This flag is
-  often used with passive endpoints.
-
 *FI_PROV_ATTR_ONLY*
 : Indicates that the caller is only querying for what providers are
   potentially available.  All providers will return exactly one
@@ -666,6 +683,12 @@ input flags.  Valid flags include the following.
   The fabric_attr member will have the prov_name and prov_version
   values filled in.
 
+*FI_SOURCE*
+: Indicates that the node and service parameters specify the local
+  source address to associate with an endpoint.  If specified, either
+  the node and/or service parameter must be non-NULL.  This flag is
+  often used with passive endpoints.
+
 # RETURN VALUE
 
 fi_getinfo() returns 0 on success. On error, fi_getinfo() returns a
@@ -685,13 +708,13 @@ via fi_freeinfo().
 : The specified endpoint or domain capability or operation flags are
   invalid.
 
-*FI_ENOMEM*
-: Indicates that there was insufficient memory to complete the operation.
-
 *FI_ENODATA*
 : Indicates that no providers could be found which support the requested
   fabric information.
 
+*FI_ENOMEM*
+: Indicates that there was insufficient memory to complete the operation.
+
 # NOTES
 
 If hints are provided, the operation will be controlled by the values
diff --git a/man/fi_mr.3.md b/man/fi_mr.3.md
index 54ebc72..8ece6dd 100644
--- a/man/fi_mr.3.md
+++ b/man/fi_mr.3.md
@@ -692,6 +692,17 @@ configure registration caches.
   transfers (such as sending elements of an array to peer(s)), and the larger
   region is access infrequently.  By default merging regions is disabled.
 
+*FI_MR_CACHE_MONITOR*
+: The cache monitor is responsible for detecting changes made between the
+  virtual addresses used by an application and the underlying physical pages.
+  Valid monitor options are: userfaultfd, memhooks, and disabled.  Selecting
+  disabled will turn off the registration cache.  Userfaultfd is a Linux
+  kernel feature used to report virtual to physical address mapping changes
+  to user space.  Memhooks operates by intercepting relevant memory
+  allocation and deallocation calls which may result in the mappings changing,
+  such as malloc, mmap, free, etc.  Note that memhooks operates at the elf
+  linker layer, and does not use glibc memory hooks.
+
 # SEE ALSO
 
 [`fi_getinfo`(3)](fi_getinfo.3.html),
diff --git a/man/fi_provider.7.md b/man/fi_provider.7.md
index 1689a58..7a71340 100644
--- a/man/fi_provider.7.md
+++ b/man/fi_provider.7.md
@@ -64,6 +64,13 @@ This distribution of libfabric contains the following providers
   hardware interface for inter-instance communication on EC2.
   See [`fi_efa`(7)](fi_efa.7.html) for more information.
 
+*SHM*
+: A provider for intranode communication using shared memory.
+  The provider makes use of the Linux kernel feature Cross Memory
+  Attach (CMA) which allows processes to have full access to another
+  process' address space.
+  See [`fi_shm`(7)](fi_shm.7.html) for more information. 
+
 ## Utility providers
 
 *RxM*
@@ -71,6 +78,11 @@ This distribution of libfabric contains the following providers
   endpoints emulated over MSG endpoints of a core provider.
   See [`fi_rxm`(7)](fi_rxm.7.html) for more information.
 
+*RxD*
+: The RxD provider (ofi_rxd) is a utility provider that supports RDM
+  endpoints emulated over DGRAM endpoints of a core provider.
+  See [`fi_rxd`(7)](fi_rxd.7.html) for more information.
+
 ## Special providers
 
 *Hook*
diff --git a/man/fi_shm.7.md b/man/fi_shm.7.md
index 4fa4a5a..3fbc999 100644
--- a/man/fi_shm.7.md
+++ b/man/fi_shm.7.md
@@ -108,8 +108,14 @@ EPs must be bound to both RX and TX CQs.
 No support for counters.
 
 # RUNTIME PARAMETERS
-
-No runtime parameters are currently defined.
+*FI_SHM_DISABLE_CMA*
+: Force disable use of CMA (Cross Memory Attach) in shm environment. CMA is a
+  Linux feature for copying data directly between two processes without the use
+  of intermediate buffering. This requires the processes to have full access to
+  the peer's address space (the same permissions required to perform a ptrace).
+  CMA is enabled by default but checked for availability during run-time.
+  For more information see the CMA [`man pages`]
+  (https://linux.die.net/man/2/process_vm_writev)
 
 # SEE ALSO
 
diff --git a/man/fi_verbs.7.md b/man/fi_verbs.7.md
index 4c239be..27ba370 100644
--- a/man/fi_verbs.7.md
+++ b/man/fi_verbs.7.md
@@ -181,13 +181,21 @@ The verbs provider checks for the following environment variables.
 *FI_VERBS_CQREAD_BUNCH_SIZE*
 : The number of entries to be read from the verbs completion queue at a time (default: 8).
 
+*FI_VERBS_PREFER_XRC*
+: Prioritize XRC transport fi_info before RC transport fi_info (default: 0, RC fi_info will be before XRC fi_info)
+
+*FI_VERBS_GID_IDX*
+: The GID index to use (default: 0)
+
+*FI_VERBS_DEVICE_NAME*
+: Specify a specific verbs device to use by name
+
+### Variables specific to MSG endpoints
+
 *FI_VERBS_IFACE*
 : The prefix or the full name of the network interface associated with the verbs
   device (default: ib)
 
-*FI_VERBS_PREFER_XRC*
-: Prioritize XRC transport fi_info before RC transport fi_info (default: 0, RC fi_info will be before XRC fi_info)
-
 ### Variables specific to DGRAM endpoints
 
 *FI_VERBS_DGRAM_USE_NAME_SERVER*
@@ -198,9 +206,6 @@ The verbs provider checks for the following environment variables.
 *FI_VERBS_NAME_SERVER_PORT*
 : The port on which Name Server thread listens incoming connections and requests (default: 5678)
 
-*FI_VERBS_GID_IDX*
-: The GID index to use (default: 0)
-
 ### Environment variables notes
 The fi_info utility would give the up-to-date information on environment variables:
 fi_info -p verbs -e
diff --git a/man/man3/fi_domain.3 b/man/man3/fi_domain.3
index 24972cf..873135a 100644
--- a/man/man3/fi_domain.3
+++ b/man/man3/fi_domain.3
@@ -1,7 +1,7 @@
 .\"t
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_domain" "3" "2020\-01\-07" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_domain" "3" "2020\-02\-07" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -173,19 +173,37 @@ Applications which can guarantee serialization in their access of
 provider allocated resources and interfaces enables a provider to
 eliminate lower\-level locks.
 .TP
-.B \f[I]FI_THREAD_UNSPEC\f[]
-This value indicates that no threading model has been defined.
-It may be used on input hints to the fi_getinfo call.
-When specified, providers will return a threading model that allows for
-the greatest level of parallelism.
+.B \f[I]FI_THREAD_COMPLETION\f[]
+The completion threading model is intended for providers that make use
+of manual progress.
+Applications must serialize access to all objects that are associated
+through the use of having a shared completion structure.
+This includes endpoint, transmit context, receive context, completion
+queue, counter, wait set, and poll set objects.
 .RS
 .RE
+.PP
+For example, threads must serialize access to an endpoint and its bound
+completion queue(s) and/or counters.
+Access to endpoints that share the same completion queue must also be
+serialized.
+.PP
+The use of FI_THREAD_COMPLETION can increase parallelism over
+FI_THREAD_SAFE, but requires the use of isolated resources.
 .TP
-.B \f[I]FI_THREAD_SAFE\f[]
-A thread safe serialization model allows a multi\-threaded application
-to access any allocated resources through any interface without
-restriction.
-All providers are required to support FI_THREAD_SAFE.
+.B \f[I]FI_THREAD_DOMAIN\f[]
+A domain serialization model requires applications to serialize access
+to all objects belonging to a domain.
+.RS
+.RE
+.TP
+.B \f[I]FI_THREAD_ENDPOINT\f[]
+The endpoint threading model is similar to FI_THREAD_FID, but with the
+added restriction that serialization is required when accessing the same
+endpoint, even if multiple transmit and receive contexts are used.
+Conceptually, FI_THREAD_ENDPOINT maps well to providers that implement
+fabric services in hardware but use a single command queue to access
+different data flows.
 .RS
 .RE
 .TP
@@ -217,37 +235,19 @@ Conceptually, FI_THREAD_FID maps well to providers that implement fabric
 services in hardware and provide separate command queues to different
 data flows.
 .TP
-.B \f[I]FI_THREAD_ENDPOINT\f[]
-The endpoint threading model is similar to FI_THREAD_FID, but with the
-added restriction that serialization is required when accessing the same
-endpoint, even if multiple transmit and receive contexts are used.
-Conceptually, FI_THREAD_ENDPOINT maps well to providers that implement
-fabric services in hardware but use a single command queue to access
-different data flows.
-.RS
-.RE
-.TP
-.B \f[I]FI_THREAD_COMPLETION\f[]
-The completion threading model is intended for providers that make use
-of manual progress.
-Applications must serialize access to all objects that are associated
-through the use of having a shared completion structure.
-This includes endpoint, transmit context, receive context, completion
-queue, counter, wait set, and poll set objects.
+.B \f[I]FI_THREAD_SAFE\f[]
+A thread safe serialization model allows a multi\-threaded application
+to access any allocated resources through any interface without
+restriction.
+All providers are required to support FI_THREAD_SAFE.
 .RS
 .RE
-.PP
-For example, threads must serialize access to an endpoint and its bound
-completion queue(s) and/or counters.
-Access to endpoints that share the same completion queue must also be
-serialized.
-.PP
-The use of FI_THREAD_COMPLETION can increase parallelism over
-FI_THREAD_SAFE, but requires the use of isolated resources.
 .TP
-.B \f[I]FI_THREAD_DOMAIN\f[]
-A domain serialization model requires applications to serialize access
-to all objects belonging to a domain.
+.B \f[I]FI_THREAD_UNSPEC\f[]
+This value indicates that no threading model has been defined.
+It may be used on input hints to the fi_getinfo call.
+When specified, providers will return a threading model that allows for
+the greatest level of parallelism.
 .RS
 .RE
 .SS Progress Models (control_progress / data_progress)
@@ -282,12 +282,6 @@ and acknowledgement processing.
 To balance between performance and ease of use, two progress models are
 defined.
 .TP
-.B \f[I]FI_PROGRESS_UNSPEC\f[]
-This value indicates that no progress model has been defined.
-It may be used on input hints to the fi_getinfo call.
-.RS
-.RE
-.TP
 .B \f[I]FI_PROGRESS_AUTO\f[]
 This progress model indicates that the provider will make forward
 progress on an asynchronous operation without further intervention by
@@ -329,6 +323,12 @@ events for the operations.
 For example, an endpoint that acts purely as the target of RMA or atomic
 operations that uses manual progress may still need application
 assistance to process received operations.
+.TP
+.B \f[I]FI_PROGRESS_UNSPEC\f[]
+This value indicates that no progress model has been defined.
+It may be used on input hints to the fi_getinfo call.
+.RS
+.RE
 .SS Resource Management (resource_mgmt)
 .PP
 Resource management (RM) is provider and protocol support to protect
@@ -352,12 +352,6 @@ protection against overruns.
 However, such protection is not guaranteed.
 The following values for resource management are defined.
 .TP
-.B \f[I]FI_RM_UNSPEC\f[]
-This value indicates that no resource management model has been defined.
-It may be used on input hints to the fi_getinfo call.
-.RS
-.RE
-.TP
 .B \f[I]FI_RM_DISABLED\f[]
 The provider is free to select an implementation and protocol that does
 not protect against resource overruns.
@@ -369,6 +363,12 @@ The application is responsible for resource protection.
 Resource management is enabled for this provider domain.
 .RS
 .RE
+.TP
+.B \f[I]FI_RM_UNSPEC\f[]
+This value indicates that no resource management model has been defined.
+It may be used on input hints to the fi_getinfo call.
+.RS
+.RE
 .PP
 The behavior of the various resource management options depends on
 whether the endpoint is reliable or unreliable, as well as provider and
@@ -604,11 +604,6 @@ Specifies the type of address vectors that are usable with this domain.
 For additional details on AV type, see \f[C]fi_av\f[](3).
 The following values may be specified.
 .TP
-.B \f[I]FI_AV_UNSPEC\f[]
-Any address vector format is requested and supported.
-.RS
-.RE
-.TP
 .B \f[I]FI_AV_MAP\f[]
 Only address vectors of type AV map are requested or supported.
 .RS
@@ -618,6 +613,11 @@ Only address vectors of type AV map are requested or supported.
 Only address vectors of type AV index are requested or supported.
 .RS
 .RE
+.TP
+.B \f[I]FI_AV_UNSPEC\f[]
+Any address vector format is requested and supported.
+.RS
+.RE
 .PP
 Address vectors are only used by connectionless endpoints.
 Applications that require the use of a specific type of address vector
@@ -634,32 +634,30 @@ Defines memory registration specific mode bits used with this domain.
 Full details on MR mode options are available in \f[C]fi_mr\f[](3).
 The following values may be specified.
 .TP
-.B \f[I]FI_MR_LOCAL\f[]
-The provider is optimized around having applications register memory for
-locally accessed data buffers.
-Data buffers used in send and receive operations and as the source
-buffer for RMA and atomic operations must be registered by the
-application for access domains opened with this capability.
+.B \f[I]FI_MR_ALLOCATED\f[]
+Indicates that memory registration occurs on allocated data buffers, and
+physical pages must back all virtual addresses being registered.
 .RS
 .RE
 .TP
-.B \f[I]FI_MR_RAW\f[]
-The provider requires additional setup as part of their memory
-registration process.
-This mode is required by providers that use a memory key that is larger
-than 64\-bits.
+.B \f[I]FI_MR_ENDPOINT\f[]
+Memory registration occurs at the endpoint level, rather than domain.
 .RS
 .RE
 .TP
-.B \f[I]FI_MR_VIRT_ADDR\f[]
-Registered memory regions are referenced by peers using the virtual
-address of the registered memory region, rather than a 0\-based offset.
+.B \f[I]FI_MR_LOCAL\f[]
+The provider is optimized around having applications register memory for
+locally accessed data buffers.
+Data buffers used in send and receive operations and as the source
+buffer for RMA and atomic operations must be registered by the
+application for access domains opened with this capability.
 .RS
 .RE
 .TP
-.B \f[I]FI_MR_ALLOCATED\f[]
-Indicates that memory registration occurs on allocated data buffers, and
-physical pages must back all virtual addresses being registered.
+.B \f[I]FI_MR_MMU_NOTIFY\f[]
+Indicates that the application is responsible for notifying the provider
+when the page tables referencing a registered memory region may have
+been updated.
 .RS
 .RE
 .TP
@@ -668,10 +666,11 @@ Memory registration keys are selected and returned by the provider.
 .RS
 .RE
 .TP
-.B \f[I]FI_MR_MMU_NOTIFY\f[]
-Indicates that the application is responsible for notifying the provider
-when the page tables referencing a registered memory region may have
-been updated.
+.B \f[I]FI_MR_RAW\f[]
+The provider requires additional setup as part of their memory
+registration process.
+This mode is required by providers that use a memory key that is larger
+than 64\-bits.
 .RS
 .RE
 .TP
@@ -681,11 +680,6 @@ must be explicitly enabled after being bound to any counter.
 .RS
 .RE
 .TP
-.B \f[I]FI_MR_ENDPOINT\f[]
-Memory registration occurs at the endpoint level, rather than domain.
-.RS
-.RE
-.TP
 .B \f[I]FI_MR_UNSPEC\f[]
 Defined for compatibility \-\- library versions 1.4 and earlier.
 Setting mr_mode to 0 indicates that FI_MR_BASIC or FI_MR_SCALABLE are
@@ -693,6 +687,12 @@ requested and supported.
 .RS
 .RE
 .TP
+.B \f[I]FI_MR_VIRT_ADDR\f[]
+Registered memory regions are referenced by peers using the virtual
+address of the registered memory region, rather than a 0\-based offset.
+.RS
+.RE
+.TP
 .B \f[I]FI_MR_BASIC\f[]
 Defined for compatibility \-\- library versions 1.4 and earlier.
 Only basic memory registration operations are requested or supported.
diff --git a/man/man3/fi_endpoint.3 b/man/man3/fi_endpoint.3
index 0ea352f..7591e09 100644
--- a/man/man3/fi_endpoint.3
+++ b/man/man3/fi_endpoint.3
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_endpoint" "3" "2019\-10\-08" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_endpoint" "3" "2020\-02\-07" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -356,13 +356,6 @@ This is specified using fi_ep_bind flags.
 The following flags may be OR\[aq]ed together when binding an endpoint
 to a completion domain CQ.
 .TP
-.B \f[I]FI_TRANSMIT\f[]
-Directs the completion of outbound data transfer requests to the
-specified completion queue.
-This includes send message, RMA, and atomic operations.
-.RS
-.RE
-.TP
 .B \f[I]FI_RECV\f[]
 Directs the notification of inbound data transfers to the specified
 completion queue.
@@ -396,6 +389,13 @@ avoid writing a CQ completion entry for every operation.
 .PP
 See Notes section below for additional information on how this flag
 interacts with the FI_CONTEXT and FI_CONTEXT2 mode bits.
+.TP
+.B \f[I]FI_TRANSMIT\f[]
+Directs the completion of outbound data transfer requests to the
+specified completion queue.
+This includes send message, RMA, and atomic operations.
+.RS
+.RE
 .PP
 An endpoint may optionally be bound to a completion counter.
 Associating an endpoint with a counter is in addition to binding the EP
@@ -403,20 +403,6 @@ with a CQ.
 When binding an endpoint to a counter, the following flags may be
 specified.
 .TP
-.B \f[I]FI_SEND\f[]
-Increments the specified counter whenever a message transfer initiated
-over the endpoint has completed successfully or in error.
-Sent messages include both tagged and normal message operations.
-.RS
-.RE
-.TP
-.B \f[I]FI_RECV\f[]
-Increments the specified counter whenever a message is received over the
-endpoint.
-Received messages include both tagged and normal message operations.
-.RS
-.RE
-.TP
 .B \f[I]FI_READ\f[]
 Increments the specified counter whenever an RMA read, atomic fetch, or
 atomic compare operation initiated from the endpoint has completed
@@ -424,10 +410,10 @@ successfully or in error.
 .RS
 .RE
 .TP
-.B \f[I]FI_WRITE\f[]
-Increments the specified counter whenever an RMA write or base atomic
-operation initiated from the endpoint has completed successfully or in
-error.
+.B \f[I]FI_RECV\f[]
+Increments the specified counter whenever a message is received over the
+endpoint.
+Received messages include both tagged and normal message operations.
 .RS
 .RE
 .TP
@@ -448,6 +434,20 @@ Use of this flag requires that the endpoint be created using
 FI_RMA_EVENT.
 .RS
 .RE
+.TP
+.B \f[I]FI_SEND\f[]
+Increments the specified counter whenever a message transfer initiated
+over the endpoint has completed successfully or in error.
+Sent messages include both tagged and normal message operations.
+.RS
+.RE
+.TP
+.B \f[I]FI_WRITE\f[]
+Increments the specified counter whenever an RMA write or base atomic
+operation initiated from the endpoint has completed successfully or in
+error.
+.RS
+.RE
 .PP
 An endpoint may only be bound to a single CQ or counter for a given type
 of operation.
@@ -542,6 +542,13 @@ struct fi_info.
 The following control commands and arguments may be assigned to an
 endpoint.
 .TP
+.B **FI_BACKLOG \- int *value**
+This option only applies to passive endpoints.
+It is used to set the connection request backlog for listening
+endpoints.
+.RS
+.RE
+.TP
 .B **FI_GETOPSFLAG \-\- uint64_t *flags**
 Used to retrieve the current value of flags associated with the data
 transfer operations initiated on the endpoint.
@@ -551,6 +558,17 @@ See below for a list of control flags.
 .RS
 .RE
 .TP
+.B \f[B]FI_GETWAIT \-\- void **\f[]
+This command allows the user to retrieve the file descriptor associated
+with a socket endpoint.
+The fi_control arg parameter should be an address where a pointer to the
+returned file descriptor will be written.
+See fi_eq.3 for addition details using fi_control with FI_GETWAIT.
+The file descriptor may be used for notification that the endpoint is
+ready to send or receive data.
+.RS
+.RE
+.TP
 .B **FI_SETOPSFLAG \-\- uint64_t *flags**
 Used to change the data transfer operation flags associated with an
 endpoint.
@@ -562,24 +580,6 @@ attributes that were set when the endpoint was created.
 Valid control flags are defined below.
 .RS
 .RE
-.TP
-.B **FI_BACKLOG \- int *value**
-This option only applies to passive endpoints.
-It is used to set the connection request backlog for listening
-endpoints.
-.RS
-.RE
-.TP
-.B \f[I]FI_GETWAIT (void **)\f[]
-This command allows the user to retrieve the file descriptor associated
-with a socket endpoint.
-The fi_control arg parameter should be an address where a pointer to the
-returned file descriptor will be written.
-See fi_eq.3 for addition details using fi_control with FI_GETWAIT.
-The file descriptor may be used for notification that the endpoint is
-ready to send or receive data.
-.RS
-.RE
 .SS fi_getopt / fi_setopt
 .PP
 Endpoint protocol operations may be retrieved using fi_getopt or set
@@ -594,35 +594,6 @@ The following option levels and option names and parameters are defined.
 \f[I]FI_OPT_ENDPOINT\f[]
 \[bu] .RS 2
 .TP
-.B \f[I]FI_OPT_MIN_MULTI_RECV \- size_t\f[]
-Defines the minimum receive buffer space available when the receive
-buffer is released by the provider (see FI_MULTI_RECV).
-Modifying this value is only guaranteed to set the minimum buffer space
-needed on receives posted after the value has been changed.
-It is recommended that applications that want to override the default
-MIN_MULTI_RECV value set this option before enabling the corresponding
-endpoint.
-.RS
-.RE
-.RE
-\[bu] .RS 2
-.TP
-.B \f[I]FI_OPT_CM_DATA_SIZE \- size_t\f[]
-Defines the size of available space in CM messages for user\-defined
-data.
-This value limits the amount of data that applications can exchange
-between peer endpoints using the fi_connect, fi_accept, and fi_reject
-operations.
-The size returned is dependent upon the properties of the endpoint,
-except in the case of passive endpoints, in which the size reflects the
-maximum size of the data that may be present as part of a connection
-request event.
-This option is read only.
-.RS
-.RE
-.RE
-\[bu] .RS 2
-.TP
 .B \f[I]FI_OPT_BUFFERED_LIMIT \- size_t\f[]
 Defines the maximum size of a buffered message that will be reported to
 users as part of a receive completion when the FI_BUFFERED_RECV mode is
@@ -659,6 +630,35 @@ latency for the overall transfer of a large message.
 .RS
 .RE
 .RE
+\[bu] .RS 2
+.TP
+.B \f[I]FI_OPT_CM_DATA_SIZE \- size_t\f[]
+Defines the size of available space in CM messages for user\-defined
+data.
+This value limits the amount of data that applications can exchange
+between peer endpoints using the fi_connect, fi_accept, and fi_reject
+operations.
+The size returned is dependent upon the properties of the endpoint,
+except in the case of passive endpoints, in which the size reflects the
+maximum size of the data that may be present as part of a connection
+request event.
+This option is read only.
+.RS
+.RE
+.RE
+\[bu] .RS 2
+.TP
+.B \f[I]FI_OPT_MIN_MULTI_RECV \- size_t\f[]
+Defines the minimum receive buffer space available when the receive
+buffer is released by the provider (see FI_MULTI_RECV).
+Modifying this value is only guaranteed to set the minimum buffer space
+needed on receives posted after the value has been changed.
+It is recommended that applications that want to override the default
+MIN_MULTI_RECV value set this option before enabling the corresponding
+endpoint.
+.RS
+.RE
+.RE
 .SS fi_tc_dscp_set
 .PP
 This call converts a DSCP defined value into a libfabric traffic class
@@ -727,10 +727,11 @@ If specified, indicates the type of fabric interface communication
 desired.
 Supported types are:
 .TP
-.B \f[I]FI_EP_UNSPEC\f[]
-The type of endpoint is not specified.
-This is usually provided as input, with other attributes of the endpoint
-or the provider selecting the type.
+.B \f[I]FI_EP_DGRAM\f[]
+Supports a connectionless, unreliable datagram communication.
+Message boundaries are maintained, but the maximum message size may be
+limited to the fabric MTU.
+Flow control is not guaranteed.
 .RS
 .RE
 .TP
@@ -740,14 +741,6 @@ flow control that maintains message boundaries.
 .RS
 .RE
 .TP
-.B \f[I]FI_EP_DGRAM\f[]
-Supports a connectionless, unreliable datagram communication.
-Message boundaries are maintained, but the maximum message size may be
-limited to the fabric MTU.
-Flow control is not guaranteed.
-.RS
-.RE
-.TP
 .B \f[I]FI_EP_RDM\f[]
 Reliable datagram message.
 Provides a reliable, unconnected data transfer service with flow control
@@ -755,6 +748,16 @@ that maintains message boundaries.
 .RS
 .RE
 .TP
+.B \f[I]FI_EP_SOCK_DGRAM\f[]
+A connectionless, unreliable datagram endpoint with UDP socket\-like
+semantics.
+FI_EP_SOCK_DGRAM is most useful for applications designed around using
+UDP sockets.
+See the SOCKET ENDPOINT section for additional details and restrictions
+that apply to datagram socket endpoints.
+.RS
+.RE
+.TP
 .B \f[I]FI_EP_SOCK_STREAM\f[]
 Data streaming endpoint with TCP socket\-like semantics.
 Provides a reliable, connection\-oriented data transfer service that
@@ -766,13 +769,10 @@ that apply to stream endpoints.
 .RS
 .RE
 .TP
-.B \f[I]FI_EP_SOCK_DGRAM\f[]
-A connectionless, unreliable datagram endpoint with UDP socket\-like
-semantics.
-FI_EP_SOCK_DGRAM is most useful for applications designed around using
-UDP sockets.
-See the SOCKET ENDPOINT section for additional details and restrictions
-that apply to datagram socket endpoints.
+.B \f[I]FI_EP_UNSPEC\f[]
+The type of endpoint is not specified.
+This is usually provided as input, with other attributes of the endpoint
+or the provider selecting the type.
 .RS
 .RE
 .SS Protocol
@@ -785,16 +785,19 @@ Provider specific protocols are also allowed.
 Provider specific protocols will be indicated by having the upper bit of
 the protocol value set to one.
 .TP
-.B \f[I]FI_PROTO_UNSPEC\f[]
-The protocol is not specified.
-This is usually provided as input, with other attributes of the socket
-or the provider selecting the actual protocol.
+.B \f[I]FI_PROTO_GNI\f[]
+Protocol runs over Cray GNI low\-level interface.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_RDMA_CM_IB_RC\f[]
-The protocol runs over Infiniband reliable\-connected queue pairs, using
-the RDMA CM protocol for connection establishment.
+.B \f[I]FI_PROTO_IB_RDM\f[]
+Reliable\-datagram protocol implemented over InfiniBand
+reliable\-connected queue pairs.
+.RS
+.RE
+.TP
+.B \f[I]FI_PROTO_IB_UD\f[]
+The protocol runs over Infiniband unreliable datagram queue pairs.
 .RS
 .RE
 .TP
@@ -803,8 +806,16 @@ The protocol runs over the Internet wide area RDMA protocol transport.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_IB_UD\f[]
-The protocol runs over Infiniband unreliable datagram queue pairs.
+.B \f[I]FI_PROTO_IWARP_RDM\f[]
+Reliable\-datagram protocol implemented over iWarp reliable\-connected
+queue pairs.
+.RS
+.RE
+.TP
+.B \f[I]FI_PROTO_NETWORKDIRECT\f[]
+Protocol runs over Microsoft NetworkDirect service provider interface.
+This adds reliable\-datagram semantics over the NetworkDirect
+connection\- oriented endpoint semantics.
 .RS
 .RE
 .TP
@@ -816,33 +827,24 @@ interfaces.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_UDP\f[]
-The protocol sends and receives UDP datagrams.
-For example, an endpoint using \f[I]FI_PROTO_UDP\f[] will be able to
-communicate with a remote peer that is using Berkeley
-\f[I]SOCK_DGRAM\f[] sockets using \f[I]IPPROTO_UDP\f[].
-.RS
-.RE
-.TP
-.B \f[I]FI_PROTO_SOCK_TCP\f[]
-The protocol is layered over TCP packets.
-.RS
-.RE
-.TP
-.B \f[I]FI_PROTO_IWARP_RDM\f[]
-Reliable\-datagram protocol implemented over iWarp reliable\-connected
-queue pairs.
+.B \f[I]FI_PROTO_PSMX2\f[]
+The protocol is based on an Intel proprietary protocol known as PSM2,
+performance scaled messaging version 2.
+PSMX2 is an extended version of the PSM2 protocol to support the
+libfabric interfaces.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_IB_RDM\f[]
-Reliable\-datagram protocol implemented over InfiniBand
-reliable\-connected queue pairs.
+.B \f[I]FI_PROTO_RDMA_CM_IB_RC\f[]
+The protocol runs over Infiniband reliable\-connected queue pairs, using
+the RDMA CM protocol for connection establishment.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_GNI\f[]
-Protocol runs over Cray GNI low\-level interface.
+.B \f[I]FI_PROTO_RXD\f[]
+Reliable\-datagram protocol implemented over datagram endpoints.
+RXD is a libfabric utility component that adds RDM endpoint semantics
+over DGRAM endpoint semantics.
 .RS
 .RE
 .TP
@@ -853,25 +855,23 @@ over MSG endpoint semantics.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_RXD\f[]
-Reliable\-datagram protocol implemented over datagram endpoints.
-RXD is a libfabric utility component that adds RDM endpoint semantics
-over DGRAM endpoint semantics.
+.B \f[I]FI_PROTO_SOCK_TCP\f[]
+The protocol is layered over TCP packets.
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_NETWORKDIRECT\f[]
-Protocol runs over Microsoft NetworkDirect service provider interface.
-This adds reliable\-datagram semantics over the NetworkDirect
-connection\- oriented endpoint semantics.
+.B \f[I]FI_PROTO_UDP\f[]
+The protocol sends and receives UDP datagrams.
+For example, an endpoint using \f[I]FI_PROTO_UDP\f[] will be able to
+communicate with a remote peer that is using Berkeley
+\f[I]SOCK_DGRAM\f[] sockets using \f[I]IPPROTO_UDP\f[].
 .RS
 .RE
 .TP
-.B \f[I]FI_PROTO_PSMX2\f[]
-The protocol is based on an Intel proprietary protocol known as PSM2,
-performance scaled messaging version 2.
-PSMX2 is an extended version of the PSM2 protocol to support the
-libfabric interfaces.
+.B \f[I]FI_PROTO_UNSPEC\f[]
+The protocol is not specified.
+This is usually provided as input, with other attributes of the socket
+or the provider selecting the actual protocol.
 .RS
 .RE
 .SS protocol_version \- Protocol Version
@@ -1083,8 +1083,21 @@ The requested capabilities of the context.
 The capabilities must be a subset of those requested of the associated
 endpoint.
 See the CAPABILITIES section of fi_getinfo(3) for capability details.
-If the caps field is 0 on input to fi_getinfo(3), the caps value from
-the fi_info structure will be used.
+If the caps field is 0 on input to fi_getinfo(3), the applicable
+capability bits from the fi_info structure will be used.
+.PP
+The following capabilities apply to the transmit attributes: FI_MSG,
+FI_RMA, FI_TAGGED, FI_ATOMIC, FI_READ, FI_WRITE, FI_SEND, FI_HMEM,
+FI_TRIGGER, FI_FENCE, FI_MULTICAST, FI_RMA_PMEM, and FI_NAMED_RX_CTX.
+.PP
+Many applications will be able to ignore this field and rely solely on
+the fi_info::caps field.
+Use of this field provides fine grained control over the transmit
+capabilities associated with an endpoint.
+It is useful when handling scalable endpoints, with multiple transmit
+contexts, for example, and allows configuring a specific transmit
+context with fewer capabilities than that supported by the endpoint or
+other transmit contexts.
 .SS mode
 .PP
 The operational mode bits of the context.
@@ -1124,90 +1137,71 @@ Message ordering requires matching ordering semantics on the receiving
 side of a data transfer operation in order to guarantee that ordering is
 met.
 .TP
-.B \f[I]FI_ORDER_NONE\f[]
-No ordering is specified.
-This value may be used as input in order to obtain the default message
-order supported by the provider.
-FI_ORDER_NONE is an alias for the value 0.
-.RS
-.RE
-.TP
-.B \f[I]FI_ORDER_RAR\f[]
-Read after read.
-If set, RMA and atomic read operations are transmitted in the order
-submitted relative to other RMA and atomic read operations.
-If not set, RMA and atomic reads may be transmitted out of order from
-their submission.
-.RS
-.RE
-.TP
-.B \f[I]FI_ORDER_RAW\f[]
-Read after write.
-If set, RMA and atomic read operations are transmitted in the order
-submitted relative to RMA and atomic write operations.
-If not set, RMA and atomic reads may be transmitted ahead of RMA and
-atomic writes.
+.B \f[I]FI_ORDER_ATOMIC_RAR\f[]
+Atomic read after read.
+If set, atomic fetch operations are transmitted in the order submitted
+relative to other atomic fetch operations.
+If not set, atomic fetches may be transmitted out of order from their
+submission.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_RAS\f[]
-Read after send.
-If set, RMA and atomic read operations are transmitted in the order
-submitted relative to message send operations, including tagged sends.
-If not set, RMA and atomic reads may be transmitted ahead of sends.
+.B \f[I]FI_ORDER_ATOMIC_RAW\f[]
+Atomic read after write.
+If set, atomic fetch operations are transmitted in the order submitted
+relative to atomic update operations.
+If not set, atomic fetches may be transmitted ahead of atomic updates.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_WAR\f[]
-Write after read.
-If set, RMA and atomic write operations are transmitted in the order
-submitted relative to RMA and atomic read operations.
-If not set, RMA and atomic writes may be transmitted ahead of RMA and
-atomic reads.
+.B \f[I]FI_ORDER_ATOMIC_WAR\f[]
+RMA write after read.
+If set, atomic update operations are transmitted in the order submitted
+relative to atomic fetch operations.
+If not set, atomic updates may be transmitted ahead of atomic fetches.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_WAW\f[]
-Write after write.
-If set, RMA and atomic write operations are transmitted in the order
-submitted relative to other RMA and atomic write operations.
-If not set, RMA and atomic writes may be transmitted out of order from
-their submission.
+.B \f[I]FI_ORDER_ATOMIC_WAW\f[]
+RMA write after write.
+If set, atomic update operations are transmitted in the order submitted
+relative to other atomic update operations.
+If not atomic updates may be transmitted out of order from their
+submission.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_WAS\f[]
-Write after send.
-If set, RMA and atomic write operations are transmitted in the order
-submitted relative to message send operations, including tagged sends.
-If not set, RMA and atomic writes may be transmitted ahead of sends.
+.B \f[I]FI_ORDER_NONE\f[]
+No ordering is specified.
+This value may be used as input in order to obtain the default message
+order supported by the provider.
+FI_ORDER_NONE is an alias for the value 0.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_SAR\f[]
-Send after read.
-If set, message send operations, including tagged sends, are transmitted
-in order submitted relative to RMA and atomic read operations.
-If not set, message sends may be transmitted ahead of RMA and atomic
-reads.
+.B \f[I]FI_ORDER_RAR\f[]
+Read after read.
+If set, RMA and atomic read operations are transmitted in the order
+submitted relative to other RMA and atomic read operations.
+If not set, RMA and atomic reads may be transmitted out of order from
+their submission.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_SAW\f[]
-Send after write.
-If set, message send operations, including tagged sends, are transmitted
-in order submitted relative to RMA and atomic write operations.
-If not set, message sends may be transmitted ahead of RMA and atomic
-writes.
+.B \f[I]FI_ORDER_RAS\f[]
+Read after send.
+If set, RMA and atomic read operations are transmitted in the order
+submitted relative to message send operations, including tagged sends.
+If not set, RMA and atomic reads may be transmitted ahead of sends.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_SAS\f[]
-Send after send.
-If set, message send operations, including tagged sends, are transmitted
-in the order submitted relative to other message send.
-If not set, message sends may be transmitted out of order from their
-submission.
+.B \f[I]FI_ORDER_RAW\f[]
+Read after write.
+If set, RMA and atomic read operations are transmitted in the order
+submitted relative to RMA and atomic write operations.
+If not set, RMA and atomic reads may be transmitted ahead of RMA and
+atomic writes.
 .RS
 .RE
 .TP
@@ -1245,37 +1239,56 @@ submission.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_RAR\f[]
-Atomic read after read.
-If set, atomic fetch operations are transmitted in the order submitted
-relative to other atomic fetch operations.
-If not set, atomic fetches may be transmitted out of order from their
+.B \f[I]FI_ORDER_SAR\f[]
+Send after read.
+If set, message send operations, including tagged sends, are transmitted
+in order submitted relative to RMA and atomic read operations.
+If not set, message sends may be transmitted ahead of RMA and atomic
+reads.
+.RS
+.RE
+.TP
+.B \f[I]FI_ORDER_SAS\f[]
+Send after send.
+If set, message send operations, including tagged sends, are transmitted
+in the order submitted relative to other message send.
+If not set, message sends may be transmitted out of order from their
 submission.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_RAW\f[]
-Atomic read after write.
-If set, atomic fetch operations are transmitted in the order submitted
-relative to atomic update operations.
-If not set, atomic fetches may be transmitted ahead of atomic updates.
+.B \f[I]FI_ORDER_SAW\f[]
+Send after write.
+If set, message send operations, including tagged sends, are transmitted
+in order submitted relative to RMA and atomic write operations.
+If not set, message sends may be transmitted ahead of RMA and atomic
+writes.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_WAR\f[]
-RMA write after read.
-If set, atomic update operations are transmitted in the order submitted
-relative to atomic fetch operations.
-If not set, atomic updates may be transmitted ahead of atomic fetches.
+.B \f[I]FI_ORDER_WAR\f[]
+Write after read.
+If set, RMA and atomic write operations are transmitted in the order
+submitted relative to RMA and atomic read operations.
+If not set, RMA and atomic writes may be transmitted ahead of RMA and
+atomic reads.
 .RS
 .RE
 .TP
-.B \f[I]FI_ORDER_ATOMIC_WAW\f[]
-RMA write after write.
-If set, atomic update operations are transmitted in the order submitted
-relative to other atomic update operations.
-If not atomic updates may be transmitted out of order from their
-submission.
+.B \f[I]FI_ORDER_WAS\f[]
+Write after send.
+If set, RMA and atomic write operations are transmitted in the order
+submitted relative to message send operations, including tagged sends.
+If not set, RMA and atomic writes may be transmitted ahead of sends.
+.RS
+.RE
+.TP
+.B \f[I]FI_ORDER_WAW\f[]
+Write after write.
+If set, RMA and atomic write operations are transmitted in the order
+submitted relative to other RMA and atomic write operations.
+If not set, RMA and atomic writes may be transmitted out of order from
+their submission.
 .RS
 .RE
 .SS comp_order \- Completion Ordering
@@ -1365,6 +1378,21 @@ distributed fairly across the applications.
 .RS
 .RE
 .TP
+.B \f[I]FI_TC_BULK_DATA\f[]
+This class is intended for large data transfers associated with I/O and
+is present to separate sustained I/O transfers from other application
+inter\-process communications.
+.RS
+.RE
+.TP
+.B \f[I]FI_TC_DEDICATED_ACCESS\f[]
+This class operates at the highest priority, except the management
+class.
+It carries a high bandwidth allocation, minimum latency targets, and the
+highest scheduling and arbitration priority.
+.RS
+.RE
+.TP
 .B \f[I]FI_TC_LOW_LATENCY\f[]
 This class supports low latency, low jitter data patterns typically
 caused by transactional data exchanges, barrier synchronizations, and
@@ -1377,18 +1405,11 @@ excessive bandwidth at high priority.
 .RS
 .RE
 .TP
-.B \f[I]FI_TC_DEDICATED_ACCESS\f[]
-This class operates at the highest priority, except the management
-class.
-It carries a high bandwidth allocation, minimum latency targets, and the
-highest scheduling and arbitration priority.
-.RS
-.RE
-.TP
-.B \f[I]FI_TC_BULK_DATA\f[]
-This class is intended for large data transfers associated with I/O and
-is present to separate sustained I/O transfers from other application
-inter\-process communications.
+.B \f[I]FI_TC_NETWORK_CTRL\f[]
+This class is intended for traffic directly related to fabric (network)
+management, which is critical to the correct operation of the network.
+Its use is typically restricted to privileged network management
+applications.
 .RS
 .RE
 .TP
@@ -1401,14 +1422,6 @@ priority and should not interfere with higher priority workflows.
 .RS
 .RE
 .TP
-.B \f[I]FI_TC_NETWORK_CTRL\f[]
-This class is intended for traffic directly related to fabric (network)
-management, which is critical to the correct operation of the network.
-Its use is typically restricted to privileged network management
-applications.
-.RS
-.RE
-.TP
 .B \f[I]fi_tc_dscp_set / fi_tc_dscp_get\f[]
 DSCP values are supported via the DSCP get and set functions.
 The definitions for DSCP values are outside the scope of libfabric.
@@ -1441,8 +1454,22 @@ The requested capabilities of the context.
 The capabilities must be a subset of those requested of the associated
 endpoint.
 See the CAPABILITIES section if fi_getinfo(3) for capability details.
-If the caps field is 0 on input to fi_getinfo(3), the caps value from
-the fi_info structure will be used.
+If the caps field is 0 on input to fi_getinfo(3), the applicable
+capability bits from the fi_info structure will be used.
+.PP
+The following capabilities apply to the receive attributes: FI_MSG,
+FI_RMA, FI_TAGGED, FI_ATOMIC, FI_REMOTE_READ, FI_REMOTE_WRITE, FI_RECV,
+FI_HMEM, FI_TRIGGER, FI_RMA_PMEM, FI_DIRECTED_RECV, FI_VARIABLE_MSG,
+FI_MULTI_RECV, FI_SOURCE, FI_RMA_EVENT, and FI_SOURCE_ERROR.
+.PP
+Many applications will be able to ignore this field and rely solely on
+the fi_info::caps field.
+Use of this field provides fine grained control over the receive
+capabilities associated with an endpoint.
+It is useful when handling scalable endpoints, with multiple receive
+contexts, for example, and allows configuring a specific receive context
+with fewer capabilities than that supported by the endpoint or other
+receive contexts.
 .SS mode
 .PP
 The operational mode bits of the context.
@@ -1481,6 +1508,14 @@ FI_ORDER_ATOMIC_WAW.
 For a description of completion ordering, see the comp_order field in
 the \f[I]Transmit Context Attribute\f[] section.
 .TP
+.B \f[I]FI_ORDER_DATA\f[]
+When set, this bit indicates that received data is written into memory
+in order.
+Data ordering applies to memory accessed as part of a single operation
+and between operations if message ordering is guaranteed.
+.RS
+.RE
+.TP
 .B \f[I]FI_ORDER_NONE\f[]
 No ordering is defined for completed operations.
 Receive operations may complete in any order, regardless of their
@@ -1493,14 +1528,6 @@ Receive operations complete in the order in which they are processed by
 the receive context, based on the receive side msg_order attribute.
 .RS
 .RE
-.TP
-.B \f[I]FI_ORDER_DATA\f[]
-When set, this bit indicates that received data is written into memory
-in order.
-Data ordering applies to memory accessed as part of a single operation
-and between operations if message ordering is guaranteed.
-.RS
-.RE
 .SS total_buffered_recv
 .PP
 This field is supported for backwards compatibility purposes.
@@ -1723,6 +1750,30 @@ data transfer operations, where a flags parameter is not available.
 Data transfer operations that take flags as input override the op_flags
 value of transmit or receive context attributes of an endpoint.
 .TP
+.B \f[I]FI_COMMIT_COMPLETE\f[]
+Indicates that a completion should not be generated (locally or at the
+peer) until the result of an operation have been made persistent.
+See \f[C]fi_cq\f[](3) for additional details on completion semantics.
+.RS
+.RE
+.TP
+.B \f[I]FI_COMPLETION\f[]
+Indicates that a completion queue entry should be written for data
+transfer operations.
+This flag only applies to operations issued on an endpoint that was
+bound to a completion queue with the FI_SELECTIVE_COMPLETION flag set,
+otherwise, it is ignored.
+See the fi_ep_bind section above for more detail.
+.RS
+.RE
+.TP
+.B \f[I]FI_DELIVERY_COMPLETE\f[]
+Indicates that a completion should be generated when the operation has
+been processed by the destination endpoint(s).
+See \f[C]fi_cq\f[](3) for additional details on completion semantics.
+.RS
+.RE
+.TP
 .B \f[I]FI_INJECT\f[]
 Indicates that all outbound data buffers should be returned to the
 user\[aq]s control immediately after a data transfer call returns, even
@@ -1735,6 +1786,21 @@ This limit is indicated using inject_size (see inject_size above).
 .RS
 .RE
 .TP
+.B \f[I]FI_INJECT_COMPLETE\f[]
+Indicates that a completion should be generated when the source
+buffer(s) may be reused.
+See \f[C]fi_cq\f[](3) for additional details on completion semantics.
+.RS
+.RE
+.TP
+.B \f[I]FI_MULTICAST\f[]
+Indicates that data transfers will target multicast addresses by
+default.
+Any fi_addr_t passed into a data transfer operation will be treated as a
+multicast address.
+.RS
+.RE
+.TP
 .B \f[I]FI_MULTI_RECV\f[]
 Applies to posted receive operations.
 This flag allows the user to post a single buffer that will receive
@@ -1750,51 +1816,12 @@ space falls below the specified minimum (see FI_OPT_MIN_MULTI_RECV).
 .RS
 .RE
 .TP
-.B \f[I]FI_COMPLETION\f[]
-Indicates that a completion queue entry should be written for data
-transfer operations.
-This flag only applies to operations issued on an endpoint that was
-bound to a completion queue with the FI_SELECTIVE_COMPLETION flag set,
-otherwise, it is ignored.
-See the fi_ep_bind section above for more detail.
-.RS
-.RE
-.TP
-.B \f[I]FI_INJECT_COMPLETE\f[]
-Indicates that a completion should be generated when the source
-buffer(s) may be reused.
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
-.TP
 .B \f[I]FI_TRANSMIT_COMPLETE\f[]
 Indicates that a completion should be generated when the transmit
 operation has completed relative to the local provider.
 See \f[C]fi_cq\f[](3) for additional details on completion semantics.
 .RS
 .RE
-.TP
-.B \f[I]FI_DELIVERY_COMPLETE\f[]
-Indicates that a completion should be generated when the operation has
-been processed by the destination endpoint(s).
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
-.TP
-.B \f[I]FI_COMMIT_COMPLETE\f[]
-Indicates that a completion should not be generated (locally or at the
-peer) until the result of an operation have been made persistent.
-See \f[C]fi_cq\f[](3) for additional details on completion semantics.
-.RS
-.RE
-.TP
-.B \f[I]FI_MULTICAST\f[]
-Indicates that data transfers will target multicast addresses by
-default.
-Any fi_addr_t passed into a data transfer operation will be treated as a
-multicast address.
-.RS
-.RE
 .SH NOTES
 .PP
 Users should call fi_close to release all resources allocated to the
diff --git a/man/man3/fi_getinfo.3 b/man/man3/fi_getinfo.3
index fafc690..4d457e8 100644
--- a/man/man3/fi_getinfo.3
+++ b/man/man3/fi_getinfo.3
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_getinfo" "3" "2019\-09\-25" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_getinfo" "3" "2020\-02\-07" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -325,80 +325,78 @@ Applications may use this feature to request a minimal set of
 requirements, then check the returned capabilities to enable additional
 optimizations.
 .TP
-.B \f[I]FI_MSG\f[]
-Specifies that an endpoint should support sending and receiving messages
-or datagrams.
-Message capabilities imply support for send and/or receive queues.
+.B \f[I]FI_ATOMIC\f[]
+Specifies that the endpoint supports some set of atomic operations.
 Endpoints supporting this capability support operations defined by
-struct fi_ops_msg.
+struct fi_ops_atomic.
+In the absence of any relevant flags, FI_ATOMIC implies the ability to
+initiate and be the target of remote atomic reads and writes.
+Applications can use the FI_READ, FI_WRITE, FI_REMOTE_READ, and
+FI_REMOTE_WRITE flags to restrict the types of atomic operations
+supported by an endpoint.
 .RS
 .RE
-.PP
-The caps may be used to specify or restrict the type of messaging
-operations that are supported.
-In the absence of any relevant flags, FI_MSG implies the ability to send
-and receive messages.
-Applications can use the FI_SEND and FI_RECV flags to optimize an
-endpoint as send\-only or receive\-only.
 .TP
-.B \f[I]FI_RMA\f[]
-Specifies that the endpoint should support RMA read and write
-operations.
-Endpoints supporting this capability support operations defined by
-struct fi_ops_rma.
-In the absence of any relevant flags, FI_RMA implies the ability to
-initiate and be the target of remote memory reads and writes.
-Applications can use the FI_READ, FI_WRITE, FI_REMOTE_READ, and
-FI_REMOTE_WRITE flags to restrict the types of RMA operations supported
-by an endpoint.
+.B \f[I]FI_DIRECTED_RECV\f[]
+Requests that the communication endpoint use the source address of an
+incoming message when matching it with a receive buffer.
+If this capability is not set, then the src_addr parameter for msg and
+tagged receive operations is ignored.
 .RS
 .RE
 .TP
-.B \f[I]FI_TAGGED\f[]
-Specifies that the endpoint should handle tagged message transfers.
-Tagged message transfers associate a user\-specified key or tag with
-each message that is used for matching purposes at the remote side.
-Endpoints supporting this capability support operations defined by
-struct fi_ops_tagged.
-In the absence of any relevant flags, FI_TAGGED implies the ability to
-send and receive tagged messages.
-Applications can use the FI_SEND and FI_RECV flags to optimize an
-endpoint as send\-only or receive\-only.
+.B \f[I]FI_FENCE\f[]
+Indicates that the endpoint support the FI_FENCE flag on data transfer
+operations.
+Support requires tracking that all previous transmit requests to a
+specified remote endpoint complete prior to initiating the fenced
+operation.
+Fenced operations are often used to enforce ordering between operations
+that are not otherwise guaranteed by the underlying provider or
+protocol.
 .RS
 .RE
 .TP
-.B \f[I]FI_ATOMIC\f[]
-Specifies that the endpoint supports some set of atomic operations.
-Endpoints supporting this capability support operations defined by
-struct fi_ops_atomic.
-In the absence of any relevant flags, FI_ATOMIC implies the ability to
-initiate and be the target of remote atomic reads and writes.
-Applications can use the FI_READ, FI_WRITE, FI_REMOTE_READ, and
-FI_REMOTE_WRITE flags to restrict the types of atomic operations
-supported by an endpoint.
+.B \f[I]FI_HMEM\f[]
+Specifies that the endpoint should support transfers to and from device
+memory.
 .RS
 .RE
 .TP
-.B \f[I]FI_MULTICAST\f[]
-Indicates that the endpoint support multicast data transfers.
-This capability must be paired with at least one other data transfer
-capability, (e.g.
-FI_MSG, FI_SEND, FI_RECV, ...).
+.B \f[I]FI_LOCAL_COMM\f[]
+Indicates that the endpoint support host local communication.
+This flag may be used in conjunction with FI_REMOTE_COMM to indicate
+that local and remote communication are required.
+If neither FI_LOCAL_COMM or FI_REMOTE_COMM are specified, then the
+provider will indicate support for the configuration that minimally
+affects performance.
+Providers that set FI_LOCAL_COMM but not FI_REMOTE_COMM, for example a
+shared memory provider, may only be used to communication between
+processes on the same system.
 .RS
 .RE
 .TP
-.B \f[I]FI_NAMED_RX_CTX\f[]
-Requests that endpoints which support multiple receive contexts allow an
-initiator to target (or name) a specific receive context as part of a
-data transfer operation.
+.B \f[I]FI_MSG\f[]
+Specifies that an endpoint should support sending and receiving messages
+or datagrams.
+Message capabilities imply support for send and/or receive queues.
+Endpoints supporting this capability support operations defined by
+struct fi_ops_msg.
 .RS
 .RE
+.PP
+The caps may be used to specify or restrict the type of messaging
+operations that are supported.
+In the absence of any relevant flags, FI_MSG implies the ability to send
+and receive messages.
+Applications can use the FI_SEND and FI_RECV flags to optimize an
+endpoint as send\-only or receive\-only.
 .TP
-.B \f[I]FI_DIRECTED_RECV\f[]
-Requests that the communication endpoint use the source address of an
-incoming message when matching it with a receive buffer.
-If this capability is not set, then the src_addr parameter for msg and
-tagged receive operations is ignored.
+.B \f[I]FI_MULTICAST\f[]
+Indicates that the endpoint support multicast data transfers.
+This capability must be paired with FI_MSG.
+Aplications can use FI_SEND and FI_RECV to optimize multicast as
+send\-only or receive\-only.
 .RS
 .RE
 .TP
@@ -408,16 +406,10 @@ posting receive buffers.
 .RS
 .RE
 .TP
-.B \f[I]FI_SOURCE\f[]
-Requests that the endpoint return source addressing data as part of its
-completion data.
-This capability only applies to connectionless endpoints.
-Note that returning source address information may require that the
-provider perform address translation and/or look\-up based on data
-available in the underlying protocol in order to provide the requested
-data, which may adversely affect performance.
-The performance impact may be greater for address vectors of type
-FI_AV_TABLE.
+.B \f[I]FI_NAMED_RX_CTX\f[]
+Requests that endpoints which support multiple receive contexts allow an
+initiator to target (or name) a specific receive context as part of a
+data transfer operation.
 .RS
 .RE
 .TP
@@ -428,21 +420,6 @@ This flag requires that FI_RMA and/or FI_ATOMIC be set.
 .RS
 .RE
 .TP
-.B \f[I]FI_WRITE\f[]
-Indicates that the user requires an endpoint capable of initiating
-writes against remote memory regions.
-This flag requires that FI_RMA and/or FI_ATOMIC be set.
-.RS
-.RE
-.TP
-.B \f[I]FI_SEND\f[]
-Indicates that the user requires an endpoint capable of sending message
-data transfers.
-Message transfers include base message operations as well as tagged
-message functionality.
-.RS
-.RE
-.TP
 .B \f[I]FI_RECV\f[]
 Indicates that the user requires an endpoint capable of receiving
 message data transfers.
@@ -451,6 +428,16 @@ message functionality.
 .RS
 .RE
 .TP
+.B \f[I]FI_REMOTE_COMM\f[]
+Indicates that the endpoint support communication with endpoints located
+at remote nodes (across the fabric).
+See FI_LOCAL_COMM for additional details.
+Providers that set FI_REMOTE_COMM but not FI_LOCAL_COMM, for example
+NICs that lack loopback support, cannot be used to communicate with
+processes on the same system.
+.RS
+.RE
+.TP
 .B \f[I]FI_REMOTE_READ\f[]
 Indicates that the user requires an endpoint capable of receiving read
 memory operations from remote endpoints.
@@ -465,6 +452,19 @@ This flag requires that FI_RMA and/or FI_ATOMIC be set.
 .RS
 .RE
 .TP
+.B \f[I]FI_RMA\f[]
+Specifies that the endpoint should support RMA read and write
+operations.
+Endpoints supporting this capability support operations defined by
+struct fi_ops_rma.
+In the absence of any relevant flags, FI_RMA implies the ability to
+initiate and be the target of remote memory reads and writes.
+Applications can use the FI_READ, FI_WRITE, FI_REMOTE_READ, and
+FI_REMOTE_WRITE flags to restrict the types of RMA operations supported
+by an endpoint.
+.RS
+.RE
+.TP
 .B \f[I]FI_RMA_EVENT\f[]
 Requests that an endpoint support the generation of completion events
 when it is the target of an RMA and/or atomic operation.
@@ -473,51 +473,41 @@ on the endpoint.
 .RS
 .RE
 .TP
-.B \f[I]FI_SHARED_AV\f[]
-Requests or indicates support for address vectors which may be shared
-among multiple processes.
-.RS
-.RE
-.TP
-.B \f[I]FI_TRIGGER\f[]
-Indicates that the endpoint should support triggered operations.
-Endpoints support this capability must meet the usage model as described
-by fi_trigger.3.
+.B \f[I]FI_RMA_PMEM\f[]
+Indicates that the provider is \[aq]persistent memory aware\[aq] and
+supports RMA operations to and from persistent memory.
+Persistent memory aware providers must support registration of memory
+that is backed by non\- volatile memory, RMA transfers to/from
+persistent memory, and enhanced completion semantics.
+This flag requires that FI_RMA be set.
+This capability is experimental.
 .RS
 .RE
 .TP
-.B \f[I]FI_FENCE\f[]
-Indicates that the endpoint support the FI_FENCE flag on data transfer
-operations.
-Support requires tracking that all previous transmit requests to a
-specified remote endpoint complete prior to initiating the fenced
-operation.
-Fenced operations are often used to enforce ordering between operations
-that are not otherwise guaranteed by the underlying provider or
-protocol.
+.B \f[I]FI_SEND\f[]
+Indicates that the user requires an endpoint capable of sending message
+data transfers.
+Message transfers include base message operations as well as tagged
+message functionality.
 .RS
 .RE
 .TP
-.B \f[I]FI_LOCAL_COMM\f[]
-Indicates that the endpoint support host local communication.
-This flag may be used in conjunction with FI_REMOTE_COMM to indicate
-that local and remote communication are required.
-If neither FI_LOCAL_COMM or FI_REMOTE_COMM are specified, then the
-provider will indicate support for the configuration that minimally
-affects performance.
-Providers that set FI_LOCAL_COMM but not FI_REMOTE_COMM, for example a
-shared memory provider, may only be used to communication between
-processes on the same system.
+.B \f[I]FI_SHARED_AV\f[]
+Requests or indicates support for address vectors which may be shared
+among multiple processes.
 .RS
 .RE
 .TP
-.B \f[I]FI_REMOTE_COMM\f[]
-Indicates that the endpoint support communication with endpoints located
-at remote nodes (across the fabric).
-See FI_LOCAL_COMM for additional details.
-Providers that set FI_REMOTE_COMM but not FI_LOCAL_COMM, for example
-NICs that lack loopback support, cannot be used to communicate with
-processes on the same system.
+.B \f[I]FI_SOURCE\f[]
+Requests that the endpoint return source addressing data as part of its
+completion data.
+This capability only applies to connectionless endpoints.
+Note that returning source address information may require that the
+provider perform address translation and/or look\-up based on data
+available in the underlying protocol in order to provide the requested
+data, which may adversely affect performance.
+The performance impact may be greater for address vectors of type
+FI_AV_TABLE.
 .RS
 .RE
 .TP
@@ -532,14 +522,23 @@ vector, which may adversely affect performance.
 .RS
 .RE
 .TP
-.B \f[I]FI_RMA_PMEM\f[]
-Indicates that the provider is \[aq]persistent memory aware\[aq] and
-supports RMA operations to and from persistent memory.
-Persistent memory aware providers must support registration of memory
-that is backed by non\- volatile memory, RMA transfers to/from
-persistent memory, and enhanced completion semantics.
-This flag requires that FI_RMA be set.
-This capability is experimental.
+.B \f[I]FI_TAGGED\f[]
+Specifies that the endpoint should handle tagged message transfers.
+Tagged message transfers associate a user\-specified key or tag with
+each message that is used for matching purposes at the remote side.
+Endpoints supporting this capability support operations defined by
+struct fi_ops_tagged.
+In the absence of any relevant flags, FI_TAGGED implies the ability to
+send and receive tagged messages.
+Applications can use the FI_SEND and FI_RECV flags to optimize an
+endpoint as send\-only or receive\-only.
+.RS
+.RE
+.TP
+.B \f[I]FI_TRIGGER\f[]
+Indicates that the endpoint should support triggered operations.
+Endpoints support this capability must meet the usage model as described
+by fi_trigger.3.
 .RS
 .RE
 .TP
@@ -555,17 +554,24 @@ This flag requires that FI_MSG and/or FI_TAGGED be set.
 .RS
 .RE
 .TP
-.B \f[I]FI_HMEM\f[]
-Specifies that the endpoint should support transfers to and from device
-memory.
+.B \f[I]FI_WRITE\f[]
+Indicates that the user requires an endpoint capable of initiating
+writes against remote memory regions.
+This flag requires that FI_RMA and/or FI_ATOMIC be set.
 .RS
 .RE
 .PP
-Capabilities may be grouped into two general categories: primary and
-secondary.
+Capabilities may be grouped into three general categories: primary,
+secondary, and primary modifiers.
 Primary capabilities must explicitly be requested by an application, and
 a provider must enable support for only those primary capabilities which
 were selected.
+Primary modifiers are used to limit a primary capability, such as
+restricting an endpoint to being send\-only.
+If no modifiers are specified for an applicable capability, all relevant
+modifiers are assumed.
+See above definitions for details.
+.PP
 Secondary capabilities may optionally be requested by an application.
 If requested, a provider must support the capability or fail the
 fi_getinfo request (FI_ENODATA).
@@ -573,9 +579,11 @@ A provider may optionally report non\-selected secondary capabilities if
 doing so would not compromise performance or security.
 .PP
 Primary capabilities: FI_MSG, FI_RMA, FI_TAGGED, FI_ATOMIC,
-FI_MULTICAST, FI_NAMED_RX_CTX, FI_DIRECTED_RECV, FI_READ, FI_WRITE,
-FI_RECV, FI_SEND, FI_REMOTE_READ, FI_REMOTE_WRITE, FI_VARIABLE_MSG,
-FI_HMEM.
+FI_MULTICAST, FI_NAMED_RX_CTX, FI_DIRECTED_RECV, FI_VARIABLE_MSG,
+FI_HMEM
+.PP
+Primary modifiers: FI_READ, FI_WRITE, FI_RECV, FI_SEND, FI_REMOTE_READ,
+FI_REMOTE_WRITE
 .PP
 Secondary capabilities: FI_MULTI_RECV, FI_SOURCE, FI_RMA_EVENT,
 FI_SHARED_AV, FI_TRIGGER, FI_FENCE, FI_LOCAL_COMM, FI_REMOTE_COMM,
@@ -600,6 +608,35 @@ The set of modes are listed below.
 If a NULL hints structure is provided, then the provider\[aq]s supported
 set of modes will be returned in the info structure(s).
 .TP
+.B \f[I]FI_ASYNC_IOV\f[]
+Applications can reference multiple data buffers as part of a single
+operation through the use of IO vectors (SGEs).
+Typically, the contents of an IO vector are copied by the provider into
+an internal buffer area, or directly to the underlying hardware.
+However, when a large number of IOV entries are supported, IOV buffering
+may have a negative impact on performance and memory consumption.
+The FI_ASYNC_IOV mode indicates that the application must provide the
+buffering needed for the IO vectors.
+When set, an application must not modify an IO vector of length > 1,
+including any related memory descriptor array, until the associated
+operation has completed.
+.RS
+.RE
+.TP
+.B \f[I]FI_BUFFERED_RECV\f[]
+The buffered receive mode bit indicates that the provider owns the data
+buffer(s) that are accessed by the networking layer for received
+messages.
+Typically, this implies that data must be copied from the provider
+buffer into the application buffer.
+Applications that can handle message processing from network allocated
+data buffers can set this mode bit to avoid copies.
+For full details on application requirements to support this mode, see
+the \[aq]Buffered Receives\[aq] section in \f[C]fi_msg\f[](3).
+This mode bit applies to FI_MSG and FI_TAGGED receive operations.
+.RS
+.RE
+.TP
 .B \f[I]FI_CONTEXT\f[]
 Specifies that the provider requires that applications use struct
 fi_context as their per operation context parameter for operations that
@@ -692,30 +729,6 @@ For scatter\-gather send/recv operations, the prefix buffer must be a
 contiguous region, though it may or may not be directly adjacent to the
 payload portion of the buffer.
 .TP
-.B \f[I]FI_ASYNC_IOV\f[]
-Applications can reference multiple data buffers as part of a single
-operation through the use of IO vectors (SGEs).
-Typically, the contents of an IO vector are copied by the provider into
-an internal buffer area, or directly to the underlying hardware.
-However, when a large number of IOV entries are supported, IOV buffering
-may have a negative impact on performance and memory consumption.
-The FI_ASYNC_IOV mode indicates that the application must provide the
-buffering needed for the IO vectors.
-When set, an application must not modify an IO vector of length > 1,
-including any related memory descriptor array, until the associated
-operation has completed.
-.RS
-.RE
-.TP
-.B \f[I]FI_RX_CQ_DATA\f[]
-This mode bit only applies to data transfers that set FI_REMOTE_CQ_DATA.
-When set, a data transfer that carries remote CQ data will consume a
-receive buffer at the target.
-This is true even for operations that would normally not consume posted
-receive buffers, such as RMA write operations.
-.RS
-.RE
-.TP
 .B \f[I]FI_NOTIFY_FLAGS_ONLY\f[]
 This bit indicates that general completion flags may not be set by the
 provider, and are not needed by the application.
@@ -736,17 +749,12 @@ contexts that have the same set of capability flags.
 .RS
 .RE
 .TP
-.B \f[I]FI_BUFFERED_RECV\f[]
-The buffered receive mode bit indicates that the provider owns the data
-buffer(s) that are accessed by the networking layer for received
-messages.
-Typically, this implies that data must be copied from the provider
-buffer into the application buffer.
-Applications that can handle message processing from network allocated
-data buffers can set this mode bit to avoid copies.
-For full details on application requirements to support this mode, see
-the \[aq]Buffered Receives\[aq] section in \f[C]fi_msg\f[](3).
-This mode bit applies to FI_MSG and FI_TAGGED receive operations.
+.B \f[I]FI_RX_CQ_DATA\f[]
+This mode bit only applies to data transfers that set FI_REMOTE_CQ_DATA.
+When set, a data transfer that carries remote CQ data will consume a
+receive buffer at the target.
+This is true even for operations that would normally not consume posted
+receive buffers, such as RMA write operations.
 .RS
 .RE
 .SH ADDRESSING FORMATS
@@ -764,51 +772,32 @@ In some cases, a selected addressing format may need to be translated or
 mapped into an address which is native to the fabric.
 See \f[C]fi_av\f[](3).
 .TP
-.B \f[I]FI_FORMAT_UNSPEC\f[]
-FI_FORMAT_UNSPEC indicates that a provider specific address format
-should be selected.
-Provider specific addresses may be protocol specific or a vendor
-proprietary format.
-Applications that select FI_FORMAT_UNSPEC should be prepared to treat
-returned addressing data as opaque.
-FI_FORMAT_UNSPEC targets apps which make use of an out of band address
-exchange.
-Applications which use FI_FORMAT_UNSPEC may use fi_getname() to obtain a
-provider specific address assigned to an allocated endpoint.
+.B \f[I]FI_ADDR_BGQ\f[]
+Address is an IBM proprietary format that is used with their Blue Gene Q
+systems.
 .RS
 .RE
 .TP
-.B \f[I]FI_SOCKADDR\f[]
-Address is of type sockaddr.
-The specific socket address format will be determined at run time by
-interfaces examining the sa_family field.
-.RS
-.RE
-.TP
-.B \f[I]FI_SOCKADDR_IN\f[]
-Address is of type sockaddr_in (IPv4).
+.B \f[I]FI_ADDR_EFA\f[]
+Address is an Amazon Elastic Fabric Adapter (EFA) proprietary format.
 .RS
 .RE
 .TP
-.B \f[I]FI_SOCKADDR_IN6\f[]
-Address is of type sockaddr_in6 (IPv6).
-.RS
-.RE
-.TP
-.B \f[I]FI_SOCKADDR_IB\f[]
-Address is of type sockaddr_ib (defined in Linux kernel source)
+.B \f[I]FI_ADDR_GNI\f[]
+Address is a Cray proprietary format that is used with their GNI
+protocol.
 .RS
 .RE
 .TP
 .B \f[I]FI_ADDR_PSMX\f[]
-Address is an Intel proprietary format that is used with their PSMX
-(extended performance scaled messaging) protocol.
+Address is an Intel proprietary format used with their Performance
+Scaled Messaging protocol.
 .RS
 .RE
 .TP
-.B \f[I]FI_ADDR_GNI\f[]
-Address is a Cray proprietary format that is used with their GNI
-protocol.
+.B \f[I]FI_ADDR_PSMX2\f[]
+Address is an Intel proprietary format used with their Performance
+Scaled Messaging protocol version 2.
 .RS
 .RE
 .TP
@@ -832,6 +821,48 @@ fi_sockaddr://10.31.6.12:7471?qos=3
 Since the string formatted address does not contain any provider
 information, the prov_name field of the fabric attribute structure
 should be used to filter by provider if necessary.
+.TP
+.B \f[I]FI_FORMAT_UNSPEC\f[]
+FI_FORMAT_UNSPEC indicates that a provider specific address format
+should be selected.
+Provider specific addresses may be protocol specific or a vendor
+proprietary format.
+Applications that select FI_FORMAT_UNSPEC should be prepared to treat
+returned addressing data as opaque.
+FI_FORMAT_UNSPEC targets apps which make use of an out of band address
+exchange.
+Applications which use FI_FORMAT_UNSPEC may use fi_getname() to obtain a
+provider specific address assigned to an allocated endpoint.
+.RS
+.RE
+.TP
+.B \f[I]FI_SOCKADDR\f[]
+Address is of type sockaddr.
+The specific socket address format will be determined at run time by
+interfaces examining the sa_family field.
+.RS
+.RE
+.TP
+.B \f[I]FI_SOCKADDR_IB\f[]
+Address is of type sockaddr_ib (defined in Linux kernel source)
+.RS
+.RE
+.TP
+.B \f[I]FI_SOCKADDR_IN\f[]
+Address is of type sockaddr_in (IPv4).
+.RS
+.RE
+.TP
+.B \f[I]FI_SOCKADDR_IN6\f[]
+Address is of type sockaddr_in6 (IPv6).
+.RS
+.RE
+.TP
+.B \f[I]FI_ADDR_PSMX\f[]
+Address is an Intel proprietary format that is used with their PSMX
+(extended performance scaled messaging) protocol.
+.RS
+.RE
 .SH FLAGS
 .PP
 The operation of the fi_getinfo call may be controlled through the use
@@ -845,15 +876,6 @@ Use of this flag will suppress any lengthy name resolution protocol.
 .RS
 .RE
 .TP
-.B \f[I]FI_SOURCE\f[]
-Indicates that the node and service parameters specify the local source
-address to associate with an endpoint.
-If specified, either the node and/or service parameter must be
-non\-NULL.
-This flag is often used with passive endpoints.
-.RS
-.RE
-.TP
 .B \f[I]FI_PROV_ATTR_ONLY\f[]
 Indicates that the caller is only querying for what providers are
 potentially available.
@@ -865,6 +887,15 @@ The fabric_attr member will have the prov_name and prov_version values
 filled in.
 .RS
 .RE
+.TP
+.B \f[I]FI_SOURCE\f[]
+Indicates that the node and service parameters specify the local source
+address to associate with an endpoint.
+If specified, either the node and/or service parameter must be
+non\-NULL.
+This flag is often used with passive endpoints.
+.RS
+.RE
 .SH RETURN VALUE
 .PP
 fi_getinfo() returns 0 on success.
@@ -887,16 +918,16 @@ invalid.
 .RS
 .RE
 .TP
-.B \f[I]FI_ENOMEM\f[]
-Indicates that there was insufficient memory to complete the operation.
-.RS
-.RE
-.TP
 .B \f[I]FI_ENODATA\f[]
 Indicates that no providers could be found which support the requested
 fabric information.
 .RS
 .RE
+.TP
+.B \f[I]FI_ENOMEM\f[]
+Indicates that there was insufficient memory to complete the operation.
+.RS
+.RE
 .SH NOTES
 .PP
 If hints are provided, the operation will be controlled by the values
diff --git a/man/man3/fi_mr.3 b/man/man3/fi_mr.3
index d04f268..5376e3e 100644
--- a/man/man3/fi_mr.3
+++ b/man/man3/fi_mr.3
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_mr" "3" "2019\-09\-27" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_mr" "3" "2020\-02\-24" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -838,6 +838,22 @@ is access infrequently.
 By default merging regions is disabled.
 .RS
 .RE
+.TP
+.B \f[I]FI_MR_CACHE_MONITOR\f[]
+The cache monitor is responsible for detecting changes made between the
+virtual addresses used by an application and the underlying physical
+pages.
+Valid monitor options are: userfaultfd, memhooks, and disabled.
+Selecting disabled will turn off the registration cache.
+Userfaultfd is a Linux kernel feature used to report virtual to physical
+address mapping changes to user space.
+Memhooks operates by intercepting relevant memory allocation and
+deallocation calls which may result in the mappings changing, such as
+malloc, mmap, free, etc.
+Note that memhooks operates at the elf linker layer, and does not use
+glibc memory hooks.
+.RS
+.RE
 .SH SEE ALSO
 .PP
 \f[C]fi_getinfo\f[](3), \f[C]fi_endpoint\f[](3), \f[C]fi_domain\f[](3),
diff --git a/man/man7/fi_efa.7 b/man/man7/fi_efa.7
index 47a7b01..b5927fd 100644
--- a/man/man7/fi_efa.7
+++ b/man/man7/fi_efa.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_efa" "7" "2020\-02\-04" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_efa" "7" "2020\-02\-18" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -160,7 +160,7 @@ Size of any cq created, in number of entries.
 .B \f[I]FI_EFA_MR_CACHE_ENABLE\f[]
 Enables using the mr cache and in\-line registration instead of a bounce
 buffer for iov\[aq]s larger than max_memcpy_size.
-Defaults to false.
+Defaults to true.
 When disabled, only uses a bounce buffer
 .RS
 .RE
diff --git a/man/man7/fi_provider.7 b/man/man7/fi_provider.7
index 73a7190..0b1c126 100644
--- a/man/man7/fi_provider.7
+++ b/man/man7/fi_provider.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_provider" "7" "2019\-06\-15" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_provider" "7" "2020\-02\-13" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -73,6 +73,15 @@ hardware interface for inter\-instance communication on EC2.
 See \f[C]fi_efa\f[](7) for more information.
 .RS
 .RE
+.TP
+.B \f[I]SHM\f[]
+A provider for intranode communication using shared memory.
+The provider makes use of the Linux kernel feature Cross Memory Attach
+(CMA) which allows processes to have full access to another process\[aq]
+address space.
+See \f[C]fi_shm\f[](7) for more information.
+.RS
+.RE
 .SS Utility providers
 .TP
 .B \f[I]RxM\f[]
@@ -81,6 +90,13 @@ endpoints emulated over MSG endpoints of a core provider.
 See \f[C]fi_rxm\f[](7) for more information.
 .RS
 .RE
+.TP
+.B \f[I]RxD\f[]
+The RxD provider (ofi_rxd) is a utility provider that supports RDM
+endpoints emulated over DGRAM endpoints of a core provider.
+See \f[C]fi_rxd\f[](7) for more information.
+.RS
+.RE
 .SS Special providers
 .TP
 .B \f[I]Hook\f[]
diff --git a/man/man7/fi_shm.7 b/man/man7/fi_shm.7
index 944c615..b7497ac 100644
--- a/man/man7/fi_shm.7
+++ b/man/man7/fi_shm.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_shm" "7" "2019\-10\-23" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_shm" "7" "2020\-02\-07" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -124,8 +124,18 @@ EPs must be bound to both RX and TX CQs.
 .PP
 No support for counters.
 .SH RUNTIME PARAMETERS
-.PP
-No runtime parameters are currently defined.
+.TP
+.B \f[I]FI_SHM_DISABLE_CMA\f[]
+Force disable use of CMA (Cross Memory Attach) in shm environment.
+CMA is a Linux feature for copying data directly between two processes
+without the use of intermediate buffering.
+This requires the processes to have full access to the peer\[aq]s
+address space (the same permissions required to perform a ptrace).
+CMA is enabled by default but checked for availability during run\-time.
+For more information see the CMA [\f[C]man\ pages\f[]]
+(https://linux.die.net/man/2/process_vm_writev)
+.RS
+.RE
 .SH SEE ALSO
 .PP
 \f[C]fabric\f[](7), \f[C]fi_provider\f[](7), \f[C]fi_getinfo\f[](3)
diff --git a/man/man7/fi_verbs.7 b/man/man7/fi_verbs.7
index 545307b..9baddb4 100644
--- a/man/man7/fi_verbs.7
+++ b/man/man7/fi_verbs.7
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 1.19.2.4
 .\"
-.TH "fi_verbs" "7" "2019\-07\-18" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
+.TH "fi_verbs" "7" "2020\-02\-27" "Libfabric Programmer\[aq]s Manual" "\@VERSION\@"
 .hy
 .SH NAME
 .PP
@@ -199,17 +199,28 @@ time (default: 8).
 .RS
 .RE
 .TP
-.B \f[I]FI_VERBS_IFACE\f[]
-The prefix or the full name of the network interface associated with the
-verbs device (default: ib)
-.RS
-.RE
-.TP
 .B \f[I]FI_VERBS_PREFER_XRC\f[]
 Prioritize XRC transport fi_info before RC transport fi_info (default:
 0, RC fi_info will be before XRC fi_info)
 .RS
 .RE
+.TP
+.B \f[I]FI_VERBS_GID_IDX\f[]
+The GID index to use (default: 0)
+.RS
+.RE
+.TP
+.B \f[I]FI_VERBS_DEVICE_NAME\f[]
+Specify a specific verbs device to use by name
+.RS
+.RE
+.SS Variables specific to MSG endpoints
+.TP
+.B \f[I]FI_VERBS_IFACE\f[]
+The prefix or the full name of the network interface associated with the
+verbs device (default: ib)
+.RS
+.RE
 .SS Variables specific to DGRAM endpoints
 .TP
 .B \f[I]FI_VERBS_DGRAM_USE_NAME_SERVER\f[]
@@ -225,11 +236,6 @@ The port on which Name Server thread listens incoming connections and
 requests (default: 5678)
 .RS
 .RE
-.TP
-.B \f[I]FI_VERBS_GID_IDX\f[]
-The GID index to use (default: 0)
-.RS
-.RE
 .SS Environment variables notes
 .PP
 The fi_info utility would give the up\-to\-date information on
diff --git a/prov/efa/Makefile.include b/prov/efa/Makefile.include
index 3a05fff..42f5cd5 100644
--- a/prov/efa/Makefile.include
+++ b/prov/efa/Makefile.include
@@ -40,6 +40,7 @@ _efa_files = \
 	prov/efa/src/efa_fabric.c \
 	prov/efa/src/efa_msg.c \
 	prov/efa/src/efa_mr.c \
+	prov/efa/src/efa_rma.c \
 	prov/efa/src/rxr/rxr_attr.c	\
 	prov/efa/src/rxr/rxr_init.c	\
 	prov/efa/src/rxr/rxr_fabric.c	\
@@ -48,14 +49,22 @@ _efa_files = \
 	prov/efa/src/rxr/rxr_ep.c	\
 	prov/efa/src/rxr/rxr_cntr.c	\
 	prov/efa/src/rxr/rxr_rma.c	\
-	prov/efa/src/rxr/rxr_msg.c
+	prov/efa/src/rxr/rxr_msg.c	\
+	prov/efa/src/rxr/rxr_pkt_entry.c \
+	prov/efa/src/rxr/rxr_pkt_type_rts.c \
+	prov/efa/src/rxr/rxr_pkt_type_data.c \
+	prov/efa/src/rxr/rxr_pkt_type_misc.c \
+	prov/efa/src/rxr/rxr_pkt_cmd.c
 
 _efa_headers = \
 	prov/efa/src/efa.h \
 	prov/efa/src/rxr/rxr.h \
 	prov/efa/src/rxr/rxr_cntr.h \
 	prov/efa/src/rxr/rxr_rma.h \
-	prov/efa/src/rxr/rxr_msg.h
+	prov/efa/src/rxr/rxr_msg.h \
+	prov/efa/src/rxr/rxr_pkt_entry.h \
+	prov/efa/src/rxr/rxr_pkt_type.h \
+	prov/efa/src/rxr/rxr_pkt_cmd.h
 
 efa_CPPFLAGS += \
 	-I$(top_srcdir)/prov/efa/src/ \
diff --git a/prov/efa/configure.m4 b/prov/efa/configure.m4
index 751fa6c..4d3478c 100644
--- a/prov/efa/configure.m4
+++ b/prov/efa/configure.m4
@@ -49,7 +49,10 @@ AC_DEFUN([FI_EFA_CONFIGURE],[
 				[$efa_PREFIX],
 				[$efa_LIBDIR],
 				[efa_happy=1],
-				[efa_happy=0])
+				[
+					efa_happy=0
+					AC_MSG_WARN([The EFA provider requires rdma-core v27 or newer.])
+				])
 	      ])
 
 	AS_IF([test x"$enable_efa" != x"no"],
@@ -64,6 +67,13 @@ AC_DEFUN([FI_EFA_CONFIGURE],[
 				[efa_happy=0])
 	      ])
 
+	AS_IF([test x"$enable_efa" != x"no"],
+	      [AC_CHECK_MEMBER(struct efadv_device_attr.max_rdma_size,
+			      [AC_DEFINE([HAVE_RDMA_SIZE], [1], [efadv_device_attr has max_rdma_size])],
+			      [],
+			      [[#include <infiniband/efadv.h>]])
+	      ])
+
 	AS_IF([test $efa_happy -eq 1 ], [$1], [$2])
 
 	efa_CPPFLAGS="$efa_ibverbs_CPPFLAGS $efadv_CPPFLAGS"
diff --git a/prov/efa/src/efa.h b/prov/efa/src/efa.h
index 643da4f..9afa002 100644
--- a/prov/efa/src/efa.h
+++ b/prov/efa/src/efa.h
@@ -57,6 +57,7 @@
 #include <rdma/fi_errno.h>
 
 #include <infiniband/verbs.h>
+#include <infiniband/efadv.h>
 
 #include "ofi.h"
 #include "ofi_enosys.h"
@@ -88,7 +89,7 @@
 
 #define EFA_DEF_CQ_SIZE 1024
 #define EFA_MR_IOV_LIMIT 1
-#define EFA_MR_SUPPORTED_PERMISSIONS (FI_SEND | FI_RECV)
+#define EFA_MR_SUPPORTED_PERMISSIONS (FI_SEND | FI_RECV | FI_REMOTE_READ)
 
 /*
  * Multiplier to give some room in the device memory registration limits
@@ -183,10 +184,14 @@ struct efa_context {
 	struct ibv_context	*ibv_ctx;
 	uint64_t		max_mr_size;
 	uint16_t		inline_buf_size;
+	uint16_t		max_wr_rdma_sge;
+	uint32_t		max_rdma_size;
+	uint32_t		device_caps;
 };
 
 struct efa_qp {
 	struct ibv_qp	*ibv_qp;
+	struct ibv_qp_ex *ibv_qp_ex;
 	struct efa_ep	*ep;
 	uint32_t	qp_num;
 };
@@ -232,6 +237,7 @@ typedef struct efa_conn *
 
 struct efa_av {
 	struct fid_av		*shm_rdm_av;
+	fi_addr_t		shm_rdm_addr_map[EFA_SHM_MAX_AV_COUNT];
 	struct efa_domain       *domain;
 	struct efa_ep           *ep;
 	size_t			used;
@@ -298,6 +304,7 @@ extern const struct efa_ep_domain efa_dgrm_domain;
 
 extern struct fi_ops_cm efa_ep_cm_ops;
 extern struct fi_ops_msg efa_ep_msg_ops;
+extern struct fi_ops_rma efa_ep_rma_ops;
 
 int efa_device_init(void);
 void efa_device_free(void);
@@ -332,4 +339,22 @@ ssize_t efa_cq_readfrom(struct fid_cq *cq_fid, void *buf, size_t count, fi_addr_
 
 ssize_t efa_cq_readerr(struct fid_cq *cq_fid, struct fi_cq_err_entry *entry, uint64_t flags);
 
+static inline
+bool efa_support_rdma_read(struct fid_ep *ep_fid)
+{
+	struct efa_ep *efa_ep;
+
+	efa_ep = container_of(ep_fid, struct efa_ep, util_ep.ep_fid);
+	return efa_ep->domain->ctx->device_caps & EFADV_DEVICE_ATTR_CAPS_RDMA_READ;
+}
+
+static inline
+size_t efa_max_rdma_size(struct fid_ep *ep_fid)
+{
+	struct efa_ep *efa_ep;
+
+	efa_ep = container_of(ep_fid, struct efa_ep, util_ep.ep_fid);
+	return efa_ep->domain->ctx->max_rdma_size;
+}
+
 #endif /* EFA_H */
diff --git a/prov/efa/src/efa_av.c b/prov/efa/src/efa_av.c
index 0cd8aa2..82b1f9f 100644
--- a/prov/efa/src/efa_av.c
+++ b/prov/efa/src/efa_av.c
@@ -309,8 +309,11 @@ int efa_av_insert_addr(struct efa_av *av, struct efa_ep_addr *addr,
 			"Insert %s to shm provider's av. addr = %" PRIu64
 			" rdm_fiaddr = %" PRIu64 " shm_rdm_fiaddr = %" PRIu64
 			"\n", smr_name, *(uint64_t *)addr, *fi_addr, shm_fiaddr);
+
+		assert(shm_fiaddr < EFA_SHM_MAX_AV_COUNT);
 		av_entry->local_mapping = 1;
 		av_entry->shm_rdm_addr = shm_fiaddr;
+		av->shm_rdm_addr_map[shm_fiaddr] = av_entry->rdm_addr;
 
 		/*
 		 * Walk through all the EPs that bound to the AV,
@@ -520,6 +523,9 @@ static int efa_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
 				ret = fi_av_remove(av->shm_rdm_av, &av_entry->shm_rdm_addr, 1, flags);
 				if (ret)
 					goto err_free_av_entry;
+
+				assert(av_entry->shm_rdm_addr < EFA_SHM_MAX_AV_COUNT);
+				av->shm_rdm_addr_map[av_entry->shm_rdm_addr] = FI_ADDR_UNSPEC;
 			}
 			HASH_DEL(av->av_map, av_entry);
 			free(av_entry);
@@ -624,7 +630,7 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 	struct util_av_attr util_attr;
 	size_t universe_size;
 	struct fi_av_attr av_attr;
-	int ret, retv;
+	int i, ret, retv;
 
 	if (!attr)
 		return -FI_EINVAL;
@@ -684,9 +690,13 @@ int efa_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
 				goto err_close_rdm_av;
 			}
 			av_attr.count = rxr_env.shm_av_size;
+			assert(av_attr.type == FI_AV_TABLE);
 			ret = fi_av_open(rxr_domain->shm_domain, &av_attr, &av->shm_rdm_av, context);
 			if (ret)
 				goto err_close_rdm_av;
+
+			for (i = 0; i < EFA_SHM_MAX_AV_COUNT; ++i)
+				av->shm_rdm_addr_map[i] = FI_ADDR_UNSPEC;
 		}
 	} else {
 		// Currently the domain is set to efa for only dgram
diff --git a/prov/efa/src/efa_ep.c b/prov/efa/src/efa_ep.c
index bd7bdcc..84a913a 100644
--- a/prov/efa/src/efa_ep.c
+++ b/prov/efa/src/efa_ep.c
@@ -91,29 +91,37 @@ static int efa_ep_modify_qp_rst2rts(struct efa_qp *qp)
 				      IBV_QP_STATE | IBV_QP_SQ_PSN);
 }
 
-static int efa_ep_create_qp(struct efa_ep *ep,
-			    struct ibv_pd *ibv_pd,
-			    struct ibv_qp_init_attr *init_attr)
+static int efa_ep_create_qp_ex(struct efa_ep *ep,
+			       struct ibv_pd *ibv_pd,
+			       struct ibv_qp_init_attr_ex *init_attr_ex)
 {
-	struct efa_domain *domain = ep->domain;
+	struct efa_domain *domain;
 	struct efa_qp *qp;
+	struct efadv_qp_init_attr efa_attr = {};
 	int err;
 
+	domain = ep->domain;
 	qp = calloc(1, sizeof(*qp));
 	if (!qp)
 		return -FI_ENOMEM;
 
-	if (init_attr->qp_type == IBV_QPT_UD)
-		qp->ibv_qp = ibv_create_qp(ibv_pd, init_attr);
-	else
-		qp->ibv_qp = efadv_create_driver_qp(ibv_pd, init_attr,
-						    EFADV_QP_DRIVER_TYPE_SRD);
+	if (init_attr_ex->qp_type == IBV_QPT_UD) {
+		qp->ibv_qp = ibv_create_qp_ex(ibv_pd->context, init_attr_ex);
+	} else {
+		assert(init_attr_ex->qp_type == IBV_QPT_DRIVER);
+		efa_attr.driver_qp_type = EFADV_QP_DRIVER_TYPE_SRD;
+		qp->ibv_qp = efadv_create_qp_ex(ibv_pd->context, init_attr_ex, &efa_attr,
+						sizeof(struct efadv_qp_init_attr));
+	}
+
 	if (!qp->ibv_qp) {
 		EFA_WARN(FI_LOG_EP_CTRL, "ibv_create_qp failed\n");
 		err = -EINVAL;
 		goto err_free_qp;
 	}
 
+	qp->ibv_qp_ex = ibv_qp_to_qp_ex(qp->ibv_qp);
+
 	err = efa_ep_modify_qp_rst2rts(qp);
 	if (err)
 		goto err_destroy_qp;
@@ -337,7 +345,7 @@ static int efa_ep_setflags(struct fid_ep *ep_fid, uint64_t flags)
 
 static int efa_ep_enable(struct fid_ep *ep_fid)
 {
-	struct ibv_qp_init_attr attr = { 0 };
+	struct ibv_qp_init_attr_ex attr_ex = { 0 };
 	const struct fi_info *efa_info;
 	struct ibv_pd *ibv_pd;
 	struct efa_ep *ep;
@@ -369,29 +377,42 @@ static int efa_ep_enable(struct fid_ep *ep_fid)
 	}
 
 	if (ep->scq) {
-		attr.cap.max_send_wr = ep->info->tx_attr->size;
-		attr.cap.max_send_sge = ep->info->tx_attr->iov_limit;
-		attr.send_cq = ep->scq->ibv_cq;
+		attr_ex.cap.max_send_wr = ep->info->tx_attr->size;
+		attr_ex.cap.max_send_sge = ep->info->tx_attr->iov_limit;
+		attr_ex.send_cq = ep->scq->ibv_cq;
 		ibv_pd = ep->scq->domain->ibv_pd;
 	} else {
-		attr.send_cq = ep->rcq->ibv_cq;
+		attr_ex.send_cq = ep->rcq->ibv_cq;
 		ibv_pd = ep->rcq->domain->ibv_pd;
 	}
 
 	if (ep->rcq) {
-		attr.cap.max_recv_wr = ep->info->rx_attr->size;
-		attr.cap.max_recv_sge = ep->info->rx_attr->iov_limit;
-		attr.recv_cq = ep->rcq->ibv_cq;
+		attr_ex.cap.max_recv_wr = ep->info->rx_attr->size;
+		attr_ex.cap.max_recv_sge = ep->info->rx_attr->iov_limit;
+		attr_ex.recv_cq = ep->rcq->ibv_cq;
 	} else {
-		attr.recv_cq = ep->scq->ibv_cq;
+		attr_ex.recv_cq = ep->scq->ibv_cq;
 	}
 
-	attr.cap.max_inline_data = ep->domain->ctx->inline_buf_size;
-	attr.qp_type = ep->domain->rdm ? IBV_QPT_DRIVER : IBV_QPT_UD;
-	attr.qp_context = ep;
-	attr.sq_sig_all = 1;
+	attr_ex.cap.max_inline_data = ep->domain->ctx->inline_buf_size;
 
-	return efa_ep_create_qp(ep, ibv_pd, &attr);
+	if (ep->domain->rdm) {
+		attr_ex.qp_type = IBV_QPT_DRIVER;
+		attr_ex.comp_mask = IBV_QP_INIT_ATTR_PD | IBV_QP_INIT_ATTR_SEND_OPS_FLAGS;
+		attr_ex.send_ops_flags = IBV_QP_EX_WITH_SEND;
+		if (efa_support_rdma_read(&ep->util_ep.ep_fid))
+			attr_ex.send_ops_flags |= IBV_QP_EX_WITH_RDMA_READ;
+		attr_ex.pd = ibv_pd;
+	} else {
+		attr_ex.qp_type = IBV_QPT_UD;
+		attr_ex.comp_mask = IBV_QP_INIT_ATTR_PD;
+		attr_ex.pd = ibv_pd;
+	}
+
+	attr_ex.qp_context = ep;
+	attr_ex.sq_sig_all = 1;
+
+	return efa_ep_create_qp_ex(ep, ibv_pd, &attr_ex);
 }
 
 static int efa_ep_control(struct fid *fid, int command, void *arg)
@@ -501,19 +522,6 @@ void efa_ep_progress(struct util_ep *ep)
 	fastlock_release(&ep->lock);
 }
 
-static struct fi_ops_rma efa_ep_rma_ops = {
-	.size = sizeof(struct fi_ops_rma),
-	.read = fi_no_rma_read,
-	.readv = fi_no_rma_readv,
-	.readmsg = fi_no_rma_readmsg,
-	.write = fi_no_rma_write,
-	.writev = fi_no_rma_writev,
-	.writemsg = fi_no_rma_writemsg,
-	.inject = fi_no_rma_inject,
-	.writedata = fi_no_rma_writedata,
-	.injectdata = fi_no_rma_injectdata,
-};
-
 static struct fi_ops_atomic efa_ep_atomic_ops = {
 	.size = sizeof(struct fi_ops_atomic),
 	.write = fi_no_atomic_write,
diff --git a/prov/efa/src/efa_fabric.c b/prov/efa/src/efa_fabric.c
index 710fdf9..fa2fc07 100644
--- a/prov/efa/src/efa_fabric.c
+++ b/prov/efa/src/efa_fabric.c
@@ -52,13 +52,20 @@
 #include <ofi_util.h>
 
 #include "efa.h"
+#if HAVE_EFA_DL
+#include <ofi_shm.h>
+#endif
 
 #define EFA_FABRIC_PREFIX "EFA-"
 
 #define EFA_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
 
-#define EFA_RDM_CAPS (FI_MSG | FI_RECV | FI_SEND | FI_SOURCE | EFA_DOMAIN_CAPS)
-#define EFA_DGRM_CAPS (FI_MSG | FI_RECV | FI_SEND | FI_SOURCE | EFA_DOMAIN_CAPS)
+#define EFA_RDM_TX_CAPS (OFI_TX_MSG_CAPS)
+#define EFA_RDM_RX_CAPS (OFI_RX_MSG_CAPS | FI_SOURCE)
+#define EFA_DGRM_TX_CAPS (OFI_TX_MSG_CAPS)
+#define EFA_DGRM_RX_CAPS (OFI_RX_MSG_CAPS | FI_SOURCE)
+#define EFA_RDM_CAPS (EFA_RDM_TX_CAPS | EFA_RDM_RX_CAPS | EFA_DOMAIN_CAPS)
+#define EFA_DGRM_CAPS (EFA_DGRM_TX_CAPS | EFA_DGRM_RX_CAPS | EFA_DOMAIN_CAPS)
 
 #define EFA_TX_OP_FLAGS (FI_TRANSMIT_COMPLETE)
 
@@ -71,7 +78,7 @@
 
 #define EFA_NO_DEFAULT -1
 
-#define EFA_DEF_MR_CACHE_ENABLE 0
+#define EFA_DEF_MR_CACHE_ENABLE 1
 #define EFA_DEF_MR_CACHE_MERGE_REGIONS 1
 
 int efa_mr_cache_enable		= EFA_DEF_MR_CACHE_ENABLE;
@@ -117,6 +124,7 @@ const struct fi_ep_attr efa_ep_attr = {
 };
 
 const struct fi_rx_attr efa_dgrm_rx_attr = {
+	.caps			= EFA_DGRM_RX_CAPS,
 	.mode			= FI_MSG_PREFIX | EFA_RX_MODE,
 	.op_flags		= EFA_RX_DGRM_OP_FLAGS,
 	.msg_order		= EFA_MSG_ORDER,
@@ -126,6 +134,7 @@ const struct fi_rx_attr efa_dgrm_rx_attr = {
 };
 
 const struct fi_rx_attr efa_rdm_rx_attr = {
+	.caps			= EFA_RDM_RX_CAPS,
 	.mode			= EFA_RX_MODE,
 	.op_flags		= EFA_RX_RDM_OP_FLAGS,
 	.msg_order		= EFA_MSG_ORDER,
@@ -135,6 +144,7 @@ const struct fi_rx_attr efa_rdm_rx_attr = {
 };
 
 const struct fi_tx_attr efa_dgrm_tx_attr = {
+	.caps			= EFA_DGRM_TX_CAPS,
 	.mode			= FI_MSG_PREFIX,
 	.op_flags		= EFA_TX_OP_FLAGS,
 	.msg_order		= EFA_MSG_ORDER,
@@ -144,6 +154,7 @@ const struct fi_tx_attr efa_dgrm_tx_attr = {
 };
 
 const struct fi_tx_attr efa_rdm_tx_attr = {
+	.caps			= EFA_RDM_TX_CAPS,
 	.mode			= 0,
 	.op_flags		= EFA_TX_OP_FLAGS,
 	.msg_order		= EFA_MSG_ORDER,
@@ -465,6 +476,9 @@ static int efa_get_device_attrs(struct efa_context *ctx, struct fi_info *info)
 	struct ibv_port_attr port_attr;
 	int ret;
 
+	memset(&efadv_attr, 0, sizeof(efadv_attr));
+	memset(&device_attr, 0, sizeof(device_attr));
+
 	base_attr = &device_attr.ibv_attr;
 	ret = -ibv_query_device(ctx->ibv_ctx, base_attr);
 	if (ret) {
@@ -472,13 +486,23 @@ static int efa_get_device_attrs(struct efa_context *ctx, struct fi_info *info)
 		return ret;
 	}
 
-	ret = -efadv_query_device(ctx->ibv_ctx, &efadv_attr, sizeof(efadv_attr));
+	ret = -efadv_query_device(ctx->ibv_ctx, &efadv_attr,
+				  sizeof(efadv_attr));
 	if (ret) {
 		EFA_INFO_ERRNO(FI_LOG_FABRIC, "efadv_query_device", ret);
 		return ret;
 	}
 
 	ctx->inline_buf_size = efadv_attr.inline_buf_size;
+	ctx->max_wr_rdma_sge = base_attr->max_sge_rd;
+
+#ifdef HAVE_RDMA_SIZE
+	ctx->max_rdma_size = efadv_attr.max_rdma_size;
+	ctx->device_caps = efadv_attr.device_caps;
+#else
+	ctx->max_rdma_size = 0;
+	ctx->device_caps = 0;
+#endif
 
 	ctx->max_mr_size			= base_attr->max_mr_size;
 	info->domain_attr->cq_cnt		= base_attr->max_cq;
@@ -506,7 +530,19 @@ static int efa_get_device_attrs(struct efa_context *ctx, struct fi_info *info)
 
 	info->tx_attr->iov_limit = efadv_attr.max_sq_sge;
 	info->tx_attr->size = align_down_to_power_of_2(efadv_attr.max_sq_wr);
-	info->tx_attr->inject_size = efadv_attr.inline_buf_size;
+	if (info->ep_attr->type == FI_EP_RDM) {
+		info->tx_attr->inject_size = efadv_attr.inline_buf_size;
+	} else if (info->ep_attr->type == FI_EP_DGRAM) {
+                /*
+                 * Currently, there is no mechanism for EFA layer (lower layer)
+                 * to discard completions internally and FI_INJECT is not optional,
+                 * it can only be disabled by setting inject_size to 0. RXR
+                 * layer does not have this issue as completions can be read from
+                 * the EFA layer and discarded in the RXR layer. For dgram
+                 * endpoint, inject size needs to be set to 0
+                 */
+		info->tx_attr->inject_size = 0;
+	}
 	info->rx_attr->iov_limit = efadv_attr.max_rq_sge;
 	info->rx_attr->size = align_down_to_power_of_2(efadv_attr.max_rq_wr / info->rx_attr->iov_limit);
 
@@ -617,8 +653,6 @@ static int efa_alloc_info(struct efa_context *ctx, struct fi_info **info,
 
 	fi->ep_attr->protocol	= FI_PROTO_EFA;
 	fi->ep_attr->type	= ep_dom->type;
-	fi->tx_attr->caps	= ep_dom->caps;
-	fi->rx_attr->caps	= ep_dom->caps;
 
 	ret = efa_get_device_attrs(ctx, fi);
 	if (ret)
@@ -756,6 +790,7 @@ static int efa_set_fi_address(const char *node, const char *service, uint64_t fl
 	struct efa_ep_addr tmp_addr;
 	void *dest_addr = NULL;
 	int ret = FI_SUCCESS;
+	struct fi_info *cur;
 
 	if (flags & FI_SOURCE) {
 		if (hints && hints->dest_addr)
@@ -772,15 +807,17 @@ static int efa_set_fi_address(const char *node, const char *service, uint64_t fl
 	}
 
 	if (dest_addr) {
-		fi->dest_addr = malloc(EFA_EP_ADDR_LEN);
-		if (!fi->dest_addr)
-			return -FI_ENOMEM;
-		memcpy(fi->dest_addr, dest_addr, EFA_EP_ADDR_LEN);
+		for (cur = fi; cur; cur = cur->next) {
+			cur->dest_addr = malloc(EFA_EP_ADDR_LEN);
+			if (!cur->dest_addr) {
+				for (; fi->dest_addr; fi = fi->next)
+					free(fi->dest_addr);
+				return -FI_ENOMEM;
+			}
+			memcpy(cur->dest_addr, dest_addr, EFA_EP_ADDR_LEN);
+			cur->dest_addrlen = EFA_EP_ADDR_LEN;
+		}
 	}
-
-	if (fi->dest_addr)
-		fi->dest_addrlen = EFA_EP_ADDR_LEN;
-
 	return ret;
 }
 
diff --git a/prov/efa/src/efa_mr.c b/prov/efa/src/efa_mr.c
index 95c69cb..e756960 100644
--- a/prov/efa/src/efa_mr.c
+++ b/prov/efa/src/efa_mr.c
@@ -247,6 +247,9 @@ static int efa_mr_reg(struct fid *fid, const void *buf, size_t len,
 	if (access & FI_RECV)
 		fi_ibv_access |= IBV_ACCESS_LOCAL_WRITE;
 
+	if (access & FI_REMOTE_READ)
+		fi_ibv_access |= IBV_ACCESS_REMOTE_READ;
+
 	md->mr = ibv_reg_mr(md->domain->ibv_pd, (void *)buf, len,
 			    fi_ibv_access);
 	if (!md->mr) {
diff --git a/prov/efa/src/efa_rma.c b/prov/efa/src/efa_rma.c
new file mode 100644
index 0000000..37c6bc5
--- /dev/null
+++ b/prov/efa/src/efa_rma.c
@@ -0,0 +1,134 @@
+/*
+ * Copyright (c) 2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <stdlib.h>
+#include <string.h>
+#include <ofi_mem.h>
+#include <ofi_iov.h>
+#include "efa.h"
+
+static
+ssize_t efa_rma_post_read(struct efa_ep *ep, const struct fi_msg_rma *msg, uint64_t flags)
+{
+	struct efa_qp *qp;
+	struct efa_conn *conn;
+	int lkey;
+
+	if (OFI_UNLIKELY(msg->iov_count > ep->domain->ctx->max_wr_rdma_sge)) {
+		EFA_WARN(FI_LOG_CQ, "invalid iov_count!\n");
+		return -FI_EINVAL;
+	}
+
+	if (OFI_UNLIKELY(msg->rma_iov_count > ep->domain->ctx->max_wr_rdma_sge)) {
+		EFA_WARN(FI_LOG_CQ, "invalid rma_iov_count!\n");
+		return -FI_EINVAL;
+	}
+
+	if (OFI_UNLIKELY(ofi_total_iov_len(msg->msg_iov, msg->iov_count)
+			 > ep->domain->ctx->max_rdma_size)) {
+		EFA_WARN(FI_LOG_CQ, "maximum rdma_size exceeded!\n");
+		return -FI_EINVAL;
+	}
+
+	/* caller must provide desc because EFA require FI_MR_LOCAL */
+	assert(msg->desc && msg->desc[0]);
+
+	qp = ep->qp;
+	ibv_wr_start(qp->ibv_qp_ex);
+	qp->ibv_qp_ex->wr_id = (uintptr_t)msg->context;
+	ibv_wr_rdma_read(qp->ibv_qp_ex, msg->rma_iov[0].key, msg->rma_iov[0].addr);
+
+	lkey = (uint32_t)(uintptr_t)msg->desc[0];
+	ibv_wr_set_sge(qp->ibv_qp_ex, lkey, (uint64_t)msg->msg_iov[0].iov_base, msg->msg_iov[0].iov_len);
+	conn = ep->av->addr_to_conn(ep->av, msg->addr);
+	ibv_wr_set_ud_addr(qp->ibv_qp_ex, conn->ah.ibv_ah, conn->ep_addr.qpn, EFA_QKEY);
+	return ibv_wr_complete(qp->ibv_qp_ex);
+}
+
+static
+ssize_t efa_rma_readmsg(struct fid_ep *ep_fid, const struct fi_msg_rma *msg, uint64_t flags)
+{
+	struct efa_ep *ep = container_of(ep_fid, struct efa_ep, util_ep.ep_fid);
+
+	return efa_rma_post_read(ep, msg, flags);
+}
+
+static
+ssize_t efa_rma_readv(struct fid_ep *ep, const struct iovec *iov, void **desc,
+		      size_t iov_count, fi_addr_t src_addr, uint64_t addr,
+		      uint64_t key, void *context)
+{
+	struct fi_rma_iov rma_iov;
+	struct fi_msg_rma msg;
+
+	rma_iov.addr = addr;
+	rma_iov.len = ofi_total_iov_len(iov, iov_count);
+	rma_iov.key = key;
+
+	memset(&msg, 0, sizeof(msg));
+	msg.msg_iov = iov;
+	msg.desc = desc;
+	msg.iov_count = iov_count;
+	msg.addr = src_addr;
+	msg.context = context;
+	msg.rma_iov = &rma_iov;
+	msg.rma_iov_count = 1;
+
+	return efa_rma_readmsg(ep, &msg, 0);
+}
+
+static
+ssize_t efa_rma_read(struct fid_ep *ep, void *buf, size_t len, void *desc,
+		     fi_addr_t src_addr, uint64_t addr, uint64_t key,
+		     void *context)
+{
+	struct iovec iov;
+
+	iov.iov_base = (void *)buf;
+	iov.iov_len = len;
+	return efa_rma_readv(ep, &iov, &desc, 1, src_addr, addr, key, context);
+}
+
+struct fi_ops_rma efa_ep_rma_ops = {
+	.size = sizeof(struct fi_ops_rma),
+	.read = efa_rma_read,
+	.readv = efa_rma_readv,
+	.readmsg = efa_rma_readmsg,
+	.write = fi_no_rma_write,
+	.writev = fi_no_rma_writev,
+	.writemsg = fi_no_rma_writemsg,
+	.inject = fi_no_rma_inject,
+	.writedata = fi_no_rma_writedata,
+	.injectdata = fi_no_rma_injectdata,
+};
+
diff --git a/prov/efa/src/rxr/rxr.h b/prov/efa/src/rxr/rxr.h
index 35b4e73..1ab6ff2 100644
--- a/prov/efa/src/rxr/rxr.h
+++ b/prov/efa/src/rxr/rxr.h
@@ -63,6 +63,8 @@
 #include <ofi_perf.h>
 
 #include <sys/wait.h>
+#include "rxr_pkt_entry.h"
+#include "rxr_pkt_type.h"
 
 #define RXR_MAJOR_VERSION	(2)
 #define RXR_MINOR_VERSION	(0)
@@ -73,6 +75,13 @@
 
 #ifdef ENABLE_EFA_POISONING
 extern const uint32_t rxr_poison_value;
+static inline void rxr_poison_mem_region(uint32_t *ptr, size_t size)
+{
+	int i;
+
+	for (i = 0; i < size / sizeof(rxr_poison_value); i++)
+		memcpy(ptr + i, &rxr_poison_value, sizeof(rxr_poison_value));
+}
 #endif
 
 /*
@@ -218,16 +227,6 @@ enum rxr_lower_ep_type {
 	SHM_EP,
 };
 
-enum rxr_pkt_type {
-	RXR_RTS_PKT = 1,
-	RXR_CONNACK_PKT,
-	RXR_CTS_PKT,
-	RXR_DATA_PKT,
-	RXR_READRSP_PKT,
-	RXR_RMA_CONTEXT_PKT,
-	RXR_EOR_PKT,
-};
-
 /* RMA context packet types which are used only on local EP */
 enum rxr_rma_context_pkt_type {
 	RXR_SHM_RMA_READ = 1,
@@ -235,20 +234,6 @@ enum rxr_rma_context_pkt_type {
 	RXR_SHM_LARGE_READ,
 };
 
-/* pkt_entry types for rx pkts */
-enum rxr_pkt_entry_type {
-	RXR_PKT_ENTRY_POSTED = 1,   /* entries that are posted to the core */
-	RXR_PKT_ENTRY_UNEXP,        /* entries used to stage unexpected msgs */
-	RXR_PKT_ENTRY_OOO	    /* entries used to stage out-of-order RTS */
-};
-
-/* pkt_entry state for retransmit tracking */
-enum rxr_pkt_entry_state {
-	RXR_PKT_ENTRY_FREE = 0,
-	RXR_PKT_ENTRY_IN_USE,
-	RXR_PKT_ENTRY_RNR_RETRANSMIT,
-};
-
 enum rxr_x_entry_type {
 	RXR_TX_ENTRY = 1,
 	RXR_RX_ENTRY,
@@ -640,207 +625,6 @@ struct rxr_ep {
 #define rxr_rx_flags(rxr_ep) ((rxr_ep)->util_ep.rx_op_flags)
 #define rxr_tx_flags(rxr_ep) ((rxr_ep)->util_ep.tx_op_flags)
 
-/*
- * Packet fields common to all rxr packets. The other packet headers below must
- * be changed if this is updated.
- */
-struct rxr_base_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_base_hdr check");
-#endif
-
-/*
- * RTS packet structure: rts_hdr, cq_data (optional), src_addr(optional),  data.
- */
-struct rxr_rts_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	/* TODO: need to add msg_id -> tx_id mapping to remove tx_id */
-	uint16_t credit_request;
-	uint8_t addrlen;
-	uint8_t rma_iov_count;
-	uint32_t tx_id;
-	uint32_t msg_id;
-	uint64_t tag;
-	uint64_t data_len;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_rts_hdr) == 32, "rxr_rts_hdr check");
-#endif
-
-/*
- * EOR packet, used to acknowledge the sender that large message
- * copy has been finished.
- */
-struct rxr_eor_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t tx_id;
-	uint32_t rx_id;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_eor_hdr) == 12, "rxr_eor_hdr check");
-#endif
-
-
-struct rxr_connack_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-}; /* 4 bytes */
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_connack_hdr check");
-#endif
-
-struct rxr_cts_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint8_t pad[4];
-	/* TODO: need to add msg_id -> tx_id/rx_id mapping */
-	uint32_t tx_id;
-	uint32_t rx_id;
-	uint64_t window;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_cts_hdr) == 24, "rxr_cts_hdr check");
-#endif
-
-struct rxr_data_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	/* TODO: need to add msg_id -> tx_id/rx_id mapping */
-	uint32_t rx_id;
-	uint64_t seg_size;
-	uint64_t seg_offset;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_data_hdr) == 24, "rxr_data_hdr check");
-#endif
-
-/*
- * Control header without completion data. We will send more data with the RTS
- * packet if RXR_REMOTE_CQ_DATA is not set.
- */
-struct rxr_ctrl_hdr {
-	union {
-		struct rxr_base_hdr base_hdr;
-		struct rxr_rts_hdr rts_hdr;
-		struct rxr_connack_hdr connack_hdr;
-		struct rxr_cts_hdr cts_hdr;
-	};
-};
-
-/*
- * Control header with completion data. CQ data length is static.
- */
-#define RXR_CQ_DATA_SIZE (8)
-struct rxr_ctrl_cq_hdr {
-	union {
-		struct rxr_base_hdr base_hdr;
-		struct rxr_rts_hdr rts_hdr;
-		struct rxr_connack_hdr connack_hdr;
-		struct rxr_cts_hdr cts_hdr;
-	};
-	uint64_t cq_data;
-};
-
-/*
- * There are three packet types:
- * - Control packet with completion queue data
- * - Control packet without completion queue data
- * - Data packet
- *
- * All start with rxr_base_hdr so it is safe to cast between them to check
- * values in that structure.
- */
-struct rxr_ctrl_cq_pkt {
-	struct rxr_ctrl_cq_hdr hdr;
-	char data[];
-};
-
-struct rxr_ctrl_pkt {
-	struct rxr_ctrl_hdr hdr;
-	char data[];
-};
-
-struct rxr_data_pkt {
-	struct rxr_data_hdr hdr;
-	char data[];
-};
-
-/*
- * RMA context packet, used to differentiate the normal RMA read, normal RMA
- * write, and the RMA read in two-sided large message transfer
- */
-struct rxr_rma_context_pkt {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint32_t tx_id;
-	uint8_t rma_context_type;
-};
-
-struct rxr_pkt_entry {
-	/* for rx/tx_entry queued_pkts list */
-	struct dlist_entry entry;
-#if ENABLE_DEBUG
-	/* for tx/rx debug list or posted buf list */
-	struct dlist_entry dbg_entry;
-#endif
-	void *x_entry; /* pointer to rxr rx/tx entry */
-	size_t pkt_size;
-	struct fid_mr *mr;
-	fi_addr_t addr;
-	void *pkt; /* rxr_ctrl_*_pkt, or rxr_data_pkt */
-	enum rxr_pkt_entry_type type;
-	enum rxr_pkt_entry_state state;
-#if ENABLE_DEBUG
-/* pad to cache line size of 64 bytes */
-	uint8_t pad[48];
-#endif
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-#if ENABLE_DEBUG
-static_assert(sizeof(struct rxr_pkt_entry) == 128, "rxr_pkt_entry check");
-#else
-static_assert(sizeof(struct rxr_pkt_entry) == 64, "rxr_pkt_entry check");
-#endif
-#endif
-
-OFI_DECL_RECVWIN_BUF(struct rxr_pkt_entry*, rxr_robuf, uint32_t);
-DECLARE_FREESTACK(struct rxr_robuf, rxr_robuf_fs);
-
-#define RXR_CTRL_HDR_SIZE		(sizeof(struct rxr_ctrl_cq_hdr))
-
-#define RXR_CTRL_HDR_SIZE_NO_CQ		(sizeof(struct rxr_ctrl_hdr))
-
-#define RXR_CONNACK_HDR_SIZE		(sizeof(struct rxr_connack_hdr))
-
-#define RXR_CTS_HDR_SIZE		(sizeof(struct rxr_cts_hdr))
-
-#define RXR_DATA_HDR_SIZE		(sizeof(struct rxr_data_hdr))
-
 static inline void rxr_copy_shm_cq_entry(struct fi_cq_tagged_entry *cq_tagged_entry,
 					 struct fi_cq_data_entry *shm_cq_entry)
 {
@@ -903,94 +687,6 @@ struct rxr_tx_entry *rxr_ep_alloc_tx_entry(struct rxr_ep *rxr_ep,
 					   uint64_t tag,
 					   uint64_t flags);
 
-static inline void
-rxr_copy_pkt_entry(struct rxr_ep *ep,
-		   struct rxr_pkt_entry *dest,
-		   struct rxr_pkt_entry *src,
-		   enum rxr_pkt_entry_type type)
-{
-	FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
-	       "Copying packet (type %d) out of posted buffer\n", type);
-	assert(src->type == RXR_PKT_ENTRY_POSTED);
-	memcpy(dest, src, sizeof(struct rxr_pkt_entry));
-	dest->pkt = (struct rxr_pkt *)((char *)dest + sizeof(*dest));
-	memcpy(dest->pkt, src->pkt, ep->mtu_size);
-	dest->type = type;
-	dlist_init(&dest->entry);
-#if ENABLE_DEBUG
-	dlist_init(&dest->dbg_entry);
-#endif
-	dest->state = RXR_PKT_ENTRY_IN_USE;
-}
-
-static inline struct rxr_pkt_entry*
-rxr_get_pkt_entry(struct rxr_ep *ep, struct ofi_bufpool *pkt_pool)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	void *mr = NULL;
-
-	pkt_entry = ofi_buf_alloc_ex(pkt_pool, &mr);
-	if (!pkt_entry)
-		return NULL;
-#ifdef ENABLE_EFA_POISONING
-	memset(pkt_entry, 0, sizeof(*pkt_entry));
-#endif
-	dlist_init(&pkt_entry->entry);
-#if ENABLE_DEBUG
-	dlist_init(&pkt_entry->dbg_entry);
-#endif
-	pkt_entry->mr = (struct fid_mr *)mr;
-	pkt_entry->pkt = (struct rxr_pkt *)((char *)pkt_entry +
-			  sizeof(*pkt_entry));
-#ifdef ENABLE_EFA_POISONING
-	memset(pkt_entry->pkt, 0, ep->mtu_size);
-#endif
-	pkt_entry->state = RXR_PKT_ENTRY_IN_USE;
-
-	return pkt_entry;
-}
-
-#ifdef ENABLE_EFA_POISONING
-static inline void rxr_poison_mem_region(uint32_t *ptr, size_t size)
-{
-	int i;
-
-	for (i = 0; i < size / sizeof(rxr_poison_value); i++)
-		memcpy(ptr + i, &rxr_poison_value, sizeof(rxr_poison_value));
-}
-#endif
-
-static inline void rxr_release_tx_pkt_entry(struct rxr_ep *ep,
-					    struct rxr_pkt_entry *pkt)
-{
-	struct rxr_peer *peer;
-
-#if ENABLE_DEBUG
-	dlist_remove(&pkt->dbg_entry);
-#endif
-	/*
-	 * Decrement rnr_queued_pkts counter and reset backoff for this peer if
-	 * we get a send completion for a retransmitted packet.
-	 */
-	if (OFI_UNLIKELY(pkt->state == RXR_PKT_ENTRY_RNR_RETRANSMIT)) {
-		peer = rxr_ep_get_peer(ep, pkt->addr);
-		peer->rnr_queued_pkt_cnt--;
-		peer->timeout_interval = 0;
-		peer->rnr_timeout_exp = 0;
-		if (peer->rnr_state & RXR_PEER_IN_BACKOFF)
-			dlist_remove(&peer->rnr_entry);
-		peer->rnr_state = 0;
-		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-		       "reset backoff timer for peer: %" PRIu64 "\n",
-		       pkt->addr);
-	}
-#ifdef ENABLE_EFA_POISONING
-	rxr_poison_mem_region((uint32_t *)pkt, ep->tx_pkt_pool_entry_sz);
-#endif
-	pkt->state = RXR_PKT_ENTRY_FREE;
-	ofi_buf_free(pkt);
-}
-
 static inline void rxr_release_tx_entry(struct rxr_ep *ep,
 					struct rxr_tx_entry *tx_entry)
 {
@@ -1026,26 +722,6 @@ static inline void *rxr_pkt_start(struct rxr_pkt_entry *pkt_entry)
 	return (void *)((char *)pkt_entry + sizeof(*pkt_entry));
 }
 
-static inline struct rxr_base_hdr *rxr_get_base_hdr(void *pkt)
-{
-	return (struct rxr_base_hdr *)pkt;
-}
-
-static inline struct rxr_rts_hdr *rxr_get_rts_hdr(void *pkt)
-{
-	return (struct rxr_rts_hdr *)pkt;
-}
-
-static inline struct rxr_connack_hdr *rxr_get_connack_hdr(void *pkt)
-{
-	return (struct rxr_connack_hdr *)pkt;
-}
-
-static inline struct rxr_cts_hdr *rxr_get_cts_hdr(void *pkt)
-{
-	return (struct rxr_cts_hdr *)pkt;
-}
-
 static inline struct rxr_ctrl_cq_pkt *rxr_get_ctrl_cq_pkt(void *pkt)
 {
 	return (struct rxr_ctrl_cq_pkt *)pkt;
@@ -1056,11 +732,6 @@ static inline struct rxr_ctrl_pkt *rxr_get_ctrl_pkt(void *pkt)
 	return (struct rxr_ctrl_pkt *)pkt;
 }
 
-static inline struct rxr_data_pkt *rxr_get_data_pkt(void *pkt)
-{
-	return (struct rxr_data_pkt *)pkt;
-}
-
 static inline int rxr_match_addr(fi_addr_t addr, fi_addr_t match_addr)
 {
 	return (addr == FI_ADDR_UNSPEC || addr == match_addr);
@@ -1094,41 +765,6 @@ static inline void rxr_ep_dec_tx_pending(struct rxr_ep *ep,
 #endif
 }
 
-/*
- * Helper function to compute the maximum payload of the RTS header based on
- * the RTS header flags. The header may have a length greater than the possible
- * RTS payload size if it is a large message.
- */
-static inline uint64_t rxr_get_rts_data_size(struct rxr_ep *ep,
-					     struct rxr_rts_hdr *rts_hdr)
-{
-	/*
-	 * read RTS contain no data, because data is on remote EP.
-	 */
-	if (rts_hdr->flags & RXR_READ_REQ)
-		return 0;
-
-	if (rts_hdr->flags & RXR_SHM_HDR)
-		return (rts_hdr->flags & RXR_SHM_HDR_DATA) ? rts_hdr->data_len : 0;
-
-	size_t max_payload_size;
-
-	if (rts_hdr->flags & RXR_REMOTE_CQ_DATA)
-		max_payload_size = ep->mtu_size - RXR_CTRL_HDR_SIZE;
-	else
-		max_payload_size = ep->mtu_size - RXR_CTRL_HDR_SIZE_NO_CQ;
-
-	if (rts_hdr->flags & RXR_REMOTE_SRC_ADDR)
-		max_payload_size -= rts_hdr->addrlen;
-
-	if (rts_hdr->flags & RXR_WRITE)
-		max_payload_size -= rts_hdr->rma_iov_count *
-					sizeof(struct fi_rma_iov);
-
-	return (rts_hdr->data_len > max_payload_size)
-		? max_payload_size : rts_hdr->data_len;
-}
-
 static inline size_t rxr_get_rx_pool_chunk_cnt(struct rxr_ep *ep)
 {
 	return MIN(ep->core_rx_size, ep->rx_size);
@@ -1151,30 +787,6 @@ static inline int rxr_need_sas_ordering(struct rxr_ep *ep)
 		rxr_env.enable_sas_ordering);
 }
 
-static inline void rxr_release_rx_pkt_entry(struct rxr_ep *ep,
-					    struct rxr_pkt_entry *pkt_entry)
-{
-	if (pkt_entry->type == RXR_PKT_ENTRY_POSTED) {
-		struct rxr_peer *peer;
-
-		peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-		assert(peer);
-		if (peer->is_local)
-			ep->rx_bufs_shm_to_post++;
-		else
-			ep->rx_bufs_efa_to_post++;
-	}
-#if ENABLE_DEBUG
-	dlist_remove(&pkt_entry->dbg_entry);
-#endif
-#ifdef ENABLE_EFA_POISONING
-	/* the same pool size is used for all types of rx pkt_entries */
-	rxr_poison_mem_region((uint32_t *)pkt_entry, ep->rx_pkt_pool_entry_sz);
-#endif
-	pkt_entry->state = RXR_PKT_ENTRY_FREE;
-	ofi_buf_free(pkt_entry);
-}
-
 /* Initialization functions */
 void rxr_reset_rx_tx_to_core(const struct fi_info *user_info,
 			     struct fi_info *core_info);
@@ -1194,18 +806,7 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 /* EP sub-functions */
 void rxr_ep_progress(struct util_ep *util_ep);
 void rxr_ep_progress_internal(struct rxr_ep *rxr_ep);
-struct rxr_pkt_entry *rxr_ep_get_pkt_entry(struct rxr_ep *rxr_ep,
-					   struct ofi_bufpool *pkt_pool);
 int rxr_ep_post_buf(struct rxr_ep *ep, uint64_t flags, enum rxr_lower_ep_type lower_ep);
-ssize_t rxr_ep_send_msg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
-			const struct fi_msg *msg, uint64_t flags);
-ssize_t rxr_ep_post_data(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry);
-void rxr_ep_init_connack_pkt_entry(struct rxr_ep *ep,
-				   struct rxr_pkt_entry *pkt_entry,
-				   fi_addr_t addr);
-void rxr_ep_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
-				    uint64_t size, int request,
-				    int *window, int *credits);
 
 int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep,
 				 struct rxr_tx_entry *tx_entry);
@@ -1213,15 +814,6 @@ int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep,
 void rxr_inline_mr_reg(struct rxr_domain *rxr_domain,
 		       struct rxr_tx_entry *tx_entry);
 
-char *rxr_ep_init_rts_hdr(struct rxr_ep *ep,
-			  struct rxr_tx_entry *tx_entry,
-			  struct rxr_pkt_entry *pkt_entry);
-
-void rxr_ep_init_cts_pkt_entry(struct rxr_ep *ep,
-			       struct rxr_rx_entry *rx_entry,
-			       struct rxr_pkt_entry *pkt_entry,
-			       uint64_t size,
-			       int *credits);
 struct rxr_rx_entry *rxr_ep_get_new_unexp_rx_entry(struct rxr_ep *ep,
 						   struct rxr_pkt_entry *unexp_entry);
 struct rxr_rx_entry *rxr_ep_split_rx_entry(struct rxr_ep *ep,
@@ -1230,24 +822,12 @@ struct rxr_rx_entry *rxr_ep_split_rx_entry(struct rxr_ep *ep,
 					   struct rxr_pkt_entry *pkt_entry);
 int rxr_ep_efa_addr_to_str(const void *addr, char *temp_name);
 
-int rxr_ep_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry,
-			      int ctrl_type, bool inject);
-
-#if ENABLE_DEBUG
-void rxr_ep_print_pkt(char *prefix,
-		      struct rxr_ep *ep,
-		      struct rxr_base_hdr *hdr);
-#endif
-
 /* CQ sub-functions */
 int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 			   ssize_t prov_errno);
 int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 			   ssize_t prov_errno);
 int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err);
-ssize_t rxr_cq_post_cts(struct rxr_ep *ep,
-			struct rxr_rx_entry *rx_entry,
-			uint64_t size);
 
 void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 				struct rxr_rx_entry *rx_entry);
@@ -1259,34 +839,15 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 void rxr_cq_write_tx_completion(struct rxr_ep *ep,
 				struct rxr_tx_entry *tx_entry);
 
-ssize_t rxr_cq_recv_shm_large_message(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry);
-void rxr_cq_process_shm_large_message(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-				      struct rxr_rts_hdr *rts_hdr, char *data);
-
-void rxr_cq_process_shm_large_message(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-				      struct rxr_rts_hdr *rts_hdr, char *data);
-
-char *rxr_cq_read_rts_hdr(struct rxr_ep *ep,
-			  struct rxr_rx_entry *rx_entry,
-			  struct rxr_pkt_entry *pkt_entry);
-
-int rxr_cq_handle_rts_with_data(struct rxr_ep *ep,
-				struct rxr_rx_entry *rx_entry,
-				struct rxr_pkt_entry *pkt_entry,
-				char *data, size_t data_size);
-
-int rxr_cq_handle_pkt_with_data(struct rxr_ep *ep,
-				struct rxr_rx_entry *rx_entry,
-				struct rxr_pkt_entry *pkt_entry,
-				char *data, size_t seg_offset,
-				size_t seg_size);
+void rxr_cq_handle_tx_completion(struct rxr_ep *ep,
+				 struct rxr_tx_entry *tx_entry);
 
-void rxr_cq_handle_pkt_recv_completion(struct rxr_ep *ep,
-				       struct fi_cq_data_entry *comp,
-				       fi_addr_t src_addr);
+int rxr_cq_reorder_msg(struct rxr_ep *ep,
+		       struct rxr_peer *peer,
+		       struct rxr_pkt_entry *pkt_entry);
 
-void rxr_cq_handle_pkt_send_completion(struct rxr_ep *rxr_ep,
-				       struct fi_cq_data_entry *comp);
+void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
+					  struct rxr_peer *peer);
 
 void rxr_cq_handle_shm_rma_write_data(struct rxr_ep *ep,
 				      struct fi_cq_data_entry *shm_comp,
@@ -1371,66 +932,6 @@ static inline void rxr_rm_tx_cq_check(struct rxr_ep *ep, struct util_cq *tx_cq)
 	fastlock_release(&tx_cq->cq_lock);
 }
 
-static inline ssize_t rxr_ep_sendv_pkt(struct rxr_ep *ep,
-				       struct rxr_pkt_entry *pkt_entry,
-				       fi_addr_t addr, const struct iovec *iov,
-				       void **desc, size_t count,
-				       uint64_t flags)
-{
-	struct fi_msg msg;
-	struct rxr_peer *peer;
-
-	msg.msg_iov = iov;
-	msg.desc = desc;
-	msg.iov_count = count;
-	peer = rxr_ep_get_peer(ep, addr);
-	msg.addr = peer->is_local ? peer->shm_fiaddr : addr;
-	msg.context = pkt_entry;
-	msg.data = 0;
-
-	return rxr_ep_send_msg(ep, pkt_entry, &msg, flags);
-}
-
-/* rxr_pkt_start currently expects data pkt right after pkt hdr */
-static inline ssize_t rxr_ep_send_pkt_flags(struct rxr_ep *ep,
-					    struct rxr_pkt_entry *pkt_entry,
-					    fi_addr_t addr, uint64_t flags)
-{
-	struct iovec iov;
-	void *desc;
-
-	iov.iov_base = rxr_pkt_start(pkt_entry);
-	iov.iov_len = pkt_entry->pkt_size;
-
-	if (rxr_ep_get_peer(ep, addr)->is_local)
-		desc = NULL;
-	else
-		desc = rxr_ep_mr_local(ep) ? fi_mr_desc(pkt_entry->mr) : NULL;
-
-	return rxr_ep_sendv_pkt(ep, pkt_entry, addr, &iov, &desc, 1, flags);
-}
-
-static inline ssize_t rxr_ep_inject_pkt(struct rxr_ep *ep,
-					struct rxr_pkt_entry *pkt_entry,
-					fi_addr_t addr)
-{
-	struct rxr_peer *peer;
-
-	/* currently only EOR packet is injected using shm ep */
-	peer = rxr_ep_get_peer(ep, addr);
-	assert(peer);
-	assert(rxr_env.enable_shm_transfer && peer->is_local);
-	return fi_inject(ep->shm_ep, rxr_pkt_start(pkt_entry), pkt_entry->pkt_size,
-			 peer->shm_fiaddr);
-}
-
-static inline ssize_t rxr_ep_send_pkt(struct rxr_ep *ep,
-				      struct rxr_pkt_entry *pkt_entry,
-				      fi_addr_t addr)
-{
-	return rxr_ep_send_pkt_flags(ep, pkt_entry, addr, 0);
-}
-
 static inline bool rxr_peer_timeout_expired(struct rxr_ep *ep,
 					    struct rxr_peer *peer,
 					    uint64_t ts)
@@ -1440,7 +941,6 @@ static inline bool rxr_peer_timeout_expired(struct rxr_ep *ep,
 					  (1 << peer->rnr_timeout_exp))));
 }
 
-
 /* Performance counter declarations */
 #ifdef RXR_PERF_ENABLED
 #define RXR_PERF_FOREACH(DECL)	\
diff --git a/prov/efa/src/rxr/rxr_attr.c b/prov/efa/src/rxr/rxr_attr.c
index b749f06..7592881 100644
--- a/prov/efa/src/rxr/rxr_attr.c
+++ b/prov/efa/src/rxr/rxr_attr.c
@@ -37,10 +37,10 @@
 const uint32_t rxr_poison_value = 0xdeadbeef;
 #endif
 
-#define RXR_EP_CAPS (FI_MSG | FI_TAGGED | FI_RECV | FI_SEND | FI_READ \
-		     | FI_WRITE | FI_REMOTE_READ | FI_REMOTE_WRITE \
-		     | FI_DIRECTED_RECV | FI_SOURCE | FI_MULTI_RECV \
-		     | FI_RMA | FI_LOCAL_COMM | FI_REMOTE_COMM)
+#define RXR_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS)
+#define RXR_RX_CAPS (OFI_RX_MSG_CAPS | FI_TAGGED | OFI_RX_RMA_CAPS | \
+		     FI_SOURCE | FI_MULTI_RECV | FI_DIRECTED_RECV)
+#define RXR_DOM_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
 
 /* TODO: Add support for true FI_DELIVERY_COMPLETE */
 #define RXR_TX_OP_FLAGS (FI_INJECT | FI_COMPLETION | FI_TRANSMIT_COMPLETE | \
@@ -48,7 +48,7 @@ const uint32_t rxr_poison_value = 0xdeadbeef;
 #define RXR_RX_OP_FLAGS (FI_COMPLETION)
 
 struct fi_tx_attr rxr_tx_attr = {
-	.caps = RXR_EP_CAPS,
+	.caps = RXR_TX_CAPS,
 	.msg_order = FI_ORDER_SAS,
 	.op_flags = RXR_TX_OP_FLAGS,
 	.comp_order = FI_ORDER_NONE,
@@ -58,7 +58,7 @@ struct fi_tx_attr rxr_tx_attr = {
 };
 
 struct fi_rx_attr rxr_rx_attr = {
-	.caps = RXR_EP_CAPS,
+	.caps = RXR_RX_CAPS,
 	.msg_order = FI_ORDER_SAS,
 	.op_flags = RXR_RX_OP_FLAGS,
 	.comp_order = FI_ORDER_NONE,
@@ -95,7 +95,7 @@ struct fi_domain_attr rxr_domain_attr = {
 	.max_ep_tx_ctx = 1,
 	.max_ep_rx_ctx = 1,
 	.cq_data_size = RXR_CQ_DATA_SIZE,
-	.caps = FI_LOCAL_COMM | FI_REMOTE_COMM
+	.caps = RXR_DOM_CAPS
 };
 
 struct fi_fabric_attr rxr_fabric_attr = {
@@ -103,7 +103,7 @@ struct fi_fabric_attr rxr_fabric_attr = {
 };
 
 struct fi_info rxr_info = {
-	.caps = RXR_EP_CAPS,
+	.caps = RXR_TX_CAPS | RXR_RX_CAPS | RXR_DOM_CAPS,
 	.addr_format = FI_FORMAT_UNSPEC,
 	.tx_attr = &rxr_tx_attr,
 	.rx_attr = &rxr_rx_attr,
diff --git a/prov/efa/src/rxr/rxr_cq.c b/prov/efa/src/rxr/rxr_cq.c
index 8e95cb3..b707051 100644
--- a/prov/efa/src/rxr/rxr_cq.c
+++ b/prov/efa/src/rxr/rxr_cq.c
@@ -119,10 +119,10 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 	dlist_foreach_container_safe(&rx_entry->queued_pkts,
 				     struct rxr_pkt_entry,
 				     pkt_entry, entry, tmp)
-		rxr_release_tx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
 
 	if (rx_entry->unexp_rts_pkt) {
-		rxr_release_rx_pkt_entry(ep, rx_entry->unexp_rts_pkt);
+		rxr_pkt_entry_release_rx(ep, rx_entry->unexp_rts_pkt);
 		rx_entry->unexp_rts_pkt = NULL;
 	}
 
@@ -203,7 +203,7 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	dlist_foreach_container_safe(&tx_entry->queued_pkts,
 				     struct rxr_pkt_entry,
 				     pkt_entry, entry, tmp)
-		rxr_release_tx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
 
 	err_entry.flags = tx_entry->cq_entry.flags;
 	err_entry.op_context = tx_entry->cq_entry.op_context;
@@ -361,9 +361,9 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 		 */
 		if (err_entry.flags & FI_SEND) {
 			rxr_ep_dec_tx_pending(ep, peer, 1);
-			rxr_release_tx_pkt_entry(ep, pkt_entry);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
 		} else if (err_entry.flags & FI_RECV) {
-			rxr_release_rx_pkt_entry(ep, pkt_entry);
+			rxr_pkt_entry_release_rx(ep, pkt_entry);
 		} else {
 			assert(0 && "unknown err_entry flags in CONNACK packet");
 		}
@@ -376,7 +376,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 		 * Since we don't have any context besides the error code,
 		 * we will write to the eq instead.
 		 */
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		goto write_err;
 	}
 
@@ -393,7 +393,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 		    rxr_ep_domain(ep)->resource_mgmt == FI_RM_ENABLED) {
 			ret = rxr_cq_handle_tx_error(ep, tx_entry,
 						     err_entry.prov_errno);
-			rxr_release_tx_pkt_entry(ep, pkt_entry);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
 			return ret;
 		}
 
@@ -415,7 +415,7 @@ int rxr_cq_handle_cq_error(struct rxr_ep *ep, ssize_t err)
 		    rxr_ep_domain(ep)->resource_mgmt == FI_RM_ENABLED) {
 			ret = rxr_cq_handle_rx_error(ep, rx_entry,
 						     err_entry.prov_errno);
-			rxr_release_tx_pkt_entry(ep, pkt_entry);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
 			return ret;
 		}
 		rxr_cq_queue_pkt(ep, &rx_entry->queued_pkts, pkt_entry);
@@ -436,70 +436,6 @@ write_err:
 	return 0;
 }
 
-static int rxr_cq_match_recv(struct dlist_entry *item, const void *arg)
-{
-	const struct rxr_pkt_entry *pkt_entry = arg;
-	struct rxr_rx_entry *rx_entry;
-
-	rx_entry = container_of(item, struct rxr_rx_entry, entry);
-
-	return rxr_match_addr(rx_entry->addr, pkt_entry->addr);
-}
-
-static int rxr_cq_match_trecv(struct dlist_entry *item, const void *arg)
-{
-	struct rxr_pkt_entry *pkt_entry = (struct rxr_pkt_entry *)arg;
-	struct rxr_rx_entry *rx_entry;
-	uint64_t match_tag;
-
-	rx_entry = container_of(item, struct rxr_rx_entry, entry);
-
-	match_tag = rxr_get_rts_hdr(pkt_entry->pkt)->tag;
-
-	return rxr_match_addr(rx_entry->addr, pkt_entry->addr) &&
-	       rxr_match_tag(rx_entry->cq_entry.tag, rx_entry->ignore,
-			     match_tag);
-}
-
-static void rxr_cq_post_connack(struct rxr_ep *ep,
-				struct rxr_peer *peer,
-				fi_addr_t addr)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	ssize_t ret;
-
-	if (peer->state == RXR_PEER_ACKED)
-		return;
-
-	pkt_entry = rxr_get_pkt_entry(ep, ep->tx_pkt_efa_pool);
-	if (OFI_UNLIKELY(!pkt_entry))
-		return;
-
-	rxr_ep_init_connack_pkt_entry(ep, pkt_entry, addr);
-
-	/*
-	 * TODO: Once we start using a core's selective completion capability,
-	 * post the CONNACK packets without FI_COMPLETION.
-	 */
-	ret = rxr_ep_send_pkt(ep, pkt_entry, addr);
-
-	/*
-	 * Skip sending this connack on error and try again when processing the
-	 * next RTS from this peer containing the source information
-	 */
-	if (OFI_UNLIKELY(ret)) {
-		rxr_release_tx_pkt_entry(ep, pkt_entry);
-		if (ret == -FI_EAGAIN)
-			return;
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"Failed to send a CONNACK packet: ret %zd\n", ret);
-	} else {
-		peer->state = RXR_PEER_ACKED;
-	}
-
-	return;
-}
-
 void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 				struct rxr_rx_entry *rx_entry)
 {
@@ -591,7 +527,7 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 		else if (ep->util_ep.caps & FI_RMA_EVENT)
 			efa_cntr_report_rx_completion(&ep->util_ep, rx_entry->cq_entry.flags);
 
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
 
@@ -637,7 +573,7 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 		 * do not call rxr_release_rx_entry here because
 		 * caller will release
 		 */
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
 
@@ -645,234 +581,13 @@ void rxr_cq_handle_rx_completion(struct rxr_ep *ep,
 		rxr_msg_multi_recv_handle_completion(ep, rx_entry);
 
 	rxr_cq_write_rx_completion(ep, rx_entry);
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
 	return;
 }
 
-ssize_t rxr_cq_recv_shm_large_message(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_rma_context_pkt *rma_context_pkt;
-	struct fi_msg_rma msg;
-	struct iovec msg_iov[RXR_IOV_LIMIT];
-	struct fi_rma_iov rma_iov[RXR_IOV_LIMIT];
-	fi_addr_t src_shm_fiaddr;
-	uint64_t remain_len;
-	struct rxr_peer *peer;
-	int ret, i;
-
-	if (rx_entry->state == RXR_RX_QUEUED_SHM_LARGE_READ)
-		return 0;
-
-	pkt_entry = rxr_get_pkt_entry(ep, ep->tx_pkt_shm_pool);
-	assert(pkt_entry);
-
-	pkt_entry->x_entry = (void *)rx_entry;
-	pkt_entry->addr = rx_entry->addr;
-	rma_context_pkt = (struct rxr_rma_context_pkt *)pkt_entry->pkt;
-	rma_context_pkt->type = RXR_RMA_CONTEXT_PKT;
-	rma_context_pkt->version = RXR_PROTOCOL_VERSION;
-	rma_context_pkt->rma_context_type = RXR_SHM_LARGE_READ;
-	rma_context_pkt->tx_id = rx_entry->tx_id;
-
-	peer = rxr_ep_get_peer(ep, rx_entry->addr);
-	src_shm_fiaddr = peer->shm_fiaddr;
-
-	memset(&msg, 0, sizeof(msg));
-
-	remain_len = rx_entry->total_len;
-
-	for (i = 0; i < rx_entry->rma_iov_count; i++) {
-		rma_iov[i].addr = rx_entry->rma_iov[i].addr;
-		rma_iov[i].len = rx_entry->rma_iov[i].len;
-		rma_iov[i].key = 0;
-	}
-
-	/*
-	 * shm provider will compare #bytes CMA copied with total length of recv buffer
-	 * (msg_iov here). If they are not equal, an error is returned when reading shm
-	 * provider's cq. So shrink the total length of recv buffer if applicable
-	 */
-	for (i = 0; i < rx_entry->iov_count; i++) {
-		msg_iov[i].iov_base = (void *)rx_entry->iov[i].iov_base;
-		msg_iov[i].iov_len = (remain_len < rx_entry->iov[i].iov_len) ?
-					remain_len : rx_entry->iov[i].iov_len;
-		remain_len -= msg_iov[i].iov_len;
-		if (remain_len == 0)
-			break;
-	}
-
-	msg.msg_iov = msg_iov;
-	msg.iov_count = rx_entry->iov_count;
-	msg.desc = NULL;
-	msg.addr = src_shm_fiaddr;
-	msg.context = pkt_entry;
-	msg.rma_iov = rma_iov;
-	msg.rma_iov_count = rx_entry->rma_iov_count;
-
-	ret = fi_readmsg(ep->shm_ep, &msg, 0);
-
-	return ret;
-}
-
-void rxr_cq_process_shm_large_message(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-				      struct rxr_rts_hdr *rts_hdr, char *data)
-{
-	struct iovec *iovec_ptr;
-	int ret, i;
-
-	/* get iov_count of sender first */
-	memcpy(&rx_entry->rma_iov_count, data, sizeof(size_t));
-	data += sizeof(size_t);
-
-	iovec_ptr = (struct iovec *)data;
-	for (i = 0; i < rx_entry->rma_iov_count; i++) {
-		iovec_ptr = iovec_ptr + i;
-		rx_entry->rma_iov[i].addr = (intptr_t) iovec_ptr->iov_base;
-		rx_entry->rma_iov[i].len = iovec_ptr->iov_len;
-		rx_entry->rma_iov[i].key = 0;
-	}
-
-	ret = rxr_cq_recv_shm_large_message(ep, rx_entry);
-
-	if (OFI_UNLIKELY(ret)) {
-		if (ret == -FI_EAGAIN) {
-			rx_entry->state = RXR_RX_QUEUED_SHM_LARGE_READ;
-			dlist_insert_tail(&rx_entry->queued_entry,  &ep->rx_entry_queued_list);
-			return;
-		}
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"A large message RMA READ failed over shm provider.\n");
-		if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-			assert(0 && "failed to write err cq entry");
-	}
-}
-
-char *rxr_cq_read_rts_hdr(struct rxr_ep *ep,
-			  struct rxr_rx_entry *rx_entry,
-			  struct rxr_pkt_entry *pkt_entry)
-{
-	char *data;
-	struct rxr_rts_hdr *rts_hdr = NULL;
-	/*
-	 * Use the correct header and grab CQ data and data, but ignore the
-	 * source_address since that has been fetched and processed already
-	 */
-
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-
-	rx_entry->addr = pkt_entry->addr;
-	rx_entry->tx_id = rts_hdr->tx_id;
-	rx_entry->msg_id = rts_hdr->msg_id;
-	rx_entry->total_len = rts_hdr->data_len;
-	rx_entry->cq_entry.tag = rts_hdr->tag;
-
-	if (rts_hdr->flags & RXR_REMOTE_CQ_DATA) {
-		rx_entry->cq_entry.flags |= FI_REMOTE_CQ_DATA;
-		data = rxr_get_ctrl_cq_pkt(rts_hdr)->data + rts_hdr->addrlen;
-		rx_entry->cq_entry.data =
-				rxr_get_ctrl_cq_pkt(rts_hdr)->hdr.cq_data;
-	} else {
-		rx_entry->cq_entry.data = 0;
-		data = rxr_get_ctrl_pkt(rts_hdr)->data + rts_hdr->addrlen;
-	}
-
-	return data;
-}
-
-int rxr_cq_process_msg_rts(struct rxr_ep *ep,
-			   struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_peer *peer;
-	struct rxr_rts_hdr *rts_hdr;
-	struct dlist_entry *match;
-	struct rxr_rx_entry *rx_entry;
-	char *data;
-	size_t data_size;
-
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-
-	if (rts_hdr->flags & RXR_TAGGED) {
-		match = dlist_find_first_match(&ep->rx_tagged_list,
-					       &rxr_cq_match_trecv,
-					       (void *)pkt_entry);
-	} else {
-		match = dlist_find_first_match(&ep->rx_list,
-					       &rxr_cq_match_recv,
-					       (void *)pkt_entry);
-	}
-
-	if (OFI_UNLIKELY(!match)) {
-		rx_entry = rxr_ep_get_new_unexp_rx_entry(ep, pkt_entry);
-		if (!rx_entry) {
-			FI_WARN(&rxr_prov, FI_LOG_CQ,
-				"RX entries exhausted.\n");
-			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
-			return -FI_ENOBUFS;
-		}
-
-		/* we are not releasing pkt_entry here because it will be
-		 * processed later
-		 */
-		pkt_entry = rx_entry->unexp_rts_pkt;
-		rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-		rxr_cq_read_rts_hdr(ep, rx_entry, pkt_entry);
-		return 0;
-	}
-
-	rx_entry = container_of(match, struct rxr_rx_entry, entry);
-	if (rx_entry->rxr_flags & RXR_MULTI_RECV_POSTED) {
-		rx_entry = rxr_ep_split_rx_entry(ep, rx_entry,
-						 NULL, pkt_entry);
-		if (OFI_UNLIKELY(!rx_entry)) {
-			FI_WARN(&rxr_prov, FI_LOG_CQ,
-				"RX entries exhausted.\n");
-			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
-			return -FI_ENOBUFS;
-		}
-	}
-
-	rx_entry->state = RXR_RX_MATCHED;
-
-	if (!(rx_entry->fi_flags & FI_MULTI_RECV) ||
-	    !rxr_msg_multi_recv_buffer_available(ep, rx_entry->master_entry))
-		dlist_remove(match);
-
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(peer);
-
-	data = rxr_cq_read_rts_hdr(ep, rx_entry, pkt_entry);
-	if (peer->is_local && !(rts_hdr->flags & RXR_SHM_HDR_DATA)) {
-		rxr_cq_process_shm_large_message(ep, rx_entry, rts_hdr, data);
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
-		return 0;
-	}
-
-	data_size = rxr_get_rts_data_size(ep, rts_hdr);
-	return rxr_cq_handle_rts_with_data(ep, rx_entry,
-					   pkt_entry, data,
-					   data_size);
-}
-
-static int rxr_cq_process_rts(struct rxr_ep *ep,
-			      struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rts_hdr *rts_hdr;
-
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-
-	if (rts_hdr->flags & RXR_READ_REQ)
-		return rxr_rma_proc_read_rts(ep, pkt_entry);
-
-	if (rts_hdr->flags & RXR_WRITE)
-		return rxr_rma_proc_write_rts(ep, pkt_entry);
-
-	return rxr_cq_process_msg_rts(ep, pkt_entry);
-}
-
-static int rxr_cq_reorder_msg(struct rxr_ep *ep,
-			      struct rxr_peer *peer,
-			      struct rxr_pkt_entry *pkt_entry)
+int rxr_cq_reorder_msg(struct rxr_ep *ep,
+		       struct rxr_peer *peer,
+		       struct rxr_pkt_entry *pkt_entry)
 {
 	struct rxr_rts_hdr *rts_hdr;
 	struct rxr_pkt_entry *ooo_entry;
@@ -900,15 +615,15 @@ static int rxr_cq_reorder_msg(struct rxr_ep *ep,
 
 	if (OFI_LIKELY(rxr_env.rx_copy_ooo)) {
 		assert(pkt_entry->type == RXR_PKT_ENTRY_POSTED);
-		ooo_entry = rxr_get_pkt_entry(ep, ep->rx_ooo_pkt_pool);
+		ooo_entry = rxr_pkt_entry_alloc(ep, ep->rx_ooo_pkt_pool);
 		if (OFI_UNLIKELY(!ooo_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unable to allocate rx_pkt_entry for OOO msg\n");
 			return -FI_ENOMEM;
 		}
-		rxr_copy_pkt_entry(ep, ooo_entry, pkt_entry, RXR_PKT_ENTRY_OOO);
+		rxr_pkt_entry_copy(ep, ooo_entry, pkt_entry, RXR_PKT_ENTRY_OOO);
 		rts_hdr = rxr_get_rts_hdr(ooo_entry->pkt);
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 	} else {
 		ooo_entry = pkt_entry;
 	}
@@ -917,8 +632,8 @@ static int rxr_cq_reorder_msg(struct rxr_ep *ep,
 	return 1;
 }
 
-static void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
-						 struct rxr_peer *peer)
+void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
+					  struct rxr_peer *peer)
 {
 	struct rxr_pkt_entry *pending_pkt;
 	struct rxr_rts_hdr *rts_hdr;
@@ -935,8 +650,8 @@ static void rxr_cq_proc_pending_items_in_recvwin(struct rxr_ep *ep,
 		FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
 		       "Processing msg_id %d from robuf\n", rts_hdr->msg_id);
 
-		/* rxr_cq_process_rts will write error cq entry if needed */
-		ret = rxr_cq_process_rts(ep, pending_pkt);
+		/* rxr_pkt_proc_rts() will write error cq entry if needed */
+		ret = rxr_pkt_proc_rts(ep, pending_pkt);
 		if (OFI_UNLIKELY(ret)) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Error processing msg_id %d from robuf: %s\n",
@@ -988,299 +703,6 @@ void rxr_cq_handle_shm_rma_write_data(struct rxr_ep *ep, struct fi_cq_data_entry
 	rxr_release_rx_entry(ep, rx_entry);
 }
 
-/*
- * Sender handles the acknowledgment (RXR_EOR_PKT) from receiver on the completion
- * of the large message copy via fi_readmsg operation
- */
-static void rxr_cq_handle_eor(struct rxr_ep *ep,
-				  struct fi_cq_data_entry *comp,
-				  struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_eor_hdr *shm_eor;
-	struct rxr_tx_entry *tx_entry;
-
-	shm_eor = (struct rxr_eor_hdr *)pkt_entry->pkt;
-
-	/* pre-post buf used here, so can NOT track back to tx_entry with x_entry */
-	tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, shm_eor->tx_id);
-	rxr_cq_write_tx_completion(ep, tx_entry);
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
-}
-
-static void rxr_cq_handle_rts(struct rxr_ep *ep,
-			      struct fi_cq_data_entry *comp,
-			      struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rts_hdr *rts_hdr;
-	struct rxr_peer *peer;
-	int ret;
-
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(peer);
-
-	if (rxr_env.enable_shm_transfer && peer->is_local) {
-		/* no need to reorder msg for shm_ep
-		 * rxr_cq_process_rts will write error cq entry if needed
-		 */
-		rxr_cq_process_rts(ep, pkt_entry);
-		return;
-	}
-
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-
-	if (ep->core_caps & FI_SOURCE)
-		rxr_cq_post_connack(ep, peer, pkt_entry->addr);
-
-	if (rxr_need_sas_ordering(ep)) {
-		ret = rxr_cq_reorder_msg(ep, peer, pkt_entry);
-		if (ret == 1) {
-			/* Packet was queued */
-			return;
-		} else if (OFI_UNLIKELY(ret == -FI_EALREADY)) {
-			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
-				"Invalid msg_id: %" PRIu32
-				" robuf->exp_msg_id: %" PRIu32 "\n",
-			       rts_hdr->msg_id, peer->robuf->exp_msg_id);
-			if (!rts_hdr->addrlen)
-				efa_eq_write_error(&ep->util_ep, FI_EIO, ret);
-			rxr_release_rx_pkt_entry(ep, pkt_entry);
-			return;
-		} else if (OFI_UNLIKELY(ret == -FI_ENOMEM)) {
-			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
-			return;
-		} else if (OFI_UNLIKELY(ret < 0)) {
-			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
-				"Unknown error %d processing RTS packet msg_id: %"
-				PRIu32 "\n", ret, rts_hdr->msg_id);
-			efa_eq_write_error(&ep->util_ep, FI_EIO, ret);
-			return;
-		}
-
-		/* processing the expected packet */
-		ofi_recvwin_slide(peer->robuf);
-	}
-
-	/* rxr_cq_process_rts will write error cq entry if needed */
-	ret = rxr_cq_process_rts(ep, pkt_entry);
-	if (OFI_UNLIKELY(ret))
-		return;
-
-	/* process pending items in reorder buff */
-	if (rxr_need_sas_ordering(ep))
-		rxr_cq_proc_pending_items_in_recvwin(ep, peer);
-}
-
-static void rxr_cq_handle_connack(struct rxr_ep *ep,
-				  struct fi_cq_data_entry *comp,
-				  struct rxr_pkt_entry *pkt_entry,
-				  fi_addr_t src_addr)
-{
-	struct rxr_peer *peer;
-
-	/*
-	 * We don't really need any information from the actual connack packet
-	 * itself, just the src_addr from the CQE
-	 */
-	assert(src_addr != FI_ADDR_NOTAVAIL);
-	peer = rxr_ep_get_peer(ep, src_addr);
-	peer->state = RXR_PEER_ACKED;
-	FI_DBG(&rxr_prov, FI_LOG_CQ,
-	       "CONNACK received from %" PRIu64 "\n", src_addr);
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
-}
-
-int rxr_cq_handle_rts_with_data(struct rxr_ep *ep,
-				struct rxr_rx_entry *rx_entry,
-				struct rxr_pkt_entry *pkt_entry,
-				char *data, size_t data_size)
-{
-	struct rxr_rts_hdr *rts_hdr;
-	int64_t bytes_left, bytes_copied;
-	ssize_t ret;
-
-
-	/* rx_entry->cq_entry.len is total recv buffer size.
-	 * rx_entry->total_len is from rts_hdr and is total send buffer size.
-	 * if send buffer size < recv buffer size, we adjust value of rx_entry->cq_entry.len.
-	 * if send buffer size > recv buffer size, we have a truncated message.
-	 */
-	if (rx_entry->cq_entry.len > rx_entry->total_len)
-		rx_entry->cq_entry.len = rx_entry->total_len;
-
-	bytes_copied = ofi_copy_to_iov(rx_entry->iov, rx_entry->iov_count,
-				       0, data, data_size);
-
-	if (OFI_UNLIKELY(bytes_copied < data_size)) {
-		/* recv buffer is not big enough to hold rts, this must be a truncated message */
-		assert(bytes_copied == rx_entry->cq_entry.len &&
-		       rx_entry->cq_entry.len < rx_entry->total_len);
-		rx_entry->bytes_done = bytes_copied;
-		bytes_left = 0;
-	} else {
-		assert(bytes_copied == data_size);
-		rx_entry->bytes_done = data_size;
-		bytes_left = rx_entry->total_len - data_size;
-	}
-
-	assert(bytes_left >= 0);
-	if (!bytes_left) {
-		/* rxr_cq_handle_rx_completion() releases pkt_entry, thus
-		 * we do not release it here.
-		 */
-		rxr_cq_handle_rx_completion(ep, pkt_entry, rx_entry);
-		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
-		rxr_release_rx_entry(ep, rx_entry);
-		return 0;
-	}
-
-#if ENABLE_DEBUG
-	dlist_insert_tail(&rx_entry->rx_pending_entry, &ep->rx_pending_list);
-	ep->rx_pending++;
-#endif
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-	rx_entry->state = RXR_RX_RECV;
-	if (rts_hdr->flags & RXR_CREDIT_REQUEST)
-		rx_entry->credit_request = rts_hdr->credit_request;
-	else
-		rx_entry->credit_request = rxr_env.tx_min_credits;
-	ret = rxr_ep_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
-	return ret;
-}
-
-int rxr_cq_handle_pkt_with_data(struct rxr_ep *ep,
-				struct rxr_rx_entry *rx_entry,
-				struct rxr_pkt_entry *pkt_entry,
-				char *data, size_t seg_offset,
-				size_t seg_size)
-{
-	struct rxr_peer *peer;
-	int64_t bytes_left, bytes_copied;
-	ssize_t ret = 0;
-
-#if ENABLE_DEBUG
-	int pkt_type = rxr_get_base_hdr(pkt_entry->pkt)->type;
-	assert(pkt_type == RXR_DATA_PKT || pkt_type == RXR_READRSP_PKT);
-#endif
-	/* we are sinking message for CANCEL/DISCARD entry */
-	if (OFI_LIKELY(!(rx_entry->rxr_flags & RXR_RECV_CANCEL)) &&
-	    rx_entry->cq_entry.len > seg_offset) {
-		bytes_copied = ofi_copy_to_iov(rx_entry->iov, rx_entry->iov_count,
-					       seg_offset, data, seg_size);
-		if (bytes_copied != MIN(seg_size, rx_entry->cq_entry.len - seg_offset)) {
-			FI_WARN(&rxr_prov, FI_LOG_CQ, "wrong size! bytes_copied: %ld\n",
-				bytes_copied);
-			if (rxr_cq_handle_rx_error(ep, rx_entry, -FI_EINVAL))
-				assert(0 && "error writing error cq entry for EOR\n");
-		}
-	}
-
-	rx_entry->bytes_done += seg_size;
-
-	peer = rxr_ep_get_peer(ep, rx_entry->addr);
-	peer->rx_credits += ofi_div_ceil(seg_size, ep->max_data_payload_size);
-
-	rx_entry->window -= seg_size;
-	if (ep->available_data_bufs < rxr_get_rx_pool_chunk_cnt(ep))
-		ep->available_data_bufs++;
-
-	/* bytes_done is total bytes sent/received, which could be larger than
-	 * to bytes copied to recv buffer (for truncated messages).
-	 * rx_entry->total_len is from rts_hdr and is the size of send buffer,
-	 * thus we always have:
-	 *             rx_entry->total >= rx_entry->bytes_done
-	 */
-	bytes_left = rx_entry->total_len - rx_entry->bytes_done;
-	assert(bytes_left >= 0);
-	if (!bytes_left) {
-#if ENABLE_DEBUG
-		dlist_remove(&rx_entry->rx_pending_entry);
-		ep->rx_pending--;
-#endif
-		rxr_cq_handle_rx_completion(ep, pkt_entry, rx_entry);
-
-		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
-		rxr_release_rx_entry(ep, rx_entry);
-		return 0;
-	}
-
-	if (!rx_entry->window) {
-		assert(rx_entry->state == RXR_RX_RECV);
-		ret = rxr_ep_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
-	}
-
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
-	return ret;
-}
-
-static void rxr_cq_handle_readrsp(struct rxr_ep *ep,
-				  struct fi_cq_data_entry *comp,
-				  struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_readrsp_pkt *readrsp_pkt = NULL;
-	struct rxr_readrsp_hdr *readrsp_hdr = NULL;
-	struct rxr_rx_entry *rx_entry = NULL;
-
-	readrsp_pkt = (struct rxr_readrsp_pkt *)pkt_entry->pkt;
-	readrsp_hdr = &readrsp_pkt->hdr;
-	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, readrsp_hdr->rx_id);
-	assert(rx_entry->cq_entry.flags & FI_READ);
-	rx_entry->tx_id = readrsp_hdr->tx_id;
-	rxr_cq_handle_pkt_with_data(ep, rx_entry, pkt_entry,
-				    readrsp_pkt->data,
-				    0, readrsp_hdr->seg_size);
-}
-
-static void rxr_cq_handle_cts(struct rxr_ep *ep,
-			      struct fi_cq_data_entry *comp,
-			      struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_peer *peer;
-	struct rxr_cts_hdr *cts_pkt;
-	struct rxr_tx_entry *tx_entry;
-
-	cts_pkt = (struct rxr_cts_hdr *)pkt_entry->pkt;
-	if (cts_pkt->flags & RXR_READ_REQ)
-		tx_entry = ofi_bufpool_get_ibuf(ep->readrsp_tx_entry_pool, cts_pkt->tx_id);
-	else
-		tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, cts_pkt->tx_id);
-
-	tx_entry->rx_id = cts_pkt->rx_id;
-	tx_entry->window = cts_pkt->window;
-
-	/* Return any excess tx_credits that were borrowed for the request */
-	peer = rxr_ep_get_peer(ep, tx_entry->addr);
-	tx_entry->credit_allocated = ofi_div_ceil(cts_pkt->window, ep->max_data_payload_size);
-	if (tx_entry->credit_allocated < tx_entry->credit_request)
-		peer->tx_credits += tx_entry->credit_request - tx_entry->credit_allocated;
-
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
-
-	if (tx_entry->state != RXR_TX_SEND) {
-		tx_entry->state = RXR_TX_SEND;
-		dlist_insert_tail(&tx_entry->entry, &ep->tx_pending_list);
-	}
-	return;
-}
-
-static void rxr_cq_handle_data(struct rxr_ep *ep,
-			       struct fi_cq_data_entry *comp,
-			       struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_data_pkt *data_pkt;
-	struct rxr_rx_entry *rx_entry;
-	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
-
-	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool,
-					 data_pkt->hdr.rx_id);
-
-	rxr_cq_handle_pkt_with_data(ep, rx_entry,
-				    pkt_entry,
-				    data_pkt->data,
-				    data_pkt->hdr.seg_offset,
-				    data_pkt->hdr.seg_size);
-}
-
 void rxr_cq_write_tx_completion(struct rxr_ep *ep,
 				struct rxr_tx_entry *tx_entry)
 {
@@ -1332,116 +754,7 @@ void rxr_cq_write_tx_completion(struct rxr_ep *ep,
 	return;
 }
 
-fi_addr_t rxr_cq_insert_addr_from_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
-{
-	int i, ret;
-	void *raw_address;
-	fi_addr_t rdm_addr;
-	struct rxr_rts_hdr *rts_hdr;
-	struct efa_ep *efa_ep;
-
-	assert(rxr_get_base_hdr(pkt_entry->pkt)->type == RXR_RTS_PKT);
-
-	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-	assert(rts_hdr->flags & RXR_REMOTE_SRC_ADDR);
-	assert(rts_hdr->addrlen > 0);
-	if (rxr_get_base_hdr(pkt_entry->pkt)->version !=
-	    RXR_PROTOCOL_VERSION) {
-		char buffer[ep->core_addrlen * 3];
-		int length = 0;
-
-		for (i = 0; i < ep->core_addrlen; i++)
-			length += sprintf(&buffer[length], "%02x ",
-					  ep->core_addr[i]);
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"Host %s:Invalid protocol version %d. Expected protocol version %d.\n",
-			buffer,
-			rxr_get_base_hdr(pkt_entry->pkt)->version,
-			RXR_PROTOCOL_VERSION);
-		efa_eq_write_error(&ep->util_ep, FI_EIO, -FI_EINVAL);
-		fprintf(stderr, "Invalid protocol version %d. Expected protocol version %d. %s:%d\n",
-			rxr_get_base_hdr(pkt_entry->pkt)->version,
-			RXR_PROTOCOL_VERSION, __FILE__, __LINE__);
-		abort();
-	}
-
-	raw_address = (rts_hdr->flags & RXR_REMOTE_CQ_DATA) ?
-		      rxr_get_ctrl_cq_pkt(rts_hdr)->data
-		      : rxr_get_ctrl_pkt(rts_hdr)->data;
-
-	ret = efa_av_insert_addr(efa_ep->av, (struct efa_ep_addr *)raw_address,
-				&rdm_addr, 0, NULL);
-	if (OFI_UNLIKELY(ret != 0)) {
-		efa_eq_write_error(&ep->util_ep, FI_EINVAL, ret);
-		return -1;
-	}
-
-	return rdm_addr;
-}
-
-void rxr_cq_handle_pkt_recv_completion(struct rxr_ep *ep,
-				       struct fi_cq_data_entry *cq_entry,
-				       fi_addr_t src_addr)
-{
-	struct rxr_peer *peer;
-	struct rxr_pkt_entry *pkt_entry;
-
-	pkt_entry = (struct rxr_pkt_entry *)cq_entry->op_context;
-
-#if ENABLE_DEBUG
-	dlist_remove(&pkt_entry->dbg_entry);
-	dlist_insert_tail(&pkt_entry->dbg_entry, &ep->rx_pkt_list);
-#ifdef ENABLE_RXR_PKT_DUMP
-	rxr_ep_print_pkt("Received", ep, (struct rxr_base_hdr *)pkt_entry->pkt);
-#endif
-#endif
-	if (OFI_UNLIKELY(src_addr == FI_ADDR_NOTAVAIL))
-		pkt_entry->addr = rxr_cq_insert_addr_from_rts(ep, pkt_entry);
-	else
-		pkt_entry->addr = src_addr;
-
-	assert(rxr_get_base_hdr(pkt_entry->pkt)->version ==
-	       RXR_PROTOCOL_VERSION);
-
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-
-	if (rxr_env.enable_shm_transfer && peer->is_local)
-		ep->posted_bufs_shm--;
-	else
-		ep->posted_bufs_efa--;
-
-	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
-	case RXR_RTS_PKT:
-		rxr_cq_handle_rts(ep, cq_entry, pkt_entry);
-		return;
-	case RXR_EOR_PKT:
-		rxr_cq_handle_eor(ep, cq_entry, pkt_entry);
-		return;
-	case RXR_CONNACK_PKT:
-		rxr_cq_handle_connack(ep, cq_entry, pkt_entry, src_addr);
-		return;
-	case RXR_CTS_PKT:
-		rxr_cq_handle_cts(ep, cq_entry, pkt_entry);
-		return;
-	case RXR_DATA_PKT:
-		rxr_cq_handle_data(ep, cq_entry, pkt_entry);
-		return;
-	case RXR_READRSP_PKT:
-		rxr_cq_handle_readrsp(ep, cq_entry, pkt_entry);
-		return;
-	default:
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"invalid control pkt type %d\n",
-			rxr_get_base_hdr(pkt_entry->pkt)->type);
-		assert(0 && "invalid control pkt type");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
-		return;
-	}
-	return;
-}
-
-static int rxr_send_completion_mr_dereg(struct rxr_tx_entry *tx_entry)
+int rxr_send_completion_mr_dereg(struct rxr_tx_entry *tx_entry)
 {
 	int i, ret = 0;
 
@@ -1455,182 +768,58 @@ static int rxr_send_completion_mr_dereg(struct rxr_tx_entry *tx_entry)
 	return ret;
 }
 
-void rxr_cq_handle_rma_context_pkt(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+void rxr_cq_handle_tx_completion(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 {
-	struct rxr_tx_entry *tx_entry = NULL;
-	struct rxr_rx_entry *rx_entry = NULL;
-	struct rxr_rma_context_pkt *rma_context_pkt;
 	int ret;
+	struct rxr_peer *peer;
 
-	assert(rxr_get_base_hdr(pkt_entry->pkt)->version == RXR_PROTOCOL_VERSION);
-
-	rma_context_pkt = (struct rxr_rma_context_pkt *)pkt_entry->pkt;
-
-	switch (rma_context_pkt->rma_context_type) {
-	case RXR_SHM_RMA_READ:
-	case RXR_SHM_RMA_WRITE:
-		/* Completion of RMA READ/WRTITE operations that apps call */
-		tx_entry = pkt_entry->x_entry;
-
-		if (tx_entry->fi_flags & FI_COMPLETION) {
-			rxr_cq_write_tx_completion(ep, tx_entry);
-		} else {
-			efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-			rxr_release_tx_entry(ep, tx_entry);
-		}
-		rxr_release_tx_pkt_entry(ep, pkt_entry);
-		break;
-	case RXR_SHM_LARGE_READ:
-		/*
-		 * This must be on the receiver (remote) side of two-sided message
-		 * transfer, which is also the initiator of RMA READ.
-		 * We get RMA READ completion for previously issued
-		 * fi_read operation over shm provider, which means
-		 * receiver side has received all data from sender
-		 */
-		rx_entry = pkt_entry->x_entry;
-		rx_entry->cq_entry.len = rx_entry->total_len;
-		rx_entry->bytes_done = rx_entry->total_len;
+	if (tx_entry->state == RXR_TX_SEND)
+		dlist_remove(&tx_entry->entry);
 
-		ret = rxr_ep_post_ctrl_or_queue(ep, RXR_TX_ENTRY, rx_entry, RXR_EOR_PKT, 1);
-		if (ret) {
-			if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-				assert(0 && "error writing error cq entry for EOR\n");
+	if (tx_entry->state == RXR_TX_SEND &&
+	    efa_mr_cache_enable && rxr_ep_mr_local(ep)) {
+		ret = rxr_send_completion_mr_dereg(tx_entry);
+		if (OFI_UNLIKELY(ret)) {
+			FI_WARN(&rxr_prov, FI_LOG_MR,
+				"In-line memory deregistration failed with error: %s.\n",
+				fi_strerror(-ret));
 		}
-
-		if (rx_entry->fi_flags & FI_MULTI_RECV)
-			rxr_msg_multi_recv_handle_completion(ep, rx_entry);
-		rxr_cq_write_rx_completion(ep, rx_entry);
-		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
-		if (OFI_LIKELY(!ret))
-			rxr_release_rx_entry(ep, rx_entry);
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
-		break;
-	default:
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid rma_context_type in RXR_RMA_CONTEXT_PKT %d\n",
-			rma_context_pkt->rma_context_type);
-		assert(0 && "invalid RXR_RMA_CONTEXT_PKT rma_context_type\n");
 	}
-}
 
-void rxr_cq_handle_pkt_send_completion(struct rxr_ep *ep, struct fi_cq_data_entry *comp)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_tx_entry *tx_entry = NULL;
-	struct rxr_peer *peer;
-	struct rxr_rts_hdr *rts_hdr = NULL;
-	struct rxr_readrsp_hdr *readrsp_hdr = NULL;
-	uint32_t tx_id;
-	int ret;
-
-	pkt_entry = (struct rxr_pkt_entry *)comp->op_context;
-	assert(rxr_get_base_hdr(pkt_entry->pkt)->version ==
-	       RXR_PROTOCOL_VERSION);
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	peer->tx_credits += tx_entry->credit_allocated;
 
-	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
-	case RXR_RTS_PKT:
+	if (tx_entry->cq_entry.flags & FI_READ) {
 		/*
-		 * for FI_READ, it is possible (though does not happen very offen) that at the point
-		 * tx_entry has been released. The reason is, for FI_READ:
-		 *     1. only the initator side will send a RTS.
-		 *     2. the initator side will receive data packet. When all data was received,
-		 *        it will release the tx_entry
-		 * Therefore, if it so happens that all data was received before we got the send
-		 * completion notice, we will have a released tx_entry at this point.
-		 * Nonetheless, because for FI_READ tx_entry will be release in rxr_handle_rx_completion,
-		 * we will ignore it here.
-		 *
-		 * For shm provider, we will write completion for small & medium  message, as data has
-		 * been sent in the RTS packet; for large message, will wait for the EOR packet
+		 * this must be on remote side
+		 * see explaination on rxr_cq_handle_rx_completion
 		 */
-		rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-		if (!(rts_hdr->flags & RXR_READ_REQ)) {
-			tx_id = rts_hdr->tx_id;
-			tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, tx_id);
-			tx_entry->bytes_acked += rxr_get_rts_data_size(ep, rts_hdr);
-		}
-		break;
-	case RXR_CONNACK_PKT:
-		break;
-	case RXR_CTS_PKT:
-		break;
-	case RXR_DATA_PKT:
-		tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-		tx_entry->bytes_acked +=
-			rxr_get_data_pkt(pkt_entry->pkt)->hdr.seg_size;
-		break;
-	case RXR_READRSP_PKT:
-		readrsp_hdr = rxr_get_readrsp_hdr(pkt_entry->pkt);
-		tx_id = readrsp_hdr->tx_id;
-		tx_entry = ofi_bufpool_get_ibuf(ep->readrsp_tx_entry_pool, tx_id);
-		assert(tx_entry->cq_entry.flags & FI_READ);
-		tx_entry->bytes_acked += readrsp_hdr->seg_size;
-		break;
-	case RXR_RMA_CONTEXT_PKT:
-		rxr_cq_handle_rma_context_pkt(ep, pkt_entry);
-		return;
-	default:
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"invalid control pkt type %d\n",
-			rxr_get_base_hdr(pkt_entry->pkt)->type);
-		assert(0 && "invalid control pkt type");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
-		return;
-	}
-
-	if (tx_entry && tx_entry->total_len == tx_entry->bytes_acked) {
-		if (tx_entry->state == RXR_TX_SEND)
-			dlist_remove(&tx_entry->entry);
-		if (tx_entry->state == RXR_TX_SEND &&
-		    efa_mr_cache_enable && rxr_ep_mr_local(ep)) {
-			ret = rxr_send_completion_mr_dereg(tx_entry);
-			if (OFI_UNLIKELY(ret)) {
-				FI_WARN(&rxr_prov, FI_LOG_MR,
-					"In-line memory deregistration failed with error: %s.\n",
-					fi_strerror(-ret));
-			}
-		}
+		struct rxr_rx_entry *rx_entry = NULL;
 
-		peer->tx_credits += tx_entry->credit_allocated;
+		rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, tx_entry->rma_loc_rx_id);
+		assert(rx_entry);
+		assert(rx_entry->state == RXR_RX_WAIT_READ_FINISH);
 
-		if (tx_entry->cq_entry.flags & FI_READ) {
-			/*
-			 * this must be on remote side
-			 * see explaination on rxr_cq_handle_rx_completion
-			 */
-			struct rxr_rx_entry *rx_entry = NULL;
-
-			rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, tx_entry->rma_loc_rx_id);
-			assert(rx_entry);
-			assert(rx_entry->state == RXR_RX_WAIT_READ_FINISH);
-
-			if (ep->util_ep.caps & FI_RMA_EVENT) {
-				rx_entry->cq_entry.len = rx_entry->total_len;
-				rx_entry->bytes_done = rx_entry->total_len;
-				efa_cntr_report_rx_completion(&ep->util_ep, rx_entry->cq_entry.flags);
-			}
+		if (ep->util_ep.caps & FI_RMA_EVENT) {
+			rx_entry->cq_entry.len = rx_entry->total_len;
+			rx_entry->bytes_done = rx_entry->total_len;
+			efa_cntr_report_rx_completion(&ep->util_ep, rx_entry->cq_entry.flags);
+		}
 
-			rxr_release_rx_entry(ep, rx_entry);
-			/* just release tx, do not write completion */
-			rxr_release_tx_entry(ep, tx_entry);
-		} else if (tx_entry->cq_entry.flags & FI_WRITE) {
-			if (tx_entry->fi_flags & FI_COMPLETION) {
-				rxr_cq_write_tx_completion(ep, tx_entry);
-			} else {
-				efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
-				rxr_release_tx_entry(ep, tx_entry);
-			}
-		} else {
-			assert(tx_entry->cq_entry.flags & FI_SEND);
+		rxr_release_rx_entry(ep, rx_entry);
+		/* just release tx, do not write completion */
+		rxr_release_tx_entry(ep, tx_entry);
+	} else if (tx_entry->cq_entry.flags & FI_WRITE) {
+		if (tx_entry->fi_flags & FI_COMPLETION) {
 			rxr_cq_write_tx_completion(ep, tx_entry);
+		} else {
+			efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
+			rxr_release_tx_entry(ep, tx_entry);
 		}
+	} else {
+		assert(tx_entry->cq_entry.flags & FI_SEND);
+		rxr_cq_write_tx_completion(ep, tx_entry);
 	}
-
-	rxr_release_tx_pkt_entry(ep, pkt_entry);
-	if (!peer->is_local)
-		rxr_ep_dec_tx_pending(ep, peer, 0);
-	return;
 }
 
 static int rxr_cq_close(struct fid *fid)
diff --git a/prov/efa/src/rxr/rxr_ep.c b/prov/efa/src/rxr/rxr_ep.c
index f139cd9..2305d7f 100644
--- a/prov/efa/src/rxr/rxr_ep.c
+++ b/prov/efa/src/rxr/rxr_ep.c
@@ -42,132 +42,7 @@
 #include "efa.h"
 #include "rxr_msg.h"
 #include "rxr_rma.h"
-
-#define RXR_PKT_DUMP_DATA_LEN 64
-
-#if ENABLE_DEBUG
-static void rxr_ep_print_rts_pkt(struct rxr_ep *ep,
-				 char *prefix, struct rxr_rts_hdr *rts_hdr)
-{
-	char str[RXR_PKT_DUMP_DATA_LEN * 4];
-	size_t str_len = RXR_PKT_DUMP_DATA_LEN * 4, l;
-	uint8_t *src;
-	uint8_t *data;
-	int i;
-
-	str[str_len - 1] = '\0';
-
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "%s RxR RTS packet - version: %"	PRIu8
-	       " flags: %"	PRIu16
-	       " tx_id: %"	PRIu32
-	       " msg_id: %"	PRIu32
-	       " tag: %lx data_len: %"	PRIu64 "\n",
-	       prefix, rts_hdr->version, rts_hdr->flags, rts_hdr->tx_id,
-	       rts_hdr->msg_id, rts_hdr->tag, rts_hdr->data_len);
-
-	if ((rts_hdr->flags & RXR_REMOTE_CQ_DATA) &&
-	    (rts_hdr->flags & RXR_REMOTE_SRC_ADDR)) {
-		src = (uint8_t *)((struct rxr_ctrl_cq_pkt *)rts_hdr)->data;
-		data = src + rts_hdr->addrlen;
-	} else if (!(rts_hdr->flags & RXR_REMOTE_CQ_DATA) &&
-		   (rts_hdr->flags & RXR_REMOTE_SRC_ADDR)) {
-		src = (uint8_t *)((struct rxr_ctrl_pkt *)rts_hdr)->data;
-		data = src + rts_hdr->addrlen;
-	} else if ((rts_hdr->flags & RXR_REMOTE_CQ_DATA) &&
-		   !(rts_hdr->flags & RXR_REMOTE_SRC_ADDR)) {
-		data = (uint8_t *)((struct rxr_ctrl_cq_pkt *)rts_hdr)->data;
-	} else {
-		data = (uint8_t *)((struct rxr_ctrl_pkt *)rts_hdr)->data;
-	}
-
-	if (rts_hdr->flags & RXR_REMOTE_CQ_DATA)
-		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-		       "\tcq_data: %08lx\n",
-		       ((struct rxr_ctrl_cq_hdr *)rts_hdr)->cq_data);
-
-	if (rts_hdr->flags & RXR_REMOTE_SRC_ADDR) {
-		l = snprintf(str, str_len, "\tsrc_addr: ");
-		for (i = 0; i < rts_hdr->addrlen; i++)
-			l += snprintf(str + l, str_len - l, "%02x ", src[i]);
-		FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
-	}
-
-	l = snprintf(str, str_len, ("\tdata:    "));
-	for (i = 0; i < MIN(rxr_get_rts_data_size(ep, rts_hdr),
-			    RXR_PKT_DUMP_DATA_LEN); i++)
-		l += snprintf(str + l, str_len - l, "%02x ", data[i]);
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
-}
-
-static void rxr_ep_print_connack_pkt(char *prefix,
-				     struct rxr_connack_hdr *connack_hdr)
-{
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "%s RxR CONNACK packet - version: %" PRIu8
-	       " flags: %x\n", prefix, connack_hdr->version,
-	       connack_hdr->flags);
-}
-
-static void rxr_ep_print_cts_pkt(char *prefix, struct rxr_cts_hdr *cts_hdr)
-{
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "%s RxR CTS packet - version: %"	PRIu8
-	       " flags: %x tx_id: %" PRIu32
-	       " rx_id: %"	   PRIu32
-	       " window: %"	   PRIu64
-	       "\n", prefix, cts_hdr->version, cts_hdr->flags,
-	       cts_hdr->tx_id, cts_hdr->rx_id, cts_hdr->window);
-}
-
-static void rxr_ep_print_data_pkt(char *prefix, struct rxr_data_pkt *data_pkt)
-{
-	char str[RXR_PKT_DUMP_DATA_LEN * 4];
-	size_t str_len = RXR_PKT_DUMP_DATA_LEN * 4, l;
-	int i;
-
-	str[str_len - 1] = '\0';
-
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "%s RxR DATA packet -  version: %" PRIu8
-	       " flags: %x rx_id: %" PRIu32
-	       " seg_size: %"	     PRIu64
-	       " seg_offset: %"	     PRIu64
-	       "\n", prefix, data_pkt->hdr.version, data_pkt->hdr.flags,
-	       data_pkt->hdr.rx_id, data_pkt->hdr.seg_size,
-	       data_pkt->hdr.seg_offset);
-
-	l = snprintf(str, str_len, ("\tdata:    "));
-	for (i = 0; i < MIN(data_pkt->hdr.seg_size, RXR_PKT_DUMP_DATA_LEN);
-	     i++)
-		l += snprintf(str + l, str_len - l, "%02x ",
-			      ((uint8_t *)data_pkt->data)[i]);
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
-}
-
-void rxr_ep_print_pkt(char *prefix, struct rxr_ep *ep, struct rxr_base_hdr *hdr)
-{
-	switch (hdr->type) {
-	case RXR_RTS_PKT:
-		rxr_ep_print_rts_pkt(ep, prefix, (struct rxr_rts_hdr *)hdr);
-		break;
-	case RXR_CONNACK_PKT:
-		rxr_ep_print_connack_pkt(prefix, (struct rxr_connack_hdr *)hdr);
-		break;
-	case RXR_CTS_PKT:
-		rxr_ep_print_cts_pkt(prefix, (struct rxr_cts_hdr *)hdr);
-		break;
-	case RXR_DATA_PKT:
-		rxr_ep_print_data_pkt(prefix, (struct rxr_data_pkt *)hdr);
-		break;
-	default:
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid ctl pkt type %d\n",
-			rxr_get_base_hdr(hdr)->type);
-		assert(0);
-		return;
-	}
-}
-#endif
+#include "rxr_pkt_cmd.h"
 
 struct rxr_rx_entry *rxr_ep_rx_entry_init(struct rxr_ep *ep,
 					  struct rxr_rx_entry *rx_entry,
@@ -265,15 +140,15 @@ struct rxr_rx_entry *rxr_ep_get_new_unexp_rx_entry(struct rxr_ep *ep,
 	uint32_t op;
 
 	if (rxr_env.rx_copy_unexp && pkt_entry->type == RXR_PKT_ENTRY_POSTED) {
-		unexp_entry = rxr_get_pkt_entry(ep, ep->rx_unexp_pkt_pool);
+		unexp_entry = rxr_pkt_entry_alloc(ep, ep->rx_unexp_pkt_pool);
 		if (OFI_UNLIKELY(!unexp_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unable to allocate rx_pkt_entry for unexp msg\n");
 			return NULL;
 		}
-		rxr_copy_pkt_entry(ep, unexp_entry, pkt_entry,
+		rxr_pkt_entry_copy(ep, unexp_entry, pkt_entry,
 				   RXR_PKT_ENTRY_UNEXP);
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
 	} else {
 		unexp_entry = pkt_entry;
 	}
@@ -364,10 +239,10 @@ int rxr_ep_post_buf(struct rxr_ep *ep, uint64_t flags, enum rxr_lower_ep_type lo
 
 	switch (lower_ep_type) {
 	case SHM_EP:
-		rx_pkt_entry = rxr_get_pkt_entry(ep, ep->rx_pkt_shm_pool);
+		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->rx_pkt_shm_pool);
 		break;
 	case EFA_EP:
-		rx_pkt_entry = rxr_get_pkt_entry(ep, ep->rx_pkt_efa_pool);
+		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->rx_pkt_efa_pool);
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
@@ -403,7 +278,7 @@ int rxr_ep_post_buf(struct rxr_ep *ep, uint64_t flags, enum rxr_lower_ep_type lo
 		msg.desc = &desc;
 		ret = fi_recvmsg(ep->shm_ep, &msg, flags);
 		if (OFI_UNLIKELY(ret)) {
-			rxr_release_rx_pkt_entry(ep, rx_pkt_entry);
+			rxr_pkt_entry_release_rx(ep, rx_pkt_entry);
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"failed to post buf for shm  %d (%s)\n", -ret,
 				fi_strerror(-ret));
@@ -421,7 +296,7 @@ int rxr_ep_post_buf(struct rxr_ep *ep, uint64_t flags, enum rxr_lower_ep_type lo
 		msg.desc = &desc;
 		ret = fi_recvmsg(ep->rdm_ep, &msg, flags);
 		if (OFI_UNLIKELY(ret)) {
-			rxr_release_rx_pkt_entry(ep, rx_pkt_entry);
+			rxr_pkt_entry_release_rx(ep, rx_pkt_entry);
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"failed to post buf %d (%s)\n", -ret,
 				fi_strerror(-ret));
@@ -531,249 +406,6 @@ struct rxr_tx_entry *rxr_ep_alloc_tx_entry(struct rxr_ep *rxr_ep,
 	return tx_entry;
 }
 
-/*
- * Copies all consecutive small iov's into one buffer. If the function reaches
- * an iov greater than the max memcpy size, it will end, only copying up to
- * that iov.
- */
-static size_t rxr_copy_from_iov(void *buf, uint64_t remaining_len,
-				struct rxr_tx_entry *tx_entry)
-{
-	struct iovec *tx_iov = tx_entry->iov;
-	uint64_t done = 0, len;
-
-	while (tx_entry->iov_index < tx_entry->iov_count &&
-	       done < remaining_len) {
-		len = tx_iov[tx_entry->iov_index].iov_len;
-		if (tx_entry->mr[tx_entry->iov_index])
-			break;
-
-		len -= tx_entry->iov_offset;
-
-		/*
-		 * If the amount to be written surpasses the remaining length,
-		 * copy up to the remaining length and return, else copy the
-		 * entire iov and continue.
-		 */
-		if (done + len > remaining_len) {
-			len = remaining_len - done;
-			memcpy((char *)buf + done,
-			       (char *)tx_iov[tx_entry->iov_index].iov_base +
-			       tx_entry->iov_offset, len);
-			tx_entry->iov_offset += len;
-			done += len;
-			break;
-		}
-		memcpy((char *)buf + done,
-		       (char *)tx_iov[tx_entry->iov_index].iov_base +
-		       tx_entry->iov_offset, len);
-		tx_entry->iov_index++;
-		tx_entry->iov_offset = 0;
-		done += len;
-	}
-	return done;
-}
-
-ssize_t rxr_ep_send_msg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
-			const struct fi_msg *msg, uint64_t flags)
-{
-	struct rxr_peer *peer;
-	size_t ret;
-
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(ep->tx_pending <= ep->max_outstanding_tx);
-
-	if (ep->tx_pending == ep->max_outstanding_tx)
-		return -FI_EAGAIN;
-
-	if (peer->rnr_state & RXR_PEER_IN_BACKOFF)
-		return -FI_EAGAIN;
-
-#if ENABLE_DEBUG
-	dlist_insert_tail(&pkt_entry->dbg_entry, &ep->tx_pkt_list);
-#ifdef ENABLE_RXR_PKT_DUMP
-	rxr_ep_print_pkt("Sent", ep, (struct rxr_base_hdr *)pkt_entry->pkt);
-#endif
-#endif
-	if (rxr_env.enable_shm_transfer && peer->is_local) {
-		ret = fi_sendmsg(ep->shm_ep, msg, flags);
-	} else {
-		ret = fi_sendmsg(ep->rdm_ep, msg, flags);
-		if (OFI_LIKELY(!ret))
-			rxr_ep_inc_tx_pending(ep, peer);
-	}
-
-	return ret;
-}
-
-static ssize_t rxr_ep_send_data_pkt_entry(struct rxr_ep *ep,
-					  struct rxr_tx_entry *tx_entry,
-					  struct rxr_pkt_entry *pkt_entry,
-					  struct rxr_data_pkt *data_pkt)
-{
-	uint64_t payload_size;
-
-	payload_size = MIN(tx_entry->total_len - tx_entry->bytes_sent,
-			   ep->max_data_payload_size);
-	payload_size = MIN(payload_size, tx_entry->window);
-	data_pkt->hdr.seg_size = payload_size;
-
-	pkt_entry->pkt_size = ofi_copy_from_iov(data_pkt->data,
-						payload_size,
-						tx_entry->iov,
-						tx_entry->iov_count,
-						tx_entry->bytes_sent);
-	assert(pkt_entry->pkt_size == payload_size);
-
-	pkt_entry->pkt_size += RXR_DATA_HDR_SIZE;
-	pkt_entry->addr = tx_entry->addr;
-
-	return rxr_ep_send_pkt_flags(ep, pkt_entry, tx_entry->addr,
-				     tx_entry->send_flags);
-}
-
-/* If mr local is not set, will skip copying and only send user buffers */
-static ssize_t rxr_ep_mr_send_data_pkt_entry(struct rxr_ep *ep,
-					     struct rxr_tx_entry *tx_entry,
-					     struct rxr_pkt_entry *pkt_entry,
-					     struct rxr_data_pkt *data_pkt)
-{
-	/* The user's iov */
-	struct iovec *tx_iov = tx_entry->iov;
-	/* The constructed iov to be passed to sendv
-	 * and corresponding fid_mrs
-	 */
-	struct iovec iov[ep->core_iov_limit];
-	void *desc[ep->core_iov_limit];
-	/* Constructed iov's total size */
-	uint64_t payload_size = 0;
-	/* pkt_entry offset to write data into */
-	uint64_t pkt_used = 0;
-	/* Remaining size that can fit in the constructed iov */
-	uint64_t remaining_len = MIN(tx_entry->window,
-				     ep->max_data_payload_size);
-	/* The constructed iov's index */
-	size_t i = 0;
-	size_t len = 0;
-
-	ssize_t ret;
-
-	/* Assign packet header in constructed iov */
-	iov[i].iov_base = rxr_pkt_start(pkt_entry);
-	iov[i].iov_len = RXR_DATA_HDR_SIZE;
-	desc[i] = rxr_ep_mr_local(ep) ? fi_mr_desc(pkt_entry->mr) : NULL;
-	i++;
-
-	/*
-	 * Loops until payload size is at max, all user iovs are sent, the
-	 * constructed iov count is greater than the core iov limit, or the tx
-	 * entry window is exhausted.  Each iteration fills one entry of the
-	 * iov to be sent.
-	 */
-	while (tx_entry->iov_index < tx_entry->iov_count &&
-	       remaining_len > 0 && i < ep->core_iov_limit) {
-		if (!rxr_ep_mr_local(ep) ||
-		    /* from the inline registration post-RTS */
-		    tx_entry->mr[tx_entry->iov_index] ||
-		    /* from application-provided descriptor */
-		    tx_entry->desc[tx_entry->iov_index]) {
-			iov[i].iov_base =
-				(char *)tx_iov[tx_entry->iov_index].iov_base +
-				tx_entry->iov_offset;
-			if (rxr_ep_mr_local(ep))
-				desc[i] = tx_entry->desc[tx_entry->iov_index] ?
-					  tx_entry->desc[tx_entry->iov_index] :
-					  fi_mr_desc(tx_entry->mr[tx_entry->iov_index]);
-
-			len = tx_iov[tx_entry->iov_index].iov_len
-			      - tx_entry->iov_offset;
-			if (len > remaining_len) {
-				len = remaining_len;
-				tx_entry->iov_offset += len;
-			} else {
-				tx_entry->iov_index++;
-				tx_entry->iov_offset = 0;
-			}
-			iov[i].iov_len = len;
-		} else {
-			/*
-			 * Copies any consecutive small iov's, returning size
-			 * written while updating iov index and offset
-			 */
-			len = rxr_copy_from_iov((char *)data_pkt->data +
-						 pkt_used,
-						 remaining_len,
-						 tx_entry);
-
-			iov[i].iov_base = (char *)data_pkt->data + pkt_used;
-			iov[i].iov_len = len;
-			desc[i] = fi_mr_desc(pkt_entry->mr);
-			pkt_used += len;
-		}
-		payload_size += len;
-		remaining_len -= len;
-		i++;
-	}
-	data_pkt->hdr.seg_size = (uint16_t)payload_size;
-	pkt_entry->pkt_size = payload_size + RXR_DATA_HDR_SIZE;
-	pkt_entry->addr = tx_entry->addr;
-
-	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-	       "Sending an iov count, %zu with payload size: %lu.\n",
-	       i, payload_size);
-	ret = rxr_ep_sendv_pkt(ep, pkt_entry, tx_entry->addr,
-			       (const struct iovec *)iov,
-			       desc, i, tx_entry->send_flags);
-	return ret;
-}
-
-ssize_t rxr_ep_post_data(struct rxr_ep *rxr_ep,
-			 struct rxr_tx_entry *tx_entry)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_data_pkt *data_pkt;
-	ssize_t ret;
-
-	pkt_entry = rxr_get_pkt_entry(rxr_ep, rxr_ep->tx_pkt_efa_pool);
-
-	if (OFI_UNLIKELY(!pkt_entry))
-		return -FI_ENOMEM;
-
-	pkt_entry->x_entry = (void *)tx_entry;
-	pkt_entry->addr = tx_entry->addr;
-
-	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
-
-	data_pkt->hdr.type = RXR_DATA_PKT;
-	data_pkt->hdr.version = RXR_PROTOCOL_VERSION;
-	data_pkt->hdr.flags = 0;
-
-	data_pkt->hdr.rx_id = tx_entry->rx_id;
-
-	/*
-	 * Data packets are sent in order so using bytes_sent is okay here.
-	 */
-	data_pkt->hdr.seg_offset = tx_entry->bytes_sent;
-
-	if (efa_mr_cache_enable) {
-		ret = rxr_ep_mr_send_data_pkt_entry(rxr_ep, tx_entry, pkt_entry,
-						    data_pkt);
-	} else {
-		ret = rxr_ep_send_data_pkt_entry(rxr_ep, tx_entry, pkt_entry,
-						 data_pkt);
-	}
-
-	if (OFI_UNLIKELY(ret)) {
-		rxr_release_tx_pkt_entry(rxr_ep, pkt_entry);
-		return ret;
-	}
-	data_pkt = rxr_get_data_pkt(pkt_entry->pkt);
-	tx_entry->bytes_sent += data_pkt->hdr.seg_size;
-	tx_entry->window -= data_pkt->hdr.seg_size;
-
-	return ret;
-}
-
 void rxr_inline_mr_reg(struct rxr_domain *rxr_domain,
 		       struct rxr_tx_entry *tx_entry)
 {
@@ -810,389 +442,6 @@ void rxr_inline_mr_reg(struct rxr_domain *rxr_domain,
 	return;
 }
 
-void rxr_ep_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
-				    uint64_t size, int request,
-				    int *window, int *credits)
-{
-	struct efa_av *av;
-	int num_peers;
-
-	/*
-	 * Adjust the peer credit pool based on the current AV size, which could
-	 * have grown since the time this peer was initialized.
-	 */
-	av = rxr_ep_av(ep);
-	num_peers = av->used - 1;
-	if (num_peers && ofi_div_ceil(rxr_env.rx_window_size, num_peers) < peer->rx_credits)
-		peer->rx_credits = ofi_div_ceil(peer->rx_credits, num_peers);
-
-	/*
-	 * Allocate credits for this transfer based on the request, the number
-	 * of available data buffers, and the number of outstanding peers this
-	 * endpoint is actively tracking in the AV. Also ensure that a minimum
-	 * number of credits are allocated to the transfer so the sender can
-	 * make progress.
-	 */
-	*credits = MIN(MIN(ep->available_data_bufs, ep->posted_bufs_efa),
-		       peer->rx_credits);
-	*credits = MIN(request, *credits);
-	*credits = MAX(*credits, rxr_env.tx_min_credits);
-	*window = MIN(size, *credits * ep->max_data_payload_size);
-	if (peer->rx_credits > ofi_div_ceil(*window, ep->max_data_payload_size))
-		peer->rx_credits -= ofi_div_ceil(*window, ep->max_data_payload_size);
-}
-
-int rxr_ep_init_cts_pkt(struct rxr_ep *ep,
-			struct rxr_rx_entry *rx_entry,
-			struct rxr_pkt_entry *pkt_entry)
-{
-	int window = 0;
-	struct rxr_cts_hdr *cts_hdr;
-	struct rxr_peer *peer;
-	size_t bytes_left;
-
-	cts_hdr = (struct rxr_cts_hdr *)pkt_entry->pkt;
-	cts_hdr->type = RXR_CTS_PKT;
-	cts_hdr->version = RXR_PROTOCOL_VERSION;
-	cts_hdr->flags = 0;
-
-	if (rx_entry->cq_entry.flags & FI_READ)
-		cts_hdr->flags |= RXR_READ_REQ;
-
-	cts_hdr->tx_id = rx_entry->tx_id;
-	cts_hdr->rx_id = rx_entry->rx_id;
-
-	bytes_left = rx_entry->total_len - rx_entry->bytes_done;
-	peer = rxr_ep_get_peer(ep, rx_entry->addr);
-	rxr_ep_calc_cts_window_credits(ep, peer, bytes_left,
-				       rx_entry->credit_request,
-				       &window, &rx_entry->credit_cts);
-	cts_hdr->window = window;
-	pkt_entry->pkt_size = RXR_CTS_HDR_SIZE;
-	pkt_entry->addr = rx_entry->addr;
-	pkt_entry->x_entry = (void *)rx_entry;
-	return 0;
-}
-
-void rxr_ep_handle_cts_sent(struct rxr_ep *ep,
-			    struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rx_entry *rx_entry;
-
-	rx_entry = (struct rxr_rx_entry *)pkt_entry->x_entry;
-	rx_entry->window = rxr_get_cts_hdr(pkt_entry->pkt)->window;
-	ep->available_data_bufs -= rx_entry->credit_cts;
-
-	/*
-	 * Set a timer if available_bufs is exhausted. We may encounter a
-	 * scenario where a peer has stopped responding so we need a fallback
-	 * to replenish the credits.
-	 */
-	if (OFI_UNLIKELY(ep->available_data_bufs == 0))
-		ep->available_data_bufs_ts = ofi_gettime_us();
-}
-
-void rxr_ep_init_connack_pkt_entry(struct rxr_ep *ep,
-				   struct rxr_pkt_entry *pkt_entry,
-				   fi_addr_t addr)
-{
-	struct rxr_connack_hdr *connack_hdr;
-
-	connack_hdr = (struct rxr_connack_hdr *)pkt_entry->pkt;
-
-	connack_hdr->type = RXR_CONNACK_PKT;
-	connack_hdr->version = RXR_PROTOCOL_VERSION;
-	connack_hdr->flags = 0;
-
-	pkt_entry->pkt_size = RXR_CONNACK_HDR_SIZE;
-	pkt_entry->addr = addr;
-}
-
-/* RTS related functions */
-char *rxr_ep_init_rts_hdr(struct rxr_ep *ep,
-			  struct rxr_tx_entry *tx_entry,
-			  struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rts_hdr *rts_hdr;
-	struct rxr_peer *peer;
-	char *src;
-
-	rts_hdr = (struct rxr_rts_hdr *)pkt_entry->pkt;
-	peer = rxr_ep_get_peer(ep, tx_entry->addr);
-
-	rts_hdr->type = RXR_RTS_PKT;
-	rts_hdr->version = RXR_PROTOCOL_VERSION;
-	rts_hdr->tag = tx_entry->tag;
-
-	rts_hdr->data_len = tx_entry->total_len;
-	rts_hdr->tx_id = tx_entry->tx_id;
-	rts_hdr->msg_id = tx_entry->msg_id;
-	/*
-	 * Even with protocol versions prior to v3 that did not include a
-	 * request in the RTS, the receiver can test for this flag and decide if
-	 * it should be used as a heuristic for credit calculation. If the
-	 * receiver is on <3 protocol version, the flag and the request just get
-	 * ignored.
-	 */
-	rts_hdr->flags |= RXR_CREDIT_REQUEST;
-	rts_hdr->credit_request = tx_entry->credit_request;
-
-	if (tx_entry->fi_flags & FI_REMOTE_CQ_DATA) {
-		rts_hdr->flags = RXR_REMOTE_CQ_DATA;
-		pkt_entry->pkt_size = RXR_CTRL_HDR_SIZE;
-		rxr_get_ctrl_cq_pkt(rts_hdr)->hdr.cq_data =
-			tx_entry->cq_entry.data;
-		src = rxr_get_ctrl_cq_pkt(rts_hdr)->data;
-	} else {
-		rts_hdr->flags = 0;
-		pkt_entry->pkt_size = RXR_CTRL_HDR_SIZE_NO_CQ;
-		src = rxr_get_ctrl_pkt(rts_hdr)->data;
-	}
-
-	if (tx_entry->cq_entry.flags & FI_TAGGED)
-		rts_hdr->flags |= RXR_TAGGED;
-
-	rts_hdr->addrlen = 0;
-	if (OFI_UNLIKELY(peer->state != RXR_PEER_ACKED)) {
-		/*
-		 * This is the first communication with this peer on this
-		 * endpoint, so send the core's address for this EP in the RTS
-		 * so the remote side can insert it into its address vector.
-		 */
-		rts_hdr->addrlen = ep->core_addrlen;
-		rts_hdr->flags |= RXR_REMOTE_SRC_ADDR;
-		memcpy(src, ep->core_addr, rts_hdr->addrlen);
-		src += rts_hdr->addrlen;
-		pkt_entry->pkt_size += rts_hdr->addrlen;
-	}
-
-	return src;
-}
-
-static size_t rxr_ep_init_rts_pkt(struct rxr_ep *ep,
-				  struct rxr_tx_entry *tx_entry,
-				  struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_peer *peer;
-	struct rxr_rts_hdr *rts_hdr;
-	char *data, *src;
-	uint64_t data_len;
-	size_t mtu = ep->mtu_size;
-
-	if (tx_entry->op == ofi_op_read_req)
-		return rxr_rma_init_read_rts(ep, tx_entry, pkt_entry);
-
-	src = rxr_ep_init_rts_hdr(ep, tx_entry, pkt_entry);
-	if (tx_entry->op == ofi_op_write)
-		src = rxr_rma_init_rts_hdr(ep, tx_entry, pkt_entry, src);
-
-	peer = rxr_ep_get_peer(ep, tx_entry->addr);
-	assert(peer);
-	data = src;
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-	if (rxr_env.enable_shm_transfer && peer->is_local) {
-		rts_hdr->flags |= RXR_SHM_HDR;
-		/* will be sent over shm provider */
-		if (tx_entry->total_len <= rxr_env.shm_max_medium_size) {
-			data_len = ofi_copy_from_iov(data, rxr_env.shm_max_medium_size,
-						     tx_entry->iov, tx_entry->iov_count, 0);
-			assert(data_len == tx_entry->total_len);
-			rts_hdr->flags |= RXR_SHM_HDR_DATA;
-			pkt_entry->pkt_size += data_len;
-		} else {
-			/* rendezvous protocol
-			 * place iov_count first, then local iov
-			 */
-			memcpy(data, &tx_entry->iov_count, sizeof(size_t));
-			data += sizeof(size_t);
-			pkt_entry->pkt_size += sizeof(size_t);
-			memcpy(data, tx_entry->iov, sizeof(struct iovec) * tx_entry->iov_count);
-			pkt_entry->pkt_size += sizeof(struct iovec) * tx_entry->iov_count;
-		}
-	} else {
-		/* will be sent over efa provider */
-		data_len = ofi_copy_from_iov(data, mtu - pkt_entry->pkt_size,
-					     tx_entry->iov, tx_entry->iov_count, 0);
-		assert(data_len == rxr_get_rts_data_size(ep, rts_hdr));
-		pkt_entry->pkt_size += data_len;
-	}
-
-	assert(pkt_entry->pkt_size <= mtu);
-	pkt_entry->addr = tx_entry->addr;
-	pkt_entry->x_entry = (void *)tx_entry;
-	return 0;
-}
-
-void rxr_ep_handle_rts_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_peer *peer;
-	struct rxr_tx_entry *tx_entry;
-	size_t data_sent;
-
-	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(peer);
-	if (tx_entry->op == ofi_op_read_req) {
-		tx_entry->bytes_sent = 0;
-		tx_entry->state = RXR_TX_WAIT_READ_FINISH;
-		return;
-	}
-
-	data_sent = rxr_get_rts_data_size(ep, rxr_get_rts_hdr(pkt_entry->pkt));
-
-	tx_entry->bytes_sent += data_sent;
-
-	if ((rxr_env.enable_shm_transfer && peer->is_local) ||
-	    !(efa_mr_cache_enable && tx_entry->total_len > data_sent))
-		return;
-
-	/*
-	 * Register the data buffers inline only if the application did not
-	 * provide a descriptor with the tx op
-	 */
-	if (rxr_ep_mr_local(ep) && !tx_entry->desc[0])
-		rxr_inline_mr_reg(rxr_ep_domain(ep), tx_entry);
-
-	return;
-}
-
-int rxr_ep_init_ctrl_pkt(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
-			 int ctrl_type, struct rxr_pkt_entry *pkt_entry)
-{
-	int ret = 0;
-
-	switch (ctrl_type) {
-	case RXR_RTS_PKT:
-		ret = rxr_ep_init_rts_pkt(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
-		break;
-	case RXR_READRSP_PKT:
-		ret = rxr_rma_init_readrsp_pkt(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
-		break;
-	case RXR_CTS_PKT:
-		ret = rxr_ep_init_cts_pkt(rxr_ep, (struct rxr_rx_entry *)x_entry, pkt_entry);
-		break;
-	case RXR_EOR_PKT:
-		ret = rxr_rma_init_eor_pkt(rxr_ep, (struct rxr_rx_entry *)x_entry, pkt_entry);
-		break;
-	default:
-		ret = -FI_EINVAL;
-		assert(0 && "unknown pkt type to init");
-		break;
-	}
-
-	return ret;
-}
-
-void rxr_ep_handle_ctrl_sent(struct rxr_ep *rxr_ep, struct rxr_pkt_entry *pkt_entry)
-{
-	int ctrl_type = rxr_get_base_hdr(pkt_entry->pkt)->type;
-
-	switch (ctrl_type) {
-	case RXR_RTS_PKT:
-		rxr_ep_handle_rts_sent(rxr_ep, pkt_entry);
-		break;
-	case RXR_READRSP_PKT:
-		rxr_rma_handle_readrsp_sent(rxr_ep, pkt_entry);
-		break;
-	case RXR_CTS_PKT:
-		rxr_ep_handle_cts_sent(rxr_ep, pkt_entry);
-		break;
-	case RXR_EOR_PKT:
-		rxr_rma_handle_eor_sent(rxr_ep, pkt_entry);
-		break;
-	default:
-		assert(0 && "Unknown packet type to handle sent");
-		break;
-	}
-}
-
-static size_t rxr_ep_post_ctrl(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
-			       int ctrl_type, bool inject)
-{
-	struct rxr_pkt_entry *pkt_entry;
-	struct rxr_tx_entry *tx_entry;
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_peer *peer;
-	ssize_t err;
-	fi_addr_t addr;
-
-	if (entry_type == RXR_TX_ENTRY) {
-		tx_entry = (struct rxr_tx_entry *)x_entry;
-		addr = tx_entry->addr;
-	} else {
-		rx_entry = (struct rxr_rx_entry *)x_entry;
-		addr = rx_entry->addr;
-	}
-
-	peer = rxr_ep_get_peer(rxr_ep, addr);
-	if (peer->is_local)
-		pkt_entry = rxr_get_pkt_entry(rxr_ep, rxr_ep->tx_pkt_shm_pool);
-	else
-		pkt_entry = rxr_get_pkt_entry(rxr_ep, rxr_ep->tx_pkt_efa_pool);
-
-	if (!pkt_entry)
-		return -FI_EAGAIN;
-
-	err = rxr_ep_init_ctrl_pkt(rxr_ep, entry_type, x_entry, ctrl_type, pkt_entry);
-	if (OFI_UNLIKELY(err)) {
-		rxr_release_tx_pkt_entry(rxr_ep, pkt_entry);
-		return err;
-	}
-
-	/* if send, tx_pkt_entry will be released while handle completion
-	 * if inject, there will not be completion, therefore tx_pkt_entry has to be
-	 * released here
-	 */
-	if (inject)
-		err = rxr_ep_inject_pkt(rxr_ep, pkt_entry, addr);
-	else
-		err = rxr_ep_send_pkt(rxr_ep, pkt_entry, addr);
-
-	if (OFI_UNLIKELY(err)) {
-		rxr_release_tx_pkt_entry(rxr_ep, pkt_entry);
-		return err;
-	}
-
-	rxr_ep_handle_ctrl_sent(rxr_ep, pkt_entry);
-
-	if (inject)
-		rxr_release_tx_pkt_entry(rxr_ep, pkt_entry);
-
-	return 0;
-}
-
-int rxr_ep_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry, int ctrl_type, bool inject)
-{
-	ssize_t err;
-	struct rxr_tx_entry *tx_entry;
-	struct rxr_rx_entry *rx_entry;
-
-	err = rxr_ep_post_ctrl(ep, entry_type, x_entry, ctrl_type, inject);
-	if (err == -FI_EAGAIN) {
-		if (entry_type == RXR_TX_ENTRY) {
-			tx_entry = (struct rxr_tx_entry *)x_entry;
-			tx_entry->state = RXR_TX_QUEUED_CTRL;
-			tx_entry->queued_ctrl.type = ctrl_type;
-			tx_entry->queued_ctrl.inject = inject;
-			dlist_insert_tail(&tx_entry->queued_entry,
-					  &ep->tx_entry_queued_list);
-		} else {
-			assert(entry_type == RXR_RX_ENTRY);
-			rx_entry = (struct rxr_rx_entry *)x_entry;
-			rx_entry->state = RXR_RX_QUEUED_CTRL;
-			rx_entry->queued_ctrl.type = ctrl_type;
-			rx_entry->queued_ctrl.inject = inject;
-			dlist_insert_tail(&rx_entry->queued_entry,
-					  &ep->rx_entry_queued_list);
-		}
-
-		err = 0;
-	}
-
-	return err;
-}
-
 /* Generic send */
 int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 {
@@ -1271,12 +520,12 @@ static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 
 	dlist_foreach(&rxr_ep->rx_unexp_list, entry) {
 		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
-		rxr_release_rx_pkt_entry(rxr_ep, rx_entry->unexp_rts_pkt);
+		rxr_pkt_entry_release_rx(rxr_ep, rx_entry->unexp_rts_pkt);
 	}
 
 	dlist_foreach(&rxr_ep->rx_unexp_tagged_list, entry) {
 		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
-		rxr_release_rx_pkt_entry(rxr_ep, rx_entry->unexp_rts_pkt);
+		rxr_pkt_entry_release_rx(rxr_ep, rx_entry->unexp_rts_pkt);
 	}
 
 	dlist_foreach(&rxr_ep->rx_entry_queued_list, entry) {
@@ -1285,7 +534,7 @@ static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 		dlist_foreach_container_safe(&rx_entry->queued_pkts,
 					     struct rxr_pkt_entry,
 					     pkt, entry, tmp)
-			rxr_release_tx_pkt_entry(rxr_ep, pkt);
+			rxr_pkt_entry_release_tx(rxr_ep, pkt);
 	}
 
 	dlist_foreach(&rxr_ep->tx_entry_queued_list, entry) {
@@ -1294,17 +543,17 @@ static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 		dlist_foreach_container_safe(&tx_entry->queued_pkts,
 					     struct rxr_pkt_entry,
 					     pkt, entry, tmp)
-			rxr_release_tx_pkt_entry(rxr_ep, pkt);
+			rxr_pkt_entry_release_tx(rxr_ep, pkt);
 	}
 
 	dlist_foreach_safe(&rxr_ep->rx_pkt_list, entry, tmp) {
 		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-		rxr_release_rx_pkt_entry(rxr_ep, pkt);
+		rxr_pkt_entry_release_rx(rxr_ep, pkt);
 	}
 
 	dlist_foreach_safe(&rxr_ep->tx_pkt_list, entry, tmp) {
 		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-		rxr_release_tx_pkt_entry(rxr_ep, pkt);
+		rxr_pkt_entry_release_tx(rxr_ep, pkt);
 	}
 
 	dlist_foreach_safe(&rxr_ep->rx_posted_buf_list, entry, tmp) {
@@ -1968,7 +1217,7 @@ static inline int rxr_ep_send_queued_pkts(struct rxr_ep *ep,
 			dlist_remove(&pkt_entry->entry);
 			continue;
 		}
-		ret = rxr_ep_send_pkt(ep, pkt_entry, pkt_entry->addr);
+		ret = rxr_pkt_entry_send(ep, pkt_entry, pkt_entry->addr);
 		if (ret)
 			return ret;
 		dlist_remove(&pkt_entry->entry);
@@ -2016,10 +1265,14 @@ static inline void rxr_ep_poll_cq(struct rxr_ep *ep,
 	struct fi_cq_data_entry cq_entry;
 	fi_addr_t src_addr;
 	ssize_t ret;
+	struct efa_ep *efa_ep;
+	struct efa_av *efa_av;
 	int i;
 
 	VALGRIND_MAKE_MEM_DEFINED(&cq_entry, sizeof(struct fi_cq_data_entry));
 
+	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
+	efa_av = efa_ep->av;
 	for (i = 0; i < cqe_to_process; i++) {
 		ret = fi_cq_readfrom(cq, &cq_entry, 1, &src_addr);
 
@@ -2037,6 +1290,12 @@ static inline void rxr_ep_poll_cq(struct rxr_ep *ep,
 		if (OFI_UNLIKELY(ret == 0))
 			return;
 
+		if (is_shm_cq && src_addr != FI_ADDR_UNSPEC) {
+			/* convert SHM address to EFA address */
+			assert(src_addr < EFA_SHM_MAX_AV_COUNT);
+			src_addr = efa_av->shm_rdm_addr_map[src_addr];
+		}
+
 		if (is_shm_cq && (cq_entry.flags & FI_REMOTE_CQ_DATA)) {
 			rxr_cq_handle_shm_rma_write_data(ep, &cq_entry, src_addr);
 		} else if (cq_entry.flags & (FI_SEND | FI_READ | FI_WRITE)) {
@@ -2044,9 +1303,9 @@ static inline void rxr_ep_poll_cq(struct rxr_ep *ep,
 			if (!is_shm_cq)
 				ep->send_comps++;
 #endif
-			rxr_cq_handle_pkt_send_completion(ep, &cq_entry);
+			rxr_pkt_handle_send_completion(ep, &cq_entry);
 		} else if (cq_entry.flags & (FI_RECV | FI_REMOTE_CQ_DATA)) {
-			rxr_cq_handle_pkt_recv_completion(ep, &cq_entry, src_addr);
+			rxr_pkt_handle_recv_completion(ep, &cq_entry, src_addr);
 #if ENABLE_DEBUG
 			if (!is_shm_cq)
 				ep->recv_comps++;
@@ -2094,11 +1353,11 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 				     struct rxr_rx_entry,
 				     rx_entry, queued_entry, tmp) {
 		if (rx_entry->state == RXR_RX_QUEUED_CTRL)
-			ret = rxr_ep_post_ctrl(ep, RXR_RX_ENTRY, rx_entry,
-					       rx_entry->queued_ctrl.type,
-					       rx_entry->queued_ctrl.inject);
+			ret = rxr_pkt_post_ctrl(ep, RXR_RX_ENTRY, rx_entry,
+						rx_entry->queued_ctrl.type,
+						rx_entry->queued_ctrl.inject);
 		else if (rx_entry->state == RXR_RX_QUEUED_SHM_LARGE_READ)
-			ret = rxr_cq_recv_shm_large_message(ep, rx_entry);
+			ret = rxr_pkt_post_shm_rndzv_read(ep, rx_entry);
 		else
 			ret = rxr_ep_send_queued_pkts(ep,
 						      &rx_entry->queued_pkts);
@@ -2115,9 +1374,9 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 				     struct rxr_tx_entry,
 				     tx_entry, queued_entry, tmp) {
 		if (tx_entry->state == RXR_TX_QUEUED_CTRL)
-			ret = rxr_ep_post_ctrl(ep, RXR_TX_ENTRY, tx_entry,
-					       tx_entry->queued_ctrl.type,
-					       tx_entry->queued_ctrl.inject);
+			ret = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry,
+						tx_entry->queued_ctrl.type,
+						tx_entry->queued_ctrl.inject);
 		else if (tx_entry->state == RXR_TX_QUEUED_SHM_RMA)
 			ret = rxr_rma_post_shm_rma(ep, tx_entry);
 		else
@@ -2162,7 +1421,7 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 			 */
 			if (ep->tx_pending == ep->max_outstanding_tx)
 				goto out;
-			ret = rxr_ep_post_data(ep, tx_entry);
+			ret = rxr_pkt_post_data(ep, tx_entry);
 			if (OFI_UNLIKELY(ret)) {
 				tx_entry->send_flags &= ~FI_MORE;
 				goto tx_err;
diff --git a/prov/efa/src/rxr/rxr_init.c b/prov/efa/src/rxr/rxr_init.c
index 3bb929b..5501302 100644
--- a/prov/efa/src/rxr/rxr_init.c
+++ b/prov/efa/src/rxr/rxr_init.c
@@ -637,7 +637,7 @@ EFA_INI
 	fi_param_define(&rxr_prov, "cq_size", FI_PARAM_INT,
 			"Define the size of completion queue. (Default: 8192)");
 	fi_param_define(&rxr_prov, "mr_cache_enable", FI_PARAM_BOOL,
-			"Enables using the mr cache and in-line registration instead of a bounce buffer for iov's larger than max_memcpy_size. Defaults to false. When disabled, only uses a bounce buffer.");
+			"Enables using the mr cache and in-line registration instead of a bounce buffer for iov's larger than max_memcpy_size. Defaults to true. When disabled, only uses a bounce buffer.");
 	fi_param_define(&rxr_prov, "mr_cache_merge_regions", FI_PARAM_BOOL,
 			"Enables merging overlapping and adjacent memory registration regions. Defaults to true.");
 	fi_param_define(&rxr_prov, "mr_max_cached_count", FI_PARAM_SIZE_T,
diff --git a/prov/efa/src/rxr/rxr_msg.c b/prov/efa/src/rxr/rxr_msg.c
index 1b93171..e0acb75 100644
--- a/prov/efa/src/rxr/rxr_msg.c
+++ b/prov/efa/src/rxr/rxr_msg.c
@@ -40,6 +40,7 @@
 
 #include "rxr.h"
 #include "rxr_msg.h"
+#include "rxr_pkt_cmd.h"
 
 /**
  * This file define the msg ops functions.
@@ -104,7 +105,7 @@ ssize_t rxr_msg_generic_send(struct fid_ep *ep, const struct fi_msg *msg,
 	    rxr_need_sas_ordering(rxr_ep))
 		tx_entry->msg_id = peer->next_msg_id++;
 
-	err = rxr_ep_post_ctrl_or_queue(rxr_ep, RXR_TX_ENTRY, tx_entry, RXR_RTS_PKT, 0);
+	err = rxr_pkt_post_ctrl_or_queue(rxr_ep, RXR_TX_ENTRY, tx_entry, RXR_RTS_PKT, 0);
 	if (OFI_UNLIKELY(err)) {
 		rxr_release_tx_entry(rxr_ep, tx_entry);
 		if (!(rxr_env.enable_shm_transfer && peer->is_local) &&
@@ -415,12 +416,9 @@ int rxr_msg_handle_unexp_match(struct rxr_ep *ep,
 			       void *context, fi_addr_t addr,
 			       uint32_t op, uint64_t flags)
 {
-	struct rxr_peer *peer;
-	struct rxr_pkt_entry *pkt_entry;
 	struct rxr_rts_hdr *rts_hdr;
+	struct rxr_pkt_entry *pkt_entry;
 	uint64_t len;
-	char *data;
-	size_t data_size;
 
 	rx_entry->fi_flags = flags;
 	rx_entry->ignore = ignore;
@@ -456,18 +454,7 @@ int rxr_msg_handle_unexp_match(struct rxr_ep *ep,
 		rx_entry->ignore = ~0;
 	}
 
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	data = rxr_cq_read_rts_hdr(ep, rx_entry, pkt_entry);
-	if (peer->is_local && !(rts_hdr->flags & RXR_SHM_HDR_DATA)) {
-		rxr_cq_process_shm_large_message(ep, rx_entry, rts_hdr, data);
-		rxr_release_rx_pkt_entry(ep, pkt_entry);
-		return 0;
-	}
-
-	data_size = rxr_get_rts_data_size(ep, rts_hdr);
-	return rxr_cq_handle_rts_with_data(ep, rx_entry,
-					   pkt_entry, data,
-					   data_size);
+	return rxr_pkt_proc_matched_msg_rts(ep, rx_entry, pkt_entry);
 }
 
 /*
diff --git a/prov/efa/src/rxr/rxr_msg.h b/prov/efa/src/rxr/rxr_msg.h
index 9538709..a4e0206 100644
--- a/prov/efa/src/rxr/rxr_msg.h
+++ b/prov/efa/src/rxr/rxr_msg.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2019 Amazon.com, Inc. or its affiliates.
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
  * All rights reserved.
  *
  * This software is available to you under a choice of one of two
diff --git a/prov/efa/src/rxr/rxr_pkt_cmd.c b/prov/efa/src/rxr/rxr_pkt_cmd.c
new file mode 100644
index 0000000..e11f89f
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_cmd.c
@@ -0,0 +1,526 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "efa.h"
+#include "rxr.h"
+#include "rxr_cntr.h"
+
+/* This file implements 4 actions that can be applied to a packet:
+ *          posting,
+ *          handling send completion and,
+ *          handing recv completion.
+ *          dump (for debug only)
+ */
+
+/*
+ *  Functions used to post a packet
+ */
+ssize_t rxr_pkt_post_data(struct rxr_ep *rxr_ep,
+			  struct rxr_tx_entry *tx_entry)
+{
+	struct rxr_pkt_entry *pkt_entry;
+	struct rxr_data_pkt *data_pkt;
+	ssize_t ret;
+
+	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_efa_pool);
+
+	if (OFI_UNLIKELY(!pkt_entry))
+		return -FI_ENOMEM;
+
+	pkt_entry->x_entry = (void *)tx_entry;
+	pkt_entry->addr = tx_entry->addr;
+
+	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
+
+	data_pkt->hdr.type = RXR_DATA_PKT;
+	data_pkt->hdr.version = RXR_PROTOCOL_VERSION;
+	data_pkt->hdr.flags = 0;
+
+	data_pkt->hdr.rx_id = tx_entry->rx_id;
+
+	/*
+	 * Data packets are sent in order so using bytes_sent is okay here.
+	 */
+	data_pkt->hdr.seg_offset = tx_entry->bytes_sent;
+
+	if (efa_mr_cache_enable)
+		ret = rxr_pkt_send_data_mr_cache(rxr_ep, tx_entry, pkt_entry);
+	else
+		ret = rxr_pkt_send_data(rxr_ep, tx_entry, pkt_entry);
+
+	if (OFI_UNLIKELY(ret)) {
+		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
+		return ret;
+	}
+
+	data_pkt = rxr_get_data_pkt(pkt_entry->pkt);
+	tx_entry->bytes_sent += data_pkt->hdr.seg_size;
+	tx_entry->window -= data_pkt->hdr.seg_size;
+	return ret;
+}
+
+/*
+ *   rxr_pkt_init_ctrl() uses init functions declared in rxr_pkt_type.h
+ */
+static
+int rxr_pkt_init_ctrl(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
+		      int ctrl_type, struct rxr_pkt_entry *pkt_entry)
+{
+	int ret = 0;
+
+	switch (ctrl_type) {
+	case RXR_RTS_PKT:
+		ret = rxr_pkt_init_rts(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+		break;
+	case RXR_READRSP_PKT:
+		ret = rxr_pkt_init_readrsp(rxr_ep, (struct rxr_tx_entry *)x_entry, pkt_entry);
+		break;
+	case RXR_CTS_PKT:
+		ret = rxr_pkt_init_cts(rxr_ep, (struct rxr_rx_entry *)x_entry, pkt_entry);
+		break;
+	case RXR_EOR_PKT:
+		ret = rxr_pkt_init_eor(rxr_ep, (struct rxr_rx_entry *)x_entry, pkt_entry);
+		break;
+	default:
+		ret = -FI_EINVAL;
+		assert(0 && "unknown pkt type to init");
+		break;
+	}
+
+	return ret;
+}
+
+/*
+ *   rxr_pkt_handle_ctrl_sent() uses handle_sent() functions declared in rxr_pkt_type.h
+ */
+static
+void rxr_pkt_handle_ctrl_sent(struct rxr_ep *rxr_ep, struct rxr_pkt_entry *pkt_entry)
+{
+	int ctrl_type = rxr_get_base_hdr(pkt_entry->pkt)->type;
+
+	switch (ctrl_type) {
+	case RXR_RTS_PKT:
+		rxr_pkt_handle_rts_sent(rxr_ep, pkt_entry);
+		break;
+	case RXR_READRSP_PKT:
+		rxr_pkt_handle_readrsp_sent(rxr_ep, pkt_entry);
+		break;
+	case RXR_CTS_PKT:
+		rxr_pkt_handle_cts_sent(rxr_ep, pkt_entry);
+		break;
+	case RXR_EOR_PKT:
+		rxr_pkt_handle_eor_sent(rxr_ep, pkt_entry);
+		break;
+	default:
+		assert(0 && "Unknown packet type to handle sent");
+		break;
+	}
+}
+
+ssize_t rxr_pkt_post_ctrl(struct rxr_ep *rxr_ep, int entry_type, void *x_entry,
+			  int ctrl_type, bool inject)
+{
+	struct rxr_pkt_entry *pkt_entry;
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_peer *peer;
+	ssize_t err;
+	fi_addr_t addr;
+
+	if (entry_type == RXR_TX_ENTRY) {
+		tx_entry = (struct rxr_tx_entry *)x_entry;
+		addr = tx_entry->addr;
+	} else {
+		rx_entry = (struct rxr_rx_entry *)x_entry;
+		addr = rx_entry->addr;
+	}
+
+	peer = rxr_ep_get_peer(rxr_ep, addr);
+	if (peer->is_local)
+		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_shm_pool);
+	else
+		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_efa_pool);
+
+	if (!pkt_entry)
+		return -FI_EAGAIN;
+
+	err = rxr_pkt_init_ctrl(rxr_ep, entry_type, x_entry, ctrl_type, pkt_entry);
+	if (OFI_UNLIKELY(err)) {
+		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
+		return err;
+	}
+
+	/* if send, tx_pkt_entry will be released while handle completion
+	 * if inject, there will not be completion, therefore tx_pkt_entry has to be
+	 * released here
+	 */
+	if (inject)
+		err = rxr_pkt_entry_inject(rxr_ep, pkt_entry, addr);
+	else
+		err = rxr_pkt_entry_send(rxr_ep, pkt_entry, addr);
+
+	if (OFI_UNLIKELY(err)) {
+		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
+		return err;
+	}
+
+	rxr_pkt_handle_ctrl_sent(rxr_ep, pkt_entry);
+
+	if (inject)
+		rxr_pkt_entry_release_tx(rxr_ep, pkt_entry);
+
+	return 0;
+}
+
+ssize_t rxr_pkt_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry, int ctrl_type, bool inject)
+{
+	ssize_t err;
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_rx_entry *rx_entry;
+
+	err = rxr_pkt_post_ctrl(ep, entry_type, x_entry, ctrl_type, inject);
+	if (err == -FI_EAGAIN) {
+		if (entry_type == RXR_TX_ENTRY) {
+			tx_entry = (struct rxr_tx_entry *)x_entry;
+			tx_entry->state = RXR_TX_QUEUED_CTRL;
+			tx_entry->queued_ctrl.type = ctrl_type;
+			tx_entry->queued_ctrl.inject = inject;
+			dlist_insert_tail(&tx_entry->queued_entry,
+					  &ep->tx_entry_queued_list);
+		} else {
+			assert(entry_type == RXR_RX_ENTRY);
+			rx_entry = (struct rxr_rx_entry *)x_entry;
+			rx_entry->state = RXR_RX_QUEUED_CTRL;
+			rx_entry->queued_ctrl.type = ctrl_type;
+			rx_entry->queued_ctrl.inject = inject;
+			dlist_insert_tail(&rx_entry->queued_entry,
+					  &ep->rx_entry_queued_list);
+		}
+
+		err = 0;
+	}
+
+	return err;
+}
+
+/*
+ *   Functions used to handle packet send completion
+ */
+void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct fi_cq_data_entry *comp)
+{
+	struct rxr_pkt_entry *pkt_entry;
+	struct rxr_peer *peer;
+
+	pkt_entry = (struct rxr_pkt_entry *)comp->op_context;
+	assert(rxr_get_base_hdr(pkt_entry->pkt)->version ==
+	       RXR_PROTOCOL_VERSION);
+
+	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
+	case RXR_RTS_PKT:
+		rxr_pkt_handle_rts_send_completion(ep, pkt_entry);
+		break;
+	case RXR_CONNACK_PKT:
+		break;
+	case RXR_CTS_PKT:
+		break;
+	case RXR_DATA_PKT:
+		rxr_pkt_handle_data_send_completion(ep, pkt_entry);
+		break;
+	case RXR_READRSP_PKT:
+		rxr_pkt_handle_readrsp_send_completion(ep, pkt_entry);
+		break;
+	case RXR_RMA_CONTEXT_PKT:
+		rxr_pkt_handle_rma_context_send_completion(ep, pkt_entry);
+		return;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"invalid control pkt type %d\n",
+			rxr_get_base_hdr(pkt_entry->pkt)->type);
+		assert(0 && "invalid control pkt type");
+		rxr_cq_handle_cq_error(ep, -FI_EIO);
+		return;
+	}
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (!peer->is_local)
+		rxr_ep_dec_tx_pending(ep, peer, 0);
+	rxr_pkt_entry_release_tx(ep, pkt_entry);
+}
+
+/*
+ *  Functions used to handle packet receive completion
+ */
+static
+fi_addr_t rxr_pkt_insert_addr_from_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	int i, ret;
+	void *raw_address;
+	fi_addr_t rdm_addr;
+	struct rxr_rts_hdr *rts_hdr;
+	struct efa_ep *efa_ep;
+
+	assert(rxr_get_base_hdr(pkt_entry->pkt)->type == RXR_RTS_PKT);
+
+	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	assert(rts_hdr->flags & RXR_REMOTE_SRC_ADDR);
+	assert(rts_hdr->addrlen > 0);
+	if (rxr_get_base_hdr(pkt_entry->pkt)->version !=
+	    RXR_PROTOCOL_VERSION) {
+		char buffer[ep->core_addrlen * 3];
+		int length = 0;
+
+		for (i = 0; i < ep->core_addrlen; i++)
+			length += sprintf(&buffer[length], "%02x ",
+					  ep->core_addr[i]);
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Host %s:Invalid protocol version %d. Expected protocol version %d.\n",
+			buffer,
+			rxr_get_base_hdr(pkt_entry->pkt)->version,
+			RXR_PROTOCOL_VERSION);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, -FI_EINVAL);
+		fprintf(stderr, "Invalid protocol version %d. Expected protocol version %d. %s:%d\n",
+			rxr_get_base_hdr(pkt_entry->pkt)->version,
+			RXR_PROTOCOL_VERSION, __FILE__, __LINE__);
+		abort();
+	}
+
+	raw_address = (rts_hdr->flags & RXR_REMOTE_CQ_DATA) ?
+		      rxr_get_ctrl_cq_pkt(rts_hdr)->data
+		      : rxr_get_ctrl_pkt(rts_hdr)->data;
+
+	ret = efa_av_insert_addr(efa_ep->av, (struct efa_ep_addr *)raw_address,
+				 &rdm_addr, 0, NULL);
+	if (OFI_UNLIKELY(ret != 0)) {
+		efa_eq_write_error(&ep->util_ep, FI_EINVAL, ret);
+		return -1;
+	}
+
+	return rdm_addr;
+}
+
+void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
+				    struct fi_cq_data_entry *cq_entry,
+				    fi_addr_t src_addr)
+{
+	struct rxr_peer *peer;
+	struct rxr_pkt_entry *pkt_entry;
+
+	pkt_entry = (struct rxr_pkt_entry *)cq_entry->op_context;
+
+#if ENABLE_DEBUG
+	dlist_remove(&pkt_entry->dbg_entry);
+	dlist_insert_tail(&pkt_entry->dbg_entry, &ep->rx_pkt_list);
+#ifdef ENABLE_RXR_PKT_DUMP
+	rxr_ep_print_pkt("Received", ep, (struct rxr_base_hdr *)pkt_entry->pkt);
+#endif
+#endif
+	if (OFI_UNLIKELY(src_addr == FI_ADDR_NOTAVAIL))
+		pkt_entry->addr = rxr_pkt_insert_addr_from_rts(ep, pkt_entry);
+	else
+		pkt_entry->addr = src_addr;
+
+	assert(rxr_get_base_hdr(pkt_entry->pkt)->version ==
+	       RXR_PROTOCOL_VERSION);
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+
+	if (rxr_env.enable_shm_transfer && peer->is_local)
+		ep->posted_bufs_shm--;
+	else
+		ep->posted_bufs_efa--;
+
+	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
+	case RXR_RTS_PKT:
+		rxr_pkt_handle_rts_recv(ep, pkt_entry);
+		return;
+	case RXR_EOR_PKT:
+		rxr_pkt_handle_eor_recv(ep, pkt_entry);
+		return;
+	case RXR_CONNACK_PKT:
+		rxr_pkt_handle_connack_recv(ep, pkt_entry, src_addr);
+		return;
+	case RXR_CTS_PKT:
+		rxr_pkt_handle_cts_recv(ep, pkt_entry);
+		return;
+	case RXR_DATA_PKT:
+		rxr_pkt_handle_data_recv(ep, pkt_entry);
+		return;
+	case RXR_READRSP_PKT:
+		rxr_pkt_handle_readrsp_recv(ep, pkt_entry);
+		return;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"invalid control pkt type %d\n",
+			rxr_get_base_hdr(pkt_entry->pkt)->type);
+		assert(0 && "invalid control pkt type");
+		rxr_cq_handle_cq_error(ep, -FI_EIO);
+		return;
+	}
+}
+
+#if ENABLE_DEBUG
+
+/*
+ *  Functions used to dump packets
+ */
+
+#define RXR_PKT_DUMP_DATA_LEN 64
+
+static
+void rxr_pkt_print_rts(struct rxr_ep *ep,
+		       char *prefix, struct rxr_rts_hdr *rts_hdr)
+{
+	char str[RXR_PKT_DUMP_DATA_LEN * 4];
+	size_t str_len = RXR_PKT_DUMP_DATA_LEN * 4, l;
+	uint8_t *src;
+	uint8_t *data;
+	int i;
+
+	str[str_len - 1] = '\0';
+
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+	       "%s RxR RTS packet - version: %"	PRIu8
+	       " flags: %"	PRIu16
+	       " tx_id: %"	PRIu32
+	       " msg_id: %"	PRIu32
+	       " tag: %lx data_len: %"	PRIu64 "\n",
+	       prefix, rts_hdr->version, rts_hdr->flags, rts_hdr->tx_id,
+	       rts_hdr->msg_id, rts_hdr->tag, rts_hdr->data_len);
+
+	if ((rts_hdr->flags & RXR_REMOTE_CQ_DATA) &&
+	    (rts_hdr->flags & RXR_REMOTE_SRC_ADDR)) {
+		src = (uint8_t *)((struct rxr_ctrl_cq_pkt *)rts_hdr)->data;
+		data = src + rts_hdr->addrlen;
+	} else if (!(rts_hdr->flags & RXR_REMOTE_CQ_DATA) &&
+		   (rts_hdr->flags & RXR_REMOTE_SRC_ADDR)) {
+		src = (uint8_t *)((struct rxr_ctrl_pkt *)rts_hdr)->data;
+		data = src + rts_hdr->addrlen;
+	} else if ((rts_hdr->flags & RXR_REMOTE_CQ_DATA) &&
+		   !(rts_hdr->flags & RXR_REMOTE_SRC_ADDR)) {
+		data = (uint8_t *)((struct rxr_ctrl_cq_pkt *)rts_hdr)->data;
+	} else {
+		data = (uint8_t *)((struct rxr_ctrl_pkt *)rts_hdr)->data;
+	}
+
+	if (rts_hdr->flags & RXR_REMOTE_CQ_DATA)
+		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+		       "\tcq_data: %08lx\n",
+		       ((struct rxr_ctrl_cq_hdr *)rts_hdr)->cq_data);
+
+	if (rts_hdr->flags & RXR_REMOTE_SRC_ADDR) {
+		l = snprintf(str, str_len, "\tsrc_addr: ");
+		for (i = 0; i < rts_hdr->addrlen; i++)
+			l += snprintf(str + l, str_len - l, "%02x ", src[i]);
+		FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
+	}
+
+	l = snprintf(str, str_len, ("\tdata:    "));
+	for (i = 0; i < MIN(rxr_get_rts_data_size(ep, rts_hdr),
+			    RXR_PKT_DUMP_DATA_LEN); i++)
+		l += snprintf(str + l, str_len - l, "%02x ", data[i]);
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
+}
+
+static
+void rxr_pkt_print_connack(char *prefix,
+			   struct rxr_connack_hdr *connack_hdr)
+{
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+	       "%s RxR CONNACK packet - version: %" PRIu8
+	       " flags: %x\n", prefix, connack_hdr->version,
+	       connack_hdr->flags);
+}
+
+static
+void rxr_pkt_print_cts(char *prefix, struct rxr_cts_hdr *cts_hdr)
+{
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+	       "%s RxR CTS packet - version: %"	PRIu8
+	       " flags: %x tx_id: %" PRIu32
+	       " rx_id: %"	   PRIu32
+	       " window: %"	   PRIu64
+	       "\n", prefix, cts_hdr->version, cts_hdr->flags,
+	       cts_hdr->tx_id, cts_hdr->rx_id, cts_hdr->window);
+}
+
+static
+void rxr_pkt_print_data(char *prefix, struct rxr_data_pkt *data_pkt)
+{
+	char str[RXR_PKT_DUMP_DATA_LEN * 4];
+	size_t str_len = RXR_PKT_DUMP_DATA_LEN * 4, l;
+	int i;
+
+	str[str_len - 1] = '\0';
+
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+	       "%s RxR DATA packet -  version: %" PRIu8
+	       " flags: %x rx_id: %" PRIu32
+	       " seg_size: %"	     PRIu64
+	       " seg_offset: %"	     PRIu64
+	       "\n", prefix, data_pkt->hdr.version, data_pkt->hdr.flags,
+	       data_pkt->hdr.rx_id, data_pkt->hdr.seg_size,
+	       data_pkt->hdr.seg_offset);
+
+	l = snprintf(str, str_len, ("\tdata:    "));
+	for (i = 0; i < MIN(data_pkt->hdr.seg_size, RXR_PKT_DUMP_DATA_LEN);
+	     i++)
+		l += snprintf(str + l, str_len - l, "%02x ",
+			      ((uint8_t *)data_pkt->data)[i]);
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA, "%s\n", str);
+}
+
+void rxr_pkt_print(char *prefix, struct rxr_ep *ep, struct rxr_base_hdr *hdr)
+{
+	switch (hdr->type) {
+	case RXR_RTS_PKT:
+		rxr_pkt_print_rts(ep, prefix, (struct rxr_rts_hdr *)hdr);
+		break;
+	case RXR_CONNACK_PKT:
+		rxr_pkt_print_connack(prefix, (struct rxr_connack_hdr *)hdr);
+		break;
+	case RXR_CTS_PKT:
+		rxr_pkt_print_cts(prefix, (struct rxr_cts_hdr *)hdr);
+		break;
+	case RXR_DATA_PKT:
+		rxr_pkt_print_data(prefix, (struct rxr_data_pkt *)hdr);
+		break;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid ctl pkt type %d\n",
+			rxr_get_base_hdr(hdr)->type);
+		assert(0);
+		return;
+	}
+}
+#endif
+
diff --git a/prov/efa/src/rxr/rxr_pkt_cmd.h b/prov/efa/src/rxr/rxr_pkt_cmd.h
new file mode 100644
index 0000000..419b14f
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_cmd.h
@@ -0,0 +1,61 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _RXR_PKT_CMD_H
+#define _RXR_PKT_CMD_H
+
+#include "rxr.h"
+
+ssize_t rxr_pkt_post_data(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry);
+
+ssize_t rxr_pkt_post_ctrl(struct rxr_ep *ep, int entry_type, void *x_entry,
+			  int ctrl_type, bool inject);
+
+ssize_t rxr_pkt_post_ctrl_or_queue(struct rxr_ep *ep, int entry_type, void *x_entry,
+				   int ctrl_type, bool inject);
+
+void rxr_pkt_handle_send_completion(struct rxr_ep *ep,
+				    struct fi_cq_data_entry *cq_entry);
+
+void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
+				    struct fi_cq_data_entry *cq_entry,
+				    fi_addr_t src_addr);
+
+#if ENABLE_DEBUG
+void rxr_pkt_print(char *prefix,
+		   struct rxr_ep *ep,
+		   struct rxr_base_hdr *hdr);
+#endif
+
+#endif
+
diff --git a/prov/efa/src/rxr/rxr_pkt_entry.c b/prov/efa/src/rxr/rxr_pkt_entry.c
new file mode 100644
index 0000000..eb62d21
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_entry.c
@@ -0,0 +1,243 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <inttypes.h>
+#include <stdlib.h>
+#include <string.h>
+#include "ofi.h"
+#include <ofi_util.h>
+#include <ofi_iov.h>
+
+#include "rxr.h"
+#include "efa.h"
+#include "rxr_msg.h"
+#include "rxr_rma.h"
+
+/*
+ *   General purpose utility functions
+ */
+struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
+					  struct ofi_bufpool *pkt_pool)
+{
+	struct rxr_pkt_entry *pkt_entry;
+	void *mr = NULL;
+
+	pkt_entry = ofi_buf_alloc_ex(pkt_pool, &mr);
+	if (!pkt_entry)
+		return NULL;
+#ifdef ENABLE_EFA_POISONING
+	memset(pkt_entry, 0, sizeof(*pkt_entry));
+#endif
+	dlist_init(&pkt_entry->entry);
+#if ENABLE_DEBUG
+	dlist_init(&pkt_entry->dbg_entry);
+#endif
+	pkt_entry->mr = (struct fid_mr *)mr;
+	pkt_entry->pkt = (struct rxr_pkt *)((char *)pkt_entry +
+			  sizeof(*pkt_entry));
+#ifdef ENABLE_EFA_POISONING
+	memset(pkt_entry->pkt, 0, ep->mtu_size);
+#endif
+	pkt_entry->state = RXR_PKT_ENTRY_IN_USE;
+
+	return pkt_entry;
+}
+
+void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt)
+{
+	struct rxr_peer *peer;
+
+#if ENABLE_DEBUG
+	dlist_remove(&pkt->dbg_entry);
+#endif
+	/*
+	 * Decrement rnr_queued_pkts counter and reset backoff for this peer if
+	 * we get a send completion for a retransmitted packet.
+	 */
+	if (OFI_UNLIKELY(pkt->state == RXR_PKT_ENTRY_RNR_RETRANSMIT)) {
+		peer = rxr_ep_get_peer(ep, pkt->addr);
+		peer->rnr_queued_pkt_cnt--;
+		peer->timeout_interval = 0;
+		peer->rnr_timeout_exp = 0;
+		if (peer->rnr_state & RXR_PEER_IN_BACKOFF)
+			dlist_remove(&peer->rnr_entry);
+		peer->rnr_state = 0;
+		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+		       "reset backoff timer for peer: %" PRIu64 "\n",
+		       pkt->addr);
+	}
+#ifdef ENABLE_EFA_POISONING
+	rxr_poison_mem_region((uint32_t *)pkt, ep->tx_pkt_pool_entry_sz);
+#endif
+	pkt->state = RXR_PKT_ENTRY_FREE;
+	ofi_buf_free(pkt);
+}
+
+void rxr_pkt_entry_release_rx(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt_entry)
+{
+	if (pkt_entry->type == RXR_PKT_ENTRY_POSTED) {
+		struct rxr_peer *peer;
+
+		peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+		assert(peer);
+		if (peer->is_local)
+			ep->rx_bufs_shm_to_post++;
+		else
+			ep->rx_bufs_efa_to_post++;
+	}
+#if ENABLE_DEBUG
+	dlist_remove(&pkt_entry->dbg_entry);
+#endif
+#ifdef ENABLE_EFA_POISONING
+	/* the same pool size is used for all types of rx pkt_entries */
+	rxr_poison_mem_region((uint32_t *)pkt_entry, ep->rx_pkt_pool_entry_sz);
+#endif
+	pkt_entry->state = RXR_PKT_ENTRY_FREE;
+	ofi_buf_free(pkt_entry);
+}
+
+void rxr_pkt_entry_copy(struct rxr_ep *ep,
+			struct rxr_pkt_entry *dest,
+			struct rxr_pkt_entry *src,
+			enum rxr_pkt_entry_type type)
+{
+	FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
+	       "Copying packet (type %d) out of posted buffer\n", type);
+	assert(src->type == RXR_PKT_ENTRY_POSTED);
+	memcpy(dest, src, sizeof(struct rxr_pkt_entry));
+	dest->pkt = (struct rxr_pkt *)((char *)dest + sizeof(*dest));
+	memcpy(dest->pkt, src->pkt, ep->mtu_size);
+	dest->type = type;
+	dlist_init(&dest->entry);
+#if ENABLE_DEBUG
+	dlist_init(&dest->dbg_entry);
+#endif
+	dest->state = RXR_PKT_ENTRY_IN_USE;
+}
+
+/*
+ *   Utility functions used to send pkt over wire
+ */
+static inline
+ssize_t rxr_pkt_entry_sendmsg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry,
+			      const struct fi_msg *msg, uint64_t flags)
+{
+	struct rxr_peer *peer;
+	size_t ret;
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(ep->tx_pending <= ep->max_outstanding_tx);
+
+	if (ep->tx_pending == ep->max_outstanding_tx)
+		return -FI_EAGAIN;
+
+	if (peer->rnr_state & RXR_PEER_IN_BACKOFF)
+		return -FI_EAGAIN;
+
+#if ENABLE_DEBUG
+	dlist_insert_tail(&pkt_entry->dbg_entry, &ep->tx_pkt_list);
+#ifdef ENABLE_RXR_PKT_DUMP
+	rxr_pkt_print("Sent", ep, (struct rxr_base_hdr *)pkt_entry->pkt);
+#endif
+#endif
+	if (rxr_env.enable_shm_transfer && peer->is_local) {
+		ret = fi_sendmsg(ep->shm_ep, msg, flags);
+	} else {
+		ret = fi_sendmsg(ep->rdm_ep, msg, flags);
+		if (OFI_LIKELY(!ret))
+			rxr_ep_inc_tx_pending(ep, peer);
+	}
+
+	return ret;
+}
+
+ssize_t rxr_pkt_entry_sendv(struct rxr_ep *ep,
+			    struct rxr_pkt_entry *pkt_entry,
+			    fi_addr_t addr, const struct iovec *iov,
+			    void **desc, size_t count, uint64_t flags)
+{
+	struct fi_msg msg;
+	struct rxr_peer *peer;
+
+	msg.msg_iov = iov;
+	msg.desc = desc;
+	msg.iov_count = count;
+	peer = rxr_ep_get_peer(ep, addr);
+	msg.addr = peer->is_local ? peer->shm_fiaddr : addr;
+	msg.context = pkt_entry;
+	msg.data = 0;
+
+	return rxr_pkt_entry_sendmsg(ep, pkt_entry, &msg, flags);
+}
+
+/* rxr_pkt_start currently expects data pkt right after pkt hdr */
+ssize_t rxr_pkt_entry_send_with_flags(struct rxr_ep *ep,
+				      struct rxr_pkt_entry *pkt_entry,
+				      fi_addr_t addr, uint64_t flags)
+{
+	struct iovec iov;
+	void *desc;
+
+	iov.iov_base = rxr_pkt_start(pkt_entry);
+	iov.iov_len = pkt_entry->pkt_size;
+
+	if (rxr_ep_get_peer(ep, addr)->is_local)
+		desc = NULL;
+	else
+		desc = rxr_ep_mr_local(ep) ? fi_mr_desc(pkt_entry->mr) : NULL;
+
+	return rxr_pkt_entry_sendv(ep, pkt_entry, addr, &iov, &desc, 1, flags);
+}
+
+ssize_t rxr_pkt_entry_send(struct rxr_ep *ep,
+			   struct rxr_pkt_entry *pkt_entry,
+			   fi_addr_t addr)
+{
+	return rxr_pkt_entry_send_with_flags(ep, pkt_entry, addr, 0);
+}
+
+ssize_t rxr_pkt_entry_inject(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry,
+			     fi_addr_t addr)
+{
+	struct rxr_peer *peer;
+
+	/* currently only EOR packet is injected using shm ep */
+	peer = rxr_ep_get_peer(ep, addr);
+	assert(peer);
+	assert(rxr_env.enable_shm_transfer && peer->is_local);
+	return fi_inject(ep->shm_ep, rxr_pkt_start(pkt_entry), pkt_entry->pkt_size,
+			 peer->shm_fiaddr);
+}
diff --git a/prov/efa/src/rxr/rxr_pkt_entry.h b/prov/efa/src/rxr/rxr_pkt_entry.h
new file mode 100644
index 0000000..566843c
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_entry.h
@@ -0,0 +1,120 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _RXR_PKT_ENTRY_H
+#define _RXR_PKT_ENTRY_H
+
+#include <ofi_list.h>
+
+/* pkt_entry state for retransmit tracking */
+enum rxr_pkt_entry_state {
+	RXR_PKT_ENTRY_FREE = 0,
+	RXR_PKT_ENTRY_IN_USE,
+	RXR_PKT_ENTRY_RNR_RETRANSMIT,
+};
+
+/* pkt_entry types for rx pkts */
+enum rxr_pkt_entry_type {
+	RXR_PKT_ENTRY_POSTED = 1,   /* entries that are posted to the core */
+	RXR_PKT_ENTRY_UNEXP,        /* entries used to stage unexpected msgs */
+	RXR_PKT_ENTRY_OOO	    /* entries used to stage out-of-order RTS */
+};
+
+struct rxr_pkt_entry {
+	/* for rx/tx_entry queued_pkts list */
+	struct dlist_entry entry;
+#if ENABLE_DEBUG
+	/* for tx/rx debug list or posted buf list */
+	struct dlist_entry dbg_entry;
+#endif
+	void *x_entry; /* pointer to rxr rx/tx entry */
+	size_t pkt_size;
+	struct fid_mr *mr;
+	fi_addr_t addr;
+	void *pkt; /* rxr_ctrl_*_pkt, or rxr_data_pkt */
+	enum rxr_pkt_entry_type type;
+	enum rxr_pkt_entry_state state;
+#if ENABLE_DEBUG
+/* pad to cache line size of 64 bytes */
+	uint8_t pad[48];
+#endif
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+#if ENABLE_DEBUG
+static_assert(sizeof(struct rxr_pkt_entry) == 128, "rxr_pkt_entry check");
+#else
+static_assert(sizeof(struct rxr_pkt_entry) == 64, "rxr_pkt_entry check");
+#endif
+#endif
+
+OFI_DECL_RECVWIN_BUF(struct rxr_pkt_entry*, rxr_robuf, uint32_t);
+DECLARE_FREESTACK(struct rxr_robuf, rxr_robuf_fs);
+
+struct rxr_ep;
+
+struct rxr_tx_entry;
+
+struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
+					  struct ofi_bufpool *pkt_pool);
+
+void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_entry_release_rx(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_entry_copy(struct rxr_ep *ep,
+			struct rxr_pkt_entry *dest,
+			struct rxr_pkt_entry *src,
+			enum rxr_pkt_entry_type type);
+
+ssize_t rxr_pkt_entry_send_with_flags(struct rxr_ep *ep,
+				      struct rxr_pkt_entry *pkt_entry,
+				      fi_addr_t addr, uint64_t flags);
+
+ssize_t rxr_pkt_entry_sendv(struct rxr_ep *ep,
+			    struct rxr_pkt_entry *pkt_entry,
+			    fi_addr_t addr, const struct iovec *iov,
+			    void **desc, size_t count, uint64_t flags);
+
+ssize_t rxr_pkt_entry_send(struct rxr_ep *ep,
+			   struct rxr_pkt_entry *pkt_entry,
+			   fi_addr_t addr);
+
+ssize_t rxr_pkt_entry_inject(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry,
+			     fi_addr_t addr);
+
+#endif
+
diff --git a/prov/efa/src/rxr/rxr_pkt_type.h b/prov/efa/src/rxr/rxr_pkt_type.h
new file mode 100644
index 0000000..4464401
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_type.h
@@ -0,0 +1,405 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _RXR_PKT_TYPE_H
+#define _RXR_PKT_TYPE_H
+
+/* This header file contain the ID of all RxR packet types, and
+ * the necessary data structures and functions for each packet type
+ *
+ * RxR packet types can be classified into 3 categories:
+ *     data packet, control packet and context packet
+ *
+ * For each packet type, the following items are needed:
+ *
+ *   First, each packet type need to define a struct for its header,
+ *       and the header must be start with ```struct rxr_base_hdr```.
+ *
+ *   Second, each control packet type need to define an init()
+ *       function and a handle_sent() function. These functions
+ *       are called by rxr_pkt_post_ctrl_or_queue().
+ *
+ *   Finally, each packet type (except context packet) need to
+ *     define a handle_recv() functions which is called by
+ *     rxr_pkt_handle_recv_completion().
+ */
+
+enum rxr_pkt_type {
+	RXR_RTS_PKT = 1,
+	RXR_CONNACK_PKT,
+	RXR_CTS_PKT,
+	RXR_DATA_PKT,
+	RXR_READRSP_PKT,
+	RXR_RMA_CONTEXT_PKT,
+	RXR_EOR_PKT,
+};
+
+/*
+ *  Packet fields common to all rxr packets. The other packet headers below must
+ *  be changed if this is updated.
+ */
+struct rxr_base_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_base_hdr) == 4, "rxr_base_hdr check");
+#endif
+
+static inline struct rxr_base_hdr *rxr_get_base_hdr(void *pkt)
+{
+	return (struct rxr_base_hdr *)pkt;
+}
+
+struct rxr_ep;
+struct rxr_peer;
+struct rxr_tx_entry;
+struct rxr_rx_entry;
+
+/*
+ *  RTS packet data structures and functions. the implementation of
+ *  these functions are in rxr_pkt_type_rts.c
+ */
+struct rxr_rts_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	/* TODO: need to add msg_id -> tx_id mapping to remove tx_id */
+	uint16_t credit_request;
+	uint8_t addrlen;
+	uint8_t rma_iov_count;
+	uint32_t tx_id;
+	uint32_t msg_id;
+	uint64_t tag;
+	uint64_t data_len;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_rts_hdr) == 32, "rxr_rts_hdr check");
+#endif
+
+static inline
+struct rxr_rts_hdr *rxr_get_rts_hdr(void *pkt)
+{
+	return (struct rxr_rts_hdr *)pkt;
+}
+
+uint64_t rxr_get_rts_data_size(struct rxr_ep *ep,
+			       struct rxr_rts_hdr *rts_hdr);
+
+ssize_t rxr_pkt_init_rts(struct rxr_ep *ep,
+			 struct rxr_tx_entry *tx_entry,
+			 struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_rts_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
+
+ssize_t rxr_pkt_proc_matched_msg_rts(struct rxr_ep *ep,
+				     struct rxr_rx_entry *rx_entry,
+				     struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_rts_send_completion(struct rxr_ep *ep,
+					struct rxr_pkt_entry *pkt_entry);
+
+ssize_t rxr_pkt_post_shm_rndzv_read(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry);
+
+ssize_t rxr_pkt_proc_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
+
+
+void rxr_pkt_handle_rts_recv(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
+
+/*
+ *  CONNACK packet header and functions
+ *  implementation of the functions are in rxr_pkt_type_misc.c
+ */
+struct rxr_connack_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_connack_hdr) == 4, "rxr_connack_hdr check");
+#endif
+
+#define RXR_CONNACK_HDR_SIZE		(sizeof(struct rxr_connack_hdr))
+
+static inline
+struct rxr_connack_hdr *rxr_get_connack_hdr(void *pkt)
+{
+	return (struct rxr_connack_hdr *)pkt;
+}
+
+ssize_t rxr_pkt_init_connack(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry,
+			     fi_addr_t addr);
+
+void rxr_pkt_post_connack(struct rxr_ep *ep,
+			  struct rxr_peer *peer,
+			  fi_addr_t addr);
+
+void rxr_pkt_handle_connack_recv(struct rxr_ep *ep,
+				 struct rxr_pkt_entry *pkt_entry,
+				 fi_addr_t addr);
+/*
+ *  CTS packet data structures and functions.
+ *  Definition of the functions is in rxr_pkt_type_misc.c
+ */
+struct rxr_cts_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint8_t pad[4];
+	/* TODO: need to add msg_id -> tx_id/rx_id mapping */
+	uint32_t tx_id;
+	uint32_t rx_id;
+	uint64_t window;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_cts_hdr) == 24, "rxr_cts_hdr check");
+#endif
+
+#define RXR_CTS_HDR_SIZE		(sizeof(struct rxr_cts_hdr))
+
+static inline
+struct rxr_cts_hdr *rxr_get_cts_hdr(void *pkt)
+{
+	return (struct rxr_cts_hdr *)pkt;
+}
+
+void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
+				     uint64_t size, int request,
+				     int *window, int *credits);
+
+ssize_t rxr_pkt_init_cts(struct rxr_ep *ep,
+			 struct rxr_rx_entry *rx_entry,
+			 struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_cts_sent(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_cts_recv(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry);
+
+/*
+ *  DATA packet data structures and functions
+ *  Definition of the functions is in rxr_pkt_data.c
+ */
+struct rxr_data_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	/* TODO: need to add msg_id -> tx_id/rx_id mapping */
+	uint32_t rx_id;
+	uint64_t seg_size;
+	uint64_t seg_offset;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_data_hdr) == 24, "rxr_data_hdr check");
+#endif
+
+#define RXR_DATA_HDR_SIZE		(sizeof(struct rxr_data_hdr))
+
+struct rxr_data_pkt {
+	struct rxr_data_hdr hdr;
+	char data[];
+};
+
+static inline
+struct rxr_data_pkt *rxr_get_data_pkt(void *pkt)
+{
+	return (struct rxr_data_pkt *)pkt;
+}
+
+ssize_t rxr_pkt_send_data(struct rxr_ep *ep,
+			  struct rxr_tx_entry *tx_entry,
+			  struct rxr_pkt_entry *pkt_entry);
+
+ssize_t rxr_pkt_send_data_mr_cache(struct rxr_ep *ep,
+				   struct rxr_tx_entry *tx_entry,
+				   struct rxr_pkt_entry *pkt_entry);
+
+int rxr_pkt_proc_data(struct rxr_ep *ep,
+		      struct rxr_rx_entry *rx_entry,
+		      struct rxr_pkt_entry *pkt_entry,
+		      char *data, size_t seg_offset,
+		      size_t seg_size);
+
+void rxr_pkt_handle_data_send_completion(struct rxr_ep *ep,
+					 struct rxr_pkt_entry *pkt_entry);
+
+
+void rxr_pkt_handle_data_recv(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt_entry);
+
+/*
+ *  READRSP packet data structures and functions
+ *  The definition of functions are in rxr_pkt_type_misc.c
+ */
+struct rxr_readrsp_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint8_t pad[4];
+	uint32_t rx_id;
+	uint32_t tx_id;
+	uint64_t seg_size;
+};
+
+static inline struct rxr_readrsp_hdr *rxr_get_readrsp_hdr(void *pkt)
+{
+	return (struct rxr_readrsp_hdr *)pkt;
+}
+
+#define RXR_READRSP_HDR_SIZE	(sizeof(struct rxr_readrsp_hdr))
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_readrsp_hdr) == sizeof(struct rxr_data_hdr), "rxr_readrsp_hdr check");
+#endif
+
+struct rxr_readrsp_pkt {
+	struct rxr_readrsp_hdr hdr;
+	char data[];
+};
+
+int rxr_pkt_init_readrsp(struct rxr_ep *ep,
+			 struct rxr_tx_entry *tx_entry,
+			 struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_readrsp_sent(struct rxr_ep *ep,
+				 struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_readrsp_send_completion(struct rxr_ep *ep,
+					    struct rxr_pkt_entry *pkt_entry);
+
+void rxr_pkt_handle_readrsp_recv(struct rxr_ep *ep,
+				 struct rxr_pkt_entry *pkt_entry);
+
+/*
+ *  RMA context packet, used to differentiate the normal RMA read, normal RMA
+ *  write, and the RMA read in two-sided large message transfer
+ *  Implementation of the function is in rxr_pkt_type_misc.c
+ */
+struct rxr_rma_context_pkt {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t tx_id;
+	uint8_t rma_context_type;
+};
+
+void rxr_pkt_handle_rma_context_send_completion(struct rxr_ep *ep,
+						struct rxr_pkt_entry *pkt_entry);
+
+/*
+ *  EOR packet, used to acknowledge the sender that large message
+ *  copy has been finished.
+ *  Implementaion of the functions are in rxr_pkt_misc.c
+ */
+struct rxr_eor_hdr {
+	uint8_t type;
+	uint8_t version;
+	uint16_t flags;
+	/* end of rxr_base_hdr */
+	uint32_t tx_id;
+	uint32_t rx_id;
+};
+
+#if defined(static_assert) && defined(__x86_64__)
+static_assert(sizeof(struct rxr_eor_hdr) == 12, "rxr_eor_hdr check");
+#endif
+
+int rxr_pkt_init_eor(struct rxr_ep *ep,
+		     struct rxr_rx_entry *rx_entry,
+		     struct rxr_pkt_entry *pkt_entry);
+
+static inline
+void rxr_pkt_handle_eor_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+}
+
+void rxr_pkt_handle_eor_recv(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry);
+
+/*
+ * Control header without completion data. We will send more data with the RTS
+ * packet if RXR_REMOTE_CQ_DATA is not set.
+ */
+struct rxr_ctrl_hdr {
+	union {
+		struct rxr_base_hdr base_hdr;
+		struct rxr_rts_hdr rts_hdr;
+		struct rxr_connack_hdr connack_hdr;
+		struct rxr_cts_hdr cts_hdr;
+	};
+};
+
+#define RXR_CTRL_HDR_SIZE              (sizeof(struct rxr_ctrl_cq_hdr))
+
+struct rxr_ctrl_pkt {
+	struct rxr_ctrl_hdr hdr;
+	char data[];
+};
+
+/*
+ * Control header with completion data. CQ data length is static.
+ */
+#define RXR_CQ_DATA_SIZE (8)
+struct rxr_ctrl_cq_hdr {
+	union {
+		struct rxr_base_hdr base_hdr;
+		struct rxr_rts_hdr rts_hdr;
+		struct rxr_connack_hdr connack_hdr;
+		struct rxr_cts_hdr cts_hdr;
+	};
+	uint64_t cq_data;
+};
+
+#define RXR_CTRL_HDR_SIZE_NO_CQ                (sizeof(struct rxr_ctrl_hdr))
+
+struct rxr_ctrl_cq_pkt {
+	struct rxr_ctrl_cq_hdr hdr;
+	char data[];
+};
+
+#endif
+
diff --git a/prov/efa/src/rxr/rxr_pkt_type_data.c b/prov/efa/src/rxr/rxr_pkt_type_data.c
new file mode 100644
index 0000000..06f642e
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_type_data.c
@@ -0,0 +1,314 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "rxr.h"
+#include "rxr_msg.h"
+#include "rxr_pkt_cmd.h"
+
+/*
+ * This function contains data packet related functions
+ * Data packet is used by long message protocol.
+ */
+
+/*
+ * Functions to send data packet, including
+ */
+
+ssize_t rxr_pkt_send_data(struct rxr_ep *ep,
+			  struct rxr_tx_entry *tx_entry,
+			  struct rxr_pkt_entry *pkt_entry)
+{
+	uint64_t payload_size;
+	struct rxr_data_pkt *data_pkt;
+
+	pkt_entry->x_entry = (void *)tx_entry;
+	pkt_entry->addr = tx_entry->addr;
+
+	payload_size = MIN(tx_entry->total_len - tx_entry->bytes_sent,
+			   ep->max_data_payload_size);
+	payload_size = MIN(payload_size, tx_entry->window);
+
+	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
+	data_pkt->hdr.seg_size = payload_size;
+
+	pkt_entry->pkt_size = ofi_copy_from_iov(data_pkt->data,
+						payload_size,
+						tx_entry->iov,
+						tx_entry->iov_count,
+						tx_entry->bytes_sent);
+	assert(pkt_entry->pkt_size == payload_size);
+
+	pkt_entry->pkt_size += RXR_DATA_HDR_SIZE;
+	pkt_entry->addr = tx_entry->addr;
+
+	return rxr_pkt_entry_send_with_flags(ep, pkt_entry, pkt_entry->addr,
+					     tx_entry->send_flags);
+}
+
+/*
+ * Copies all consecutive small iov's into one buffer. If the function reaches
+ * an iov greater than the max memcpy size, it will end, only copying up to
+ * that iov.
+ */
+static size_t rxr_copy_from_iov(void *buf, uint64_t remaining_len,
+				struct rxr_tx_entry *tx_entry)
+{
+	struct iovec *tx_iov = tx_entry->iov;
+	uint64_t done = 0, len;
+
+	while (tx_entry->iov_index < tx_entry->iov_count &&
+	       done < remaining_len) {
+		len = tx_iov[tx_entry->iov_index].iov_len;
+		if (tx_entry->mr[tx_entry->iov_index])
+			break;
+
+		len -= tx_entry->iov_offset;
+
+		/*
+		 * If the amount to be written surpasses the remaining length,
+		 * copy up to the remaining length and return, else copy the
+		 * entire iov and continue.
+		 */
+		if (done + len > remaining_len) {
+			len = remaining_len - done;
+			memcpy((char *)buf + done,
+			       (char *)tx_iov[tx_entry->iov_index].iov_base +
+			       tx_entry->iov_offset, len);
+			tx_entry->iov_offset += len;
+			done += len;
+			break;
+		}
+		memcpy((char *)buf + done,
+		       (char *)tx_iov[tx_entry->iov_index].iov_base +
+		       tx_entry->iov_offset, len);
+		tx_entry->iov_index++;
+		tx_entry->iov_offset = 0;
+		done += len;
+	}
+	return done;
+}
+
+ssize_t rxr_pkt_send_data_mr_cache(struct rxr_ep *ep,
+				   struct rxr_tx_entry *tx_entry,
+				   struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_data_pkt *data_pkt;
+	/* The user's iov */
+	struct iovec *tx_iov = tx_entry->iov;
+	/* The constructed iov to be passed to sendv
+	 * and corresponding fid_mrs
+	 */
+	struct iovec iov[ep->core_iov_limit];
+	void *desc[ep->core_iov_limit];
+	/* Constructed iov's total size */
+	uint64_t payload_size = 0;
+	/* pkt_entry offset to write data into */
+	uint64_t pkt_used = 0;
+	/* Remaining size that can fit in the constructed iov */
+	uint64_t remaining_len = MIN(tx_entry->window,
+				     ep->max_data_payload_size);
+	/* The constructed iov's index */
+	size_t i = 0;
+	size_t len = 0;
+
+	ssize_t ret;
+
+	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
+	/* Assign packet header in constructed iov */
+	iov[i].iov_base = rxr_pkt_start(pkt_entry);
+	iov[i].iov_len = RXR_DATA_HDR_SIZE;
+	desc[i] = rxr_ep_mr_local(ep) ? fi_mr_desc(pkt_entry->mr) : NULL;
+	i++;
+
+	/*
+	 * Loops until payload size is at max, all user iovs are sent, the
+	 * constructed iov count is greater than the core iov limit, or the tx
+	 * entry window is exhausted.  Each iteration fills one entry of the
+	 * iov to be sent.
+	 */
+	while (tx_entry->iov_index < tx_entry->iov_count &&
+	       remaining_len > 0 && i < ep->core_iov_limit) {
+		if (!rxr_ep_mr_local(ep) ||
+		    /* from the inline registration post-RTS */
+		    tx_entry->mr[tx_entry->iov_index] ||
+		    /* from application-provided descriptor */
+		    tx_entry->desc[tx_entry->iov_index]) {
+			iov[i].iov_base =
+				(char *)tx_iov[tx_entry->iov_index].iov_base +
+				tx_entry->iov_offset;
+			if (rxr_ep_mr_local(ep))
+				desc[i] = tx_entry->desc[tx_entry->iov_index] ?
+					  tx_entry->desc[tx_entry->iov_index] :
+					  fi_mr_desc(tx_entry->mr[tx_entry->iov_index]);
+
+			len = tx_iov[tx_entry->iov_index].iov_len
+			      - tx_entry->iov_offset;
+			if (len > remaining_len) {
+				len = remaining_len;
+				tx_entry->iov_offset += len;
+			} else {
+				tx_entry->iov_index++;
+				tx_entry->iov_offset = 0;
+			}
+			iov[i].iov_len = len;
+		} else {
+			/*
+			 * Copies any consecutive small iov's, returning size
+			 * written while updating iov index and offset
+			 */
+			len = rxr_copy_from_iov((char *)data_pkt->data +
+						 pkt_used,
+						 remaining_len,
+						 tx_entry);
+
+			iov[i].iov_base = (char *)data_pkt->data + pkt_used;
+			iov[i].iov_len = len;
+			desc[i] = fi_mr_desc(pkt_entry->mr);
+			pkt_used += len;
+		}
+		payload_size += len;
+		remaining_len -= len;
+		i++;
+	}
+	data_pkt->hdr.seg_size = (uint16_t)payload_size;
+	pkt_entry->pkt_size = payload_size + RXR_DATA_HDR_SIZE;
+	pkt_entry->addr = tx_entry->addr;
+
+	FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+	       "Sending an iov count, %zu with payload size: %lu.\n",
+	       i, payload_size);
+	ret = rxr_pkt_entry_sendv(ep, pkt_entry, tx_entry->addr,
+				  (const struct iovec *)iov,
+				  desc, i, tx_entry->send_flags);
+	return ret;
+}
+
+void rxr_pkt_handle_data_send_completion(struct rxr_ep *ep,
+					 struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_tx_entry *tx_entry;
+
+	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
+	tx_entry->bytes_acked +=
+		rxr_get_data_pkt(pkt_entry->pkt)->hdr.seg_size;
+	if (tx_entry->total_len == tx_entry->bytes_acked)
+		rxr_cq_handle_tx_completion(ep, tx_entry);
+}
+
+/*
+ *  rxr_pkt_handle_data_recv() and related functions
+ */
+int rxr_pkt_proc_data(struct rxr_ep *ep,
+		      struct rxr_rx_entry *rx_entry,
+		      struct rxr_pkt_entry *pkt_entry,
+		      char *data, size_t seg_offset,
+		      size_t seg_size)
+{
+	struct rxr_peer *peer;
+	int64_t bytes_left, bytes_copied;
+	ssize_t ret = 0;
+
+#if ENABLE_DEBUG
+	int pkt_type = rxr_get_base_hdr(pkt_entry->pkt)->type;
+
+	assert(pkt_type == RXR_DATA_PKT || pkt_type == RXR_READRSP_PKT);
+#endif
+	/* we are sinking message for CANCEL/DISCARD entry */
+	if (OFI_LIKELY(!(rx_entry->rxr_flags & RXR_RECV_CANCEL)) &&
+	    rx_entry->cq_entry.len > seg_offset) {
+		bytes_copied = ofi_copy_to_iov(rx_entry->iov, rx_entry->iov_count,
+					       seg_offset, data, seg_size);
+		if (bytes_copied != MIN(seg_size, rx_entry->cq_entry.len - seg_offset)) {
+			FI_WARN(&rxr_prov, FI_LOG_CQ, "wrong size! bytes_copied: %ld\n",
+				bytes_copied);
+			if (rxr_cq_handle_rx_error(ep, rx_entry, -FI_EINVAL))
+				assert(0 && "error writing error cq entry for EOR\n");
+		}
+	}
+
+	rx_entry->bytes_done += seg_size;
+
+	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	peer->rx_credits += ofi_div_ceil(seg_size, ep->max_data_payload_size);
+
+	rx_entry->window -= seg_size;
+	if (ep->available_data_bufs < rxr_get_rx_pool_chunk_cnt(ep))
+		ep->available_data_bufs++;
+
+	/* bytes_done is total bytes sent/received, which could be larger than
+	 * to bytes copied to recv buffer (for truncated messages).
+	 * rx_entry->total_len is from rts_hdr and is the size of send buffer,
+	 * thus we always have:
+	 *             rx_entry->total >= rx_entry->bytes_done
+	 */
+	bytes_left = rx_entry->total_len - rx_entry->bytes_done;
+	assert(bytes_left >= 0);
+	if (!bytes_left) {
+#if ENABLE_DEBUG
+		dlist_remove(&rx_entry->rx_pending_entry);
+		ep->rx_pending--;
+#endif
+		rxr_cq_handle_rx_completion(ep, pkt_entry, rx_entry);
+
+		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
+		rxr_release_rx_entry(ep, rx_entry);
+		return 0;
+	}
+
+	if (!rx_entry->window) {
+		assert(rx_entry->state == RXR_RX_RECV);
+		ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
+	}
+
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+	return ret;
+}
+
+void rxr_pkt_handle_data_recv(struct rxr_ep *ep,
+			      struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_data_pkt *data_pkt;
+	struct rxr_rx_entry *rx_entry;
+
+	data_pkt = (struct rxr_data_pkt *)pkt_entry->pkt;
+
+	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool,
+					data_pkt->hdr.rx_id);
+
+	rxr_pkt_proc_data(ep, rx_entry,
+			  pkt_entry,
+			  data_pkt->data,
+			  data_pkt->hdr.seg_offset,
+			  data_pkt->hdr.seg_size);
+}
+
diff --git a/prov/efa/src/rxr/rxr_pkt_type_misc.c b/prov/efa/src/rxr/rxr_pkt_type_misc.c
new file mode 100644
index 0000000..cec9d5f
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_type_misc.c
@@ -0,0 +1,408 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "efa.h"
+#include "rxr.h"
+#include "rxr_msg.h"
+#include "rxr_cntr.h"
+#include "rxr_pkt_cmd.h"
+
+/* This file define functons for the following packet type:
+ *       CONNACK
+ *       CTS
+ *       READRSP
+ *       RMA_CONTEXT
+ *       EOR
+ */
+
+/*  CONNACK packet related functions */
+ssize_t rxr_pkt_init_connack(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry,
+			     fi_addr_t addr)
+{
+	struct rxr_connack_hdr *connack_hdr;
+
+	connack_hdr = (struct rxr_connack_hdr *)pkt_entry->pkt;
+
+	connack_hdr->type = RXR_CONNACK_PKT;
+	connack_hdr->version = RXR_PROTOCOL_VERSION;
+	connack_hdr->flags = 0;
+
+	pkt_entry->pkt_size = RXR_CONNACK_HDR_SIZE;
+	pkt_entry->addr = addr;
+	return 0;
+}
+
+void rxr_pkt_post_connack(struct rxr_ep *ep,
+			  struct rxr_peer *peer,
+			  fi_addr_t addr)
+{
+	struct rxr_pkt_entry *pkt_entry;
+	ssize_t ret;
+
+	if (peer->state == RXR_PEER_ACKED)
+		return;
+
+	pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_efa_pool);
+	if (OFI_UNLIKELY(!pkt_entry))
+		return;
+
+	rxr_pkt_init_connack(ep, pkt_entry, addr);
+
+	/*
+	 * TODO: Once we start using a core's selective completion capability,
+	 * post the CONNACK packets without FI_COMPLETION.
+	 */
+	ret = rxr_pkt_entry_send(ep, pkt_entry, addr);
+
+	/*
+	 * Skip sending this connack on error and try again when processing the
+	 * next RTS from this peer containing the source information
+	 */
+	if (OFI_UNLIKELY(ret)) {
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		if (ret == -FI_EAGAIN)
+			return;
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Failed to send a CONNACK packet: ret %zd\n", ret);
+	} else {
+		peer->state = RXR_PEER_ACKED;
+	}
+}
+
+void rxr_pkt_handle_connack_recv(struct rxr_ep *ep,
+				 struct rxr_pkt_entry *pkt_entry,
+				 fi_addr_t src_addr)
+{
+	struct rxr_peer *peer;
+
+	/*
+	 * We don't really need any information from the actual connack packet
+	 * itself, just the src_addr from the CQE
+	 */
+	assert(src_addr != FI_ADDR_NOTAVAIL);
+	peer = rxr_ep_get_peer(ep, src_addr);
+	peer->state = RXR_PEER_ACKED;
+	FI_DBG(&rxr_prov, FI_LOG_CQ,
+	       "CONNACK received from %" PRIu64 "\n", src_addr);
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+}
+
+/*  CTS packet related functions */
+void rxr_pkt_calc_cts_window_credits(struct rxr_ep *ep, struct rxr_peer *peer,
+				     uint64_t size, int request,
+				     int *window, int *credits)
+{
+	struct efa_av *av;
+	int num_peers;
+
+	/*
+	 * Adjust the peer credit pool based on the current AV size, which could
+	 * have grown since the time this peer was initialized.
+	 */
+	av = rxr_ep_av(ep);
+	num_peers = av->used - 1;
+	if (num_peers && ofi_div_ceil(rxr_env.rx_window_size, num_peers) < peer->rx_credits)
+		peer->rx_credits = ofi_div_ceil(peer->rx_credits, num_peers);
+
+	/*
+	 * Allocate credits for this transfer based on the request, the number
+	 * of available data buffers, and the number of outstanding peers this
+	 * endpoint is actively tracking in the AV. Also ensure that a minimum
+	 * number of credits are allocated to the transfer so the sender can
+	 * make progress.
+	 */
+	*credits = MIN(MIN(ep->available_data_bufs, ep->posted_bufs_efa),
+		       peer->rx_credits);
+	*credits = MIN(request, *credits);
+	*credits = MAX(*credits, rxr_env.tx_min_credits);
+	*window = MIN(size, *credits * ep->max_data_payload_size);
+	if (peer->rx_credits > ofi_div_ceil(*window, ep->max_data_payload_size))
+		peer->rx_credits -= ofi_div_ceil(*window, ep->max_data_payload_size);
+}
+
+ssize_t rxr_pkt_init_cts(struct rxr_ep *ep,
+			 struct rxr_rx_entry *rx_entry,
+			 struct rxr_pkt_entry *pkt_entry)
+{
+	int window = 0;
+	struct rxr_cts_hdr *cts_hdr;
+	struct rxr_peer *peer;
+	size_t bytes_left;
+
+	cts_hdr = (struct rxr_cts_hdr *)pkt_entry->pkt;
+	cts_hdr->type = RXR_CTS_PKT;
+	cts_hdr->version = RXR_PROTOCOL_VERSION;
+	cts_hdr->flags = 0;
+
+	if (rx_entry->cq_entry.flags & FI_READ)
+		cts_hdr->flags |= RXR_READ_REQ;
+
+	cts_hdr->tx_id = rx_entry->tx_id;
+	cts_hdr->rx_id = rx_entry->rx_id;
+
+	bytes_left = rx_entry->total_len - rx_entry->bytes_done;
+	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	rxr_pkt_calc_cts_window_credits(ep, peer, bytes_left,
+					rx_entry->credit_request,
+					&window, &rx_entry->credit_cts);
+	cts_hdr->window = window;
+	pkt_entry->pkt_size = RXR_CTS_HDR_SIZE;
+	pkt_entry->addr = rx_entry->addr;
+	pkt_entry->x_entry = (void *)rx_entry;
+	return 0;
+}
+
+void rxr_pkt_handle_cts_sent(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rx_entry *rx_entry;
+
+	rx_entry = (struct rxr_rx_entry *)pkt_entry->x_entry;
+	rx_entry->window = rxr_get_cts_hdr(pkt_entry->pkt)->window;
+	ep->available_data_bufs -= rx_entry->credit_cts;
+
+	/*
+	 * Set a timer if available_bufs is exhausted. We may encounter a
+	 * scenario where a peer has stopped responding so we need a fallback
+	 * to replenish the credits.
+	 */
+	if (OFI_UNLIKELY(ep->available_data_bufs == 0))
+		ep->available_data_bufs_ts = ofi_gettime_us();
+}
+
+void rxr_pkt_handle_cts_recv(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_peer *peer;
+	struct rxr_cts_hdr *cts_pkt;
+	struct rxr_tx_entry *tx_entry;
+
+	cts_pkt = (struct rxr_cts_hdr *)pkt_entry->pkt;
+	if (cts_pkt->flags & RXR_READ_REQ)
+		tx_entry = ofi_bufpool_get_ibuf(ep->readrsp_tx_entry_pool, cts_pkt->tx_id);
+	else
+		tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, cts_pkt->tx_id);
+
+	tx_entry->rx_id = cts_pkt->rx_id;
+	tx_entry->window = cts_pkt->window;
+
+	/* Return any excess tx_credits that were borrowed for the request */
+	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	tx_entry->credit_allocated = ofi_div_ceil(cts_pkt->window, ep->max_data_payload_size);
+	if (tx_entry->credit_allocated < tx_entry->credit_request)
+		peer->tx_credits += tx_entry->credit_request - tx_entry->credit_allocated;
+
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+
+	if (tx_entry->state != RXR_TX_SEND) {
+		tx_entry->state = RXR_TX_SEND;
+		dlist_insert_tail(&tx_entry->entry, &ep->tx_pending_list);
+	}
+}
+
+/*  READRSP packet functions */
+int rxr_pkt_init_readrsp(struct rxr_ep *ep,
+			 struct rxr_tx_entry *tx_entry,
+			 struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_readrsp_pkt *readrsp_pkt;
+	struct rxr_readrsp_hdr *readrsp_hdr;
+	size_t mtu = ep->mtu_size;
+
+	readrsp_pkt = (struct rxr_readrsp_pkt *)pkt_entry->pkt;
+	readrsp_hdr = &readrsp_pkt->hdr;
+	readrsp_hdr->type = RXR_READRSP_PKT;
+	readrsp_hdr->version = RXR_PROTOCOL_VERSION;
+	readrsp_hdr->flags = 0;
+	readrsp_hdr->tx_id = tx_entry->tx_id;
+	readrsp_hdr->rx_id = tx_entry->rx_id;
+	readrsp_hdr->seg_size = ofi_copy_from_iov(readrsp_pkt->data,
+						  mtu - RXR_READRSP_HDR_SIZE,
+						  tx_entry->iov,
+						  tx_entry->iov_count, 0);
+	pkt_entry->pkt_size = RXR_READRSP_HDR_SIZE + readrsp_hdr->seg_size;
+	pkt_entry->addr = tx_entry->addr;
+	pkt_entry->x_entry = tx_entry;
+	return 0;
+}
+
+void rxr_pkt_handle_readrsp_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_tx_entry *tx_entry;
+	size_t data_len;
+
+	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
+	data_len = rxr_get_readrsp_hdr(pkt_entry->pkt)->seg_size;
+	tx_entry->state = RXR_TX_SENT_READRSP;
+	tx_entry->bytes_sent += data_len;
+	tx_entry->window -= data_len;
+	assert(tx_entry->window >= 0);
+	if (tx_entry->bytes_sent < tx_entry->total_len) {
+		if (efa_mr_cache_enable && rxr_ep_mr_local(ep))
+			rxr_inline_mr_reg(rxr_ep_domain(ep), tx_entry);
+
+		tx_entry->state = RXR_TX_SEND;
+		dlist_insert_tail(&tx_entry->entry,
+				  &ep->tx_pending_list);
+	}
+}
+
+void rxr_pkt_handle_readrsp_send_completion(struct rxr_ep *ep,
+					    struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_readrsp_hdr *readrsp_hdr;
+
+	readrsp_hdr = (struct rxr_readrsp_hdr *)pkt_entry->pkt;
+
+	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
+	assert(tx_entry->cq_entry.flags & FI_READ);
+
+	tx_entry->bytes_acked += readrsp_hdr->seg_size;
+	if (tx_entry->total_len == tx_entry->bytes_acked)
+		rxr_cq_handle_tx_completion(ep, tx_entry);
+}
+
+void rxr_pkt_handle_readrsp_recv(struct rxr_ep *ep,
+				 struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_readrsp_pkt *readrsp_pkt = NULL;
+	struct rxr_readrsp_hdr *readrsp_hdr = NULL;
+	struct rxr_rx_entry *rx_entry = NULL;
+
+	readrsp_pkt = (struct rxr_readrsp_pkt *)pkt_entry->pkt;
+	readrsp_hdr = &readrsp_pkt->hdr;
+	rx_entry = ofi_bufpool_get_ibuf(ep->rx_entry_pool, readrsp_hdr->rx_id);
+	assert(rx_entry->cq_entry.flags & FI_READ);
+	rx_entry->tx_id = readrsp_hdr->tx_id;
+	rxr_pkt_proc_data(ep, rx_entry, pkt_entry,
+			  readrsp_pkt->data,
+			  0, readrsp_hdr->seg_size);
+}
+
+/*  RMA_CONTEXT packet functions */
+void rxr_pkt_handle_rma_context_send_completion(struct rxr_ep *ep,
+						struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_tx_entry *tx_entry = NULL;
+	struct rxr_rx_entry *rx_entry = NULL;
+	struct rxr_rma_context_pkt *rma_context_pkt;
+	int ret;
+
+	assert(rxr_get_base_hdr(pkt_entry->pkt)->version == RXR_PROTOCOL_VERSION);
+
+	rma_context_pkt = (struct rxr_rma_context_pkt *)pkt_entry->pkt;
+
+	switch (rma_context_pkt->rma_context_type) {
+	case RXR_SHM_RMA_READ:
+	case RXR_SHM_RMA_WRITE:
+		/* Completion of RMA READ/WRTITE operations that apps call */
+		tx_entry = pkt_entry->x_entry;
+
+		if (tx_entry->fi_flags & FI_COMPLETION) {
+			rxr_cq_write_tx_completion(ep, tx_entry);
+		} else {
+			efa_cntr_report_tx_completion(&ep->util_ep, tx_entry->cq_entry.flags);
+			rxr_release_tx_entry(ep, tx_entry);
+		}
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		break;
+	case RXR_SHM_LARGE_READ:
+		/*
+		 * This must be on the receiver (remote) side of two-sided message
+		 * transfer, which is also the initiator of RMA READ.
+		 * We get RMA READ completion for previously issued
+		 * fi_read operation over shm provider, which means
+		 * receiver side has received all data from sender
+		 */
+		rx_entry = pkt_entry->x_entry;
+		rx_entry->cq_entry.len = rx_entry->total_len;
+		rx_entry->bytes_done = rx_entry->total_len;
+
+		ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_TX_ENTRY, rx_entry, RXR_EOR_PKT, 1);
+		if (ret) {
+			if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
+				assert(0 && "error writing error cq entry for EOR\n");
+		}
+
+		if (rx_entry->fi_flags & FI_MULTI_RECV)
+			rxr_msg_multi_recv_handle_completion(ep, rx_entry);
+		rxr_cq_write_rx_completion(ep, rx_entry);
+		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
+		if (OFI_LIKELY(!ret))
+			rxr_release_rx_entry(ep, rx_entry);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
+		break;
+	default:
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "invalid rma_context_type in RXR_RMA_CONTEXT_PKT %d\n",
+			rma_context_pkt->rma_context_type);
+		assert(0 && "invalid RXR_RMA_CONTEXT_PKT rma_context_type\n");
+	}
+}
+
+/*  EOR packet related functions */
+int rxr_pkt_init_eor(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_eor_hdr *eor_hdr;
+
+	eor_hdr = (struct rxr_eor_hdr *)pkt_entry->pkt;
+	eor_hdr->type = RXR_EOR_PKT;
+	eor_hdr->version = RXR_PROTOCOL_VERSION;
+	eor_hdr->flags = 0;
+	eor_hdr->tx_id = rx_entry->tx_id;
+	eor_hdr->rx_id = rx_entry->rx_id;
+	pkt_entry->pkt_size = sizeof(struct rxr_eor_hdr);
+	pkt_entry->addr = rx_entry->addr;
+	pkt_entry->x_entry = rx_entry;
+	return 0;
+}
+
+/*
+ *   Sender handles the acknowledgment (RXR_EOR_PKT) from receiver on the completion
+ *   of the large message copy via fi_readmsg operation
+ */
+void rxr_pkt_handle_eor_recv(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_eor_hdr *shm_eor;
+	struct rxr_tx_entry *tx_entry;
+
+	shm_eor = (struct rxr_eor_hdr *)pkt_entry->pkt;
+
+	/* pre-post buf used here, so can NOT track back to tx_entry with x_entry */
+	tx_entry = ofi_bufpool_get_ibuf(ep->tx_entry_pool, shm_eor->tx_id);
+	rxr_cq_write_tx_completion(ep, tx_entry);
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+}
+
diff --git a/prov/efa/src/rxr/rxr_pkt_type_rts.c b/prov/efa/src/rxr/rxr_pkt_type_rts.c
new file mode 100644
index 0000000..4426e1b
--- /dev/null
+++ b/prov/efa/src/rxr/rxr_pkt_type_rts.c
@@ -0,0 +1,825 @@
+/*
+ * Copyright (c) 2019-2020 Amazon.com, Inc. or its affiliates.
+ * All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "efa.h"
+#include "rxr.h"
+#include "rxr_msg.h"
+#include "rxr_rma.h"
+#include "rxr_pkt_cmd.h"
+
+/* This file contains RTS packet related functions.
+ * RTS is used to post send, emulated read and emulated
+ * write requests.
+ */
+
+/*
+ *  Utility struct and functions
+ */
+struct rxr_pkt_rts_read_hdr {
+	uint64_t rma_initiator_rx_id;
+	uint64_t window;
+};
+
+/*
+ *   Helper function to compute the maximum payload of the RTS header based on
+ *   the RTS header flags. The header' data_len field may have a length greater
+ *   than the possible RTS payload size if it is a large message.
+ */
+uint64_t rxr_get_rts_data_size(struct rxr_ep *ep,
+			       struct rxr_rts_hdr *rts_hdr)
+{
+	size_t max_payload_size;
+
+	/*
+	 * read RTS contain no data, because data is on remote EP.
+	 */
+	if (rts_hdr->flags & RXR_READ_REQ)
+		return 0;
+
+	if (rts_hdr->flags & RXR_SHM_HDR)
+		return (rts_hdr->flags & RXR_SHM_HDR_DATA) ? rts_hdr->data_len : 0;
+
+
+	if (rts_hdr->flags & RXR_REMOTE_CQ_DATA)
+		max_payload_size = ep->mtu_size - RXR_CTRL_HDR_SIZE;
+	else
+		max_payload_size = ep->mtu_size - RXR_CTRL_HDR_SIZE_NO_CQ;
+
+	if (rts_hdr->flags & RXR_REMOTE_SRC_ADDR)
+		max_payload_size -= rts_hdr->addrlen;
+
+	if (rts_hdr->flags & RXR_WRITE)
+		max_payload_size -= rts_hdr->rma_iov_count *
+					sizeof(struct fi_rma_iov);
+
+	return (rts_hdr->data_len > max_payload_size)
+		? max_payload_size : rts_hdr->data_len;
+}
+
+/*
+ *  rxr_pkt_init_rts() and related functions.
+ */
+static
+char *rxr_pkt_init_rts_base_hdr(struct rxr_ep *ep,
+				struct rxr_tx_entry *tx_entry,
+				struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rts_hdr *rts_hdr;
+	struct rxr_peer *peer;
+	char *src;
+
+	rts_hdr = (struct rxr_rts_hdr *)pkt_entry->pkt;
+	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+
+	rts_hdr->type = RXR_RTS_PKT;
+	rts_hdr->version = RXR_PROTOCOL_VERSION;
+	rts_hdr->tag = tx_entry->tag;
+
+	rts_hdr->data_len = tx_entry->total_len;
+	rts_hdr->tx_id = tx_entry->tx_id;
+	rts_hdr->msg_id = tx_entry->msg_id;
+	/*
+	 * Even with protocol versions prior to v3 that did not include a
+	 * request in the RTS, the receiver can test for this flag and decide if
+	 * it should be used as a heuristic for credit calculation. If the
+	 * receiver is on <3 protocol version, the flag and the request just get
+	 * ignored.
+	 */
+	rts_hdr->flags |= RXR_CREDIT_REQUEST;
+	rts_hdr->credit_request = tx_entry->credit_request;
+
+	if (tx_entry->fi_flags & FI_REMOTE_CQ_DATA) {
+		rts_hdr->flags = RXR_REMOTE_CQ_DATA;
+		pkt_entry->pkt_size = RXR_CTRL_HDR_SIZE;
+		rxr_get_ctrl_cq_pkt(rts_hdr)->hdr.cq_data =
+			tx_entry->cq_entry.data;
+		src = rxr_get_ctrl_cq_pkt(rts_hdr)->data;
+	} else {
+		rts_hdr->flags = 0;
+		pkt_entry->pkt_size = RXR_CTRL_HDR_SIZE_NO_CQ;
+		src = rxr_get_ctrl_pkt(rts_hdr)->data;
+	}
+
+	if (tx_entry->cq_entry.flags & FI_TAGGED)
+		rts_hdr->flags |= RXR_TAGGED;
+
+	rts_hdr->addrlen = 0;
+	if (OFI_UNLIKELY(peer->state != RXR_PEER_ACKED)) {
+		/*
+		 * This is the first communication with this peer on this
+		 * endpoint, so send the core's address for this EP in the RTS
+		 * so the remote side can insert it into its address vector.
+		 */
+		rts_hdr->addrlen = ep->core_addrlen;
+		rts_hdr->flags |= RXR_REMOTE_SRC_ADDR;
+		memcpy(src, ep->core_addr, rts_hdr->addrlen);
+		src += rts_hdr->addrlen;
+		pkt_entry->pkt_size += rts_hdr->addrlen;
+	}
+
+	return src;
+}
+
+static
+char *rxr_pkt_init_rts_rma_hdr(struct rxr_ep *ep,
+			       struct rxr_tx_entry *tx_entry,
+			       struct rxr_pkt_entry *pkt_entry,
+			       char *hdr)
+{
+	int rmalen;
+	struct rxr_rts_hdr *rts_hdr;
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	rts_hdr->rma_iov_count = 0;
+	assert(tx_entry->cq_entry.flags & FI_RMA);
+	if (tx_entry->op == ofi_op_write) {
+		rts_hdr->flags |= RXR_WRITE;
+	} else {
+		assert(tx_entry->op == ofi_op_read_req);
+		rts_hdr->flags |= RXR_READ_REQ;
+	}
+
+	rmalen = tx_entry->rma_iov_count * sizeof(struct fi_rma_iov);
+	rts_hdr->rma_iov_count = tx_entry->rma_iov_count;
+	memcpy(hdr, tx_entry->rma_iov, rmalen);
+	hdr += rmalen;
+	pkt_entry->pkt_size += rmalen;
+
+	return hdr;
+}
+
+static
+int rxr_pkt_init_read_rts(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
+			  struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_pkt_rts_read_hdr *read_hdr;
+	char *hdr;
+
+	hdr = rxr_pkt_init_rts_base_hdr(ep, tx_entry, pkt_entry);
+	hdr = rxr_pkt_init_rts_rma_hdr(ep, tx_entry, pkt_entry, hdr);
+
+	/* no data to send, but need to send rx_id and window */
+	read_hdr = (struct rxr_pkt_rts_read_hdr *)hdr;
+	read_hdr->rma_initiator_rx_id = tx_entry->rma_loc_rx_id;
+	read_hdr->window = tx_entry->rma_window;
+	hdr += sizeof(struct rxr_pkt_rts_read_hdr);
+	pkt_entry->pkt_size += sizeof(struct rxr_pkt_rts_read_hdr);
+
+	assert(pkt_entry->pkt_size <= ep->mtu_size);
+	pkt_entry->addr = tx_entry->addr;
+	pkt_entry->x_entry = (void *)tx_entry;
+	return 0;
+}
+
+ssize_t rxr_pkt_init_rts(struct rxr_ep *ep,
+			 struct rxr_tx_entry *tx_entry,
+			 struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_peer *peer;
+	struct rxr_rts_hdr *rts_hdr;
+	char *data, *src;
+	uint64_t data_len;
+	size_t mtu = ep->mtu_size;
+
+	if (tx_entry->op == ofi_op_read_req)
+		return rxr_pkt_init_read_rts(ep, tx_entry, pkt_entry);
+
+	src = rxr_pkt_init_rts_base_hdr(ep, tx_entry, pkt_entry);
+	if (tx_entry->op == ofi_op_write)
+		src = rxr_pkt_init_rts_rma_hdr(ep, tx_entry, pkt_entry, src);
+
+	peer = rxr_ep_get_peer(ep, tx_entry->addr);
+	assert(peer);
+	data = src;
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	if (rxr_env.enable_shm_transfer && peer->is_local) {
+		rts_hdr->flags |= RXR_SHM_HDR;
+		/* will be sent over shm provider */
+		if (tx_entry->total_len <= rxr_env.shm_max_medium_size) {
+			data_len = ofi_copy_from_iov(data, rxr_env.shm_max_medium_size,
+						     tx_entry->iov, tx_entry->iov_count, 0);
+			assert(data_len == tx_entry->total_len);
+			rts_hdr->flags |= RXR_SHM_HDR_DATA;
+			pkt_entry->pkt_size += data_len;
+		} else {
+			/* rendezvous protocol
+			 * place iov_count first, then local iov
+			 */
+			memcpy(data, &tx_entry->iov_count, sizeof(size_t));
+			data += sizeof(size_t);
+			pkt_entry->pkt_size += sizeof(size_t);
+			memcpy(data, tx_entry->iov, sizeof(struct iovec) * tx_entry->iov_count);
+			pkt_entry->pkt_size += sizeof(struct iovec) * tx_entry->iov_count;
+		}
+	} else {
+		/* will be sent over efa provider */
+		data_len = ofi_copy_from_iov(data, mtu - pkt_entry->pkt_size,
+					     tx_entry->iov, tx_entry->iov_count, 0);
+		assert(data_len == rxr_get_rts_data_size(ep, rts_hdr));
+		pkt_entry->pkt_size += data_len;
+	}
+
+	assert(pkt_entry->pkt_size <= mtu);
+	pkt_entry->addr = tx_entry->addr;
+	pkt_entry->x_entry = (void *)tx_entry;
+	return 0;
+}
+
+void rxr_pkt_handle_rts_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_peer *peer;
+	struct rxr_tx_entry *tx_entry;
+	size_t data_sent;
+
+	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
+	if (tx_entry->op == ofi_op_read_req) {
+		tx_entry->bytes_sent = 0;
+		tx_entry->state = RXR_TX_WAIT_READ_FINISH;
+		return;
+	}
+
+	data_sent = rxr_get_rts_data_size(ep, rxr_get_rts_hdr(pkt_entry->pkt));
+
+	tx_entry->bytes_sent += data_sent;
+
+	if ((rxr_env.enable_shm_transfer && peer->is_local) ||
+	    !(efa_mr_cache_enable && tx_entry->total_len > data_sent))
+		return;
+
+	/*
+	 * Register the data buffers inline only if the application did not
+	 * provide a descriptor with the tx op
+	 */
+	if (rxr_ep_mr_local(ep) && !tx_entry->desc[0])
+		rxr_inline_mr_reg(rxr_ep_domain(ep), tx_entry);
+}
+
+void rxr_pkt_handle_rts_send_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rts_hdr *rts_hdr;
+	struct rxr_tx_entry *tx_entry;
+
+	/*
+	 * for FI_READ, it is possible (though does not happen very often) that at the point
+	 * tx_entry has been released. The reason is, for FI_READ:
+	 *     1. only the initator side will send a RTS.
+	 *     2. the initator side will receive data packet. When all data was received,
+	 *        it will release the tx_entry
+	 * Therefore, if it so happens that all data was received before we got the send
+	 * completion notice, we will have a released tx_entry at this point.
+	 * Nonetheless, because for FI_READ tx_entry will be release in rxr_handle_rx_completion,
+	 * we will ignore it here.
+	 */
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	if (rts_hdr->flags & RXR_READ_REQ)
+		return;
+
+	/*
+	 * For shm provider, we will write completion for small & medium  message, as data has
+	 * been sent in the RTS packet; for large message, will wait for the EOR packet
+	 */
+	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
+	tx_entry->bytes_acked += rxr_get_rts_data_size(ep, rts_hdr);
+	if (tx_entry->total_len == tx_entry->bytes_acked)
+		rxr_cq_handle_tx_completion(ep, tx_entry);
+}
+
+/*
+ *  The following section are rxr_pkt_handle_rts_recv() and
+ *  its related functions.
+ */
+static
+char *rxr_pkt_proc_rts_base_hdr(struct rxr_ep *ep,
+				struct rxr_rx_entry *rx_entry,
+				struct rxr_pkt_entry *pkt_entry)
+{
+	char *data;
+	struct rxr_rts_hdr *rts_hdr = NULL;
+	/*
+	 * Use the correct header and grab CQ data and data, but ignore the
+	 * source_address since that has been fetched and processed already
+	 */
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+
+	rx_entry->addr = pkt_entry->addr;
+	rx_entry->tx_id = rts_hdr->tx_id;
+	rx_entry->msg_id = rts_hdr->msg_id;
+	rx_entry->total_len = rts_hdr->data_len;
+	rx_entry->cq_entry.tag = rts_hdr->tag;
+
+	if (rts_hdr->flags & RXR_REMOTE_CQ_DATA) {
+		rx_entry->cq_entry.flags |= FI_REMOTE_CQ_DATA;
+		data = rxr_get_ctrl_cq_pkt(rts_hdr)->data + rts_hdr->addrlen;
+		rx_entry->cq_entry.data =
+				rxr_get_ctrl_cq_pkt(rts_hdr)->hdr.cq_data;
+	} else {
+		rx_entry->cq_entry.data = 0;
+		data = rxr_get_ctrl_pkt(rts_hdr)->data + rts_hdr->addrlen;
+	}
+
+	return data;
+}
+
+static
+char *rxr_pkt_proc_rts_rma_hdr(struct rxr_ep *ep,
+			       struct rxr_rx_entry *rx_entry,
+			       struct rxr_pkt_entry *pkt_entry,
+			       char *rma_hdr)
+{
+	uint32_t rma_access;
+	struct fi_rma_iov *rma_iov = NULL;
+	struct rxr_rts_hdr *rts_hdr;
+	int ret;
+
+	rma_iov = (struct fi_rma_iov *)rma_hdr;
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	if (rts_hdr->flags & RXR_READ_REQ) {
+		rma_access = FI_SEND;
+		rx_entry->cq_entry.flags |= (FI_RMA | FI_READ);
+	} else {
+		assert(rts_hdr->flags & RXR_WRITE);
+		rma_access = FI_RECV;
+		rx_entry->cq_entry.flags |= (FI_RMA | FI_WRITE);
+	}
+
+	assert(rx_entry->iov_count == 0);
+
+	rx_entry->iov_count = rts_hdr->rma_iov_count;
+	ret = rxr_rma_verified_copy_iov(ep, rma_iov, rts_hdr->rma_iov_count,
+					rma_access, rx_entry->iov);
+	if (ret) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "RMA address verify failed!\n");
+		rxr_cq_handle_cq_error(ep, -FI_EIO);
+	}
+
+	rx_entry->cq_entry.len = ofi_total_iov_len(&rx_entry->iov[0],
+						   rx_entry->iov_count);
+	rx_entry->cq_entry.buf = rx_entry->iov[0].iov_base;
+	return rma_hdr + rts_hdr->rma_iov_count * sizeof(struct fi_rma_iov);
+}
+
+static
+int rxr_cq_match_recv(struct dlist_entry *item, const void *arg)
+{
+	const struct rxr_pkt_entry *pkt_entry = arg;
+	struct rxr_rx_entry *rx_entry;
+
+	rx_entry = container_of(item, struct rxr_rx_entry, entry);
+
+	return rxr_match_addr(rx_entry->addr, pkt_entry->addr);
+}
+
+static
+int rxr_cq_match_trecv(struct dlist_entry *item, const void *arg)
+{
+	struct rxr_pkt_entry *pkt_entry = (struct rxr_pkt_entry *)arg;
+	struct rxr_rx_entry *rx_entry;
+	uint64_t match_tag;
+
+	rx_entry = container_of(item, struct rxr_rx_entry, entry);
+
+	match_tag = rxr_get_rts_hdr(pkt_entry->pkt)->tag;
+
+	return rxr_match_addr(rx_entry->addr, pkt_entry->addr) &&
+	       rxr_match_tag(rx_entry->cq_entry.tag, rx_entry->ignore,
+			     match_tag);
+}
+
+int rxr_pkt_proc_rts_data(struct rxr_ep *ep,
+			  struct rxr_rx_entry *rx_entry,
+			  struct rxr_pkt_entry *pkt_entry,
+			  char *data, size_t data_size)
+{
+	struct rxr_rts_hdr *rts_hdr;
+	int64_t bytes_left, bytes_copied;
+	ssize_t ret;
+
+	/* rx_entry->cq_entry.len is total recv buffer size.
+	 * rx_entry->total_len is from rts_hdr and is total send buffer size.
+	 * if send buffer size < recv buffer size, we adjust value of rx_entry->cq_entry.len.
+	 * if send buffer size > recv buffer size, we have a truncated message.
+	 */
+	if (rx_entry->cq_entry.len > rx_entry->total_len)
+		rx_entry->cq_entry.len = rx_entry->total_len;
+
+	bytes_copied = ofi_copy_to_iov(rx_entry->iov, rx_entry->iov_count,
+				       0, data, data_size);
+
+	if (OFI_UNLIKELY(bytes_copied < data_size)) {
+		/* recv buffer is not big enough to hold rts, this must be a truncated message */
+		assert(bytes_copied == rx_entry->cq_entry.len &&
+		       rx_entry->cq_entry.len < rx_entry->total_len);
+		rx_entry->bytes_done = bytes_copied;
+		bytes_left = 0;
+	} else {
+		assert(bytes_copied == data_size);
+		rx_entry->bytes_done = data_size;
+		bytes_left = rx_entry->total_len - data_size;
+	}
+
+	assert(bytes_left >= 0);
+	if (!bytes_left) {
+		/* rxr_cq_handle_rx_completion() releases pkt_entry, thus
+		 * we do not release it here.
+		 */
+		rxr_cq_handle_rx_completion(ep, pkt_entry, rx_entry);
+		rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
+		rxr_release_rx_entry(ep, rx_entry);
+		return 0;
+	}
+
+#if ENABLE_DEBUG
+	dlist_insert_tail(&rx_entry->rx_pending_entry, &ep->rx_pending_list);
+	ep->rx_pending++;
+#endif
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	rx_entry->state = RXR_RX_RECV;
+	if (rts_hdr->flags & RXR_CREDIT_REQUEST)
+		rx_entry->credit_request = rts_hdr->credit_request;
+	else
+		rx_entry->credit_request = rxr_env.tx_min_credits;
+	ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+	return ret;
+}
+
+ssize_t rxr_pkt_post_shm_rndzv_read(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry)
+{
+	struct rxr_pkt_entry *pkt_entry;
+	struct rxr_rma_context_pkt *rma_context_pkt;
+	struct fi_msg_rma msg;
+	struct iovec msg_iov[RXR_IOV_LIMIT];
+	struct fi_rma_iov rma_iov[RXR_IOV_LIMIT];
+	fi_addr_t src_shm_fiaddr;
+	uint64_t remain_len;
+	struct rxr_peer *peer;
+	int ret, i;
+
+	if (rx_entry->state == RXR_RX_QUEUED_SHM_LARGE_READ)
+		return 0;
+
+	pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_shm_pool);
+	assert(pkt_entry);
+
+	pkt_entry->x_entry = (void *)rx_entry;
+	pkt_entry->addr = rx_entry->addr;
+	rma_context_pkt = (struct rxr_rma_context_pkt *)pkt_entry->pkt;
+	rma_context_pkt->type = RXR_RMA_CONTEXT_PKT;
+	rma_context_pkt->version = RXR_PROTOCOL_VERSION;
+	rma_context_pkt->rma_context_type = RXR_SHM_LARGE_READ;
+	rma_context_pkt->tx_id = rx_entry->tx_id;
+
+	peer = rxr_ep_get_peer(ep, rx_entry->addr);
+	src_shm_fiaddr = peer->shm_fiaddr;
+
+	memset(&msg, 0, sizeof(msg));
+
+	remain_len = rx_entry->total_len;
+
+	for (i = 0; i < rx_entry->rma_iov_count; i++) {
+		rma_iov[i].addr = rx_entry->rma_iov[i].addr;
+		rma_iov[i].len = rx_entry->rma_iov[i].len;
+		rma_iov[i].key = 0;
+	}
+
+	/*
+	 * shm provider will compare #bytes CMA copied with total length of recv buffer
+	 * (msg_iov here). If they are not equal, an error is returned when reading shm
+	 * provider's cq. So shrink the total length of recv buffer if applicable
+	 */
+	for (i = 0; i < rx_entry->iov_count; i++) {
+		msg_iov[i].iov_base = (void *)rx_entry->iov[i].iov_base;
+		msg_iov[i].iov_len = (remain_len < rx_entry->iov[i].iov_len) ?
+					remain_len : rx_entry->iov[i].iov_len;
+		remain_len -= msg_iov[i].iov_len;
+		if (remain_len == 0)
+			break;
+	}
+
+	msg.msg_iov = msg_iov;
+	msg.iov_count = rx_entry->iov_count;
+	msg.desc = NULL;
+	msg.addr = src_shm_fiaddr;
+	msg.context = pkt_entry;
+	msg.rma_iov = rma_iov;
+	msg.rma_iov_count = rx_entry->rma_iov_count;
+
+	ret = fi_readmsg(ep->shm_ep, &msg, 0);
+
+	return ret;
+}
+
+void rxr_pkt_proc_shm_long_msg_rts(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+				   struct rxr_rts_hdr *rts_hdr, char *data)
+{
+	struct iovec *iovec_ptr;
+	int ret, i;
+
+	/* get iov_count of sender first */
+	memcpy(&rx_entry->rma_iov_count, data, sizeof(size_t));
+	data += sizeof(size_t);
+
+	iovec_ptr = (struct iovec *)data;
+	for (i = 0; i < rx_entry->rma_iov_count; i++) {
+		iovec_ptr = iovec_ptr + i;
+		rx_entry->rma_iov[i].addr = (intptr_t) iovec_ptr->iov_base;
+		rx_entry->rma_iov[i].len = iovec_ptr->iov_len;
+		rx_entry->rma_iov[i].key = 0;
+	}
+
+	ret = rxr_pkt_post_shm_rndzv_read(ep, rx_entry);
+
+	if (OFI_UNLIKELY(ret)) {
+		if (ret == -FI_EAGAIN) {
+			rx_entry->state = RXR_RX_QUEUED_SHM_LARGE_READ;
+			dlist_insert_tail(&rx_entry->queued_entry,  &ep->rx_entry_queued_list);
+			return;
+		}
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"A large message RMA READ failed over shm provider.\n");
+		if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
+			assert(0 && "failed to write err cq entry");
+	}
+}
+
+ssize_t rxr_pkt_proc_matched_msg_rts(struct rxr_ep *ep,
+				     struct rxr_rx_entry *rx_entry,
+				     struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_peer *peer;
+	struct rxr_rts_hdr *rts_hdr;
+	char *data;
+	size_t data_size;
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	data = rxr_pkt_proc_rts_base_hdr(ep, rx_entry, pkt_entry);
+	if (peer->is_local && !(rts_hdr->flags & RXR_SHM_HDR_DATA)) {
+		rxr_pkt_proc_shm_long_msg_rts(ep, rx_entry, rts_hdr, data);
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
+		return 0;
+	}
+
+	data_size = rxr_get_rts_data_size(ep, rts_hdr);
+	return rxr_pkt_proc_rts_data(ep, rx_entry,
+				     pkt_entry, data,
+				     data_size);
+}
+
+static
+int rxr_pkt_proc_msg_rts(struct rxr_ep *ep,
+			 struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rts_hdr *rts_hdr;
+	struct dlist_entry *match;
+	struct rxr_rx_entry *rx_entry;
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+
+	if (rts_hdr->flags & RXR_TAGGED) {
+		match = dlist_find_first_match(&ep->rx_tagged_list,
+					       &rxr_cq_match_trecv,
+					       (void *)pkt_entry);
+	} else {
+		match = dlist_find_first_match(&ep->rx_list,
+					       &rxr_cq_match_recv,
+					       (void *)pkt_entry);
+	}
+
+	if (OFI_UNLIKELY(!match)) {
+		rx_entry = rxr_ep_get_new_unexp_rx_entry(ep, pkt_entry);
+		if (!rx_entry) {
+			FI_WARN(&rxr_prov, FI_LOG_CQ,
+				"RX entries exhausted.\n");
+			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
+			return -FI_ENOBUFS;
+		}
+
+		/* we are not releasing pkt_entry here because it will be
+		 * processed later
+		 */
+		pkt_entry = rx_entry->unexp_rts_pkt;
+		rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+		rxr_pkt_proc_rts_base_hdr(ep, rx_entry, pkt_entry);
+		return 0;
+	}
+
+	rx_entry = container_of(match, struct rxr_rx_entry, entry);
+	if (rx_entry->rxr_flags & RXR_MULTI_RECV_POSTED) {
+		rx_entry = rxr_ep_split_rx_entry(ep, rx_entry,
+						 NULL, pkt_entry);
+		if (OFI_UNLIKELY(!rx_entry)) {
+			FI_WARN(&rxr_prov, FI_LOG_CQ,
+				"RX entries exhausted.\n");
+			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
+			return -FI_ENOBUFS;
+		}
+	}
+
+	rx_entry->state = RXR_RX_MATCHED;
+
+	if (!(rx_entry->fi_flags & FI_MULTI_RECV) ||
+	    !rxr_msg_multi_recv_buffer_available(ep, rx_entry->master_entry))
+		dlist_remove(match);
+
+	return rxr_pkt_proc_matched_msg_rts(ep, rx_entry, pkt_entry);
+}
+
+static
+int rxr_pkt_proc_write_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_rts_hdr *rts_hdr;
+	uint64_t tag = ~0;
+	char *rma_hdr;
+	char *data;
+	size_t data_size;
+
+	/*
+	 * rma is one sided operation, match is not expected
+	 * we need to create a rx entry upon receiving a rts
+	 */
+	rx_entry = rxr_ep_get_rx_entry(ep, NULL, 0, tag, 0, NULL, pkt_entry->addr, ofi_op_write, 0);
+	if (OFI_UNLIKELY(!rx_entry)) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"RX entries exhausted.\n");
+		efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
+		return -FI_ENOBUFS;
+	}
+
+	rx_entry->bytes_done = 0;
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+	rma_hdr = rxr_pkt_proc_rts_base_hdr(ep, rx_entry, pkt_entry);
+	data = rxr_pkt_proc_rts_rma_hdr(ep, rx_entry, pkt_entry, rma_hdr);
+	data_size = rxr_get_rts_data_size(ep, rts_hdr);
+	return rxr_pkt_proc_rts_data(ep, rx_entry,
+				     pkt_entry, data,
+				     data_size);
+}
+
+static
+int rxr_pkt_proc_read_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_tx_entry *tx_entry;
+	uint64_t tag = ~0;
+	int err = 0;
+	char *hdr;
+	struct rxr_pkt_rts_read_hdr *read_info;
+	/*
+	 * rma is one sided operation, match is not expected
+	 * we need to create a rx entry upon receiving a rts
+	 */
+	rx_entry = rxr_ep_get_rx_entry(ep, NULL, 0, tag, 0, NULL, pkt_entry->addr, ofi_op_read_rsp, 0);
+	if (OFI_UNLIKELY(!rx_entry)) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"RX entries exhausted.\n");
+		efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
+		return -FI_ENOBUFS;
+	}
+
+	rx_entry->bytes_done = 0;
+
+	hdr = (char *)rxr_pkt_proc_rts_base_hdr(ep, rx_entry, pkt_entry);
+	hdr = (char *)rxr_pkt_proc_rts_rma_hdr(ep, rx_entry, pkt_entry, hdr);
+	read_info = (struct rxr_pkt_rts_read_hdr *)hdr;
+
+	rx_entry->rma_initiator_rx_id = read_info->rma_initiator_rx_id;
+	rx_entry->window = read_info->window;
+	assert(rx_entry->window > 0);
+
+	tx_entry = rxr_rma_alloc_readrsp_tx_entry(ep, rx_entry);
+	assert(tx_entry);
+	/* the only difference between a read response packet and
+	 * a data packet is that read response packet has remote EP tx_id
+	 * which initiator EP rx_entry need to send CTS back
+	 */
+	err = rxr_pkt_post_ctrl_or_queue(ep, RXR_TX_ENTRY, tx_entry, RXR_READRSP_PKT, 0);
+	if (OFI_UNLIKELY(err)) {
+		if (rxr_cq_handle_tx_error(ep, tx_entry, err))
+			assert(0 && "failed to write err cq entry");
+		rxr_release_tx_entry(ep, tx_entry);
+		rxr_release_rx_entry(ep, rx_entry);
+	} else {
+		rx_entry->state = RXR_RX_WAIT_READ_FINISH;
+	}
+
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+	return err;
+}
+
+ssize_t rxr_pkt_proc_rts(struct rxr_ep *ep,
+			 struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rts_hdr *rts_hdr;
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+
+	if (rts_hdr->flags & RXR_READ_REQ)
+		return rxr_pkt_proc_read_rts(ep, pkt_entry);
+
+	if (rts_hdr->flags & RXR_WRITE)
+		return rxr_pkt_proc_write_rts(ep, pkt_entry);
+
+	return rxr_pkt_proc_msg_rts(ep, pkt_entry);
+}
+
+void rxr_pkt_handle_rts_recv(struct rxr_ep *ep,
+			     struct rxr_pkt_entry *pkt_entry)
+{
+	struct rxr_rts_hdr *rts_hdr;
+	struct rxr_peer *peer;
+	int ret;
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	assert(peer);
+
+	if (rxr_env.enable_shm_transfer && peer->is_local) {
+		/* no need to reorder msg for shm_ep
+		 * rxr_pkt_proc_rts will write error cq entry if needed
+		 */
+		rxr_pkt_proc_rts(ep, pkt_entry);
+		return;
+	}
+
+	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
+
+	if (ep->core_caps & FI_SOURCE)
+		rxr_pkt_post_connack(ep, peer, pkt_entry->addr);
+
+	if (rxr_need_sas_ordering(ep)) {
+		ret = rxr_cq_reorder_msg(ep, peer, pkt_entry);
+		if (ret == 1) {
+			/* Packet was queued */
+			return;
+		} else if (OFI_UNLIKELY(ret == -FI_EALREADY)) {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"Invalid msg_id: %" PRIu32
+				" robuf->exp_msg_id: %" PRIu32 "\n",
+			       rts_hdr->msg_id, peer->robuf->exp_msg_id);
+			if (!rts_hdr->addrlen)
+				efa_eq_write_error(&ep->util_ep, FI_EIO, ret);
+			rxr_pkt_entry_release_rx(ep, pkt_entry);
+			return;
+		} else if (OFI_UNLIKELY(ret == -FI_ENOMEM)) {
+			efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
+			return;
+		} else if (OFI_UNLIKELY(ret < 0)) {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"Unknown error %d processing RTS packet msg_id: %"
+				PRIu32 "\n", ret, rts_hdr->msg_id);
+			efa_eq_write_error(&ep->util_ep, FI_EIO, ret);
+			return;
+		}
+
+		/* processing the expected packet */
+		ofi_recvwin_slide(peer->robuf);
+	}
+
+	/* rxr_pkt_proc_rts will write error cq entry if needed */
+	ret = rxr_pkt_proc_rts(ep, pkt_entry);
+	if (OFI_UNLIKELY(ret))
+		return;
+
+	/* process pending items in reorder buff */
+	if (rxr_need_sas_ordering(ep))
+		rxr_cq_proc_pending_items_in_recvwin(ep, peer);
+}
+
diff --git a/prov/efa/src/rxr/rxr_rma.c b/prov/efa/src/rxr/rxr_rma.c
index 61b0b5d..093b822 100644
--- a/prov/efa/src/rxr/rxr_rma.c
+++ b/prov/efa/src/rxr/rxr_rma.c
@@ -38,33 +38,7 @@
 #include "efa.h"
 #include "rxr.h"
 #include "rxr_rma.h"
-
-char *rxr_rma_init_rts_hdr(struct rxr_ep *ep,
-			   struct rxr_tx_entry *tx_entry,
-			   struct rxr_pkt_entry *pkt_entry,
-			   char *hdr)
-{
-	int rmalen;
-	struct rxr_rts_hdr *rts_hdr;
-
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-	rts_hdr->rma_iov_count = 0;
-	assert (tx_entry->cq_entry.flags & FI_RMA);
-	if (tx_entry->op == ofi_op_write) {
-		rts_hdr->flags |= RXR_WRITE;
-	} else {
-		assert(tx_entry->op == ofi_op_read_req);
-		rts_hdr->flags |= RXR_READ_REQ;
-	}
-
-	rmalen = tx_entry->rma_iov_count * sizeof(struct fi_rma_iov);
-	rts_hdr->rma_iov_count = tx_entry->rma_iov_count;
-	memcpy(hdr, tx_entry->rma_iov, rmalen);
-	hdr += rmalen;
-	pkt_entry->pkt_size += rmalen;
-
-	return hdr;
-}
+#include "rxr_pkt_cmd.h"
 
 int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct fi_rma_iov *rma,
 			      size_t count, uint32_t flags, struct iovec *iov)
@@ -92,148 +66,6 @@ int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct fi_rma_iov *rma,
 	}
 	return 0;
 }
-
-char *rxr_rma_read_rts_hdr(struct rxr_ep *ep,
-			   struct rxr_rx_entry *rx_entry,
-			   struct rxr_pkt_entry *pkt_entry,
-			   char *rma_hdr)
-{
-	uint32_t rma_access;
-	struct fi_rma_iov *rma_iov = NULL;
-	struct rxr_rts_hdr *rts_hdr;
-	int ret;
-
-	rma_iov = (struct fi_rma_iov *)rma_hdr;
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-	if (rts_hdr->flags & RXR_READ_REQ) {
-		rma_access = FI_SEND;
-		rx_entry->cq_entry.flags |= (FI_RMA | FI_READ);
-	} else {
-		assert(rts_hdr->flags & RXR_WRITE);
-		rma_access = FI_RECV;
-		rx_entry->cq_entry.flags |= (FI_RMA | FI_WRITE);
-	}
-
-	assert(rx_entry->iov_count == 0);
-
-	rx_entry->iov_count = rts_hdr->rma_iov_count;
-	ret = rxr_rma_verified_copy_iov(ep, rma_iov, rts_hdr->rma_iov_count,
-					rma_access, rx_entry->iov);
-	if (ret) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ, "RMA address verify failed!\n");
-		rxr_cq_handle_cq_error(ep, -FI_EIO);
-	}
-
-	rx_entry->cq_entry.len = ofi_total_iov_len(&rx_entry->iov[0],
-						   rx_entry->iov_count);
-	rx_entry->cq_entry.buf = rx_entry->iov[0].iov_base;
-	return rma_hdr + rts_hdr->rma_iov_count * sizeof(struct fi_rma_iov);
-}
-
-int rxr_rma_proc_write_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_rts_hdr *rts_hdr;
-	uint64_t tag = ~0;
-	char *rma_hdr;
-	char *data;
-	size_t data_size;
-
-	/*
-	 * rma is one sided operation, match is not expected
-	 * we need to create a rx entry upon receiving a rts
-	 */
-	rx_entry = rxr_ep_get_rx_entry(ep, NULL, 0, tag, 0, NULL, pkt_entry->addr, ofi_op_write, 0);
-	if (OFI_UNLIKELY(!rx_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"RX entries exhausted.\n");
-		efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
-		return -FI_ENOBUFS;
-	}
-
-	rx_entry->bytes_done = 0;
-
-	rts_hdr = rxr_get_rts_hdr(pkt_entry->pkt);
-	rma_hdr = rxr_cq_read_rts_hdr(ep, rx_entry, pkt_entry);
-	data = rxr_rma_read_rts_hdr(ep, rx_entry, pkt_entry, rma_hdr);
-	data_size = rxr_get_rts_data_size(ep, rts_hdr);
-	return rxr_cq_handle_rts_with_data(ep, rx_entry,
-					   pkt_entry, data,
-					   data_size);
-}
-
-int rxr_rma_init_read_rts(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
-			  struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rma_read_info *read_info;
-	char *hdr;
-
-	hdr = rxr_ep_init_rts_hdr(ep, tx_entry, pkt_entry);
-	hdr = rxr_rma_init_rts_hdr(ep, tx_entry, pkt_entry, hdr);
-
-	/* no data to send, but need to send rx_id and window */
-	read_info = (struct rxr_rma_read_info *)hdr;
-	read_info->rma_initiator_rx_id = tx_entry->rma_loc_rx_id;
-	read_info->window = tx_entry->rma_window;
-	hdr += sizeof(struct rxr_rma_read_info);
-	pkt_entry->pkt_size += sizeof(struct rxr_rma_read_info);
-
-	assert(pkt_entry->pkt_size <= ep->mtu_size);
-	pkt_entry->addr = tx_entry->addr;
-	pkt_entry->x_entry = (void *)tx_entry;
-	return 0;
-}
-
-int rxr_rma_proc_read_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_tx_entry *tx_entry;
-	uint64_t tag = ~0;
-	int err = 0;
-	char *hdr;
-	struct rxr_rma_read_info *read_info;
-	/*
-	 * rma is one sided operation, match is not expected
-	 * we need to create a rx entry upon receiving a rts
-	 */
-	rx_entry = rxr_ep_get_rx_entry(ep, NULL, 0, tag, 0, NULL, pkt_entry->addr, ofi_op_read_rsp, 0);
-	if (OFI_UNLIKELY(!rx_entry)) {
-		FI_WARN(&rxr_prov, FI_LOG_CQ,
-			"RX entries exhausted.\n");
-		efa_eq_write_error(&ep->util_ep, FI_ENOBUFS, -FI_ENOBUFS);
-		return -FI_ENOBUFS;
-	}
-
-	rx_entry->bytes_done = 0;
-
-	hdr = (char *)rxr_cq_read_rts_hdr(ep, rx_entry, pkt_entry);
-	hdr = (char *)rxr_rma_read_rts_hdr(ep, rx_entry, pkt_entry, hdr);
-	read_info = (struct rxr_rma_read_info *)hdr;
-
-	rx_entry->rma_initiator_rx_id = read_info->rma_initiator_rx_id;
-	rx_entry->window = read_info->window;
-	assert(rx_entry->window > 0);
-
-	tx_entry = rxr_rma_alloc_readrsp_tx_entry(ep, rx_entry);
-	assert(tx_entry);
-	/* the only difference between a read response packet and
-	 * a data packet is that read response packet has remote EP tx_id
-	 * which initiator EP rx_entry need to send CTS back
-	 */
-	err = rxr_ep_post_ctrl_or_queue(ep, RXR_TX_ENTRY, tx_entry, RXR_READRSP_PKT, 0);
-	if (OFI_UNLIKELY(err)) {
-		if (rxr_cq_handle_tx_error(ep, tx_entry, err))
-			assert(0 && "failed to write err cq entry");
-		rxr_release_tx_entry(ep, tx_entry);
-		rxr_release_rx_entry(ep, rx_entry);
-	} else {
-		rx_entry->state = RXR_RX_WAIT_READ_FINISH;
-	}
-
-	rxr_release_rx_pkt_entry(ep, pkt_entry);
-	return err;
-}
-
 /* Upon receiving a read request, Remote EP call this function to create
  * a tx entry for sending data back.
  */
@@ -285,73 +117,6 @@ rxr_rma_alloc_readrsp_tx_entry(struct rxr_ep *rxr_ep,
 	return tx_entry;
 }
 
-int rxr_rma_init_readrsp_pkt(struct rxr_ep *ep,
-			     struct rxr_tx_entry *tx_entry,
-			     struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_readrsp_pkt *readrsp_pkt;
-	struct rxr_readrsp_hdr *readrsp_hdr;
-	size_t mtu = ep->mtu_size;
-
-	readrsp_pkt = (struct rxr_readrsp_pkt *)pkt_entry->pkt;
-	readrsp_hdr = &readrsp_pkt->hdr;
-	readrsp_hdr->type = RXR_READRSP_PKT;
-	readrsp_hdr->version = RXR_PROTOCOL_VERSION;
-	readrsp_hdr->flags = 0;
-	readrsp_hdr->tx_id = tx_entry->tx_id;
-	readrsp_hdr->rx_id = tx_entry->rx_id;
-	readrsp_hdr->seg_size = ofi_copy_from_iov(readrsp_pkt->data,
-						  mtu - RXR_READRSP_HDR_SIZE,
-						  tx_entry->iov,
-						  tx_entry->iov_count, 0);
-	pkt_entry->pkt_size = RXR_READRSP_HDR_SIZE + readrsp_hdr->seg_size;
-	pkt_entry->addr = tx_entry->addr;
-	pkt_entry->x_entry = tx_entry;
-	return 0;
-}
-
-void rxr_rma_handle_readrsp_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_tx_entry *tx_entry;
-	size_t data_len;
-
-	tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-	data_len = rxr_get_readrsp_hdr(pkt_entry->pkt)->seg_size;
-	tx_entry->state = RXR_TX_SENT_READRSP;
-	tx_entry->bytes_sent += data_len;
-	tx_entry->window -= data_len;
-	assert(tx_entry->window >= 0);
-	if (tx_entry->bytes_sent < tx_entry->total_len) {
-		if (efa_mr_cache_enable && rxr_ep_mr_local(ep))
-			rxr_inline_mr_reg(rxr_ep_domain(ep), tx_entry);
-
-		tx_entry->state = RXR_TX_SEND;
-		dlist_insert_tail(&tx_entry->entry,
-				  &ep->tx_pending_list);
-	}
-}
-
-/* EOR packet functions */
-int rxr_rma_init_eor_pkt(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry, struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_eor_hdr *eor_hdr;
-
-	eor_hdr = (struct rxr_eor_hdr *)pkt_entry->pkt;
-	eor_hdr->type = RXR_EOR_PKT;
-	eor_hdr->version = RXR_PROTOCOL_VERSION;
-	eor_hdr->flags = 0;
-	eor_hdr->tx_id = rx_entry->tx_id;
-	eor_hdr->rx_id = rx_entry->rx_id;
-	pkt_entry->pkt_size = sizeof(struct rxr_eor_hdr);
-	pkt_entry->addr = rx_entry->addr;
-	pkt_entry->x_entry = rx_entry;
-	return 0;
-}
-
-void rxr_rma_handle_eor_sent(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
-{
-}
-
 struct rxr_tx_entry *
 rxr_rma_alloc_tx_entry(struct rxr_ep *rxr_ep,
 		       const struct fi_msg_rma *msg_rma,
@@ -400,7 +165,7 @@ size_t rxr_rma_post_shm_rma(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry
 
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
 	shm_fiaddr = peer->shm_fiaddr;
-	pkt_entry = rxr_get_pkt_entry(rxr_ep, rxr_ep->tx_pkt_shm_pool);
+	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_shm_pool);
 	if (OFI_UNLIKELY(!pkt_entry))
 		return -FI_EAGAIN;
 
@@ -493,11 +258,11 @@ ssize_t rxr_rma_post_efa_read(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 
 	peer = rxr_ep_get_peer(ep, tx_entry->addr);
 	assert(peer);
-	rxr_ep_calc_cts_window_credits(ep, peer,
-				       tx_entry->total_len,
-				       tx_entry->credit_request,
-				       &window,
-				       &credits);
+	rxr_pkt_calc_cts_window_credits(ep, peer,
+					tx_entry->total_len,
+					tx_entry->credit_request,
+					&window,
+					&credits);
 
 	rx_entry->window = window;
 	rx_entry->credit_cts = credits;
@@ -523,7 +288,7 @@ ssize_t rxr_rma_post_efa_read(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 
 	tx_entry->msg_id = peer->next_msg_id++;
 
-	err = rxr_ep_post_ctrl_or_queue(ep, RXR_TX_ENTRY, tx_entry, RXR_RTS_PKT, 0);
+	err = rxr_pkt_post_ctrl_or_queue(ep, RXR_TX_ENTRY, tx_entry, RXR_RTS_PKT, 0);
 	if (OFI_UNLIKELY(err)) {
 		rxr_release_tx_entry(ep, tx_entry);
 		peer->next_msg_id--;
@@ -656,7 +421,7 @@ ssize_t rxr_rma_writemsg(struct fid_ep *ep,
 
 		tx_entry->msg_id = peer->next_msg_id++;
 
-		err = rxr_ep_post_ctrl_or_queue(rxr_ep, RXR_TX_ENTRY, tx_entry, RXR_RTS_PKT, 0);
+		err = rxr_pkt_post_ctrl_or_queue(rxr_ep, RXR_TX_ENTRY, tx_entry, RXR_RTS_PKT, 0);
 		if (OFI_UNLIKELY(err)) {
 			rxr_release_tx_entry(rxr_ep, tx_entry);
 			peer->next_msg_id--;
diff --git a/prov/efa/src/rxr/rxr_rma.h b/prov/efa/src/rxr/rxr_rma.h
index 3da3933..a03749c 100644
--- a/prov/efa/src/rxr/rxr_rma.h
+++ b/prov/efa/src/rxr/rxr_rma.h
@@ -39,82 +39,14 @@
 
 #include <rdma/fi_rma.h>
 
-struct rxr_readrsp_hdr {
-	uint8_t type;
-	uint8_t version;
-	uint16_t flags;
-	/* end of rxr_base_hdr */
-	uint8_t pad[4];
-	uint32_t rx_id;
-	uint32_t tx_id;
-	uint64_t seg_size;
-};
-
-struct rxr_readrsp_pkt {
-	struct rxr_readrsp_hdr hdr;
-	char data[];
-};
-
-static inline struct rxr_readrsp_hdr *rxr_get_readrsp_hdr(void *pkt)
-{
-	return (struct rxr_readrsp_hdr *)pkt;
-}
-
-#define RXR_READRSP_HDR_SIZE	(sizeof(struct rxr_readrsp_hdr))
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_readrsp_hdr) == sizeof(struct rxr_data_hdr), "rxr_readrsp_hdr check");
-#endif
-
-struct rxr_rma_read_info {
-	uint64_t rma_initiator_rx_id;
-	uint64_t window;
-};
-
-#if defined(static_assert) && defined(__x86_64__)
-static_assert(sizeof(struct rxr_rma_read_info) == 16, "rxr_rma_read_hdr check");
-#endif
-
-char *rxr_rma_init_rts_hdr(struct rxr_ep *ep,
-			   struct rxr_tx_entry *tx_entry,
-			   struct rxr_pkt_entry *pkt_entry,
-			   char *hdr);
-
 int rxr_rma_verified_copy_iov(struct rxr_ep *ep, struct fi_rma_iov *rma,
 			      size_t count, uint32_t flags, struct iovec *iov);
 
-char *rxr_rma_read_rts_hdr(struct rxr_ep *ep,
-			   struct rxr_rx_entry *rx_entry,
-			   struct rxr_pkt_entry *pkt_entry,
-			   char *hdr);
-
-int rxr_rma_proc_write_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
-
-int rxr_rma_init_read_rts(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
-			  struct rxr_pkt_entry *pkt_entry);
-
-int rxr_rma_proc_read_rts(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
-
 /* read response related functions */
 struct rxr_tx_entry *
 rxr_rma_alloc_readrsp_tx_entry(struct rxr_ep *rxr_ep,
 			       struct rxr_rx_entry *rx_entry);
 
-int rxr_rma_init_readrsp_pkt(struct rxr_ep *ep,
-			     struct rxr_tx_entry *tx_entry,
-			     struct rxr_pkt_entry *pkt_entry);
-
-void rxr_rma_handle_readrsp_sent(struct rxr_ep *ep,
-				 struct rxr_pkt_entry *pkt_entry);
-
-/* EOR related functions */
-int rxr_rma_init_eor_pkt(struct rxr_ep *ep,
-			 struct rxr_rx_entry *rx_entry,
-			 struct rxr_pkt_entry *pkt_entry);
-
-void rxr_rma_handle_eor_sent(struct rxr_ep *ep,
-			     struct rxr_pkt_entry *pkt_entry);
-
 size_t rxr_rma_post_shm_rma(struct rxr_ep *rxr_ep,
 			    struct rxr_tx_entry *tx_entry);
 
diff --git a/prov/mrail/src/mrail_init.c b/prov/mrail/src/mrail_init.c
index b08ba56..224e900 100644
--- a/prov/mrail/src/mrail_init.c
+++ b/prov/mrail/src/mrail_init.c
@@ -330,7 +330,36 @@ static int mrail_get_core_info(uint32_t version, const char *node, const char *s
 		FI_DBG(&mrail_prov, FI_LOG_CORE,
 		       "--- Begin fi_getinfo for rail: %zd ---\n", i);
 
-		ret = fi_getinfo(version, NULL, NULL, OFI_GETINFO_INTERNAL, core_hints, &rail_info[i]);
+		if (!hints || !hints->caps) {
+			struct fi_info *tmp_info = NULL;
+			uint64_t saved_core_hints_caps = core_hints->caps;
+			/*
+			 * Get the default caps that would be returned for empty
+			 * hints, otherwise the returned caps would only contain
+			 * those specifed in the hints (FI_SOURCE) and secondary
+			 * capabilities.
+			 */
+			core_hints->caps = 0;
+			ret = fi_getinfo(version, NULL, NULL,
+					 OFI_GETINFO_INTERNAL, core_hints,
+					 &tmp_info);
+			if (tmp_info) {
+				core_hints->caps = tmp_info->caps |
+						   saved_core_hints_caps;
+				fi_freeinfo(tmp_info);
+			} else {
+				core_hints->caps = saved_core_hints_caps;
+			}
+
+			ret = fi_getinfo(version, NULL, NULL,
+					 OFI_GETINFO_INTERNAL, core_hints,
+					 &rail_info[i]);
+			core_hints->caps = saved_core_hints_caps;
+		} else {
+			ret = fi_getinfo(version, NULL, NULL,
+					 OFI_GETINFO_INTERNAL, core_hints,
+					 &rail_info[i]);
+		}
 
 		FI_DBG(&mrail_prov, FI_LOG_CORE,
 		       "--- End fi_getinfo for rail: %zd ---\n", i);
diff --git a/prov/psm2/src/psmx2.h b/prov/psm2/src/psmx2.h
index 0ea2a89..5f698da 100644
--- a/prov/psm2/src/psmx2.h
+++ b/prov/psm2/src/psmx2.h
@@ -89,25 +89,22 @@ extern struct fi_provider psmx2_prov;
 			 FI_TRIGGER | FI_INJECT_COMPLETE | \
 			 FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)
 
-#define PSMX2_PRI_CAPS	(FI_TAGGED | FI_MSG | FI_RMA | FI_ATOMICS | \
-			 FI_NAMED_RX_CTX | FI_DIRECTED_RECV | \
-			 FI_SEND | FI_RECV | FI_READ | FI_WRITE | \
-			 FI_REMOTE_READ | FI_REMOTE_WRITE)
-
-#define PSMX2_SEC_CAPS	(FI_MULTI_RECV | FI_SOURCE | FI_RMA_EVENT | \
-			 FI_TRIGGER | FI_LOCAL_COMM | FI_REMOTE_COMM | \
-			 FI_SOURCE_ERR | FI_SHARED_AV)
-
-#define PSMX2_CAPS	(PSMX2_PRI_CAPS | PSMX2_SEC_CAPS | FI_REMOTE_CQ_DATA)
-
-#define PSMX2_RMA_CAPS	(PSMX2_CAPS & ~(FI_TAGGED | FI_MSG | FI_SEND | \
-			 FI_RECV | FI_DIRECTED_RECV | FI_MULTI_RECV))
+#define PSMX2_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS | FI_ATOMICS | \
+		       FI_NAMED_RX_CTX | FI_TRIGGER)
+#define PSMX2_RX_CAPS (FI_SOURCE | FI_SOURCE_ERR | FI_RMA_EVENT | OFI_RX_MSG_CAPS | \
+		       FI_TAGGED | OFI_RX_RMA_CAPS | FI_ATOMICS | FI_DIRECTED_RECV | \
+		       FI_MULTI_RECV | FI_TRIGGER)
+#define PSMX2_DOM_CAPS	(FI_SHARED_AV | FI_LOCAL_COMM | FI_REMOTE_COMM)
+#define PSMX2_CAPS (PSMX2_TX_CAPS | PSMX2_RX_CAPS | PSMX2_DOM_CAPS)
+
+#define PSMX2_RMA_TX_CAPS (PSMX2_TX_CAPS & ~(FI_TAGGED | FI_MSG | FI_SEND))
+#define PSMX2_RMA_RX_CAPS (PSMX2_RX_CAPS & ~(FI_TAGGED | FI_MSG | FI_RECV | \
+			   FI_DIRECTED_RECV | FI_MULTI_RECV))
+#define PSMX2_RMA_CAPS (PSMX2_RMA_TX_CAPS | PSMX2_RMA_RX_CAPS | PSMX2_DOM_CAPS)
 
 #define PSMX2_SUB_CAPS	(FI_SEND | FI_RECV | FI_READ | FI_WRITE | \
 			 FI_REMOTE_READ | FI_REMOTE_WRITE)
 
-#define PSMX2_DOM_CAPS	(FI_LOCAL_COMM | FI_REMOTE_COMM)
-
 #define PSMX2_ALL_TRX_CTXT	((void *)-1)
 #define PSMX2_MAX_MSG_SIZE	((0x1ULL << 32) - 1)
 #define PSMX2_RMA_ORDER_SIZE	(4096)
diff --git a/prov/psm2/src/psmx2_attr.c b/prov/psm2/src/psmx2_attr.c
index accd09b..18ee02a 100644
--- a/prov/psm2/src/psmx2_attr.c
+++ b/prov/psm2/src/psmx2_attr.c
@@ -46,7 +46,7 @@
  */
 
 static struct fi_tx_attr psmx2_tx_attr = {
-	.caps			= PSMX2_CAPS, /* PSMX2_RMA_CAPS */
+	.caps			= PSMX2_TX_CAPS, /* PSMX2_RMA_TX_CAPS */
 	.mode			= FI_CONTEXT, /* 0 */
 	.op_flags		= PSMX2_OP_FLAGS,
 	.msg_order		= PSMX2_MSG_ORDER,
@@ -58,7 +58,7 @@ static struct fi_tx_attr psmx2_tx_attr = {
 };
 
 static struct fi_rx_attr psmx2_rx_attr = {
-	.caps			= PSMX2_CAPS, /* PSMX2_RMA_CAPS */
+	.caps			= PSMX2_RX_CAPS, /* PSMX2_RMA_RX_CAPS */
 	.mode			= FI_CONTEXT, /* 0 */
 	.op_flags		= PSMX2_OP_FLAGS,
 	.msg_order		= PSMX2_MSG_ORDER,
@@ -244,9 +244,9 @@ alloc_info:
 			info_new->ep_attr->type = ep_type;
 			info_new->caps = PSMX2_RMA_CAPS;
 			info_new->mode = 0;
-			info_new->tx_attr->caps = PSMX2_RMA_CAPS;
+			info_new->tx_attr->caps = PSMX2_RMA_TX_CAPS;
 			info_new->tx_attr->mode = 0;
-			info_new->rx_attr->caps = PSMX2_RMA_CAPS;
+			info_new->rx_attr->caps = PSMX2_RMA_RX_CAPS;
 			info_new->rx_attr->mode = 0;
 			info_new->domain_attr->cq_data_size = 8;
 			info_out = info_new;
@@ -272,13 +272,12 @@ alloc_info:
 	 * Special arrangement to help auto tag layout selection.
 	 * See psmx2_alter_prov_info().
 	 */
-	if (!hints || !(hints->caps & FI_REMOTE_CQ_DATA)) {
+	if (!hints) {
 		info_new = fi_dupinfo(&psmx2_prov_info);
 		if (info_new) {
 			/* 64 bit tag, no CQ data */
 			info_new->addr_format = addr_format;
 			info_new->ep_attr->type = ep_type;
-			info_new->caps &= ~FI_REMOTE_CQ_DATA;
 			info_new->next = info_out;
 			info_out = info_new;
 			FI_INFO(&psmx2_prov, FI_LOG_CORE,
@@ -450,10 +449,8 @@ void psmx2_alter_prov_info(uint32_t api_version,
 		 * setting the cq_data_size field. Notice that the flag
 		 * may be cleared by ofi_alter_info().
 		 */
-		if (info->domain_attr->cq_data_size) {
-			info->caps |= FI_REMOTE_CQ_DATA;
+		if (info->domain_attr->cq_data_size)
 			cq_data_cnt++;
-		}
 
 		cnt++;
 	}
diff --git a/prov/rxd/src/rxd_attr.c b/prov/rxd/src/rxd_attr.c
index f0c6bfd..6792547 100644
--- a/prov/rxd/src/rxd_attr.c
+++ b/prov/rxd/src/rxd_attr.c
@@ -32,17 +32,16 @@
 
 #include "rxd.h"
 
-#define RXD_EP_CAPS (FI_MSG | FI_TAGGED | FI_RMA | FI_ATOMIC | FI_SOURCE |  \
-			FI_DIRECTED_RECV | FI_MULTI_RECV | FI_RMA_EVENT)
-#define RXD_TX_CAPS (FI_SEND | FI_WRITE | FI_READ)
-#define RXD_RX_CAPS (FI_RECV | FI_REMOTE_READ | FI_REMOTE_WRITE)
-#define RXD_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
+#define RXD_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS | FI_ATOMICS)
+#define RXD_RX_CAPS (FI_SOURCE | FI_RMA_EVENT | OFI_RX_MSG_CAPS | FI_TAGGED | \
+		     OFI_RX_RMA_CAPS | FI_ATOMICS | FI_DIRECTED_RECV | FI_MULTI_RECV)
 #define RXD_TX_OP_FLAGS (FI_INJECT | FI_INJECT_COMPLETE | FI_COMPLETION	|   \
 			 FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)
 #define RXD_RX_OP_FLAGS (FI_MULTI_RECV | FI_COMPLETION)
+#define RXD_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
 
 struct fi_tx_attr rxd_tx_attr = {
-	.caps = RXD_EP_CAPS | RXD_TX_CAPS,
+	.caps = RXD_TX_CAPS,
 	.op_flags = RXD_TX_OP_FLAGS,
 	.comp_order = FI_ORDER_NONE,
 	.msg_order = FI_ORDER_SAS,
@@ -53,7 +52,7 @@ struct fi_tx_attr rxd_tx_attr = {
 };
 
 struct fi_rx_attr rxd_rx_attr = {
-	.caps = RXD_EP_CAPS | RXD_RX_CAPS,
+	.caps = RXD_RX_CAPS,
 	.op_flags = RXD_RX_OP_FLAGS,
 	.comp_order = FI_ORDER_NONE,
 	.msg_order = FI_ORDER_SAS,
@@ -98,7 +97,7 @@ struct fi_fabric_attr rxd_fabric_attr = {
 };
 
 struct fi_info rxd_info = {
-	.caps = RXD_DOMAIN_CAPS | RXD_EP_CAPS | RXD_TX_CAPS | RXD_RX_CAPS,
+	.caps = RXD_DOMAIN_CAPS | RXD_TX_CAPS | RXD_RX_CAPS,
 	.addr_format = FI_FORMAT_UNSPEC,
 	.tx_attr = &rxd_tx_attr,
 	.rx_attr = &rxd_rx_attr,
diff --git a/prov/rxd/src/rxd_ep.c b/prov/rxd/src/rxd_ep.c
index 22f2ee8..a104988 100644
--- a/prov/rxd/src/rxd_ep.c
+++ b/prov/rxd/src/rxd_ep.c
@@ -709,7 +709,7 @@ static int rxd_ep_trywait(void *arg)
 
 static int rxd_ep_wait_fd_add(struct rxd_ep *rxd_ep, struct util_wait *wait)
 {
-	return ofi_wait_fd_add(wait, rxd_ep->dg_cq_fd, FI_EPOLL_IN,
+	return ofi_wait_fd_add(wait, rxd_ep->dg_cq_fd, OFI_EPOLL_IN,
 			       rxd_ep_trywait, rxd_ep,
 			       &rxd_ep->util_ep.ep_fid.fid);
 }
diff --git a/prov/rxm/src/rxm_attr.c b/prov/rxm/src/rxm_attr.c
index 9cc6f0f..d70f2dd 100644
--- a/prov/rxm/src/rxm_attr.c
+++ b/prov/rxm/src/rxm_attr.c
@@ -32,10 +32,10 @@
 
 #include "rxm.h"
 
-#define RXM_EP_CAPS (FI_MSG | FI_RMA | FI_TAGGED | FI_ATOMIC |		\
-		     FI_DIRECTED_RECV |	FI_READ | FI_WRITE | FI_RECV |	\
-		     FI_SEND | FI_REMOTE_READ | FI_REMOTE_WRITE | FI_SOURCE)
-
+#define RXM_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS | FI_ATOMICS)
+#define RXM_RX_CAPS (FI_SOURCE | OFI_RX_MSG_CAPS | FI_TAGGED | \
+		     OFI_RX_RMA_CAPS | FI_ATOMICS | FI_DIRECTED_RECV | \
+		     FI_MULTI_RECV)
 #define RXM_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
 
 // TODO have a separate "check info" against which app hints would be checked.
@@ -46,7 +46,7 @@
  * requested by the app. */
 
 struct fi_tx_attr rxm_tx_attr = {
-	.caps = RXM_EP_CAPS,
+	.caps = RXM_TX_CAPS,
 	.op_flags = RXM_PASSTHRU_TX_OP_FLAGS | RXM_TX_OP_FLAGS,
 	.msg_order = ~0x0ULL,
 	.comp_order = FI_ORDER_NONE,
@@ -56,7 +56,7 @@ struct fi_tx_attr rxm_tx_attr = {
 };
 
 struct fi_rx_attr rxm_rx_attr = {
-	.caps = RXM_EP_CAPS | FI_MULTI_RECV,
+	.caps = RXM_RX_CAPS,
 	.op_flags = RXM_PASSTHRU_RX_OP_FLAGS | RXM_RX_OP_FLAGS,
 	.msg_order = ~0x0ULL,
 	.comp_order = FI_ORDER_NONE,
@@ -103,7 +103,7 @@ struct fi_fabric_attr rxm_fabric_attr = {
 };
 
 struct fi_info rxm_info = {
-	.caps = RXM_EP_CAPS | RXM_DOMAIN_CAPS | FI_MULTI_RECV | FI_COLLECTIVE,
+	.caps = RXM_TX_CAPS | RXM_RX_CAPS | RXM_DOMAIN_CAPS | FI_COLLECTIVE,
 	.addr_format = FI_SOCKADDR,
 	.tx_attr = &rxm_tx_attr,
 	.rx_attr = &rxm_rx_attr,
diff --git a/prov/rxm/src/rxm_ep.c b/prov/rxm/src/rxm_ep.c
index 6fbe628..fdd48b8 100644
--- a/prov/rxm/src/rxm_ep.c
+++ b/prov/rxm/src/rxm_ep.c
@@ -791,6 +791,66 @@ rxm_recv_entry_get(struct rxm_ep *rxm_ep, const struct iovec *iov,
 	return recv_entry;
 }
 
+/*
+ * We don't expect to have unexpected messages when the app is using
+ * multi-recv buffers.  Optimize for that case.
+ *
+ * If there are unexpected messages waiting when we post a mult-recv buffer,
+ * we trim off the start of the buffer, treat it as a normal buffer, and pair
+ * it with an unexpected message.  We continue doing this until either no
+ * unexpected messages are left or the multi-recv buffer has been consumed.
+ */
+static ssize_t
+rxm_ep_post_mrecv(struct rxm_ep *ep, const struct iovec *iov,
+		 void **desc, void *context, uint64_t op_flags)
+{
+	struct rxm_recv_entry *recv_entry;
+	struct rxm_rx_buf *rx_buf;
+	struct iovec cur_iov = *iov;
+	int ret;
+
+	do {
+		recv_entry = rxm_recv_entry_get(ep, &cur_iov, desc, 1,
+						FI_ADDR_UNSPEC, 0, 0, context,
+						op_flags, &ep->recv_queue);
+		if (!recv_entry) {
+			ret = -FI_ENOMEM;
+			break;
+		}
+
+		rx_buf = rxm_get_unexp_msg(&ep->recv_queue, recv_entry->addr, 0,  0);
+		if (!rx_buf) {
+			dlist_insert_tail(&recv_entry->entry,
+					  &ep->recv_queue.recv_list);
+			return 0;
+		}
+
+		dlist_remove(&rx_buf->unexp_msg.entry);
+		rx_buf->recv_entry = recv_entry;
+		recv_entry->flags &= ~FI_MULTI_RECV;
+		recv_entry->total_len = MIN(cur_iov.iov_len, rx_buf->pkt.hdr.size);
+		recv_entry->rxm_iov.iov[0].iov_len = recv_entry->total_len;
+
+		cur_iov.iov_base = (uint8_t *) cur_iov.iov_base + recv_entry->total_len;
+		cur_iov.iov_len -= recv_entry->total_len;
+
+		if (rx_buf->pkt.ctrl_hdr.type != rxm_ctrl_seg)
+			ret = rxm_cq_handle_rx_buf(rx_buf);
+		else
+			ret = rxm_handle_unexp_sar(&ep->recv_queue, recv_entry,
+						   rx_buf);
+
+	} while (!ret && cur_iov.iov_len >= ep->min_multi_recv_size);
+
+	if ((cur_iov.iov_len < ep->min_multi_recv_size) ||
+	    (ret && cur_iov.iov_len != iov->iov_len)) {
+		ofi_cq_write(ep->util_ep.rx_cq, context, FI_MULTI_RECV,
+			     0, NULL, 0, 0);
+	}
+
+	return ret;
+}
+
 static ssize_t
 rxm_ep_post_recv(struct rxm_ep *rxm_ep, const struct iovec *iov,
 		 void **desc, size_t count, fi_addr_t src_addr,
@@ -800,6 +860,8 @@ rxm_ep_post_recv(struct rxm_ep *rxm_ep, const struct iovec *iov,
 	struct rxm_rx_buf *rx_buf;
 
 	assert(count <= rxm_ep->rxm_info->rx_attr->iov_limit);
+	if (op_flags & FI_MULTI_RECV)
+		return rxm_ep_post_mrecv(rxm_ep, iov, desc, context, op_flags);
 
 	recv_entry = rxm_recv_entry_get(rxm_ep, iov, desc, count, src_addr,
 					0, 0, context, op_flags,
@@ -1782,9 +1844,9 @@ static ssize_t rxm_ep_trecvmsg(struct fid_ep *ep_fid, const struct fi_msg_tagged
 
 	if (!(flags & (FI_CLAIM | FI_PEEK)) &&
 	    !(rxm_ep->rxm_info->mode & FI_BUFFERED_RECV)) {
-		return rxm_ep_post_trecv(rxm_ep, msg->msg_iov, msg->desc,
-					 msg->iov_count, msg->addr,
-					 msg->tag, msg->ignore, context, flags);
+		return rxm_ep_trecv_common(rxm_ep, msg->msg_iov, msg->desc,
+					   msg->iov_count, msg->addr,
+					   msg->tag, msg->ignore, context, flags);
 	}
 
 	ofi_ep_lock_acquire(&rxm_ep->util_ep);
@@ -2209,7 +2271,7 @@ static int rxm_ep_wait_fd_add(struct rxm_ep *rxm_ep, struct util_wait *wait)
 		return ret;
 	}
 
-	ret = ofi_wait_fd_add(wait, msg_cq_fd, FI_EPOLL_IN,
+	ret = ofi_wait_fd_add(wait, msg_cq_fd, OFI_EPOLL_IN,
 			      rxm_ep_trywait_cq, rxm_ep,
 			      &rxm_ep->util_ep.ep_fid.fid);
 	if (ret)
@@ -2226,7 +2288,7 @@ static int rxm_ep_wait_fd_add(struct rxm_ep *rxm_ep, struct util_wait *wait)
 		return ret;
 	}
 
-	return ofi_wait_fd_add(wait, msg_eq_fd, FI_EPOLL_IN, rxm_ep_trywait_eq,
+	return ofi_wait_fd_add(wait, msg_eq_fd, OFI_EPOLL_IN, rxm_ep_trywait_eq,
 			       rxm_ep, &rxm_ep->util_ep.ep_fid.fid);
 }
 
diff --git a/prov/rxm/src/rxm_init.c b/prov/rxm/src/rxm_init.c
index 08d6c6c..46d8909 100644
--- a/prov/rxm/src/rxm_init.c
+++ b/prov/rxm/src/rxm_init.c
@@ -220,14 +220,19 @@ static void rxm_alter_info(const struct fi_info *hints, struct fi_info *info)
 		if (!hints) {
 			cur->caps &= ~(FI_DIRECTED_RECV | FI_SOURCE |
 				       FI_ATOMIC);
-			cur->tx_attr->caps &= ~FI_ATOMIC;
-			cur->rx_attr->caps &= ~FI_ATOMIC;
+			cur->tx_attr->caps &= ~(FI_ATOMIC);
+			cur->rx_attr->caps &= ~(FI_DIRECTED_RECV | FI_ATOMIC |
+						FI_SOURCE);
 			cur->domain_attr->data_progress = FI_PROGRESS_MANUAL;
 		} else {
-			if (!(hints->caps & FI_DIRECTED_RECV))
+			if (!(hints->caps & FI_DIRECTED_RECV)) {
 				cur->caps &= ~FI_DIRECTED_RECV;
-			if (!(hints->caps & FI_SOURCE))
+				cur->rx_attr->caps &= ~FI_DIRECTED_RECV;
+			}
+			if (!(hints->caps & FI_SOURCE)) {
 				cur->caps &= ~FI_SOURCE;
+				cur->rx_attr->caps &= ~FI_SOURCE;
+			}
 
 			if (hints->mode & FI_BUFFERED_RECV)
 				cur->mode |= FI_BUFFERED_RECV;
diff --git a/prov/shm/src/smr.h b/prov/shm/src/smr.h
index 134e0c3..ac89be4 100644
--- a/prov/shm/src/smr.h
+++ b/prov/shm/src/smr.h
@@ -67,6 +67,11 @@
 #define SMR_MAJOR_VERSION 1
 #define SMR_MINOR_VERSION 1
 
+struct smr_env {
+	int disable_cma;
+};
+
+extern struct smr_env smr_env;
 extern struct fi_provider smr_prov;
 extern struct fi_info smr_info;
 extern struct util_prov smr_util_prov;
@@ -169,7 +174,8 @@ static inline const char *smr_no_prefix(const char *addr)
 #define SMR_RMA_ORDER (OFI_ORDER_RAR_SET | OFI_ORDER_RAW_SET | FI_ORDER_RAS |	\
 		       OFI_ORDER_WAR_SET | OFI_ORDER_WAW_SET | FI_ORDER_WAS |	\
 		       FI_ORDER_SAR | FI_ORDER_SAW)
-#define smr_fast_rma_enabled(mode, order) ((mode & FI_MR_VIRT_ADDR) && \
+#define smr_fast_rma_enabled(mode, order) (!smr_env.disable_cma && \
+			(mode & FI_MR_VIRT_ADDR) && \
 			!(order & SMR_RMA_ORDER))
 
 struct smr_ep {
diff --git a/prov/shm/src/smr_atomic.c b/prov/shm/src/smr_atomic.c
index e2fc1bf..a1cbd0d 100644
--- a/prov/shm/src/smr_atomic.c
+++ b/prov/shm/src/smr_atomic.c
@@ -149,8 +149,8 @@ static int smr_fetch_result(struct smr_ep *ep, struct smr_region *peer_smr,
 	return 0; 
 }
 
-static void smr_post_fetch_resp(struct smr_ep *ep, struct smr_cmd *cmd,
-				const struct iovec *result_iov, size_t count)
+static void smr_post_atomic_resp(struct smr_ep *ep, struct smr_cmd *cmd,
+				 const struct iovec *result_iov, size_t count)
 {
 	struct smr_cmd *pend;
 	struct smr_resp *resp;
@@ -268,13 +268,12 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 	ofi_cirque_commit(smr_cmd_queue(peer_smr));
 	peer_smr->cmd_cnt--;
 
-	if (op != ofi_op_atomic) {
-		if (flags & SMR_RMA_REQ) {
-			smr_post_fetch_resp(ep, cmd,
-				(const struct iovec *) result_iov,
-				result_count);
-			goto format_rma;
-		}
+	if (flags & SMR_RMA_REQ || op_flags & FI_DELIVERY_COMPLETE) {
+		smr_post_atomic_resp(ep, cmd,
+			(const struct iovec *) result_iov,
+			result_count);
+		goto format_rma;
+	} else if (op != ofi_op_atomic) {
 		err = smr_fetch_result(ep, peer_smr, result_iov, result_count,
 				       rma_ioc, rma_count, datatype, msg_len);
 		if (err)
diff --git a/prov/shm/src/smr_attr.c b/prov/shm/src/smr_attr.c
index a88f026..010a9fc 100644
--- a/prov/shm/src/smr_attr.c
+++ b/prov/shm/src/smr_attr.c
@@ -34,11 +34,10 @@
 
 #define SMR_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS | FI_ATOMICS)
 #define SMR_RX_CAPS (FI_SOURCE | FI_RMA_EVENT | OFI_RX_MSG_CAPS | FI_TAGGED | \
-		     OFI_RX_RMA_CAPS | FI_ATOMICS | FI_DIRECTED_RECV)
-#define SMR_TX_OP_FLAGS (FI_REMOTE_CQ_DATA | FI_COMPLETION | \
-			 FI_INJECT_COMPLETE | FI_TRANSMIT_COMPLETE | \
-			 /* TODO: support for delivery complete */ \
-			 FI_DELIVERY_COMPLETE)
+		     OFI_RX_RMA_CAPS | FI_ATOMICS | FI_DIRECTED_RECV | \
+		     FI_MULTI_RECV)
+#define SMR_TX_OP_FLAGS (FI_COMPLETION | FI_INJECT_COMPLETE | \
+			 FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)
 #define SMR_RX_OP_FLAGS (FI_COMPLETION | FI_MULTI_RECV)
 
 struct fi_tx_attr smr_tx_attr = {
@@ -53,7 +52,7 @@ struct fi_tx_attr smr_tx_attr = {
 };
 
 struct fi_rx_attr smr_rx_attr = {
-	.caps = SMR_RX_CAPS | FI_MULTI_RECV,
+	.caps = SMR_RX_CAPS,
 	.op_flags = SMR_RX_OP_FLAGS,
 	.comp_order = FI_ORDER_STRICT,
 	.msg_order = SMR_RMA_ORDER | FI_ORDER_SAS,
diff --git a/prov/shm/src/smr_init.c b/prov/shm/src/smr_init.c
index 2d7fe17..f3189fc 100644
--- a/prov/shm/src/smr_init.c
+++ b/prov/shm/src/smr_init.c
@@ -36,6 +36,16 @@
 #include "smr.h"
 #include "smr_signal.h"
 
+extern struct sigaction *old_action;
+
+struct smr_env smr_env = {
+	.disable_cma	= 0,
+};
+
+static void smr_init_env(void)
+{
+	fi_param_get_bool(&smr_prov, "disable_cma", &smr_env.disable_cma);
+}
 
 static void smr_resolve_addr(const char *node, const char *service,
 			     char **addr, size_t *addrlen)
@@ -63,11 +73,15 @@ static void smr_resolve_addr(const char *node, const char *service,
 	(*addr)[*addrlen - 1]  = '\0';
 }
 
-static int smr_get_ptrace_scope(void)
+static void smr_check_ptrace_scope(void)
 {
+	static bool init = 0;
 	FILE *file;
 	int scope, ret;
 
+	if (smr_env.disable_cma || init)
+		return;
+
 	scope = 0;
 	file = fopen("/proc/sys/kernel/yama/ptrace_scope", "r");
 	if (file) {
@@ -75,16 +89,21 @@ static int smr_get_ptrace_scope(void)
 		if (ret != 1) {
 			FI_WARN(&smr_prov, FI_LOG_CORE,
 				"Error getting value from ptrace_scope\n");
-			return -FI_EINVAL;
+			scope = 1;
+			goto out;
 		}
 		ret = fclose(file);
 		if (ret) {
 			FI_WARN(&smr_prov, FI_LOG_CORE,
 				"Error closing ptrace_scope file\n");
-			return -FI_EINVAL;
+			scope = 1;
+			goto out;
 		}
 	}
-	return scope;
+
+out:
+	smr_env.disable_cma = scope;
+	init = 1;
 }
 
 static int smr_getinfo(uint32_t version, const char *node, const char *service,
@@ -94,11 +113,12 @@ static int smr_getinfo(uint32_t version, const char *node, const char *service,
 	struct fi_info *cur;
 	uint64_t mr_mode, msg_order;
 	int fast_rma;
-	int ptrace_scope, ret;
+	int ret;
 
 	mr_mode = hints && hints->domain_attr ? hints->domain_attr->mr_mode :
 						FI_MR_VIRT_ADDR;
 	msg_order = hints && hints->tx_attr ? hints->tx_attr->msg_order : 0;
+	smr_check_ptrace_scope();
 	fast_rma = smr_fast_rma_enabled(mr_mode, msg_order);
 
 	ret = util_getinfo(&smr_util_prov, version, node, service, flags,
@@ -106,8 +126,6 @@ static int smr_getinfo(uint32_t version, const char *node, const char *service,
 	if (ret)
 		return ret;
 
-	ptrace_scope = smr_get_ptrace_scope();
-
 	for (cur = *info; cur; cur = cur->next) {
 		if (!(flags & FI_SOURCE) && !cur->dest_addr)
 			smr_resolve_addr(node, service, (char **) &cur->dest_addr,
@@ -128,7 +146,7 @@ static int smr_getinfo(uint32_t version, const char *node, const char *service,
 			cur->ep_attr->max_order_waw_size = 0;
 			cur->ep_attr->max_order_war_size = 0;
 		}
-		if (ptrace_scope != 0)
+		if (smr_env.disable_cma)
 			cur->ep_attr->max_msg_size = SMR_INJECT_SIZE;
 	}
 	return 0;
@@ -137,6 +155,7 @@ static int smr_getinfo(uint32_t version, const char *node, const char *service,
 static void smr_fini(void)
 {
 	smr_cleanup();
+	free(old_action);
 }
 
 struct fi_provider smr_prov = {
@@ -156,8 +175,17 @@ struct util_prov smr_util_prov = {
 
 SHM_INI
 {
-
+	fi_param_define(&smr_prov, "disable_cma", FI_PARAM_BOOL,
+			"Disable use of CMA (Cross Memory Attach) for \
+			copying data directly between processes (default: no)");
+	smr_init_env();
+
+	old_action = calloc(SIGRTMIN, sizeof(*old_action));
+	if (!old_action)
+		return NULL;
 	/* Signal handlers to cleanup tmpfs files on an unclean shutdown */
+	assert(SIGBUS < SIGRTMIN && SIGSEGV < SIGRTMIN
+	       && SIGTERM < SIGRTMIN && SIGINT < SIGRTMIN);
 	smr_reg_sig_hander(SIGBUS);
 	smr_reg_sig_hander(SIGSEGV);
 	smr_reg_sig_hander(SIGTERM);
diff --git a/prov/shm/src/smr_msg.c b/prov/shm/src/smr_msg.c
index c0c9f18..7167b35 100644
--- a/prov/shm/src/smr_msg.c
+++ b/prov/shm/src/smr_msg.c
@@ -205,15 +205,7 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 
 	cmd = ofi_cirque_tail(smr_cmd_queue(peer_smr));
 
-	if (total_len <= SMR_MSG_DATA_LEN) {
-		smr_format_inline(cmd, smr_peer_addr(ep->region)[peer_id].addr, iov,
-				  iov_count, op, tag, data, op_flags);
-	} else if (total_len <= SMR_INJECT_SIZE) {
-		tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
-		smr_format_inject(cmd, smr_peer_addr(ep->region)[peer_id].addr,
-				  iov, iov_count, op, tag, data, op_flags,
-				  peer_smr, tx_buf);
-	} else {
+	if (total_len > SMR_INJECT_SIZE || op_flags & FI_DELIVERY_COMPLETE) {
 		if (ofi_cirque_isfull(smr_resp_queue(ep->region))) {
 			ret = -FI_EAGAIN;
 			goto unlock_cq;
@@ -225,6 +217,14 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 			       context, ep->region, resp, pend);
 		ofi_cirque_commit(smr_resp_queue(ep->region));
 		goto commit;
+	} else if (total_len > SMR_MSG_DATA_LEN) {
+		tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
+		smr_format_inject(cmd, smr_peer_addr(ep->region)[peer_id].addr,
+				  iov, iov_count, op, tag, data, op_flags,
+				  peer_smr, tx_buf);
+	} else {
+		smr_format_inline(cmd, smr_peer_addr(ep->region)[peer_id].addr, iov,
+				  iov_count, op, tag, data, op_flags);
 	}
 	ret = smr_complete_tx(ep, context, op, cmd->msg.hdr.op_flags, 0);
 	if (ret) {
diff --git a/prov/shm/src/smr_progress.c b/prov/shm/src/smr_progress.c
index 0039ad8..5a215bb 100644
--- a/prov/shm/src/smr_progress.c
+++ b/prov/shm/src/smr_progress.c
@@ -37,8 +37,8 @@
 #include "ofi_iov.h"
 #include "smr.h"
 
-static int smr_progress_fetch(struct smr_ep *ep, struct smr_cmd *pending,
-			      uint64_t *ret)
+static int smr_progress_atomic_resp(struct smr_ep *ep, struct smr_cmd *pending,
+				    uint64_t *ret)
 {
 	struct smr_region *peer_smr;
 	size_t inj_offset, size;
@@ -49,12 +49,15 @@ static int smr_progress_fetch(struct smr_ep *ep, struct smr_cmd *pending,
 	if (fastlock_tryacquire(&peer_smr->lock))
 		return -FI_EAGAIN;
 
+	if (!(pending->msg.hdr.op_flags & SMR_RMA_REQ))
+		goto out;
+
 	inj_offset = (size_t) pending->msg.hdr.src_data;
 	tx_buf = (struct smr_inject_buf *) ((char **) peer_smr +
 					    inj_offset);
 
 	if (*ret)
-		goto out;
+		goto push;
 
 	src = pending->msg.hdr.op == ofi_op_atomic_compare ?
 	      tx_buf->buf : tx_buf->data;
@@ -67,9 +70,9 @@ static int smr_progress_fetch(struct smr_ep *ep, struct smr_cmd *pending,
 			"Incomplete atomic fetch buffer copied\n");
 		*ret = FI_EIO;
 	}
-
-out:
+push:
 	smr_freestack_push(smr_inject_pool(peer_smr), tx_buf);
+out:
 	peer_smr->cmd_cnt++;
 	fastlock_release(&peer_smr->lock);
 	return 0;
@@ -90,8 +93,9 @@ static void smr_progress_resp(struct smr_ep *ep)
 			break;
 
 		pending = (struct smr_cmd *) resp->msg_id;
-		if (pending->msg.hdr.op_flags & SMR_RMA_REQ &&
-			smr_progress_fetch(ep, pending, &resp->status))
+		if (pending->msg.hdr.op >= ofi_op_atomic &&
+		    pending->msg.hdr.op <= ofi_op_atomic_compare &&
+		    smr_progress_atomic_resp(ep, pending, &resp->status))
 				break;
 
 		ret = smr_complete_tx(ep, (void *) (uintptr_t) pending->msg.hdr.msg_id,
@@ -519,14 +523,15 @@ static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd)
 			"unidentified operation type\n");
 		err = -FI_EINVAL;
 	}
-	if (!(cmd->msg.hdr.op_flags & SMR_RMA_REQ)) {
-		ep->region->cmd_cnt++;
-	} else {
+	if (cmd->msg.hdr.data) {
 		peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.addr);
 		resp = (struct smr_resp *) ((char **) peer_smr +
 			    (size_t) cmd->msg.hdr.data);
 		resp->status = -err;
+	} else {
+		ep->region->cmd_cnt++;
 	}
+
 	if (err)
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 			"error processing atomic op\n");
diff --git a/prov/shm/src/smr_rma.c b/prov/shm/src/smr_rma.c
index fd970a1..df1a026 100644
--- a/prov/shm/src/smr_rma.c
+++ b/prov/shm/src/smr_rma.c
@@ -123,7 +123,8 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 	if (ret)
 		return ret;
 
-	cmds = 1 + !(domain->fast_rma && !(op_flags & FI_REMOTE_CQ_DATA) &&
+	cmds = 1 + !(domain->fast_rma && !(op_flags &
+		    (FI_REMOTE_CQ_DATA | FI_DELIVERY_COMPLETE)) &&
 		     rma_count == 1);
 
 	peer_smr = smr_peer_region(ep->region, peer_id);
@@ -150,15 +151,8 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 
 	total_len = ofi_total_iov_len(iov, iov_count);
 
-	if (total_len <= SMR_MSG_DATA_LEN && op == ofi_op_write) {
-		smr_format_inline(cmd, smr_peer_addr(ep->region)[peer_id].addr,
-				  iov, iov_count, op, 0, data, op_flags);
-	} else if (total_len <= SMR_INJECT_SIZE && op == ofi_op_write) {
-		tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
-		smr_format_inject(cmd, smr_peer_addr(ep->region)[peer_id].addr,
-				  iov, iov_count, op, 0, data, op_flags,
-				  peer_smr, tx_buf);
-	} else {
+	if (total_len > SMR_INJECT_SIZE || op != ofi_op_write ||
+	    op_flags & FI_DELIVERY_COMPLETE) {
 		if (ofi_cirque_isfull(smr_resp_queue(ep->region))) {
 			ret = -FI_EAGAIN;
 			goto unlock_cq;
@@ -170,6 +164,14 @@ ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 			       op_flags, context, ep->region, resp, pend);
 		ofi_cirque_commit(smr_resp_queue(ep->region));
 		comp = 0;
+	} else if (total_len > SMR_MSG_DATA_LEN) {
+		tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
+		smr_format_inject(cmd, smr_peer_addr(ep->region)[peer_id].addr,
+				  iov, iov_count, op, 0, data, op_flags,
+				  peer_smr, tx_buf);
+	} else {
+		smr_format_inline(cmd, smr_peer_addr(ep->region)[peer_id].addr,
+				  iov, iov_count, op, 0, data, op_flags);
 	}
 
 	comp_flags = cmd->msg.hdr.op_flags;
diff --git a/prov/shm/src/smr_signal.h b/prov/shm/src/smr_signal.h
index d1e6995..67f6d28 100644
--- a/prov/shm/src/smr_signal.h
+++ b/prov/shm/src/smr_signal.h
@@ -36,7 +36,7 @@
 #include <signal.h>
 #include <ofi_shm.h>
 
-struct sigaction old_action;
+struct sigaction *old_action;
 
 static void smr_handle_signal(int signum, siginfo_t *info, void *ucontext)
 {
@@ -49,7 +49,7 @@ static void smr_handle_signal(int signum, siginfo_t *info, void *ucontext)
 	}
 
 	/* Register the original signum handler, SIG_DFL or otherwise */
-	ret = sigaction(signum, &old_action, NULL);
+	ret = sigaction(signum, &old_action[signum], NULL);
 	if (ret)
 		return;
 
@@ -66,7 +66,7 @@ static void smr_reg_sig_hander(int signum)
 	action.sa_sigaction = smr_handle_signal;
 	action.sa_flags |= SA_SIGINFO;
 
-	ret = sigaction(signum, &action, &old_action);
+	ret = sigaction(signum, &action, &old_action[signum]);
 	if (ret)
 		FI_WARN(&smr_prov, FI_LOG_FABRIC,
 			"Unable to register handler for sig %d\n", signum);
diff --git a/prov/sockets/Makefile.include b/prov/sockets/Makefile.include
index 3cf4a07..2e8024c 100644
--- a/prov/sockets/Makefile.include
+++ b/prov/sockets/Makefile.include
@@ -5,6 +5,7 @@ if HAVE_SOCKETS
 AM_CPPFLAGS += -I$(top_srcdir)/prov/sockets/include -I$(top_srcdir)/prov/sockets
 
 _sockets_files =				\
+	prov/sockets/src/sock_attr.c		\
 	prov/sockets/src/sock_av.c		\
 	prov/sockets/src/sock_dom.c		\
 	prov/sockets/src/sock_mr.c		\
diff --git a/prov/sockets/include/sock.h b/prov/sockets/include/sock.h
index e762d96..3509c93 100644
--- a/prov/sockets/include/sock.h
+++ b/prov/sockets/include/sock.h
@@ -109,42 +109,11 @@
 #define SOCK_CM_DEF_RETRY (5)
 #define SOCK_CM_CONN_IN_PROGRESS ((struct sock_conn *)(0x1L))
 
-#define SOCK_EP_RDM_PRI_CAP (FI_MSG | FI_RMA | FI_TAGGED | FI_ATOMICS |	\
-			 FI_NAMED_RX_CTX | \
-			 FI_DIRECTED_RECV | \
-			 FI_READ | FI_WRITE | FI_RECV | FI_SEND | \
-			 FI_REMOTE_READ | FI_REMOTE_WRITE)
-
-#define SOCK_EP_RDM_SEC_CAP_BASE (FI_MULTI_RECV | FI_SOURCE | FI_RMA_EVENT | \
-				  FI_SHARED_AV | FI_FENCE | FI_TRIGGER)
-extern uint64_t SOCK_EP_RDM_SEC_CAP;
-
-#define SOCK_EP_RDM_CAP_BASE (SOCK_EP_RDM_PRI_CAP | SOCK_EP_RDM_SEC_CAP_BASE)
-extern uint64_t SOCK_EP_RDM_CAP;
-
-#define SOCK_EP_MSG_PRI_CAP SOCK_EP_RDM_PRI_CAP
-
-#define SOCK_EP_MSG_SEC_CAP_BASE SOCK_EP_RDM_SEC_CAP_BASE
-extern uint64_t SOCK_EP_MSG_SEC_CAP;
-
-#define SOCK_EP_MSG_CAP_BASE (SOCK_EP_MSG_PRI_CAP | SOCK_EP_MSG_SEC_CAP_BASE)
-extern uint64_t SOCK_EP_MSG_CAP;
-
-#define SOCK_EP_DGRAM_PRI_CAP (FI_MSG | FI_TAGGED | \
-			   FI_NAMED_RX_CTX | FI_DIRECTED_RECV | \
-			   FI_RECV | FI_SEND)
-
-#define SOCK_EP_DGRAM_SEC_CAP (FI_MULTI_RECV | FI_SOURCE | FI_SHARED_AV | \
-			   FI_FENCE | FI_TRIGGER)
-
-#define SOCK_EP_DGRAM_CAP (SOCK_EP_DGRAM_PRI_CAP | SOCK_EP_DGRAM_SEC_CAP)
-
 #define SOCK_EP_MSG_ORDER (OFI_ORDER_RAR_SET | OFI_ORDER_RAW_SET | FI_ORDER_RAS| \
 			   OFI_ORDER_WAR_SET | OFI_ORDER_WAW_SET | FI_ORDER_WAS | \
 			   FI_ORDER_SAR | FI_ORDER_SAW | FI_ORDER_SAS)
 
 #define SOCK_EP_COMP_ORDER (FI_ORDER_STRICT | FI_ORDER_DATA)
-#define SOCK_EP_DEFAULT_OP_FLAGS (FI_TRANSMIT_COMPLETE)
 
 #define SOCK_EP_CQ_FLAGS (FI_SEND | FI_TRANSMIT | FI_RECV | \
 			FI_SELECTIVE_COMPLETION)
@@ -182,6 +151,24 @@ enum {
 
 #define SOCK_WIRE_PROTO_VERSION (2)
 
+extern struct fi_info sock_dgram_info;
+extern struct fi_info sock_msg_info;
+
+extern struct util_prov sock_util_prov;
+extern struct fi_domain_attr sock_domain_attr;
+extern struct fi_fabric_attr sock_fabric_attr;
+extern struct fi_tx_attr sock_msg_tx_attr;
+extern struct fi_tx_attr sock_rdm_tx_attr;
+extern struct fi_tx_attr sock_dgram_tx_attr;
+extern struct fi_rx_attr sock_msg_rx_attr;
+extern struct fi_rx_attr sock_rdm_rx_attr;
+extern struct fi_rx_attr sock_dgram_rx_attr;
+extern struct fi_ep_attr sock_msg_ep_attr;
+extern struct fi_ep_attr sock_rdm_ep_attr;
+extern struct fi_ep_attr sock_dgram_ep_attr;
+extern struct fi_tx_attr sock_stx_attr;
+extern struct fi_rx_attr sock_srx_attr;
+
 struct sock_service_entry {
 	int service;
 	struct dlist_entry entry;
@@ -212,7 +199,7 @@ struct sock_conn {
 
 struct sock_conn_map {
 	struct sock_conn *table;
-	fi_epoll_t epoll_set;
+	ofi_epoll_t epoll_set;
 	void **epoll_ctxs;
 	int epoll_ctxs_sz;
 	int used;
@@ -221,7 +208,7 @@ struct sock_conn_map {
 };
 
 struct sock_conn_listener {
-	fi_epoll_t emap;
+	ofi_epoll_t emap;
 	struct fd_signal signal;
 	fastlock_t signal_lock; /* acquire before map lock */
 	pthread_t listener_thread;
@@ -229,7 +216,7 @@ struct sock_conn_listener {
 };
 
 struct sock_ep_cm_head {
-	fi_epoll_t emap;
+	ofi_epoll_t emap;
 	struct fd_signal signal;
 	fastlock_t signal_lock;
 	pthread_t listener_thread;
@@ -878,7 +865,7 @@ struct sock_pe {
 	pthread_t progress_thread;
 	volatile int do_progress;
 	struct sock_pe_entry *pe_atomic;
-	fi_epoll_t epoll_set;
+	ofi_epoll_t epoll_set;
 };
 
 typedef int (*sock_cq_report_fn) (struct sock_cq *cq, fi_addr_t addr,
@@ -1000,20 +987,7 @@ union sock_tx_op {
 };
 #define SOCK_EP_TX_ENTRY_SZ (sizeof(union sock_tx_op))
 
-int sock_verify_info(uint32_t version, const struct fi_info *hints);
-int sock_verify_fabric_attr(const struct fi_fabric_attr *attr);
-int sock_verify_domain_attr(uint32_t version, const struct fi_info *info);
-
 size_t sock_get_tx_size(size_t size);
-int sock_rdm_verify_ep_attr(const struct fi_ep_attr *ep_attr,
-			    const struct fi_tx_attr *tx_attr,
-			    const struct fi_rx_attr *rx_attr);
-int sock_dgram_verify_ep_attr(const struct fi_ep_attr *ep_attr,
-			      const struct fi_tx_attr *tx_attr,
-			      const struct fi_rx_attr *rx_attr);
-int sock_msg_verify_ep_attr(const struct fi_ep_attr *ep_attr,
-			    const struct fi_tx_attr *tx_attr,
-			    const struct fi_rx_attr *rx_attr);
 int sock_get_src_addr(union ofi_sock_ip *dest_addr,
 		      union ofi_sock_ip *src_addr);
 int sock_get_src_addr_from_hostname(union ofi_sock_ip *src_addr,
@@ -1022,12 +996,6 @@ int sock_get_src_addr_from_hostname(union ofi_sock_ip *src_addr,
 struct fi_info *sock_fi_info(uint32_t version, enum fi_ep_type ep_type,
 			     const struct fi_info *hints, void *src_addr,
 			     void *dest_addr);
-int sock_msg_fi_info(uint32_t version, void *src_addr, void *dest_addr,
-		     const struct fi_info *hints, struct fi_info **info);
-int sock_dgram_fi_info(uint32_t version, void *src_addr, void *dest_addr,
-		       const struct fi_info *hints, struct fi_info **info);
-int sock_rdm_fi_info(uint32_t version, void *src_addr, void *dest_addr,
-		     const struct fi_info *hints, struct fi_info **info);
 void free_fi_info(struct fi_info *info);
 
 int sock_msg_getinfo(uint32_t version, const char *node, const char *service,
diff --git a/prov/sockets/src/sock_attr.c b/prov/sockets/src/sock_attr.c
new file mode 100644
index 0000000..c14dc39
--- /dev/null
+++ b/prov/sockets/src/sock_attr.c
@@ -0,0 +1,252 @@
+/*
+ * Copyright (c) 2020 Intel Corporation. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "sock.h"
+
+#define SOCK_MSG_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS | \
+			  FI_ATOMICS | FI_NAMED_RX_CTX | FI_FENCE | FI_TRIGGER)
+#define SOCK_MSG_RX_CAPS (OFI_RX_MSG_CAPS | FI_TAGGED | OFI_RX_RMA_CAPS | \
+			  FI_ATOMICS | FI_DIRECTED_RECV | FI_MULTI_RECV | \
+			  FI_RMA_EVENT | FI_SOURCE | FI_TRIGGER)
+
+#define SOCK_RDM_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | OFI_TX_RMA_CAPS | \
+			  FI_ATOMICS | FI_NAMED_RX_CTX | FI_FENCE | FI_TRIGGER | \
+			  FI_RMA_PMEM)
+#define SOCK_RDM_RX_CAPS (OFI_RX_MSG_CAPS | FI_TAGGED | OFI_RX_RMA_CAPS | \
+			  FI_ATOMICS | FI_DIRECTED_RECV | FI_MULTI_RECV | \
+			  FI_RMA_EVENT | FI_SOURCE | FI_TRIGGER | FI_RMA_PMEM)
+
+#define SOCK_DGRAM_TX_CAPS (OFI_TX_MSG_CAPS | FI_TAGGED | FI_NAMED_RX_CTX | \
+			    FI_FENCE | FI_TRIGGER)
+#define SOCK_DGRAM_RX_CAPS (OFI_RX_MSG_CAPS | FI_TAGGED | FI_DIRECTED_RECV | \
+			    FI_MULTI_RECV | FI_SOURCE | FI_TRIGGER)
+
+#define SOCK_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM | FI_SHARED_AV)
+
+#define SOCK_TX_OP_FLAGS (FI_COMMIT_COMPLETE | FI_COMPLETION | \
+			  FI_DELIVERY_COMPLETE | FI_INJECT | FI_INJECT_COMPLETE | \
+			  FI_MULTICAST | FI_TRANSMIT_COMPLETE)
+#define SOCK_RX_OP_FLAGS (FI_COMMIT_COMPLETE | FI_COMPLETION | \
+			  FI_DELIVERY_COMPLETE | FI_INJECT | FI_INJECT_COMPLETE | \
+			  FI_MULTI_RECV | FI_TRANSMIT_COMPLETE)
+
+struct fi_ep_attr sock_msg_ep_attr = {
+	.type = FI_EP_MSG,
+	.protocol = FI_PROTO_SOCK_TCP,
+	.protocol_version = SOCK_WIRE_PROTO_VERSION,
+	.max_msg_size = SOCK_EP_MAX_MSG_SZ,
+	.msg_prefix_size = SOCK_EP_MSG_PREFIX_SZ,
+	.max_order_raw_size = SOCK_EP_MAX_ORDER_RAW_SZ,
+	.max_order_war_size = SOCK_EP_MAX_ORDER_WAR_SZ,
+	.max_order_waw_size = SOCK_EP_MAX_ORDER_WAW_SZ,
+	.mem_tag_format = SOCK_EP_MEM_TAG_FMT,
+	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
+	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
+};
+
+struct fi_tx_attr sock_msg_tx_attr = {
+	.caps = SOCK_MSG_TX_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = SOCK_TX_OP_FLAGS,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.inject_size = SOCK_EP_MAX_INJECT_SZ,
+	.size = SOCK_EP_TX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+	.rma_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_rx_attr sock_msg_rx_attr = {
+	.caps = SOCK_MSG_RX_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = SOCK_RX_OP_FLAGS,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.comp_order = SOCK_EP_COMP_ORDER,
+	.total_buffered_recv = SOCK_EP_MAX_BUFF_RECV,
+	.size = SOCK_EP_RX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_ep_attr sock_dgram_ep_attr = {
+	.type = FI_EP_DGRAM,
+	.protocol = FI_PROTO_SOCK_TCP,
+	.protocol_version = SOCK_WIRE_PROTO_VERSION,
+	.max_msg_size = SOCK_EP_MAX_MSG_SZ,
+	.msg_prefix_size = SOCK_EP_MSG_PREFIX_SZ,
+	.max_order_raw_size = SOCK_EP_MAX_ORDER_RAW_SZ,
+	.max_order_war_size = SOCK_EP_MAX_ORDER_WAR_SZ,
+	.max_order_waw_size = SOCK_EP_MAX_ORDER_WAW_SZ,
+	.mem_tag_format = SOCK_EP_MEM_TAG_FMT,
+	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
+	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
+};
+
+struct fi_ep_attr sock_rdm_ep_attr = {
+	.type = FI_EP_RDM,
+	.protocol = FI_PROTO_SOCK_TCP,
+	.protocol_version = SOCK_WIRE_PROTO_VERSION,
+	.max_msg_size = SOCK_EP_MAX_MSG_SZ,
+	.msg_prefix_size = SOCK_EP_MSG_PREFIX_SZ,
+	.max_order_raw_size = SOCK_EP_MAX_ORDER_RAW_SZ,
+	.max_order_war_size = SOCK_EP_MAX_ORDER_WAR_SZ,
+	.max_order_waw_size = SOCK_EP_MAX_ORDER_WAW_SZ,
+	.mem_tag_format = SOCK_EP_MEM_TAG_FMT,
+	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
+	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
+};
+
+struct fi_tx_attr sock_rdm_tx_attr = {
+	.caps = SOCK_RDM_TX_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = SOCK_TX_OP_FLAGS,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.inject_size = SOCK_EP_MAX_INJECT_SZ,
+	.size = SOCK_EP_TX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+	.rma_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_rx_attr sock_rdm_rx_attr = {
+	.caps = SOCK_RDM_RX_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = SOCK_RX_OP_FLAGS,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.comp_order = SOCK_EP_COMP_ORDER,
+	.total_buffered_recv = SOCK_EP_MAX_BUFF_RECV,
+	.size = SOCK_EP_RX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_tx_attr sock_dgram_tx_attr = {
+	.caps = SOCK_DGRAM_TX_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = SOCK_TX_OP_FLAGS,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.inject_size = SOCK_EP_MAX_INJECT_SZ,
+	.size = SOCK_EP_TX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+	.rma_iov_limit = 0,
+};
+
+struct fi_rx_attr sock_dgram_rx_attr = {
+	.caps = SOCK_DGRAM_RX_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = SOCK_RX_OP_FLAGS,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.comp_order = SOCK_EP_COMP_ORDER,
+	.total_buffered_recv = SOCK_EP_MAX_BUFF_RECV,
+	.size = SOCK_EP_RX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_tx_attr sock_stx_attr = {
+	.caps = SOCK_RDM_TX_CAPS | SOCK_RDM_RX_CAPS | SOCK_DOMAIN_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = FI_TRANSMIT_COMPLETE,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.inject_size = SOCK_EP_MAX_INJECT_SZ,
+	.size = SOCK_EP_TX_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+	.rma_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_rx_attr sock_srx_attr = {
+	.caps = SOCK_RDM_TX_CAPS | SOCK_RDM_RX_CAPS | SOCK_DOMAIN_CAPS,
+	.mode = SOCK_MODE,
+	.op_flags = 0,
+	.msg_order = SOCK_EP_MSG_ORDER,
+	.comp_order = SOCK_EP_COMP_ORDER,
+	.total_buffered_recv = 0,
+	.size = SOCK_EP_MAX_MSG_SZ,
+	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+};
+
+struct fi_domain_attr sock_domain_attr = {
+	.name = "sockets",
+	.threading = FI_THREAD_SAFE,
+	.control_progress = FI_PROGRESS_AUTO,
+	.data_progress = FI_PROGRESS_AUTO,
+	.resource_mgmt = FI_RM_ENABLED,
+	/* Provider supports basic memory registration mode */
+	.mr_mode = FI_MR_BASIC | FI_MR_SCALABLE,
+	.mr_key_size = sizeof(uint64_t),
+	.cq_data_size = sizeof(uint64_t),
+	.cq_cnt = SOCK_EP_MAX_CQ_CNT,
+	.ep_cnt = SOCK_EP_MAX_EP_CNT,
+	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
+	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
+	.max_ep_tx_ctx = SOCK_EP_MAX_TX_CNT,
+	.max_ep_rx_ctx = SOCK_EP_MAX_RX_CNT,
+	.max_ep_stx_ctx = SOCK_EP_MAX_EP_CNT,
+	.max_ep_srx_ctx = SOCK_EP_MAX_EP_CNT,
+	.cntr_cnt = SOCK_EP_MAX_CNTR_CNT,
+	.mr_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
+	.max_err_data = SOCK_MAX_ERR_CQ_EQ_DATA_SZ,
+	.mr_cnt = SOCK_DOMAIN_MR_CNT,
+	.caps = SOCK_DOMAIN_CAPS,
+};
+
+struct fi_fabric_attr sock_fabric_attr = {
+	.name = "sockets",
+	.prov_version = FI_VERSION(SOCK_MAJOR_VERSION, SOCK_MINOR_VERSION),
+};
+
+struct fi_info sock_msg_info = {
+	.caps = SOCK_MSG_TX_CAPS | SOCK_MSG_RX_CAPS | SOCK_DOMAIN_CAPS,
+	.addr_format = FI_SOCKADDR,
+	.tx_attr = &sock_msg_tx_attr,
+	.rx_attr = &sock_msg_rx_attr,
+	.ep_attr = &sock_msg_ep_attr,
+	.domain_attr = &sock_domain_attr,
+	.fabric_attr = &sock_fabric_attr
+};
+
+struct fi_info sock_rdm_info = {
+	.next = &sock_msg_info,
+	.caps = SOCK_RDM_TX_CAPS | SOCK_RDM_RX_CAPS | SOCK_DOMAIN_CAPS,
+	.addr_format = FI_SOCKADDR,
+	.tx_attr = &sock_rdm_tx_attr,
+	.rx_attr = &sock_rdm_rx_attr,
+	.ep_attr = &sock_rdm_ep_attr,
+	.domain_attr = &sock_domain_attr,
+	.fabric_attr = &sock_fabric_attr
+};
+
+struct fi_info sock_dgram_info = {
+	.next = &sock_rdm_info,
+	.caps = SOCK_DGRAM_TX_CAPS | SOCK_DGRAM_RX_CAPS | SOCK_DOMAIN_CAPS,
+	.addr_format = FI_SOCKADDR,
+	.tx_attr = &sock_dgram_tx_attr,
+	.rx_attr = &sock_dgram_rx_attr,
+	.ep_attr = &sock_dgram_ep_attr,
+	.domain_attr = &sock_domain_attr,
+	.fabric_attr = &sock_fabric_attr
+};
diff --git a/prov/sockets/src/sock_conn.c b/prov/sockets/src/sock_conn.c
index b29b43e..8c739db 100644
--- a/prov/sockets/src/sock_conn.c
+++ b/prov/sockets/src/sock_conn.c
@@ -105,7 +105,7 @@ int sock_conn_map_init(struct sock_ep *ep, int init_size)
 	if (!map->epoll_ctxs)
 		goto err1;
 
-	ret = fi_epoll_create(&map->epoll_set);
+	ret = ofi_epoll_create(&map->epoll_set);
 	if (ret < 0) {
 		SOCK_LOG_ERROR("failed to create epoll set, "
 			       "error - %d (%s)\n", ret,
@@ -157,13 +157,13 @@ void sock_conn_map_destroy(struct sock_ep_attr *ep_attr)
 	cmap->epoll_ctxs = NULL;
 	cmap->epoll_ctxs_sz = 0;
 	cmap->used = cmap->size = 0;
-	fi_epoll_close(cmap->epoll_set);
+	ofi_epoll_close(cmap->epoll_set);
 	fastlock_destroy(&cmap->lock);
 }
 
 void sock_conn_release_entry(struct sock_conn_map *map, struct sock_conn *conn)
 {
-	fi_epoll_del(map->epoll_set, conn->sock_fd);
+	ofi_epoll_del(map->epoll_set, conn->sock_fd);
 	ofi_close_socket(conn->sock_fd);
 
 	conn->address_published = 0;
@@ -210,7 +210,7 @@ static struct sock_conn *sock_conn_map_insert(struct sock_ep_attr *ep_attr,
 	                  (ep_attr->ep_type == FI_EP_MSG ?
 	                   SOCK_OPTS_KEEPALIVE : 0));
 
-	if (fi_epoll_add(map->epoll_set, conn_fd, FI_EPOLL_IN, &map->table[index]))
+	if (ofi_epoll_add(map->epoll_set, conn_fd, OFI_EPOLL_IN, &map->table[index]))
 		SOCK_LOG_ERROR("failed to add to epoll set: %d\n", conn_fd);
 
 	map->table[index].address_published = addr_published;
@@ -306,7 +306,7 @@ int sock_conn_stop_listener_thread(struct sock_conn_listener *conn_listener)
 	}
 
 	fd_signal_free(&conn_listener->signal);
-	fi_epoll_close(conn_listener->emap);
+	ofi_epoll_close(conn_listener->emap);
 	fastlock_destroy(&conn_listener->signal_lock);
 
 	return 0;
@@ -323,7 +323,7 @@ static void *sock_conn_listener_thread(void *arg)
 	socklen_t addr_size;
 
 	while (conn_listener->do_listen) {
-		num_fds = fi_epoll_wait(conn_listener->emap, ep_contexts,
+		num_fds = ofi_epoll_wait(conn_listener->emap, ep_contexts,
 		                        SOCK_EPOLL_WAIT_EVENTS, -1);
 		if (num_fds < 0) {
 			SOCK_LOG_ERROR("poll failed : %s\n", strerror(errno));
@@ -371,7 +371,7 @@ int sock_conn_start_listener_thread(struct sock_conn_listener *conn_listener)
 
 	fastlock_init(&conn_listener->signal_lock);
 
-	ret = fi_epoll_create(&conn_listener->emap);
+	ret = ofi_epoll_create(&conn_listener->emap);
 	if (ret < 0) {
 		SOCK_LOG_ERROR("failed to create epoll set\n");
 		goto err1;
@@ -383,9 +383,9 @@ int sock_conn_start_listener_thread(struct sock_conn_listener *conn_listener)
 		goto err2;
 	}
 
-	ret = fi_epoll_add(conn_listener->emap,
+	ret = ofi_epoll_add(conn_listener->emap,
 	                   conn_listener->signal.fd[FI_READ_FD],
-	                   FI_EPOLL_IN, NULL);
+	                   OFI_EPOLL_IN, NULL);
 	if (ret != 0){
 		SOCK_LOG_ERROR("failed to add signal fd to epoll\n");
 		goto err3;
@@ -404,7 +404,7 @@ err3:
 	conn_listener->do_listen = 0;
 	fd_signal_free(&conn_listener->signal);
 err2:
-	fi_epoll_close(conn_listener->emap);
+	ofi_epoll_close(conn_listener->emap);
 err1:
 	fastlock_destroy(&conn_listener->signal_lock);
 	return ret;
@@ -463,8 +463,8 @@ int sock_conn_listen(struct sock_ep_attr *ep_attr)
 	conn_handle->do_listen = 1;
 
 	fastlock_acquire(&ep_attr->domain->conn_listener.signal_lock);
-	ret = fi_epoll_add(ep_attr->domain->conn_listener.emap,
-	                   conn_handle->sock, FI_EPOLL_IN, conn_handle);
+	ret = ofi_epoll_add(ep_attr->domain->conn_listener.emap,
+	                   conn_handle->sock, OFI_EPOLL_IN, conn_handle);
 	fd_signal_set(&ep_attr->domain->conn_listener.signal);
 	fastlock_release(&ep_attr->domain->conn_listener.signal_lock);
 	if (ret) {
diff --git a/prov/sockets/src/sock_dom.c b/prov/sockets/src/sock_dom.c
index e02d5fe..8adec0c 100644
--- a/prov/sockets/src/sock_dom.c
+++ b/prov/sockets/src/sock_dom.c
@@ -46,133 +46,6 @@
 
 extern struct fi_ops_mr sock_dom_mr_ops;
 
-const struct fi_domain_attr sock_domain_attr = {
-	.name = NULL,
-	.threading = FI_THREAD_SAFE,
-	.control_progress = FI_PROGRESS_AUTO,
-	.data_progress = FI_PROGRESS_AUTO,
-	.resource_mgmt = FI_RM_ENABLED,
-	/* Provider supports basic memory registration mode */
-	.mr_mode = FI_MR_BASIC | FI_MR_SCALABLE,
-	.mr_key_size = sizeof(uint64_t),
-	.cq_data_size = sizeof(uint64_t),
-	.cq_cnt = SOCK_EP_MAX_CQ_CNT,
-	.ep_cnt = SOCK_EP_MAX_EP_CNT,
-	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
-	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
-	.max_ep_tx_ctx = SOCK_EP_MAX_TX_CNT,
-	.max_ep_rx_ctx = SOCK_EP_MAX_RX_CNT,
-	.max_ep_stx_ctx = SOCK_EP_MAX_EP_CNT,
-	.max_ep_srx_ctx = SOCK_EP_MAX_EP_CNT,
-	.cntr_cnt = SOCK_EP_MAX_CNTR_CNT,
-	.mr_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-	.max_err_data = SOCK_MAX_ERR_CQ_EQ_DATA_SZ,
-	.mr_cnt = SOCK_DOMAIN_MR_CNT,
-};
-
-int sock_verify_domain_attr(uint32_t version, const struct fi_info *info)
-{
-	const struct fi_domain_attr *attr = info->domain_attr;
-
-	if (!attr)
-		return 0;
-
-	switch (attr->threading) {
-	case FI_THREAD_UNSPEC:
-	case FI_THREAD_SAFE:
-	case FI_THREAD_FID:
-	case FI_THREAD_DOMAIN:
-	case FI_THREAD_COMPLETION:
-	case FI_THREAD_ENDPOINT:
-		break;
-	default:
-		SOCK_LOG_DBG("Invalid threading model!\n");
-		return -FI_ENODATA;
-	}
-
-	switch (attr->control_progress) {
-	case FI_PROGRESS_UNSPEC:
-	case FI_PROGRESS_AUTO:
-	case FI_PROGRESS_MANUAL:
-		break;
-
-	default:
-		SOCK_LOG_DBG("Control progress mode not supported!\n");
-		return -FI_ENODATA;
-	}
-
-	switch (attr->data_progress) {
-	case FI_PROGRESS_UNSPEC:
-	case FI_PROGRESS_AUTO:
-	case FI_PROGRESS_MANUAL:
-		break;
-
-	default:
-		SOCK_LOG_DBG("Data progress mode not supported!\n");
-		return -FI_ENODATA;
-	}
-
-	switch (attr->resource_mgmt) {
-	case FI_RM_UNSPEC:
-	case FI_RM_DISABLED:
-	case FI_RM_ENABLED:
-		break;
-
-	default:
-		SOCK_LOG_DBG("Resource mgmt not supported!\n");
-		return -FI_ENODATA;
-	}
-
-	switch (attr->av_type) {
-	case FI_AV_UNSPEC:
-	case FI_AV_MAP:
-	case FI_AV_TABLE:
-		break;
-
-	default:
-		SOCK_LOG_DBG("AV type not supported!\n");
-		return -FI_ENODATA;
-	}
-
-	if (ofi_check_mr_mode(&sock_prov, version,
-			      sock_domain_attr.mr_mode, info)) {
-		FI_INFO(&sock_prov, FI_LOG_CORE,
-			"Invalid memory registration mode\n");
-		return -FI_ENODATA;
-	}
-
-	if (attr->mr_key_size > sock_domain_attr.mr_key_size)
-		return -FI_ENODATA;
-
-	if (attr->cq_data_size > sock_domain_attr.cq_data_size)
-		return -FI_ENODATA;
-
-	if (attr->cq_cnt > sock_domain_attr.cq_cnt)
-		return -FI_ENODATA;
-
-	if (attr->ep_cnt > sock_domain_attr.ep_cnt)
-		return -FI_ENODATA;
-
-	if (attr->max_ep_tx_ctx > sock_domain_attr.max_ep_tx_ctx)
-		return -FI_ENODATA;
-
-	if (attr->max_ep_rx_ctx > sock_domain_attr.max_ep_rx_ctx)
-		return -FI_ENODATA;
-
-	if (attr->cntr_cnt > sock_domain_attr.cntr_cnt)
-		return -FI_ENODATA;
-
-	if (attr->mr_iov_limit > sock_domain_attr.mr_iov_limit)
-		return -FI_ENODATA;
-
-	if (attr->max_err_data > sock_domain_attr.max_err_data)
-		return -FI_ENODATA;
-
-	if (attr->mr_cnt > sock_domain_attr.mr_cnt)
-		return -FI_ENODATA;
-
-	return 0;
-}
 
 static int sock_dom_close(struct fid *fid)
 {
@@ -282,12 +155,8 @@ int sock_domain(struct fid_fabric *fabric, struct fi_info *info,
 	struct sock_fabric *fab;
 	int ret;
 
+	assert(info && info->domain_attr);
 	fab = container_of(fabric, struct sock_fabric, fab_fid);
-	if (info && info->domain_attr) {
-		ret = sock_verify_domain_attr(fabric->api_version, info);
-		if (ret)
-			return -FI_EINVAL;
-	}
 
 	sock_domain = calloc(1, sizeof(*sock_domain));
 	if (!sock_domain)
@@ -296,12 +165,8 @@ int sock_domain(struct fid_fabric *fabric, struct fi_info *info,
 	fastlock_init(&sock_domain->lock);
 	ofi_atomic_initialize32(&sock_domain->ref, 0);
 
-	if (info) {
-		sock_domain->info = *info;
-	} else {
-		SOCK_LOG_ERROR("invalid fi_info\n");
-		goto err1;
-	}
+	sock_domain->info = *info;
+	sock_domain->info.domain_attr = NULL;
 
 	sock_domain->dom_fid.fid.fclass = FI_CLASS_DOMAIN;
 	sock_domain->dom_fid.fid.context = context;
@@ -309,8 +174,7 @@ int sock_domain(struct fid_fabric *fabric, struct fi_info *info,
 	sock_domain->dom_fid.ops = &sock_dom_ops;
 	sock_domain->dom_fid.mr = &sock_dom_mr_ops;
 
-	if (!info->domain_attr ||
-	    info->domain_attr->data_progress == FI_PROGRESS_UNSPEC)
+	if (info->domain_attr->data_progress == FI_PROGRESS_UNSPEC)
 		sock_domain->progress_mode = FI_PROGRESS_AUTO;
 	else
 		sock_domain->progress_mode = info->domain_attr->data_progress;
@@ -324,10 +188,7 @@ int sock_domain(struct fid_fabric *fabric, struct fi_info *info,
 	sock_domain->fab = fab;
 	*dom = &sock_domain->dom_fid;
 
-	if (info->domain_attr)
-		sock_domain->attr = *(info->domain_attr);
-	else
-		sock_domain->attr = sock_domain_attr;
+	sock_domain->attr = *(info->domain_attr);
 
 	ret = ofi_mr_map_init(&sock_prov, sock_domain->attr.mr_mode,
 			      &sock_domain->mr_map);
diff --git a/prov/sockets/src/sock_ep.c b/prov/sockets/src/sock_ep.c
index 0317bbf..c19d924 100644
--- a/prov/sockets/src/sock_ep.c
+++ b/prov/sockets/src/sock_ep.c
@@ -64,31 +64,6 @@ extern struct fi_ops_ep sock_ep_ops;
 extern struct fi_ops sock_ep_fi_ops;
 extern struct fi_ops_ep sock_ctx_ep_ops;
 
-extern const struct fi_domain_attr sock_domain_attr;
-extern const struct fi_fabric_attr sock_fabric_attr;
-
-const struct fi_tx_attr sock_stx_attr = {
-	.caps = SOCK_EP_RDM_CAP_BASE,
-	.mode = SOCK_MODE,
-	.op_flags = FI_TRANSMIT_COMPLETE,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.inject_size = SOCK_EP_MAX_INJECT_SZ,
-	.size = SOCK_EP_TX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-	.rma_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
-const struct fi_rx_attr sock_srx_attr = {
-	.caps = SOCK_EP_RDM_CAP_BASE,
-	.mode = SOCK_MODE,
-	.op_flags = 0,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.comp_order = SOCK_EP_COMP_ORDER,
-	.total_buffered_recv = 0,
-	.size = SOCK_EP_MAX_MSG_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
 static void sock_tx_ctx_close(struct sock_tx_ctx *tx_ctx)
 {
 	if (tx_ctx->comp.send_cq)
@@ -705,7 +680,7 @@ static int sock_ep_close(struct fid *fid)
 
 	if (sock_ep->attr->conn_handle.do_listen) {
 		fastlock_acquire(&sock_ep->attr->domain->conn_listener.signal_lock);
-		fi_epoll_del(sock_ep->attr->domain->conn_listener.emap,
+		ofi_epoll_del(sock_ep->attr->domain->conn_listener.emap,
 		             sock_ep->attr->conn_handle.sock);
 		fastlock_release(&sock_ep->attr->domain->conn_listener.signal_lock);
 		ofi_close_socket(sock_ep->attr->conn_handle.sock);
@@ -1623,16 +1598,10 @@ int sock_alloc_endpoint(struct fid_domain *domain, struct fi_info *info,
 	struct sock_rx_ctx *rx_ctx;
 	struct sock_domain *sock_dom;
 
+	assert(info);
 	sock_dom = container_of(domain, struct sock_domain, dom_fid);
-	if (info) {
-		ret = sock_verify_info(sock_dom->fab->fab_fid.api_version, info);
-		if (ret) {
-			SOCK_LOG_DBG("Cannot support requested options!\n");
-			return -FI_EINVAL;
-		}
-	}
 
-	sock_ep = (struct sock_ep *) calloc(1, sizeof(*sock_ep));
+	sock_ep = calloc(1, sizeof(*sock_ep));
 	if (!sock_ep)
 		return -FI_ENOMEM;
 
@@ -1672,52 +1641,50 @@ int sock_alloc_endpoint(struct fid_domain *domain, struct fi_info *info,
 	sock_ep->attr->fclass = fclass;
 	*ep = sock_ep;
 
-	if (info) {
-		sock_ep->attr->info.caps = info->caps;
-		sock_ep->attr->info.addr_format = FI_SOCKADDR_IN;
+	sock_ep->attr->info.caps = info->caps;
+	sock_ep->attr->info.addr_format = info->addr_format;
 
-		if (info->ep_attr) {
-			sock_ep->attr->ep_type = info->ep_attr->type;
-			sock_ep->attr->ep_attr.tx_ctx_cnt = info->ep_attr->tx_ctx_cnt;
-			sock_ep->attr->ep_attr.rx_ctx_cnt = info->ep_attr->rx_ctx_cnt;
-		}
-
-		if (info->src_addr) {
-			sock_ep->attr->src_addr = calloc(1, sizeof(*sock_ep->
-							 attr->src_addr));
-			if (!sock_ep->attr->src_addr) {
-				ret = -FI_ENOMEM;
-				goto err2;
-			}
-			memcpy(sock_ep->attr->src_addr, info->src_addr,
-			       info->src_addrlen);
-		}
+	if (info->ep_attr) {
+		sock_ep->attr->ep_type = info->ep_attr->type;
+		sock_ep->attr->ep_attr.tx_ctx_cnt = info->ep_attr->tx_ctx_cnt;
+		sock_ep->attr->ep_attr.rx_ctx_cnt = info->ep_attr->rx_ctx_cnt;
+	}
 
-		if (info->dest_addr) {
-			sock_ep->attr->dest_addr = calloc(1, sizeof(*sock_ep->
-							  attr->dest_addr));
-			if (!sock_ep->attr->dest_addr) {
-				ret = -FI_ENOMEM;
-				goto err2;
-			}
-			memcpy(sock_ep->attr->dest_addr, info->dest_addr,
-			       info->dest_addrlen);
+	if (info->src_addr) {
+		sock_ep->attr->src_addr = calloc(1, sizeof(*sock_ep->
+							   attr->src_addr));
+		if (!sock_ep->attr->src_addr) {
+			ret = -FI_ENOMEM;
+			goto err2;
 		}
+		memcpy(sock_ep->attr->src_addr, info->src_addr,
+			info->src_addrlen);
+	}
 
-		if (info->tx_attr) {
-			sock_ep->tx_attr = *info->tx_attr;
-			if (!(sock_ep->tx_attr.op_flags & (FI_INJECT_COMPLETE |
-			     FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)))
-                        	sock_ep->tx_attr.op_flags |= FI_TRANSMIT_COMPLETE;
-			sock_ep->tx_attr.size = sock_ep->tx_attr.size ?
-				sock_ep->tx_attr.size : SOCK_EP_TX_SZ;
+	if (info->dest_addr) {
+		sock_ep->attr->dest_addr = calloc(1, sizeof(*sock_ep->
+							    attr->dest_addr));
+		if (!sock_ep->attr->dest_addr) {
+			ret = -FI_ENOMEM;
+			goto err2;
 		}
+		memcpy(sock_ep->attr->dest_addr, info->dest_addr,
+			info->dest_addrlen);
+	}
 
-		if (info->rx_attr)
-			sock_ep->rx_attr = *info->rx_attr;
-		sock_ep->attr->info.handle = info->handle;
+	if (info->tx_attr) {
+		sock_ep->tx_attr = *info->tx_attr;
+		if (!(sock_ep->tx_attr.op_flags & (FI_INJECT_COMPLETE |
+			FI_TRANSMIT_COMPLETE | FI_DELIVERY_COMPLETE)))
+			sock_ep->tx_attr.op_flags |= FI_TRANSMIT_COMPLETE;
+		sock_ep->tx_attr.size = sock_ep->tx_attr.size ?
+			sock_ep->tx_attr.size : SOCK_EP_TX_SZ;
 	}
 
+	if (info->rx_attr)
+		sock_ep->rx_attr = *info->rx_attr;
+	sock_ep->attr->info.handle = info->handle;
+
 	if (!sock_ep->attr->src_addr && sock_ep_assign_src_addr(sock_ep, info)) {
 		SOCK_LOG_ERROR("failed to get src_address\n");
 		ret = -FI_EINVAL;
diff --git a/prov/sockets/src/sock_ep_dgram.c b/prov/sockets/src/sock_ep_dgram.c
index 3b7e62a..7a18532 100644
--- a/prov/sockets/src/sock_ep_dgram.c
+++ b/prov/sockets/src/sock_ep_dgram.c
@@ -56,235 +56,6 @@
 #define SOCK_LOG_DBG(...) _SOCK_LOG_DBG(FI_LOG_EP_CTRL, __VA_ARGS__)
 #define SOCK_LOG_ERROR(...) _SOCK_LOG_ERROR(FI_LOG_EP_CTRL, __VA_ARGS__)
 
-const struct fi_ep_attr sock_dgram_ep_attr = {
-	.type = FI_EP_DGRAM,
-	.protocol = FI_PROTO_SOCK_TCP,
-	.protocol_version = SOCK_WIRE_PROTO_VERSION,
-	.max_msg_size = SOCK_EP_MAX_MSG_SZ,
-	.msg_prefix_size = SOCK_EP_MSG_PREFIX_SZ,
-	.max_order_raw_size = SOCK_EP_MAX_ORDER_RAW_SZ,
-	.max_order_war_size = SOCK_EP_MAX_ORDER_WAR_SZ,
-	.max_order_waw_size = SOCK_EP_MAX_ORDER_WAW_SZ,
-	.mem_tag_format = SOCK_EP_MEM_TAG_FMT,
-	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
-	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
-};
-
-const struct fi_tx_attr sock_dgram_tx_attr = {
-	.caps = SOCK_EP_DGRAM_CAP,
-	.mode = SOCK_MODE,
-	.op_flags = SOCK_EP_DEFAULT_OP_FLAGS,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.inject_size = SOCK_EP_MAX_INJECT_SZ,
-	.size = SOCK_EP_TX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-	.rma_iov_limit = 0,
-};
-
-const struct fi_rx_attr sock_dgram_rx_attr = {
-	.caps = SOCK_EP_DGRAM_CAP,
-	.mode = SOCK_MODE,
-	.op_flags = 0,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.comp_order = SOCK_EP_COMP_ORDER,
-	.total_buffered_recv = SOCK_EP_MAX_BUFF_RECV,
-	.size = SOCK_EP_RX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
-static int sock_dgram_verify_rx_attr(const struct fi_rx_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if ((attr->caps | SOCK_EP_DGRAM_CAP) != SOCK_EP_DGRAM_CAP)
-		return -FI_ENODATA;
-
-	if ((attr->msg_order | SOCK_EP_MSG_ORDER) != SOCK_EP_MSG_ORDER)
-		return -FI_ENODATA;
-
-	if ((attr->comp_order | SOCK_EP_COMP_ORDER) != SOCK_EP_COMP_ORDER)
-		return -FI_ENODATA;
-
-	if (attr->total_buffered_recv > sock_dgram_rx_attr.total_buffered_recv)
-		return -FI_ENODATA;
-
-	if (sock_get_tx_size(attr->size) >
-	     sock_get_tx_size(sock_dgram_rx_attr.size))
-		return -FI_ENODATA;
-
-	if (attr->iov_limit > sock_dgram_rx_attr.iov_limit)
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-static int sock_dgram_verify_tx_attr(const struct fi_tx_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if ((attr->caps | SOCK_EP_DGRAM_CAP) != SOCK_EP_DGRAM_CAP)
-		return -FI_ENODATA;
-
-	if ((attr->msg_order | SOCK_EP_MSG_ORDER) != SOCK_EP_MSG_ORDER)
-		return -FI_ENODATA;
-
-	if (attr->inject_size > sock_dgram_tx_attr.inject_size)
-		return -FI_ENODATA;
-
-	if (sock_get_tx_size(attr->size) >
-	     sock_get_tx_size(sock_dgram_tx_attr.size))
-		return -FI_ENODATA;
-
-	if (attr->iov_limit > sock_dgram_tx_attr.iov_limit)
-		return -FI_ENODATA;
-
-	if (attr->rma_iov_limit > sock_dgram_tx_attr.rma_iov_limit)
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-int sock_dgram_verify_ep_attr(const struct fi_ep_attr *ep_attr,
-			      const struct fi_tx_attr *tx_attr,
-			      const struct fi_rx_attr *rx_attr)
-{
-	if (ep_attr) {
-		switch (ep_attr->protocol) {
-		case FI_PROTO_UNSPEC:
-		case FI_PROTO_SOCK_TCP:
-			break;
-		default:
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->protocol_version &&
-		    (ep_attr->protocol_version != sock_dgram_ep_attr.protocol_version))
-			return -FI_ENODATA;
-
-		if (ep_attr->max_msg_size > sock_dgram_ep_attr.max_msg_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->msg_prefix_size > sock_dgram_ep_attr.msg_prefix_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->max_order_raw_size >
-		   sock_dgram_ep_attr.max_order_raw_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->max_order_war_size >
-		   sock_dgram_ep_attr.max_order_war_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->max_order_waw_size >
-		   sock_dgram_ep_attr.max_order_waw_size)
-			return -FI_ENODATA;
-
-		if ((ep_attr->tx_ctx_cnt > SOCK_EP_MAX_TX_CNT) &&
-		    ep_attr->tx_ctx_cnt != FI_SHARED_CONTEXT)
-			return -FI_ENODATA;
-
-		if ((ep_attr->rx_ctx_cnt > SOCK_EP_MAX_RX_CNT) &&
-		    ep_attr->rx_ctx_cnt != FI_SHARED_CONTEXT)
-			return -FI_ENODATA;
-	}
-
-	if (sock_dgram_verify_tx_attr(tx_attr) ||
-			sock_dgram_verify_rx_attr(rx_attr))
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-int sock_dgram_fi_info(uint32_t version, void *src_addr, void *dest_addr,
-		       const struct fi_info *hints, struct fi_info **info)
-{
-	*info = sock_fi_info(version, FI_EP_DGRAM, hints, src_addr, dest_addr);
-	if (!*info)
-		return -FI_ENOMEM;
-
-	*(*info)->tx_attr = sock_dgram_tx_attr;
-	(*info)->tx_attr->size = sock_get_tx_size(sock_dgram_tx_attr.size);
-	*(*info)->rx_attr = sock_dgram_rx_attr;
-	(*info)->rx_attr->size = sock_get_tx_size(sock_dgram_rx_attr.size);
-	*(*info)->ep_attr = sock_dgram_ep_attr;
-
-	if (hints && hints->ep_attr) {
-		if (hints->ep_attr->rx_ctx_cnt)
-			(*info)->ep_attr->rx_ctx_cnt = hints->ep_attr->rx_ctx_cnt;
-		if (hints->ep_attr->tx_ctx_cnt)
-			(*info)->ep_attr->tx_ctx_cnt = hints->ep_attr->tx_ctx_cnt;
-	}
-
-	if (hints && hints->rx_attr) {
-		(*info)->rx_attr->op_flags |= hints->rx_attr->op_flags;
-		if (hints->rx_attr->caps)
-			(*info)->rx_attr->caps = SOCK_EP_DGRAM_SEC_CAP |
-							hints->rx_attr->caps;
-	}
-
-	if (hints && hints->tx_attr) {
-		(*info)->tx_attr->op_flags |= hints->tx_attr->op_flags;
-		if (hints->tx_attr->caps)
-			(*info)->tx_attr->caps = SOCK_EP_DGRAM_SEC_CAP |
-							hints->tx_attr->caps;
-	}
-
-	(*info)->caps = SOCK_EP_DGRAM_CAP |
-			(*info)->rx_attr->caps | (*info)->tx_attr->caps;
-	if (hints && hints->caps) {
-		(*info)->caps = SOCK_EP_DGRAM_SEC_CAP | hints->caps;
-		(*info)->rx_attr->caps = SOCK_EP_DGRAM_SEC_CAP |
-			((*info)->rx_attr->caps & (*info)->caps);
-		(*info)->tx_attr->caps = SOCK_EP_DGRAM_SEC_CAP |
-			((*info)->tx_attr->caps & (*info)->caps);
-	}
-	return 0;
-}
-
-static int sock_dgram_endpoint(struct fid_domain *domain, struct fi_info *info,
-		struct sock_ep **ep, void *context, size_t fclass)
-{
-	int ret;
-
-	if (info) {
-		if (info->ep_attr) {
-			ret = sock_dgram_verify_ep_attr(info->ep_attr,
-						      info->tx_attr,
-						      info->rx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-
-		if (info->tx_attr) {
-			ret = sock_dgram_verify_tx_attr(info->tx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-
-		if (info->rx_attr) {
-			ret = sock_dgram_verify_rx_attr(info->rx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-	}
-
-	ret = sock_alloc_endpoint(domain, info, ep, context, fclass);
-	if (ret)
-		return ret;
-
-	if (!info || !info->ep_attr)
-		(*ep)->attr->ep_attr = sock_dgram_ep_attr;
-
-	if (!info || !info->tx_attr)
-		(*ep)->tx_attr = sock_dgram_tx_attr;
-
-	if (!info || !info->rx_attr)
-		(*ep)->rx_attr = sock_dgram_rx_attr;
-
-	return 0;
-}
 
 int sock_dgram_ep(struct fid_domain *domain, struct fi_info *info,
 		struct fid_ep **ep, void *context)
@@ -292,7 +63,7 @@ int sock_dgram_ep(struct fid_domain *domain, struct fi_info *info,
 	int ret;
 	struct sock_ep *endpoint;
 
-	ret = sock_dgram_endpoint(domain, info, &endpoint, context, FI_CLASS_EP);
+	ret = sock_alloc_endpoint(domain, info, &endpoint, context, FI_CLASS_EP);
 	if (ret)
 		return ret;
 
@@ -306,7 +77,7 @@ int sock_dgram_sep(struct fid_domain *domain, struct fi_info *info,
 	int ret;
 	struct sock_ep *endpoint;
 
-	ret = sock_dgram_endpoint(domain, info, &endpoint, context, FI_CLASS_SEP);
+	ret = sock_alloc_endpoint(domain, info, &endpoint, context, FI_CLASS_SEP);
 	if (ret)
 		return ret;
 
diff --git a/prov/sockets/src/sock_ep_msg.c b/prov/sockets/src/sock_ep_msg.c
index 6b84b40..594650c 100644
--- a/prov/sockets/src/sock_ep_msg.c
+++ b/prov/sockets/src/sock_ep_msg.c
@@ -59,195 +59,6 @@
 #define SOCK_LOG_DBG(...) _SOCK_LOG_DBG(FI_LOG_EP_CTRL, __VA_ARGS__)
 #define SOCK_LOG_ERROR(...) _SOCK_LOG_ERROR(FI_LOG_EP_CTRL, __VA_ARGS__)
 
-static const struct fi_ep_attr sock_msg_ep_attr = {
-	.type = FI_EP_MSG,
-	.protocol = FI_PROTO_SOCK_TCP,
-	.protocol_version = SOCK_WIRE_PROTO_VERSION,
-	.max_msg_size = SOCK_EP_MAX_MSG_SZ,
-	.msg_prefix_size = SOCK_EP_MSG_PREFIX_SZ,
-	.max_order_raw_size = SOCK_EP_MAX_ORDER_RAW_SZ,
-	.max_order_war_size = SOCK_EP_MAX_ORDER_WAR_SZ,
-	.max_order_waw_size = SOCK_EP_MAX_ORDER_WAW_SZ,
-	.mem_tag_format = SOCK_EP_MEM_TAG_FMT,
-	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
-	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
-};
-
-static const struct fi_tx_attr sock_msg_tx_attr = {
-	.caps = SOCK_EP_MSG_CAP_BASE,
-	.mode = SOCK_MODE,
-	.op_flags = SOCK_EP_DEFAULT_OP_FLAGS,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.inject_size = SOCK_EP_MAX_INJECT_SZ,
-	.size = SOCK_EP_TX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-	.rma_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
-static const struct fi_rx_attr sock_msg_rx_attr = {
-	.caps = SOCK_EP_MSG_CAP_BASE,
-	.mode = SOCK_MODE,
-	.op_flags = 0,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.comp_order = SOCK_EP_COMP_ORDER,
-	.total_buffered_recv = SOCK_EP_MAX_BUFF_RECV,
-	.size = SOCK_EP_RX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
-static int sock_msg_verify_rx_attr(const struct fi_rx_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if ((attr->caps | SOCK_EP_MSG_CAP) != SOCK_EP_MSG_CAP)
-		return -FI_ENODATA;
-
-	if ((attr->msg_order | SOCK_EP_MSG_ORDER) != SOCK_EP_MSG_ORDER)
-		return -FI_ENODATA;
-
-	if ((attr->comp_order | SOCK_EP_COMP_ORDER) != SOCK_EP_COMP_ORDER)
-		return -FI_ENODATA;
-
-	if (attr->total_buffered_recv > sock_msg_rx_attr.total_buffered_recv)
-		return -FI_ENODATA;
-
-	if (sock_get_tx_size(attr->size) >
-	     sock_get_tx_size(sock_msg_rx_attr.size))
-		return -FI_ENODATA;
-
-	if (attr->iov_limit > sock_msg_rx_attr.iov_limit)
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-static int sock_msg_verify_tx_attr(const struct fi_tx_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if ((attr->caps | SOCK_EP_MSG_CAP) != SOCK_EP_MSG_CAP)
-		return -FI_ENODATA;
-
-	if ((attr->msg_order | SOCK_EP_MSG_ORDER) != SOCK_EP_MSG_ORDER)
-		return -FI_ENODATA;
-
-	if (attr->inject_size > sock_msg_tx_attr.inject_size)
-		return -FI_ENODATA;
-
-	if (sock_get_tx_size(attr->size) >
-	     sock_get_tx_size(sock_msg_tx_attr.size))
-		return -FI_ENODATA;
-
-	if (attr->iov_limit > sock_msg_tx_attr.iov_limit)
-		return -FI_ENODATA;
-
-	if (attr->rma_iov_limit > sock_msg_tx_attr.rma_iov_limit)
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-int sock_msg_verify_ep_attr(const struct fi_ep_attr *ep_attr,
-			    const struct fi_tx_attr *tx_attr,
-			    const struct fi_rx_attr *rx_attr)
-{
-	if (ep_attr) {
-		switch (ep_attr->protocol) {
-		case FI_PROTO_UNSPEC:
-		case FI_PROTO_SOCK_TCP:
-			break;
-		default:
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->protocol_version &&
-		    (ep_attr->protocol_version != sock_msg_ep_attr.protocol_version))
-			return -FI_ENODATA;
-
-		if (ep_attr->max_msg_size > sock_msg_ep_attr.max_msg_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->msg_prefix_size > sock_msg_ep_attr.msg_prefix_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->max_order_raw_size >
-		   sock_msg_ep_attr.max_order_raw_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->max_order_war_size >
-		   sock_msg_ep_attr.max_order_war_size)
-			return -FI_ENODATA;
-
-		if (ep_attr->max_order_waw_size >
-		   sock_msg_ep_attr.max_order_waw_size)
-			return -FI_ENODATA;
-
-		if ((ep_attr->tx_ctx_cnt > SOCK_EP_MAX_TX_CNT) &&
-		    ep_attr->tx_ctx_cnt != FI_SHARED_CONTEXT)
-			return -FI_ENODATA;
-
-		if ((ep_attr->rx_ctx_cnt > SOCK_EP_MAX_RX_CNT) &&
-		    ep_attr->rx_ctx_cnt != FI_SHARED_CONTEXT)
-			return -FI_ENODATA;
-
-		if (ep_attr->auth_key_size &&
-		    (ep_attr->auth_key_size != sock_msg_ep_attr.auth_key_size))
-			return -FI_ENODATA;
-	}
-
-	if (sock_msg_verify_tx_attr(tx_attr) || sock_msg_verify_rx_attr(rx_attr))
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-int sock_msg_fi_info(uint32_t version, void *src_addr, void *dest_addr,
-		     const struct fi_info *hints, struct fi_info **info)
-{
-	*info = sock_fi_info(version, FI_EP_MSG, hints, src_addr, dest_addr);
-	if (!*info)
-		return -FI_ENOMEM;
-
-	*(*info)->tx_attr = sock_msg_tx_attr;
-	(*info)->tx_attr->size = sock_get_tx_size(sock_msg_tx_attr.size);
-	*(*info)->rx_attr = sock_msg_rx_attr;
-	(*info)->rx_attr->size = sock_get_tx_size(sock_msg_rx_attr.size);
-	*(*info)->ep_attr = sock_msg_ep_attr;
-
-	if (hints && hints->ep_attr) {
-		if (hints->ep_attr->rx_ctx_cnt)
-			(*info)->ep_attr->rx_ctx_cnt = hints->ep_attr->rx_ctx_cnt;
-		if (hints->ep_attr->tx_ctx_cnt)
-			(*info)->ep_attr->tx_ctx_cnt = hints->ep_attr->tx_ctx_cnt;
-	}
-
-	if (hints && hints->rx_attr) {
-		(*info)->rx_attr->op_flags |= hints->rx_attr->op_flags;
-		if (hints->rx_attr->caps)
-			(*info)->rx_attr->caps = SOCK_EP_MSG_SEC_CAP |
-							hints->rx_attr->caps;
-	}
-
-	if (hints && hints->tx_attr) {
-		(*info)->tx_attr->op_flags |= hints->tx_attr->op_flags;
-		if (hints->tx_attr->caps)
-			(*info)->tx_attr->caps = SOCK_EP_MSG_SEC_CAP |
-							hints->tx_attr->caps;
-	}
-
-	(*info)->caps = SOCK_EP_MSG_CAP |
-		(*info)->rx_attr->caps | (*info)->tx_attr->caps;
-	if (hints && hints->caps) {
-		(*info)->caps = SOCK_EP_MSG_SEC_CAP | hints->caps;
-		(*info)->rx_attr->caps = SOCK_EP_MSG_SEC_CAP |
-			((*info)->rx_attr->caps & (*info)->caps);
-		(*info)->tx_attr->caps = SOCK_EP_MSG_SEC_CAP |
-			((*info)->tx_attr->caps & (*info)->caps);
-	}
-	return 0;
-}
 
 static int sock_ep_cm_getname(fid_t fid, void *addr, size_t *addrlen)
 {
@@ -418,7 +229,7 @@ static void sock_ep_cm_monitor_handle(struct sock_ep_cm_head *cm_head,
 
 	/* Mark the handle as monitored before adding it to the pollset */
 	handle->monitored = 1;
-	ret = fi_epoll_add(cm_head->emap, handle->sock_fd,
+	ret = ofi_epoll_add(cm_head->emap, handle->sock_fd,
 	                   events, handle);
 	if (ret) {
 		SOCK_LOG_ERROR("failed to monitor fd %d: %d\n",
@@ -439,7 +250,7 @@ sock_ep_cm_unmonitor_handle_locked(struct sock_ep_cm_head *cm_head,
 	int ret;
 
 	if (handle->monitored) {
-		ret = fi_epoll_del(cm_head->emap, handle->sock_fd);
+		ret = ofi_epoll_del(cm_head->emap, handle->sock_fd);
 		if (ret)
 			SOCK_LOG_ERROR("failed to unmonitor fd %d: %d\n",
 			               handle->sock_fd, ret);
@@ -717,7 +528,7 @@ static int sock_ep_cm_connect(struct fid_ep *ep, const void *addr,
 	/* Monitor the connection */
 	_ep->attr->cm.state = SOCK_CM_STATE_REQUESTED;
 	handle->sock_fd = sock_fd;
-	sock_ep_cm_monitor_handle(cm_head, handle, FI_EPOLL_IN);
+	sock_ep_cm_monitor_handle(cm_head, handle, OFI_EPOLL_IN);
 
 	return 0;
 close_socket:
@@ -782,7 +593,7 @@ static int sock_ep_cm_accept(struct fid_ep *ep, const void *param, size_t paraml
 		}
 	}
 	/* Monitor the handle prior to report the event */
-	sock_ep_cm_monitor_handle(cm_head, handle, FI_EPOLL_IN);
+	sock_ep_cm_monitor_handle(cm_head, handle, OFI_EPOLL_IN);
 	sock_ep_enable(ep);
 
 	memset(&cm_entry, 0, sizeof(cm_entry));
@@ -823,65 +634,22 @@ struct fi_ops_cm sock_ep_cm_ops = {
 	.join = fi_no_join,
 };
 
-static int sock_msg_endpoint(struct fid_domain *domain, struct fi_info *info,
-		struct sock_ep **ep, void *context, size_t fclass)
+int sock_msg_ep(struct fid_domain *domain, struct fi_info *info,
+		struct fid_ep **ep, void *context)
 {
-	int ret;
+	struct sock_ep *endpoint;
 	struct sock_pep *pep;
+	int ret;
 
-	if (info) {
-		if (info->ep_attr) {
-			ret = sock_msg_verify_ep_attr(info->ep_attr,
-						      info->tx_attr,
-						      info->rx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-
-		if (info->tx_attr) {
-			ret = sock_msg_verify_tx_attr(info->tx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-
-		if (info->rx_attr) {
-			ret = sock_msg_verify_rx_attr(info->rx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-	}
-
-	ret = sock_alloc_endpoint(domain, info, ep, context, fclass);
+	ret = sock_alloc_endpoint(domain, info, &endpoint, context, FI_CLASS_EP);
 	if (ret)
 		return ret;
 
 	if (info && info->handle && info->handle->fclass == FI_CLASS_PEP) {
 		pep = container_of(info->handle, struct sock_pep, pep.fid);
-		memcpy((*ep)->attr->src_addr, &pep->src_addr, sizeof *(*ep)->attr->src_addr);
+		*endpoint->attr->src_addr = pep->src_addr;
 	}
 
-	if (!info || !info->ep_attr)
-		(*ep)->attr->ep_attr = sock_msg_ep_attr;
-
-	if (!info || !info->tx_attr)
-		(*ep)->tx_attr = sock_msg_tx_attr;
-
-	if (!info || !info->rx_attr)
-		(*ep)->rx_attr = sock_msg_rx_attr;
-
-	return 0;
-}
-
-int sock_msg_ep(struct fid_domain *domain, struct fi_info *info,
-		struct fid_ep **ep, void *context)
-{
-	int ret;
-	struct sock_ep *endpoint;
-
-	ret = sock_msg_endpoint(domain, info, &endpoint, context, FI_CLASS_EP);
-	if (ret)
-		return ret;
-
 	*ep = &endpoint->ep;
 	return 0;
 }
@@ -945,8 +713,8 @@ static struct fi_info *sock_ep_msg_get_info(struct sock_pep *pep,
 	struct fi_info hints;
 	uint64_t requested, supported;
 
-	requested = req->caps & SOCK_EP_MSG_PRI_CAP;
-	supported = pep->info.caps & SOCK_EP_MSG_PRI_CAP;
+	requested = req->caps & sock_msg_info.caps;
+	supported = pep->info.caps & sock_msg_info.caps;
 	supported = (supported & FI_RMA) ?
 		(supported | FI_REMOTE_READ | FI_REMOTE_WRITE) : supported;
 	if ((requested | supported) != supported)
@@ -1172,7 +940,7 @@ static void *sock_pep_listener_thread(void *data)
 		handle->pep = pep;
 
 		/* Monitor the connection */
-		sock_ep_cm_monitor_handle(&pep->cm_head, handle, FI_EPOLL_IN);
+		sock_ep_cm_monitor_handle(&pep->cm_head, handle, OFI_EPOLL_IN);
 	}
 
 	SOCK_LOG_DBG("PEP listener thread exiting\n");
@@ -1284,7 +1052,7 @@ int sock_msg_sep(struct fid_domain *domain, struct fi_info *info,
 	int ret;
 	struct sock_ep *endpoint;
 
-	ret = sock_msg_endpoint(domain, info, &endpoint, context, FI_CLASS_SEP);
+	ret = sock_alloc_endpoint(domain, info, &endpoint, context, FI_CLASS_SEP);
 	if (ret)
 		return ret;
 
@@ -1299,52 +1067,40 @@ int sock_msg_passive_ep(struct fid_fabric *fabric, struct fi_info *info,
 	struct sock_pep *_pep;
 	struct addrinfo hints, *result;
 
-	if (info) {
-		ret = sock_verify_info(fabric->api_version, info);
-		if (ret) {
-			SOCK_LOG_DBG("Cannot support requested options!\n");
-			return ret;
-		}
-	}
-
+	assert(info);
 	_pep = calloc(1, sizeof(*_pep));
 	if (!_pep)
 		return -FI_ENOMEM;
 
-	if (info) {
-		if (info->src_addr) {
-			memcpy(&_pep->src_addr, info->src_addr,
-				info->src_addrlen);
+	if (info->src_addr) {
+		memcpy(&_pep->src_addr, info->src_addr,
+			info->src_addrlen);
+	} else {
+		memset(&hints, 0, sizeof(hints));
+		hints.ai_socktype = SOCK_STREAM;
+		hints.ai_family = ofi_get_sa_family(info);
+		if (!hints.ai_family)
+			hints.ai_family = AF_INET;
+
+		if (hints.ai_family == AF_INET) {
+			ret = getaddrinfo("127.0.0.1", NULL, &hints,
+						&result);
+		} else if (hints.ai_family == AF_INET6) {
+			ret = getaddrinfo("::1", NULL, &hints, &result);
 		} else {
-			memset(&hints, 0, sizeof(hints));
-			hints.ai_socktype = SOCK_STREAM;
-			hints.ai_family = ofi_get_sa_family(info);
-			if (!hints.ai_family)
-				hints.ai_family = AF_INET;
-
-			if (hints.ai_family == AF_INET) {
-				ret = getaddrinfo("127.0.0.1", NULL, &hints,
-						  &result);
-			} else if (hints.ai_family == AF_INET6) {
-				ret = getaddrinfo("::1", NULL, &hints, &result);
-			} else {
-				ret = getaddrinfo("localhost", NULL, &hints,
-						  &result);
-			}
-			if (ret) {
-				ret = -FI_EINVAL;
-				SOCK_LOG_DBG("getaddrinfo failed!\n");
-				goto err;
-			}
-			memcpy(&_pep->src_addr, result->ai_addr,
-				result->ai_addrlen);
-			freeaddrinfo(result);
+			ret = getaddrinfo("localhost", NULL, &hints,
+						&result);
 		}
-		_pep->info = *info;
-	} else {
-		SOCK_LOG_ERROR("invalid fi_info\n");
-		goto err;
+		if (ret) {
+			ret = -FI_EINVAL;
+			SOCK_LOG_DBG("getaddrinfo failed!\n");
+			goto err;
+		}
+		memcpy(&_pep->src_addr, result->ai_addr,
+			result->ai_addrlen);
+		freeaddrinfo(result);
 	}
+	_pep->info = *info;
 
 	ret = socketpair(AF_UNIX, SOCK_STREAM, 0, _pep->cm.signal_fds);
 	if (ret) {
@@ -1410,7 +1166,7 @@ static void *sock_ep_cm_thread(void *arg)
 	while (cm_head->do_listen) {
 		sock_ep_cm_check_closing_rejected_list(cm_head);
 
-		num_fds = fi_epoll_wait(cm_head->emap, ep_contexts,
+		num_fds = ofi_epoll_wait(cm_head->emap, ep_contexts,
 		                        SOCK_EPOLL_WAIT_EVENTS, -1);
 		if (num_fds < 0) {
 			SOCK_LOG_ERROR("poll failed : %s\n", strerror(errno));
@@ -1452,7 +1208,7 @@ int sock_ep_cm_start_thread(struct sock_ep_cm_head *cm_head)
 	fastlock_init(&cm_head->signal_lock);
 	dlist_init(&cm_head->msg_list);
 
-	int ret = fi_epoll_create(&cm_head->emap);
+	int ret = ofi_epoll_create(&cm_head->emap);
 	if (ret < 0) {
 		SOCK_LOG_ERROR("failed to create epoll set\n");
 		goto err1;
@@ -1465,9 +1221,9 @@ int sock_ep_cm_start_thread(struct sock_ep_cm_head *cm_head)
 		goto err2;
 	}
 
-	ret = fi_epoll_add(cm_head->emap,
+	ret = ofi_epoll_add(cm_head->emap,
 	                   cm_head->signal.fd[FI_READ_FD],
-	                   FI_EPOLL_IN, NULL);
+	                   OFI_EPOLL_IN, NULL);
 	if (ret != 0){
 		SOCK_LOG_ERROR("failed to add signal fd to epoll\n");
 		goto err3;
@@ -1486,7 +1242,7 @@ err3:
 	cm_head->do_listen = 0;
 	fd_signal_free(&cm_head->signal);
 err2:
-	fi_epoll_close(cm_head->emap);
+	ofi_epoll_close(cm_head->emap);
 err1:
 	return ret;
 }
@@ -1519,7 +1275,7 @@ void sock_ep_cm_stop_thread(struct sock_ep_cm_head *cm_head)
 			pthread_join(cm_head->listener_thread, NULL)) {
 		SOCK_LOG_DBG("pthread join failed\n");
 	}
-	fi_epoll_close(cm_head->emap);
+	ofi_epoll_close(cm_head->emap);
 	fd_signal_free(&cm_head->signal);
 	fastlock_destroy(&cm_head->signal_lock);
 }
diff --git a/prov/sockets/src/sock_ep_rdm.c b/prov/sockets/src/sock_ep_rdm.c
index 700f6b9..ede4bba 100644
--- a/prov/sockets/src/sock_ep_rdm.c
+++ b/prov/sockets/src/sock_ep_rdm.c
@@ -57,278 +57,6 @@
 #define SOCK_LOG_DBG(...) _SOCK_LOG_DBG(FI_LOG_EP_CTRL, __VA_ARGS__)
 #define SOCK_LOG_ERROR(...) _SOCK_LOG_ERROR(FI_LOG_EP_CTRL, __VA_ARGS__)
 
-const struct fi_ep_attr sock_rdm_ep_attr = {
-	.type = FI_EP_RDM,
-	.protocol = FI_PROTO_SOCK_TCP,
-	.protocol_version = SOCK_WIRE_PROTO_VERSION,
-	.max_msg_size = SOCK_EP_MAX_MSG_SZ,
-	.msg_prefix_size = SOCK_EP_MSG_PREFIX_SZ,
-	.max_order_raw_size = SOCK_EP_MAX_ORDER_RAW_SZ,
-	.max_order_war_size = SOCK_EP_MAX_ORDER_WAR_SZ,
-	.max_order_waw_size = SOCK_EP_MAX_ORDER_WAW_SZ,
-	.mem_tag_format = SOCK_EP_MEM_TAG_FMT,
-	.tx_ctx_cnt = SOCK_EP_MAX_TX_CNT,
-	.rx_ctx_cnt = SOCK_EP_MAX_RX_CNT,
-};
-
-const struct fi_tx_attr sock_rdm_tx_attr = {
-	.caps = SOCK_EP_RDM_CAP_BASE,
-	.mode = SOCK_MODE,
-	.op_flags = SOCK_EP_DEFAULT_OP_FLAGS,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.inject_size = SOCK_EP_MAX_INJECT_SZ,
-	.size = SOCK_EP_TX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-	.rma_iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
-const struct fi_rx_attr sock_rdm_rx_attr = {
-	.caps = SOCK_EP_RDM_CAP_BASE,
-	.mode = SOCK_MODE,
-	.op_flags = 0,
-	.msg_order = SOCK_EP_MSG_ORDER,
-	.comp_order = SOCK_EP_COMP_ORDER,
-	.total_buffered_recv = SOCK_EP_MAX_BUFF_RECV,
-	.size = SOCK_EP_RX_SZ,
-	.iov_limit = SOCK_EP_MAX_IOV_LIMIT,
-};
-
-static int sock_rdm_verify_rx_attr(const struct fi_rx_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if ((attr->caps | SOCK_EP_RDM_CAP) != SOCK_EP_RDM_CAP) {
-		SOCK_LOG_DBG("Unsupported RDM rx caps\n");
-		return -FI_ENODATA;
-	}
-
-	if ((attr->msg_order | SOCK_EP_MSG_ORDER) != SOCK_EP_MSG_ORDER) {
-		SOCK_LOG_DBG("Unsuported rx message order\n");
-		return -FI_ENODATA;
-	}
-
-	if ((attr->comp_order | SOCK_EP_COMP_ORDER) != SOCK_EP_COMP_ORDER) {
-		SOCK_LOG_DBG("Unsuported rx completion order\n");
-		return -FI_ENODATA;
-	}
-
-	if (attr->total_buffered_recv > sock_rdm_rx_attr.total_buffered_recv) {
-		SOCK_LOG_DBG("Buffered receive size too large\n");
-		return -FI_ENODATA;
-	}
-
-	if (sock_get_tx_size(attr->size) >
-	     sock_get_tx_size(sock_rdm_rx_attr.size)) {
-		SOCK_LOG_DBG("Rx size too large\n");
-		return -FI_ENODATA;
-	}
-
-	if (attr->iov_limit > sock_rdm_rx_attr.iov_limit) {
-		SOCK_LOG_DBG("Rx iov limit too large\n");
-		return -FI_ENODATA;
-	}
-
-	return 0;
-}
-
-static int sock_rdm_verify_tx_attr(const struct fi_tx_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if ((attr->caps | SOCK_EP_RDM_CAP) != SOCK_EP_RDM_CAP) {
-		SOCK_LOG_DBG("Unsupported RDM tx caps\n");
-		return -FI_ENODATA;
-	}
-
-	if ((attr->msg_order | SOCK_EP_MSG_ORDER) != SOCK_EP_MSG_ORDER) {
-		SOCK_LOG_DBG("Unsupported tx message order\n");
-		return -FI_ENODATA;
-	}
-
-	if (attr->inject_size > sock_rdm_tx_attr.inject_size) {
-		SOCK_LOG_DBG("Inject size too large\n");
-		return -FI_ENODATA;
-	}
-
-	if (sock_get_tx_size(attr->size) >
-	     sock_get_tx_size(sock_rdm_tx_attr.size)) {
-		SOCK_LOG_DBG("Tx size too large\n");
-		return -FI_ENODATA;
-	}
-
-	if (attr->iov_limit > sock_rdm_tx_attr.iov_limit) {
-		SOCK_LOG_DBG("Tx iov limit too large\n");
-		return -FI_ENODATA;
-	}
-
-	if (attr->rma_iov_limit > sock_rdm_tx_attr.rma_iov_limit) {
-		SOCK_LOG_DBG("RMA iov limit too large\n");
-		return -FI_ENODATA;
-	}
-
-	return 0;
-}
-
-int sock_rdm_verify_ep_attr(const struct fi_ep_attr *ep_attr,
-			    const struct fi_tx_attr *tx_attr,
-			    const struct fi_rx_attr *rx_attr)
-{
-	int ret;
-
-	if (ep_attr) {
-		switch (ep_attr->protocol) {
-		case FI_PROTO_UNSPEC:
-		case FI_PROTO_SOCK_TCP:
-			break;
-		default:
-			SOCK_LOG_DBG("Unsupported protocol\n");
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->protocol_version &&
-		    (ep_attr->protocol_version != sock_rdm_ep_attr.protocol_version)) {
-			SOCK_LOG_DBG("Invalid protocol version\n");
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->max_msg_size > sock_rdm_ep_attr.max_msg_size) {
-			SOCK_LOG_DBG("Message size too large\n");
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->msg_prefix_size > sock_rdm_ep_attr.msg_prefix_size) {
-			SOCK_LOG_DBG("Msg prefix size not supported\n");
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->max_order_raw_size >
-		   sock_rdm_ep_attr.max_order_raw_size) {
-			SOCK_LOG_DBG("RAW order size too large\n");
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->max_order_war_size >
-		   sock_rdm_ep_attr.max_order_war_size) {
-			SOCK_LOG_DBG("WAR order size too large\n");
-			return -FI_ENODATA;
-		}
-
-		if (ep_attr->max_order_waw_size >
-		   sock_rdm_ep_attr.max_order_waw_size) {
-			SOCK_LOG_DBG("WAW order size too large\n");
-			return -FI_ENODATA;
-		}
-
-		if ((ep_attr->tx_ctx_cnt > SOCK_EP_MAX_TX_CNT) &&
-		    ep_attr->tx_ctx_cnt != FI_SHARED_CONTEXT)
-			return -FI_ENODATA;
-
-		if ((ep_attr->rx_ctx_cnt > SOCK_EP_MAX_RX_CNT) &&
-		    ep_attr->rx_ctx_cnt != FI_SHARED_CONTEXT)
-			return -FI_ENODATA;
-	}
-
-	ret = sock_rdm_verify_tx_attr(tx_attr);
-	if (ret)
-		return ret;
-
-	ret = sock_rdm_verify_rx_attr(rx_attr);
-	if (ret)
-		return ret;
-
-	return 0;
-}
-
-int sock_rdm_fi_info(uint32_t version, void *src_addr, void *dest_addr,
-		     const struct fi_info *hints, struct fi_info **info)
-{
-	*info = sock_fi_info(version, FI_EP_RDM, hints, src_addr, dest_addr);
-	if (!*info)
-		return -FI_ENOMEM;
-
-	*(*info)->tx_attr = sock_rdm_tx_attr;
-	(*info)->tx_attr->size = sock_get_tx_size(sock_rdm_tx_attr.size);
-	*(*info)->rx_attr = sock_rdm_rx_attr;
-	(*info)->rx_attr->size = sock_get_tx_size(sock_rdm_rx_attr.size);
-	*(*info)->ep_attr = sock_rdm_ep_attr;
-
-	if (hints && hints->ep_attr) {
-		if (hints->ep_attr->rx_ctx_cnt)
-			(*info)->ep_attr->rx_ctx_cnt = hints->ep_attr->rx_ctx_cnt;
-		if (hints->ep_attr->tx_ctx_cnt)
-			(*info)->ep_attr->tx_ctx_cnt = hints->ep_attr->tx_ctx_cnt;
-	}
-
-	if (hints && hints->rx_attr) {
-		(*info)->rx_attr->op_flags |= hints->rx_attr->op_flags;
-		if (hints->rx_attr->caps)
-			(*info)->rx_attr->caps = SOCK_EP_RDM_SEC_CAP |
-							hints->rx_attr->caps;
-	}
-
-	if (hints && hints->tx_attr) {
-		(*info)->tx_attr->op_flags |= hints->tx_attr->op_flags;
-		if (hints->tx_attr->caps)
-			(*info)->tx_attr->caps = SOCK_EP_RDM_SEC_CAP |
-							hints->tx_attr->caps;
-	}
-
-	(*info)->caps = SOCK_EP_RDM_CAP |
-			(*info)->rx_attr->caps | (*info)->tx_attr->caps;
-	if (hints && hints->caps) {
-		(*info)->caps = SOCK_EP_RDM_SEC_CAP | hints->caps;
-		(*info)->rx_attr->caps = SOCK_EP_RDM_SEC_CAP |
-			((*info)->rx_attr->caps & (*info)->caps);
-		(*info)->tx_attr->caps = SOCK_EP_RDM_SEC_CAP |
-			((*info)->tx_attr->caps & (*info)->caps);
-	}
-	return 0;
-}
-
-static int sock_rdm_endpoint(struct fid_domain *domain, struct fi_info *info,
-		struct sock_ep **ep, void *context, size_t fclass)
-{
-	int ret;
-
-	if (info) {
-		if (info->ep_attr) {
-			ret = sock_rdm_verify_ep_attr(info->ep_attr,
-						      info->tx_attr,
-						      info->rx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-
-		if (info->tx_attr) {
-			ret = sock_rdm_verify_tx_attr(info->tx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-
-		if (info->rx_attr) {
-			ret = sock_rdm_verify_rx_attr(info->rx_attr);
-			if (ret)
-				return -FI_EINVAL;
-		}
-	}
-
-	ret = sock_alloc_endpoint(domain, info, ep, context, fclass);
-	if (ret)
-		return ret;
-
-	if (!info || !info->ep_attr)
-		(*ep)->attr->ep_attr = sock_rdm_ep_attr;
-
-	if (!info || !info->tx_attr)
-		(*ep)->tx_attr = sock_rdm_tx_attr;
-
-	if (!info || !info->rx_attr)
-		(*ep)->rx_attr = sock_rdm_rx_attr;
-
-	return 0;
-}
 
 int sock_rdm_ep(struct fid_domain *domain, struct fi_info *info,
 		struct fid_ep **ep, void *context)
@@ -336,7 +64,7 @@ int sock_rdm_ep(struct fid_domain *domain, struct fi_info *info,
 	int ret;
 	struct sock_ep *endpoint;
 
-	ret = sock_rdm_endpoint(domain, info, &endpoint, context, FI_CLASS_EP);
+	ret = sock_alloc_endpoint(domain, info, &endpoint, context, FI_CLASS_EP);
 	if (ret)
 		return ret;
 
@@ -350,11 +78,10 @@ int sock_rdm_sep(struct fid_domain *domain, struct fi_info *info,
 	int ret;
 	struct sock_ep *endpoint;
 
-	ret = sock_rdm_endpoint(domain, info, &endpoint, context, FI_CLASS_SEP);
+	ret = sock_alloc_endpoint(domain, info, &endpoint, context, FI_CLASS_SEP);
 	if (ret)
 		return ret;
 
 	*sep = &endpoint->ep;
 	return 0;
 }
-
diff --git a/prov/sockets/src/sock_fabric.c b/prov/sockets/src/sock_fabric.c
index 3a60310..53f4591 100644
--- a/prov/sockets/src/sock_fabric.c
+++ b/prov/sockets/src/sock_fabric.c
@@ -66,23 +66,9 @@ int sock_keepalive_time = INT_MAX;
 int sock_keepalive_intvl = INT_MAX;
 int sock_keepalive_probes = INT_MAX;
 
-uint64_t SOCK_EP_RDM_SEC_CAP = SOCK_EP_RDM_SEC_CAP_BASE;
-uint64_t SOCK_EP_RDM_CAP = SOCK_EP_RDM_CAP_BASE;
-uint64_t SOCK_EP_MSG_SEC_CAP = SOCK_EP_MSG_SEC_CAP_BASE;
-uint64_t SOCK_EP_MSG_CAP = SOCK_EP_MSG_CAP_BASE;
-
-
-const struct fi_fabric_attr sock_fabric_attr = {
-	.fabric = NULL,
-	.name = NULL,
-	.prov_name = NULL,
-	.prov_version = FI_VERSION(SOCK_MAJOR_VERSION, SOCK_MINOR_VERSION),
-};
-
 static struct dlist_entry sock_fab_list;
 static struct dlist_entry sock_dom_list;
 static fastlock_t sock_list_lock;
-static struct slist sock_addr_list;
 static int read_default_params;
 
 void sock_dom_add_to_list(struct sock_domain *domain)
@@ -206,101 +192,6 @@ struct sock_fabric *sock_fab_list_head(void)
 	return fabric;
 }
 
-int sock_verify_fabric_attr(const struct fi_fabric_attr *attr)
-{
-	if (!attr)
-		return 0;
-
-	if (attr->prov_version) {
-		if (attr->prov_version !=
-		   FI_VERSION(SOCK_MAJOR_VERSION, SOCK_MINOR_VERSION))
-			return -FI_ENODATA;
-	}
-
-	return 0;
-}
-
-int sock_verify_info(uint32_t version, const struct fi_info *hints)
-{
-	uint64_t caps;
-	enum fi_ep_type ep_type;
-	int ret;
-	struct sock_domain *domain;
-	struct sock_fabric *fabric;
-
-	if (!hints)
-		return 0;
-
-	ep_type = hints->ep_attr ? hints->ep_attr->type : FI_EP_UNSPEC;
-	switch (ep_type) {
-	case FI_EP_UNSPEC:
-	case FI_EP_MSG:
-		caps = SOCK_EP_MSG_CAP;
-		ret = sock_msg_verify_ep_attr(hints->ep_attr,
-					      hints->tx_attr,
-					      hints->rx_attr);
-		break;
-	case FI_EP_DGRAM:
-		caps = SOCK_EP_DGRAM_CAP;
-		ret = sock_dgram_verify_ep_attr(hints->ep_attr,
-						hints->tx_attr,
-						hints->rx_attr);
-		break;
-	case FI_EP_RDM:
-		caps = SOCK_EP_RDM_CAP;
-		ret = sock_rdm_verify_ep_attr(hints->ep_attr,
-					      hints->tx_attr,
-					      hints->rx_attr);
-		break;
-	default:
-		ret = -FI_ENODATA;
-	}
-	if (ret)
-		return ret;
-
-	if ((caps | hints->caps) != caps) {
-		SOCK_LOG_DBG("Unsupported capabilities\n");
-		return -FI_ENODATA;
-	}
-
-	switch (hints->addr_format) {
-	case FI_FORMAT_UNSPEC:
-	case FI_SOCKADDR:
-	case FI_SOCKADDR_IN:
-	case FI_SOCKADDR_IN6:
-		break;
-	default:
-		SOCK_LOG_DBG("Unsupported address format\n");
-		return -FI_ENODATA;
-	}
-
-	if (hints->domain_attr && hints->domain_attr->domain) {
-		domain = container_of(hints->domain_attr->domain,
-				      struct sock_domain, dom_fid);
-		if (!sock_dom_check_list(domain)) {
-			SOCK_LOG_DBG("no matching domain\n");
-			return -FI_ENODATA;
-		}
-	}
-	ret = sock_verify_domain_attr(version, hints);
-	if (ret)
-		return ret;
-
-	if (hints->fabric_attr && hints->fabric_attr->fabric) {
-		fabric = container_of(hints->fabric_attr->fabric,
-				      struct sock_fabric, fab_fid);
-		if (!sock_fab_check_list(fabric)) {
-			SOCK_LOG_DBG("no matching fabric\n");
-			return -FI_ENODATA;
-		}
-	}
-	ret = sock_verify_fabric_attr(hints->fabric_attr);
-	if (ret)
-		return ret;
-
-	return 0;
-}
-
 static int sock_trywait(struct fid_fabric *fabric, struct fid **fids, int count)
 {
 	/* we're always ready to wait! */
@@ -417,275 +308,16 @@ out:
 	return ret;
 }
 
-static int sock_fi_checkinfo(const struct fi_info *info,
-			     const struct fi_info *hints)
-{
-	if (hints && hints->domain_attr && hints->domain_attr->name &&
-             strcmp(info->domain_attr->name, hints->domain_attr->name))
-		return -FI_ENODATA;
-
-	if (hints && hints->fabric_attr && hints->fabric_attr->name &&
-             strcmp(info->fabric_attr->name, hints->fabric_attr->name))
-		return -FI_ENODATA;
-
-	return 0;
-}
-
-static int sock_ep_getinfo(uint32_t version, const char *node,
-			   const char *service, uint64_t flags,
-			   const struct fi_info *hints, enum fi_ep_type ep_type,
-			   struct fi_info **info)
-{
-	struct addrinfo ai, *rai = NULL;
-	union ofi_sock_ip *src_addr = NULL, *dest_addr = NULL;
-	union ofi_sock_ip sip;
-	int ret;
-
-	memset(&ai, 0, sizeof(ai));
-	ai.ai_socktype = SOCK_STREAM;
-	ai.ai_family = ofi_get_sa_family(hints);
-	if (flags & FI_NUMERICHOST)
-		ai.ai_flags |= AI_NUMERICHOST;
-
-	if (flags & FI_SOURCE) {
-		ai.ai_flags |= AI_PASSIVE;
-		ret = getaddrinfo(node, service, &ai, &rai);
-		if (ret) {
-			SOCK_LOG_DBG("getaddrinfo failed!\n");
-			return -FI_ENODATA;
-		}
-		src_addr = (union ofi_sock_ip *) rai->ai_addr;
-		if (hints && hints->dest_addr)
-			dest_addr = hints->dest_addr;
-	} else {
-		if (node || service) {
-			ret = getaddrinfo(node, service, &ai, &rai);
-			if (ret) {
-				SOCK_LOG_DBG("getaddrinfo failed!\n");
-				return -FI_ENODATA;
-			}
-			dest_addr = (union ofi_sock_ip *) rai->ai_addr;
-		} else if (hints) {
-			dest_addr = hints->dest_addr;
-		}
-
-		if (hints && hints->src_addr)
-			src_addr = hints->src_addr;
-	}
-
-	if (dest_addr && !src_addr) {
-		ret = sock_get_src_addr(dest_addr, &sip);
-		if (!ret)
-			src_addr = &sip;
-	}
-
-	if (dest_addr) {
-		ofi_straddr_log(&sock_prov, FI_LOG_INFO, FI_LOG_CORE,
-				"dest addr: ", dest_addr);
-	}
-	if (src_addr) {
-		ofi_straddr_log(&sock_prov, FI_LOG_INFO, FI_LOG_CORE,
-				"src addr: ", src_addr);
-	}
-	switch (ep_type) {
-	case FI_EP_MSG:
-		ret = sock_msg_fi_info(version, src_addr, dest_addr, hints, info);
-		break;
-	case FI_EP_DGRAM:
-		ret = sock_dgram_fi_info(version, src_addr, dest_addr, hints, info);
-		break;
-	case FI_EP_RDM:
-		ret = sock_rdm_fi_info(version, src_addr, dest_addr, hints, info);
-		break;
-	default:
-		ret = -FI_ENODATA;
-		break;
-	}
-
-	if (rai)
-		freeaddrinfo(rai);
-
-	if (ret == 0) {
-		ret = sock_fi_checkinfo(*info, hints);
-		if (ret)
-			fi_freeinfo(*info);
-	}
-
-	return ret;
-}
-
-static void sock_init_addrlist(void)
-{
-	fastlock_acquire(&sock_list_lock);
-	if (slist_empty(&sock_addr_list))
-		ofi_get_list_of_addr(&sock_prov, "iface", &sock_addr_list);
-	fastlock_release(&sock_list_lock);
-}
-
-int sock_node_getinfo(uint32_t version, const char *node, const char *service,
-		      uint64_t flags, const struct fi_info *hints, struct fi_info **info,
-		      struct fi_info **tail)
-{
-	enum fi_ep_type ep_type;
-	struct fi_info *cur;
-	int ret;
-
-	if (hints && hints->ep_attr) {
-		switch (hints->ep_attr->type) {
-		case FI_EP_RDM:
-		case FI_EP_DGRAM:
-		case FI_EP_MSG:
-			ret = sock_ep_getinfo(version, node, service, flags,
-					      hints, hints->ep_attr->type, &cur);
-			if (ret) {
-				if (ret == -FI_ENODATA)
-					return ret;
-				goto err;
-			}
-
-			if (!*info)
-				*info = cur;
-			else
-				(*tail)->next = cur;
-			(*tail) = cur;
-			return 0;
-		default:
-			break;
-		}
-	}
-	for (ep_type = FI_EP_MSG; ep_type <= FI_EP_RDM; ep_type++) {
-		ret = sock_ep_getinfo(version, node, service, flags, hints,
-				      ep_type, &cur);
-		if (ret) {
-			if (ret == -FI_ENODATA)
-				continue;
-			goto err;
-		}
-
-		if (!*info)
-			*info = cur;
-		else
-			(*tail)->next = cur;
-		(*tail) = cur;
-	}
-	if (!*info) {
-		ret = -FI_ENODATA;
-		goto err_no_free;
-	}
-	return 0;
-
-err:
-	fi_freeinfo(*info);
-	*info = NULL;
-err_no_free:
-	return ret;
-}
-
-static int sock_match_src_addr(struct slist_entry *entry, const void *src_addr)
-{
-	struct ofi_addr_list_entry *host_entry =
-		container_of(entry, struct ofi_addr_list_entry, entry);
-
-        return ofi_equals_ipaddr(&host_entry->ipaddr.sa, src_addr);
-}
-
-static int sock_addr_matches_interface(struct slist *addr_list,
-				       struct sockaddr *src_addr)
-{
-	struct slist_entry *entry;
-
-	/* Always match if it's localhost */
-	if (ofi_is_loopback_addr(src_addr))
-		return 1;
-
-	entry = slist_find_first_match(addr_list, sock_match_src_addr, src_addr);
-	return entry ? 1 : 0;
-}
-
-static int sock_node_matches_interface(struct slist *addr_list, const char *node)
-{
-	union ofi_sock_ip addr;
-	struct addrinfo *rai = NULL, ai = {
-		.ai_socktype = SOCK_STREAM,
-	};
-
-	if (getaddrinfo(node, 0, &ai, &rai)) {
-		SOCK_LOG_DBG("getaddrinfo failed!\n");
-		return -FI_EINVAL;
-	}
-	if (rai->ai_addrlen > sizeof(addr)) {
-		freeaddrinfo(rai);
-		return -FI_EINVAL;
-	}
-
-	memset(&addr, 0, sizeof addr);
-	memcpy(&addr, rai->ai_addr, rai->ai_addrlen);
-	freeaddrinfo(rai);
-
-	return sock_addr_matches_interface(addr_list, &addr.sa);
-}
-
 static int sock_getinfo(uint32_t version, const char *node, const char *service,
 			uint64_t flags, const struct fi_info *hints,
 			struct fi_info **info)
 {
-	int ret = 0;
-	struct slist_entry *entry, *prev;
-	struct ofi_addr_list_entry *host_entry;
-	struct fi_info *tail;
-
-	if (!(flags & FI_SOURCE) && hints && hints->src_addr &&
-	    (hints->src_addrlen != ofi_sizeofaddr(hints->src_addr)))
-		return -FI_ENODATA;
-
-	if (((!node && !service) || (flags & FI_SOURCE)) &&
-	    hints && hints->dest_addr &&
-	    (hints->dest_addrlen != ofi_sizeofaddr(hints->dest_addr)))
-		return -FI_ENODATA;
-
-	ret = sock_verify_info(version, hints);
-	if (ret)
-		return ret;
-
-	ret = 1;
-	sock_init_addrlist();
-	if ((flags & FI_SOURCE) && node) {
-		ret = sock_node_matches_interface(&sock_addr_list, node);
-	} else if (hints && hints->src_addr) {
-		ret = sock_addr_matches_interface(&sock_addr_list,
-						  hints->src_addr);
-	}
-	if (!ret) {
-		SOCK_LOG_ERROR("Couldn't find a match with local interfaces\n");
-		return -FI_ENODATA;
-	}
-
-	*info = tail = NULL;
-	if (node ||
-	     (!(flags & FI_SOURCE) && hints && hints->src_addr) ||
-	     (!(flags & FI_SOURCE) && hints && hints->dest_addr))
-		return sock_node_getinfo(version, node, service, flags,
-					 hints, info, &tail);
-
-	(void) prev; /* Makes compiler happy */
-	slist_foreach(&sock_addr_list, entry, prev) {
-		host_entry = container_of(entry, struct ofi_addr_list_entry, entry);
-		node = host_entry->ipstr;
-		flags |= FI_SOURCE;
-		ret = sock_node_getinfo(version, node, service, flags, hints, info, &tail);
-		if (ret) {
-			if (ret == -FI_ENODATA)
-				continue;
-			return ret;
-		}
-	}
-
-	return (!*info) ? ret : 0;
+	return ofi_ip_getinfo(&sock_util_prov, version, node, service, flags,
+			      hints, info);
 }
 
 static void fi_sockets_fini(void)
 {
-	ofi_free_list_of_addr(&sock_addr_list);
 	fastlock_destroy(&sock_list_lock);
 }
 
@@ -698,6 +330,12 @@ struct fi_provider sock_prov = {
 	.cleanup = fi_sockets_fini
 };
 
+struct util_prov sock_util_prov = {
+	.prov = &sock_prov,
+	.info = &sock_dgram_info,
+	.flags = 0,
+};
+
 SOCKETS_INI
 {
 #if HAVE_SOCKETS_DL
@@ -747,11 +385,6 @@ SOCKETS_INI
 	fastlock_init(&sock_list_lock);
 	dlist_init(&sock_fab_list);
 	dlist_init(&sock_dom_list);
-	slist_init(&sock_addr_list);
-	SOCK_EP_RDM_SEC_CAP |= OFI_RMA_PMEM;
-	SOCK_EP_RDM_CAP |= OFI_RMA_PMEM;
-	SOCK_EP_MSG_SEC_CAP |= OFI_RMA_PMEM;
-	SOCK_EP_MSG_CAP |= OFI_RMA_PMEM;
 #if ENABLE_DEBUG
 	fi_param_define(&sock_prov, "dgram_drop_rate", FI_PARAM_INT,
 			"Drop every Nth dgram frame (debug only)");
diff --git a/prov/sockets/src/sock_progress.c b/prov/sockets/src/sock_progress.c
index 9d78aad..6cafe28 100644
--- a/prov/sockets/src/sock_progress.c
+++ b/prov/sockets/src/sock_progress.c
@@ -2277,7 +2277,7 @@ void sock_pe_signal(struct sock_pe *pe)
 void sock_pe_poll_add(struct sock_pe *pe, int fd)
 {
         fastlock_acquire(&pe->signal_lock);
-        if (fi_epoll_add(pe->epoll_set, fd, FI_EPOLL_IN, NULL))
+        if (ofi_epoll_add(pe->epoll_set, fd, OFI_EPOLL_IN, NULL))
 			SOCK_LOG_ERROR("failed to add to epoll set: %d\n", fd);
         fastlock_release(&pe->signal_lock);
 }
@@ -2285,7 +2285,7 @@ void sock_pe_poll_add(struct sock_pe *pe, int fd)
 void sock_pe_poll_del(struct sock_pe *pe, int fd)
 {
         fastlock_acquire(&pe->signal_lock);
-        if (fi_epoll_del(pe->epoll_set, fd))
+        if (ofi_epoll_del(pe->epoll_set, fd))
 			SOCK_LOG_DBG("failed to del from epoll set: %d\n", fd);
         fastlock_release(&pe->signal_lock);
 }
@@ -2366,7 +2366,7 @@ static int sock_pe_progress_rx_ep(struct sock_pe *pe,
 		}
 	}
 
-	num_fds = fi_epoll_wait(map->epoll_set, map->epoll_ctxs,
+	num_fds = ofi_epoll_wait(map->epoll_set, map->epoll_ctxs,
 	                        MIN(map->used, map->epoll_ctxs_sz), 0);
 	if (num_fds < 0 || num_fds == 0) {
 		if (num_fds < 0)
@@ -2577,7 +2577,7 @@ static void sock_pe_wait(struct sock_pe *pe)
 	int ret;
 	void *ep_contexts[1];
 
-	ret = fi_epoll_wait(pe->epoll_set, ep_contexts, 1, -1);
+	ret = ofi_epoll_wait(pe->epoll_set, ep_contexts, 1, -1);
 	if (ret < 0)
 		SOCK_LOG_ERROR("poll failed : %s\n", strerror(ofi_sockerr()));
 
@@ -2712,7 +2712,7 @@ struct sock_pe *sock_pe_init(struct sock_domain *domain)
 		goto err2;
 	}
 
-	if (fi_epoll_create(&pe->epoll_set) < 0) {
+	if (ofi_epoll_create(&pe->epoll_set) < 0) {
                 SOCK_LOG_ERROR("failed to create epoll set\n");
                 goto err3;
 	}
@@ -2722,9 +2722,9 @@ struct sock_pe *sock_pe_init(struct sock_domain *domain)
 			goto err4;
 
 		if (fd_set_nonblock(pe->signal_fds[SOCK_SIGNAL_RD_FD]) ||
-		    fi_epoll_add(pe->epoll_set,
+		    ofi_epoll_add(pe->epoll_set,
 				 pe->signal_fds[SOCK_SIGNAL_RD_FD],
-				 FI_EPOLL_IN, NULL))
+				 OFI_EPOLL_IN, NULL))
 			goto err5;
 
 		pe->do_progress = 1;
@@ -2741,7 +2741,7 @@ err5:
 	ofi_close_socket(pe->signal_fds[0]);
 	ofi_close_socket(pe->signal_fds[1]);
 err4:
-	fi_epoll_close(pe->epoll_set);
+	ofi_epoll_close(pe->epoll_set);
 err3:
 	ofi_bufpool_destroy(pe->atomic_rx_pool);
 err2:
@@ -2788,7 +2788,7 @@ void sock_pe_finalize(struct sock_pe *pe)
 	fastlock_destroy(&pe->lock);
 	fastlock_destroy(&pe->signal_lock);
 	pthread_mutex_destroy(&pe->list_lock);
-	fi_epoll_close(pe->epoll_set);
+	ofi_epoll_close(pe->epoll_set);
 	free(pe);
 	SOCK_LOG_DBG("Progress engine finalize: OK\n");
 }
diff --git a/prov/tcp/src/tcpx.h b/prov/tcp/src/tcpx.h
index ed3f748..1919753 100644
--- a/prov/tcp/src/tcpx.h
+++ b/prov/tcp/src/tcpx.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017 Intel Corporation, Inc.  All rights reserved.
+ * Copyright (c) 201-2020 Intel Corporation, Inc.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -121,7 +121,7 @@ struct tcpx_port_range {
 struct tcpx_conn_handle {
 	struct fid		handle;
 	struct tcpx_pep		*pep;
-	SOCKET			conn_fd;
+	SOCKET			sock;
 	bool			endian_match;
 };
 
@@ -179,7 +179,6 @@ struct tcpx_rx_ctx {
 };
 
 typedef int (*tcpx_rx_process_fn_t)(struct tcpx_xfer_entry *rx_entry);
-typedef void (*tcpx_ep_progress_func_t)(struct tcpx_ep *ep);
 typedef int (*tcpx_get_rx_func_t)(struct tcpx_ep *ep);
 
 struct stage_buf {
@@ -191,7 +190,7 @@ struct stage_buf {
 
 struct tcpx_ep {
 	struct util_ep		util_ep;
-	SOCKET			conn_fd;
+	SOCKET			sock;
 	struct tcpx_cur_rx_msg	cur_rx_msg;
 	struct tcpx_xfer_entry	*cur_rx_entry;
 	tcpx_rx_process_fn_t 	cur_rx_proc_fn;
@@ -204,12 +203,11 @@ struct tcpx_ep {
 	enum tcpx_cm_state	cm_state;
 	/* lock for protecting tx/rx queues,rma list,cm_state*/
 	fastlock_t		lock;
-	tcpx_ep_progress_func_t progress_func;
 	tcpx_get_rx_func_t	get_rx_entry[ofi_op_write + 1];
 	void (*hdr_bswap)(struct tcpx_base_hdr *hdr);
 	struct stage_buf	stage_buf;
 	size_t			min_multi_recv_size;
-	bool			send_ready_monitor;
+	bool			epoll_out_set;
 };
 
 struct tcpx_fabric {
@@ -300,8 +298,8 @@ struct tcpx_xfer_entry *
 tcpx_srx_next_xfer_entry(struct tcpx_rx_ctx *srx_ctx,
 			struct tcpx_ep *ep, size_t entry_size);
 
-void tcpx_progress(struct util_ep *util_ep);
-void tcpx_ep_progress(struct tcpx_ep *ep);
+void tcpx_progress_tx(struct tcpx_ep *ep);
+void tcpx_progress_rx(struct tcpx_ep *ep);
 int tcpx_try_func(void *util_ep);
 
 void tcpx_hdr_none(struct tcpx_base_hdr *hdr);
diff --git a/prov/tcp/src/tcpx_comm.c b/prov/tcp/src/tcpx_comm.c
index 6c2018b..43eef14 100644
--- a/prov/tcp/src/tcpx_comm.c
+++ b/prov/tcp/src/tcpx_comm.c
@@ -45,8 +45,7 @@ int tcpx_send_msg(struct tcpx_xfer_entry *tx_entry)
 	msg.msg_iov = tx_entry->iov;
 	msg.msg_iovlen = tx_entry->iov_cnt;
 
-	bytes_sent = ofi_sendmsg_tcp(tx_entry->ep->conn_fd,
-	                             &msg, MSG_NOSIGNAL);
+	bytes_sent = ofi_sendmsg_tcp(tx_entry->ep->sock, &msg, MSG_NOSIGNAL);
 	if (bytes_sent < 0)
 		return ofi_sockerr() == EPIPE ? -FI_ENOTCONN : -ofi_sockerr();
 
@@ -154,7 +153,7 @@ int tcpx_recv_msg_data(struct tcpx_xfer_entry *rx_entry)
 						     rx_entry->iov,
 						     rx_entry->iov_cnt);
 	else
-		bytes_recvd = ofi_readv_socket(rx_entry->ep->conn_fd,
+		bytes_recvd = ofi_readv_socket(rx_entry->ep->sock,
 					       rx_entry->iov,
 					       rx_entry->iov_cnt);
 	if (bytes_recvd <= 0)
diff --git a/prov/tcp/src/tcpx_conn_mgr.c b/prov/tcp/src/tcpx_conn_mgr.c
index 6f04563..b442bd3 100644
--- a/prov/tcp/src/tcpx_conn_mgr.c
+++ b/prov/tcp/src/tcpx_conn_mgr.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017 Intel Corporation. All rights reserved.
+ * Copyright (c) 2017-2020 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -130,8 +130,8 @@ static int tcpx_ep_enable_xfers(struct tcpx_ep *ep)
 		ret = -FI_EINVAL;
 		goto unlock;
 	}
-	ep->progress_func = tcpx_ep_progress;
-	ret = fi_fd_nonblock(ep->conn_fd);
+
+	ret = fi_fd_nonblock(ep->sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"failed to set socket to nonblocking\n");
@@ -140,12 +140,20 @@ static int tcpx_ep_enable_xfers(struct tcpx_ep *ep)
 	ep->cm_state = TCPX_EP_CONNECTED;
 	fastlock_release(&ep->lock);
 
-	if (ep->util_ep.rx_cq->wait) {
+	if (ep->util_ep.rx_cq) {
 		ret = ofi_wait_fd_add(ep->util_ep.rx_cq->wait,
-				      ep->conn_fd, FI_EPOLL_IN,
-				      tcpx_try_func, (void *) &ep->util_ep,
-				      NULL);
+				      ep->sock, OFI_EPOLL_IN, tcpx_try_func,
+				      (void *) &ep->util_ep,
+				      &ep->util_ep.ep_fid.fid);
+	}
+
+	if (ep->util_ep.tx_cq) {
+		ret = ofi_wait_fd_add(ep->util_ep.tx_cq->wait,
+				      ep->sock, OFI_EPOLL_IN, tcpx_try_func,
+				      (void *) &ep->util_ep,
+				      &ep->util_ep.ep_fid.fid);
 	}
+
 	return ret;
 unlock:
 	fastlock_release(&ep->lock);
@@ -160,7 +168,7 @@ static int proc_conn_resp(struct tcpx_cm_context *cm_ctx,
 	ssize_t len;
 	int ret = FI_SUCCESS;
 
-	ret = rx_cm_data(ep->conn_fd, &conn_resp, ofi_ctrl_connresp, cm_ctx);
+	ret = rx_cm_data(ep->sock, &conn_resp, ofi_ctrl_connresp, cm_ctx);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"Failed to receive connect response\n");
@@ -207,7 +215,7 @@ static void client_recv_connresp(struct util_wait *wait,
 	assert(cm_ctx->fid->fclass == FI_CLASS_EP);
 	ep = container_of(cm_ctx->fid, struct tcpx_ep, util_ep.ep_fid.fid);
 
-	ret = ofi_wait_fd_del(wait, ep->conn_fd);
+	ret = ofi_wait_fd_del(wait, ep->sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"Could not remove fd from wait\n");
@@ -254,7 +262,7 @@ static void server_send_cm_accept(struct util_wait *wait,
 	ep = container_of(cm_ctx->fid, struct tcpx_ep, util_ep.ep_fid.fid);
 
 	FI_DBG(&tcpx_prov, FI_LOG_EP_CTRL, "Send connect (accept) response\n");
-	ret = tx_cm_data(ep->conn_fd, ofi_ctrl_connresp, cm_ctx);
+	ret = tx_cm_data(ep->sock, ofi_ctrl_connresp, cm_ctx);
 	if (ret)
 		goto err;
 
@@ -264,7 +272,7 @@ static void server_send_cm_accept(struct util_wait *wait,
 	if (ret < 0)
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "Error writing to EQ\n");
 
-	ret = ofi_wait_fd_del(wait, ep->conn_fd);
+	ret = ofi_wait_fd_del(wait, ep->sock);
 	if (ret) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"Could not remove fd from wait\n");
@@ -302,7 +310,7 @@ static void server_recv_connreq(struct util_wait *wait,
 	handle  = container_of(cm_ctx->fid, struct tcpx_conn_handle, handle);
 
 	FI_DBG(&tcpx_prov, FI_LOG_EP_CTRL, "Server receive connect request\n");
-	ret = rx_cm_data(handle->conn_fd, &conn_req, ofi_ctrl_connreq, cm_ctx);
+	ret = rx_cm_data(handle->sock, &conn_req, ofi_ctrl_connreq, cm_ctx);
 	if (ret)
 		goto err1;
 
@@ -320,7 +328,7 @@ static void server_recv_connreq(struct util_wait *wait,
 	if (!cm_entry->info->dest_addr)
 		goto err3;
 
-	ret = ofi_getpeername(handle->conn_fd, cm_entry->info->dest_addr, &len);
+	ret = ofi_getpeername(handle->sock, cm_entry->info->dest_addr, &len);
 	if (ret)
 		goto err3;
 
@@ -334,7 +342,7 @@ static void server_recv_connreq(struct util_wait *wait,
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "Error writing to EQ\n");
 		goto err3;
 	}
-	ret = ofi_wait_fd_del(wait, handle->conn_fd);
+	ret = ofi_wait_fd_del(wait, handle->sock);
 	if (ret)
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL,
 			"fd deletion from ofi_wait failed\n");
@@ -346,8 +354,8 @@ err3:
 err2:
 	free(cm_entry);
 err1:
-	ofi_wait_fd_del(wait, handle->conn_fd);
-	ofi_close_socket(handle->conn_fd);
+	ofi_wait_fd_del(wait, handle->sock);
+	ofi_close_socket(handle->sock);
 	free(cm_ctx);
 	free(handle);
 }
@@ -366,23 +374,23 @@ static void client_send_connreq(struct util_wait *wait,
 	ep = container_of(cm_ctx->fid, struct tcpx_ep, util_ep.ep_fid.fid);
 
 	len = sizeof(status);
-	ret = getsockopt(ep->conn_fd, SOL_SOCKET, SO_ERROR, (char *) &status, &len);
+	ret = getsockopt(ep->sock, SOL_SOCKET, SO_ERROR, (char *) &status, &len);
 	if (ret < 0 || status) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_CTRL, "connection failure\n");
 		ret = (ret < 0)? -ofi_sockerr() : status;
 		goto err;
 	}
 
-	ret = tx_cm_data(ep->conn_fd, ofi_ctrl_connreq, cm_ctx);
+	ret = tx_cm_data(ep->sock, ofi_ctrl_connreq, cm_ctx);
 	if (ret)
 		goto err;
 
-	ret = ofi_wait_fd_del(wait, ep->conn_fd);
+	ret = ofi_wait_fd_del(wait, ep->sock);
 	if (ret)
 		goto err;
 
 	cm_ctx->type = CLIENT_RECV_CONNRESP;
-	ret = ofi_wait_fd_add(wait, ep->conn_fd, FI_EPOLL_IN,
+	ret = ofi_wait_fd_add(wait, ep->sock, OFI_EPOLL_IN,
 			      tcpx_eq_wait_try_func, NULL, cm_ctx);
 	if (ret)
 		goto err;
@@ -431,13 +439,13 @@ static void server_sock_accept(struct util_wait *wait,
 	if (!rx_req_cm_ctx)
 		goto err2;
 
-	handle->conn_fd = sock;
+	handle->sock = sock;
 	handle->handle.fclass = FI_CLASS_CONNREQ;
 	handle->pep = pep;
 	rx_req_cm_ctx->fid = &handle->handle;
 	rx_req_cm_ctx->type = SERVER_RECV_CONNREQ;
 
-	ret = ofi_wait_fd_add(wait, sock, FI_EPOLL_IN,
+	ret = ofi_wait_fd_add(wait, sock, OFI_EPOLL_IN,
 			      tcpx_eq_wait_try_func,
 			      NULL, (void *) rx_req_cm_ctx);
 	if (ret)
@@ -477,6 +485,11 @@ static void process_cm_ctx(struct util_wait *wait,
 	}
 }
 
+/* The implementation assumes that the EQ does not share a wait set with
+ * a CQ.  This is true for internally created wait sets, but not if the
+ * application manages the wait set.  To fix, we need to distinguish
+ * whether the wait_context references a fid or tcpx_cm_context.
+ */
 void tcpx_conn_mgr_run(struct util_eq *eq)
 {
 	struct util_wait_fd *wait_fd;
@@ -491,8 +504,8 @@ void tcpx_conn_mgr_run(struct util_eq *eq)
 
 	tcpx_eq = container_of(eq, struct tcpx_eq, util_eq);
 	fastlock_acquire(&tcpx_eq->close_lock);
-	num_fds = fi_epoll_wait(wait_fd->epoll_fd, wait_contexts,
-				MAX_EPOLL_EVENTS, 0);
+	num_fds = ofi_epoll_wait(wait_fd->epoll_fd, wait_contexts,
+				 MAX_EPOLL_EVENTS, 0);
 	if (num_fds < 0) {
 		fastlock_release(&tcpx_eq->close_lock);
 		return;
diff --git a/prov/tcp/src/tcpx_cq.c b/prov/tcp/src/tcpx_cq.c
index 765f825..d9b7716 100644
--- a/prov/tcp/src/tcpx_cq.c
+++ b/prov/tcp/src/tcpx_cq.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017 Intel Corporation. All rights reserved.
+ * Copyright (c) 2017-2020 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -37,6 +37,49 @@
 
 #define TCPX_DEF_CQ_SIZE (1024)
 
+
+void tcpx_cq_progress(struct util_cq *cq)
+{
+	void *wait_contexts[MAX_EPOLL_EVENTS];
+	struct fid_list_entry *fid_entry;
+	struct util_wait_fd *fdwait;
+	struct dlist_entry *item;
+	struct tcpx_ep *ep;
+	struct fid *fid;
+	int nfds, i;
+
+	fdwait = container_of(cq->wait, struct util_wait_fd, util_wait);
+
+	cq->cq_fastlock_acquire(&cq->ep_list_lock);
+	dlist_foreach(&cq->ep_list, item) {
+		fid_entry = container_of(item, struct fid_list_entry, entry);
+		ep = container_of(fid_entry->fid, struct tcpx_ep,
+				  util_ep.ep_fid.fid);
+		tcpx_try_func(&ep->util_ep);
+		tcpx_progress_tx(ep);
+		if (ep->stage_buf.off != ep->stage_buf.len)
+			tcpx_progress_rx(ep);
+	}
+
+	nfds = ofi_epoll_wait(fdwait->epoll_fd, wait_contexts,
+			      MAX_EPOLL_EVENTS, 0);
+	if (nfds <= 0)
+		goto unlock;
+
+	for (i = 0; i < nfds; i++) {
+		fid = wait_contexts[i];
+		if (fid->fclass != FI_CLASS_EP) {
+			fd_signal_reset(&fdwait->signal);
+			continue;
+		}
+
+		ep = container_of(fid, struct tcpx_ep, util_ep.ep_fid.fid);
+		tcpx_progress_rx(ep);
+	}
+unlock:
+	cq->cq_fastlock_release(&cq->ep_list_lock);
+}
+
 static void tcpx_buf_pools_destroy(struct tcpx_buf_pool *buf_pools)
 {
 	int i;
@@ -242,8 +285,9 @@ err:
 int tcpx_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 		 struct fid_cq **cq_fid, void *context)
 {
-	int ret;
 	struct tcpx_cq *tcpx_cq;
+	struct fi_cq_attr cq_attr;
+	int ret;
 
 	tcpx_cq = calloc(1, sizeof(*tcpx_cq));
 	if (!tcpx_cq)
@@ -256,8 +300,14 @@ int tcpx_cq_open(struct fid_domain *domain, struct fi_cq_attr *attr,
 	if (ret)
 		goto free_cq;
 
+	if (attr->wait_obj == FI_WAIT_NONE) {
+		cq_attr = *attr;
+		cq_attr.wait_obj = FI_WAIT_FD;
+		attr = &cq_attr;
+	}
+
 	ret = ofi_cq_init(&tcpx_prov, domain, attr, &tcpx_cq->util_cq,
-			  &ofi_cq_progress, context);
+			  &tcpx_cq_progress, context);
 	if (ret)
 		goto destroy_pool;
 
diff --git a/prov/tcp/src/tcpx_ep.c b/prov/tcp/src/tcpx_ep.c
index ff16680..d1fdc75 100644
--- a/prov/tcp/src/tcpx_ep.c
+++ b/prov/tcp/src/tcpx_ep.c
@@ -97,7 +97,7 @@ static int tcpx_ep_connect(struct fid_ep *ep, const void *addr,
 	struct tcpx_cm_context *cm_ctx;
 	int ret;
 
-	if (!addr || !tcpx_ep->conn_fd || paramlen > TCPX_MAX_CM_DATA_SIZE)
+	if (!addr || !tcpx_ep->sock || paramlen > TCPX_MAX_CM_DATA_SIZE)
 		return -FI_EINVAL;
 
 	cm_ctx = calloc(1, sizeof(*cm_ctx));
@@ -107,7 +107,7 @@ static int tcpx_ep_connect(struct fid_ep *ep, const void *addr,
 		return -FI_ENOMEM;
 	}
 
-	ret = connect(tcpx_ep->conn_fd, (struct sockaddr *) addr,
+	ret = connect(tcpx_ep->sock, (struct sockaddr *) addr,
 		      (socklen_t) ofi_sizeofaddr(addr));
 	if (ret && ofi_sockerr() != FI_EINPROGRESS) {
 		ret =  -ofi_sockerr();
@@ -122,8 +122,8 @@ static int tcpx_ep_connect(struct fid_ep *ep, const void *addr,
 		memcpy(cm_ctx->cm_data, param, paramlen);
 	}
 
-	ret = ofi_wait_fd_add(tcpx_ep->util_ep.eq->wait, tcpx_ep->conn_fd,
-			      FI_EPOLL_OUT, tcpx_eq_wait_try_func, NULL,cm_ctx);
+	ret = ofi_wait_fd_add(tcpx_ep->util_ep.eq->wait, tcpx_ep->sock,
+			      OFI_EPOLL_OUT, tcpx_eq_wait_try_func, NULL,cm_ctx);
 	if (ret)
 		goto err;
 
@@ -140,7 +140,7 @@ static int tcpx_ep_accept(struct fid_ep *ep, const void *param, size_t paramlen)
 	struct tcpx_cm_context *cm_ctx;
 	int ret;
 
-	if (tcpx_ep->conn_fd == INVALID_SOCKET)
+	if (tcpx_ep->sock == INVALID_SOCKET)
 		return -FI_EINVAL;
 
 	cm_ctx = calloc(1, sizeof(*cm_ctx));
@@ -157,8 +157,8 @@ static int tcpx_ep_accept(struct fid_ep *ep, const void *param, size_t paramlen)
 		memcpy(cm_ctx->cm_data, param, paramlen);
 	}
 
-	ret = ofi_wait_fd_add(tcpx_ep->util_ep.eq->wait, tcpx_ep->conn_fd,
-			      FI_EPOLL_OUT, tcpx_eq_wait_try_func, NULL, cm_ctx);
+	ret = ofi_wait_fd_add(tcpx_ep->util_ep.eq->wait, tcpx_ep->sock,
+			      OFI_EPOLL_OUT, tcpx_eq_wait_try_func, NULL, cm_ctx);
 	if (ret) {
 		free(cm_ctx);
 		return ret;
@@ -174,7 +174,7 @@ static int tcpx_ep_shutdown(struct fid_ep *ep, uint64_t flags)
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
 
-	ret = ofi_shutdown(tcpx_ep->conn_fd, SHUT_RDWR);
+	ret = ofi_shutdown(tcpx_ep->sock, SHUT_RDWR);
 	if (ret && ofi_sockerr() != ENOTCONN) {
 		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA, "ep shutdown unsuccessful\n");
 	}
@@ -269,7 +269,7 @@ static int tcpx_ep_getname(fid_t fid, void *addr, size_t *addrlen)
 	int ret;
 
 	tcpx_ep = container_of(fid, struct tcpx_ep, util_ep.ep_fid);
-	ret = ofi_getsockname(tcpx_ep->conn_fd, addr, (socklen_t *)addrlen);
+	ret = ofi_getsockname(tcpx_ep->sock, addr, (socklen_t *)addrlen);
 	if (ret)
 		return -ofi_sockerr();
 
@@ -283,7 +283,7 @@ static int tcpx_ep_getpeer(struct fid_ep *ep, void *addr, size_t *addrlen)
 	int ret;
 
 	tcpx_ep = container_of(ep, struct tcpx_ep, util_ep.ep_fid);
-	ret = ofi_getpeername(tcpx_ep->conn_fd, addr, (socklen_t *)addrlen);
+	ret = ofi_getpeername(tcpx_ep->sock, addr, (socklen_t *)addrlen);
 	if (ret)
 		return -ofi_sockerr();
 
@@ -360,15 +360,18 @@ static int tcpx_ep_close(struct fid *fid)
 
 	/* eq->close_lock protects from processing stale connection events */
 	fastlock_acquire(&eq->close_lock);
-	if (ep->util_ep.rx_cq->wait)
-		ofi_wait_fd_del(ep->util_ep.rx_cq->wait, ep->conn_fd);
+	if (ep->util_ep.rx_cq)
+		ofi_wait_fd_del(ep->util_ep.rx_cq->wait, ep->sock);
+
+	if (ep->util_ep.tx_cq)
+		ofi_wait_fd_del(ep->util_ep.tx_cq->wait, ep->sock);
 
 	if (ep->util_ep.eq->wait)
-		ofi_wait_fd_del(ep->util_ep.eq->wait, ep->conn_fd);
+		ofi_wait_fd_del(ep->util_ep.eq->wait, ep->sock);
 
 	fastlock_release(&eq->close_lock);
 	ofi_eq_remove_fid_events(ep->util_ep.eq, &ep->util_ep.ep_fid.fid);
-	ofi_close_socket(ep->conn_fd);
+	ofi_close_socket(ep->sock);
 	ofi_endpoint_close(&ep->util_ep);
 	fastlock_destroy(&ep->lock);
 
@@ -480,11 +483,6 @@ static struct fi_ops_ep tcpx_ep_ops = {
 	.tx_size_left = fi_no_tx_size_left,
 };
 
-static void tcpx_empty_progress(struct tcpx_ep *ep)
-{
-	/* no-op */
-}
-
 int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 		  struct fid_ep **ep_fid, void *context)
 {
@@ -498,7 +496,7 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 		return -FI_ENOMEM;
 
 	ret = ofi_endpoint_init(domain, &tcpx_util_prov, info, &ep->util_ep,
-				context, tcpx_progress);
+				context, NULL);
 	if (ret)
 		goto err1;
 
@@ -507,34 +505,33 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 			pep = container_of(info->handle, struct tcpx_pep,
 					   util_pep.pep_fid.fid);
 
-			ep->conn_fd = pep->sock;
+			ep->sock = pep->sock;
 			pep->sock = INVALID_SOCKET;
 		} else {
 			handle = container_of(info->handle,
 					      struct tcpx_conn_handle, handle);
-			ep->conn_fd = handle->conn_fd;
+			ep->sock = handle->sock;
 			ep->hdr_bswap = handle->endian_match ?
 					tcpx_hdr_none : tcpx_hdr_bswap;
 			free(handle);
 
-			ret = tcpx_setup_socket(ep->conn_fd);
+			ret = tcpx_setup_socket(ep->sock);
 			if (ret)
 				goto err3;
 		}
 	} else {
-		ep->conn_fd = ofi_socket(ofi_get_sa_family(info), SOCK_STREAM, 0);
-		if (ep->conn_fd == INVALID_SOCKET) {
+		ep->sock = ofi_socket(ofi_get_sa_family(info), SOCK_STREAM, 0);
+		if (ep->sock == INVALID_SOCKET) {
 			ret = -ofi_sockerr();
 			goto err2;
 		}
 
-		ret = tcpx_setup_socket(ep->conn_fd);
+		ret = tcpx_setup_socket(ep->sock);
 		if (ret)
 			goto err3;
 	}
 
 	ep->cm_state = TCPX_EP_CONNECTING;
-	ep->progress_func = tcpx_empty_progress;
 	ret = fastlock_init(&ep->lock);
 	if (ret)
 		goto err3;
@@ -566,7 +563,7 @@ int tcpx_endpoint(struct fid_domain *domain, struct fi_info *info,
 	ep->get_rx_entry[ofi_op_write] = tcpx_get_rx_entry_op_write;
 	return 0;
 err3:
-	ofi_close_socket(ep->conn_fd);
+	ofi_close_socket(ep->sock);
 err2:
 	ofi_endpoint_close(&ep->util_ep);
 err1:
@@ -671,7 +668,7 @@ static int tcpx_pep_listen(struct fid_pep *pep)
 	}
 
 	ret = ofi_wait_fd_add(tcpx_pep->util_pep.eq->wait, tcpx_pep->sock,
-			      FI_EPOLL_IN, tcpx_eq_wait_try_func,
+			      OFI_EPOLL_IN, tcpx_eq_wait_try_func,
 			      NULL, &tcpx_pep->cm_ctx);
 
 	tcpx_pep->util_pep.eq->wait->signal(tcpx_pep->util_pep.eq->wait);
@@ -692,15 +689,15 @@ static int tcpx_pep_reject(struct fid_pep *pep, fid_t handle,
 	hdr.type = ofi_ctrl_nack;
 	hdr.seg_size = htons((uint16_t) paramlen);
 
-	ret = ofi_send_socket(tcpx_handle->conn_fd, &hdr,
+	ret = ofi_send_socket(tcpx_handle->sock, &hdr,
 			      sizeof(hdr), MSG_NOSIGNAL);
 
 	if ((ret == sizeof(hdr)) && paramlen)
-		(void) ofi_send_socket(tcpx_handle->conn_fd, param,
+		(void) ofi_send_socket(tcpx_handle->sock, param,
 				       paramlen, MSG_NOSIGNAL);
 
-	ofi_shutdown(tcpx_handle->conn_fd, SHUT_RDWR);
-	ret = ofi_close_socket(tcpx_handle->conn_fd);
+	ofi_shutdown(tcpx_handle->sock, SHUT_RDWR);
+	ret = ofi_close_socket(tcpx_handle->sock);
 	if (ret)
 		return ret;
 
diff --git a/prov/tcp/src/tcpx_init.c b/prov/tcp/src/tcpx_init.c
index 315f91d..a3ed427 100644
--- a/prov/tcp/src/tcpx_init.c
+++ b/prov/tcp/src/tcpx_init.c
@@ -36,85 +36,15 @@
 #include "tcpx.h"
 
 #include <sys/types.h>
-#include <ifaddrs.h>
-#include <net/if.h>
 #include <ofi_util.h>
 #include <stdlib.h>
 
-#if HAVE_GETIFADDRS
-static void tcpx_getinfo_ifs(struct fi_info **info)
-{
-	struct fi_info *head = NULL, *tail = NULL, *cur;
-	struct slist addr_list;
-	size_t addrlen;
-	uint32_t addr_format;
-	struct slist_entry *entry, *prev;
-	struct ofi_addr_list_entry *addr_entry;
-
-	slist_init(&addr_list);
-
-	ofi_get_list_of_addr(&tcpx_prov, "iface", &addr_list);
-
-	(void) prev; /* Makes compiler happy */
-	slist_foreach(&addr_list, entry, prev) {
-		addr_entry = container_of(entry, struct ofi_addr_list_entry, entry);
-
-		cur = fi_dupinfo(*info);
-		if (!cur)
-			break;
-
-		if (!head)
-			head = cur;
-		else
-			tail->next = cur;
-		tail = cur;
-
-		switch (addr_entry->ipaddr.sin.sin_family) {
-		case AF_INET:
-			addrlen = sizeof(struct sockaddr_in);
-			addr_format = FI_SOCKADDR_IN;
-			break;
-		case AF_INET6:
-			addrlen = sizeof(struct sockaddr_in6);
-			addr_format = FI_SOCKADDR_IN6;
-			break;
-		default:
-			continue;
-		}
-
-		cur->src_addr = mem_dup(&addr_entry->ipaddr, addrlen);
-		if (cur->src_addr) {
-			cur->src_addrlen = addrlen;
-			cur->addr_format = addr_format;
-		}
-		/* TODO: rework util code
-		util_set_fabric_domain(&tcpx_prov, cur);
-		*/
-	}
-
-	ofi_free_list_of_addr(&addr_list);
-	fi_freeinfo(*info);
-	*info = head;
-}
-#else
-#define tcpx_getinfo_ifs(info) do{ } while(0)
-#endif
-
 static int tcpx_getinfo(uint32_t version, const char *node, const char *service,
 			uint64_t flags, const struct fi_info *hints,
 			struct fi_info **info)
 {
-	int ret;
-
-	ret = util_getinfo(&tcpx_util_prov, version, node, service, flags,
-			   hints, info);
-	if (ret)
-		return ret;
-
-	if (!(*info)->src_addr && !(*info)->dest_addr)
-		tcpx_getinfo_ifs(info);
-
-	return 0;
+	return ofi_ip_getinfo(&tcpx_util_prov, version, node, service, flags,
+			      hints, info);
 }
 
 struct tcpx_port_range port_range = {
diff --git a/prov/tcp/src/tcpx_progress.c b/prov/tcp/src/tcpx_progress.c
index 66d891f..e775b64 100644
--- a/prov/tcp/src/tcpx_progress.c
+++ b/prov/tcp/src/tcpx_progress.c
@@ -585,7 +585,7 @@ static inline int tcpx_get_next_rx_hdr(struct tcpx_ep *ep)
 	if (ep->cur_rx_msg.hdr_len == ep->cur_rx_msg.done_len)
 		return FI_SUCCESS;
 
-	ret = tcpx_comm_recv_hdr(ep->conn_fd, &ep->stage_buf, &ep->cur_rx_msg);
+	ret = tcpx_comm_recv_hdr(ep->sock, &ep->stage_buf, &ep->cur_rx_msg);
 	if (ret)
 		return ret;
 
@@ -593,12 +593,13 @@ static inline int tcpx_get_next_rx_hdr(struct tcpx_ep *ep)
 	return FI_SUCCESS;
 }
 
-static void tcpx_process_rx_msg(struct tcpx_ep *ep)
+/* Must hold ep lock */
+void tcpx_progress_rx(struct tcpx_ep *ep)
 {
 	int ret;
 
 	if (!ep->cur_rx_entry && (ep->stage_buf.len == ep->stage_buf.off)) {
-		ret = tcpx_read_to_buffer(ep->conn_fd, &ep->stage_buf);
+		ret = tcpx_read_to_buffer(ep->sock, &ep->stage_buf);
 		if (ret)
 			goto err;
 	}
@@ -629,7 +630,8 @@ err:
 		tcpx_report_error(ep, ret);
 }
 
-void tcpx_ep_progress(struct tcpx_ep *ep)
+/* Must hold ep lock */
+void tcpx_progress_tx(struct tcpx_ep *ep)
 {
 	struct tcpx_xfer_entry *tx_entry;
 	struct slist_entry *entry;
@@ -639,19 +641,6 @@ void tcpx_ep_progress(struct tcpx_ep *ep)
 		tx_entry = container_of(entry, struct tcpx_xfer_entry, entry);
 		process_tx_entry(tx_entry);
 	}
-
-	tcpx_process_rx_msg(ep);
-}
-
-void tcpx_progress(struct util_ep *util_ep)
-{
-	struct tcpx_ep *ep;
-
-	ep = container_of(util_ep, struct tcpx_ep, util_ep);
-	fastlock_acquire(&ep->lock);
-	ep->progress_func(ep);
-	fastlock_release(&ep->lock);
-	return;
 }
 
 int tcpx_try_func(void *util_ep)
@@ -662,27 +651,28 @@ int tcpx_try_func(void *util_ep)
 	int ret;
 
 	ep = container_of(util_ep, struct tcpx_ep, util_ep);
-	wait_fd = container_of(((struct util_ep *)util_ep)->rx_cq->wait,
-			       struct util_wait_fd, util_wait);
 
 	fastlock_acquire(&ep->lock);
-	if (!slist_empty(&ep->tx_queue) && !ep->send_ready_monitor) {
-		ep->send_ready_monitor = true;
-		events = FI_EPOLL_IN | FI_EPOLL_OUT;
+	if (!slist_empty(&ep->tx_queue) && !ep->epoll_out_set) {
+		ep->epoll_out_set = true;
+		events = OFI_EPOLL_IN | OFI_EPOLL_OUT;
 		goto epoll_mod;
-	} else if (slist_empty(&ep->tx_queue) && ep->send_ready_monitor) {
-		ep->send_ready_monitor = false;
-		events = FI_EPOLL_IN;
+	} else if (slist_empty(&ep->tx_queue) && ep->epoll_out_set) {
+		ep->epoll_out_set = false;
+		events = OFI_EPOLL_IN;
 		goto epoll_mod;
 	}
 	fastlock_release(&ep->lock);
 	return FI_SUCCESS;
 
 epoll_mod:
-	ret = fi_epoll_mod(wait_fd->epoll_fd, ep->conn_fd, events, NULL);
+	wait_fd = container_of(((struct util_ep *) util_ep)->tx_cq->wait,
+			       struct util_wait_fd, util_wait);
+	ret = ofi_epoll_mod(wait_fd->epoll_fd, ep->sock, events,
+			    &ep->util_ep.ep_fid.fid);
 	if (ret)
 		FI_WARN(&tcpx_prov, FI_LOG_EP_DATA,
-			"invalid op type\n");
+			"epoll modify failed\n");
 	fastlock_release(&ep->lock);
 	return ret;
 }
diff --git a/prov/udp/src/udpx_attr.c b/prov/udp/src/udpx_attr.c
index fd4d134..751d429 100644
--- a/prov/udp/src/udpx_attr.c
+++ b/prov/udp/src/udpx_attr.c
@@ -32,9 +32,12 @@
 
 #include "udpx.h"
 
+#define UDPX_TX_CAPS (OFI_TX_MSG_CAPS | FI_MULTICAST)
+#define UDPX_RX_CAPS (FI_SOURCE | OFI_RX_MSG_CAPS)
+#define UDPX_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
 
 struct fi_tx_attr udpx_tx_attr = {
-	.caps = FI_MSG | FI_SEND | FI_MULTICAST,
+	.caps = UDPX_TX_CAPS,
 	.comp_order = FI_ORDER_STRICT,
 	.inject_size = 1472,
 	.size = 1024,
@@ -42,7 +45,7 @@ struct fi_tx_attr udpx_tx_attr = {
 };
 
 struct fi_rx_attr udpx_rx_attr = {
-	.caps = FI_MSG | FI_RECV | FI_SOURCE | FI_MULTICAST,
+	.caps = UDPX_RX_CAPS,
 	.comp_order = FI_ORDER_STRICT,
 	.total_buffered_recv = (1 << 16),
 	.size = 1024,
@@ -59,7 +62,7 @@ struct fi_ep_attr udpx_ep_attr = {
 };
 
 struct fi_domain_attr udpx_domain_attr = {
-	.caps = FI_LOCAL_COMM | FI_REMOTE_COMM,
+	.caps = UDPX_DOMAIN_CAPS,
 	.name = "udp",
 	.threading = FI_THREAD_SAFE,
 	.control_progress = FI_PROGRESS_AUTO,
@@ -82,8 +85,7 @@ struct fi_fabric_attr udpx_fabric_attr = {
 };
 
 struct fi_info udpx_info = {
-	.caps = FI_MSG | FI_SEND | FI_RECV | FI_SOURCE | FI_MULTICAST |
-		FI_LOCAL_COMM | FI_REMOTE_COMM,
+	.caps = UDPX_DOMAIN_CAPS | UDPX_TX_CAPS | UDPX_RX_CAPS,
 	.addr_format = FI_SOCKADDR,
 	.tx_attr = &udpx_tx_attr,
 	.rx_attr = &udpx_rx_attr,
diff --git a/prov/udp/src/udpx_ep.c b/prov/udp/src/udpx_ep.c
index 2a1145f..08e234a 100644
--- a/prov/udp/src/udpx_ep.c
+++ b/prov/udp/src/udpx_ep.c
@@ -561,7 +561,7 @@ static int udpx_ep_close(struct fid *fid)
 		if (ep->util_ep.rx_cq->wait) {
 			wait = container_of(ep->util_ep.rx_cq->wait,
 					    struct util_wait_fd, util_wait);
-			fi_epoll_del(wait->epoll_fd, (int)ep->sock);
+			ofi_epoll_del(wait->epoll_fd, (int)ep->sock);
 		}
 		fid_list_remove(&ep->util_ep.rx_cq->ep_list,
 				&ep->util_ep.rx_cq->ep_list_lock,
@@ -604,8 +604,8 @@ static int udpx_ep_bind_cq(struct udpx_ep *ep, struct util_cq *cq,
 
 			wait = container_of(cq->wait,
 					    struct util_wait_fd, util_wait);
-			ret = fi_epoll_add(wait->epoll_fd, (int)ep->sock,
-					   FI_EPOLL_IN, &ep->util_ep.ep_fid.fid);
+			ret = ofi_epoll_add(wait->epoll_fd, (int)ep->sock,
+					   OFI_EPOLL_IN, &ep->util_ep.ep_fid.fid);
 			if (ret)
 				return ret;
 		} else {
diff --git a/prov/udp/src/udpx_init.c b/prov/udp/src/udpx_init.c
index 8952147..020c54c 100644
--- a/prov/udp/src/udpx_init.c
+++ b/prov/udp/src/udpx_init.c
@@ -36,81 +36,14 @@
 #include "udpx.h"
 
 #include <sys/types.h>
-#include <ifaddrs.h>
-#include <net/if.h>
 
 
-#if HAVE_GETIFADDRS
-static void udpx_getinfo_ifs(struct fi_info **info)
-{
-	struct fi_info *head = NULL, *tail = NULL, *cur;
-	struct slist addr_list;
-	size_t addrlen;
-	uint32_t addr_format;
-	struct slist_entry *entry, *prev;
-	struct ofi_addr_list_entry *addr_entry;
-
-	slist_init(&addr_list);
-
-	ofi_get_list_of_addr(&udpx_prov, "iface", &addr_list);
-
-	(void) prev; /* Makes compiler happy */
-	slist_foreach(&addr_list, entry, prev) {
-		addr_entry = container_of(entry, struct ofi_addr_list_entry, entry);
-
-		cur = fi_dupinfo(*info);
-		if (!cur)
-			break;
-
-		if (!head)
-			head = cur;
-		else
-			tail->next = cur;
-		tail = cur;
-
-		switch (addr_entry->ipaddr.sin.sin_family) {
-		case AF_INET:
-			addrlen = sizeof(struct sockaddr_in);
-			addr_format = FI_SOCKADDR_IN;
-			break;
-		case AF_INET6:
-			addrlen = sizeof(struct sockaddr_in6);
-			addr_format = FI_SOCKADDR_IN6;
-			break;
-		default:
-			continue;
-		}
-
-		cur->src_addr = mem_dup(&addr_entry->ipaddr, addrlen);
-		if (cur->src_addr) {
-			cur->src_addrlen = addrlen;
-			cur->addr_format = addr_format;
-		}
-	}
-
-	ofi_free_list_of_addr(&addr_list);
-	fi_freeinfo(*info);
-	*info = head;
-}
-#else
-#define udpx_getinfo_ifs(info) do{}while(0)
-#endif
-
 static int udpx_getinfo(uint32_t version, const char *node, const char *service,
 			uint64_t flags, const struct fi_info *hints,
 			struct fi_info **info)
 {
-	int ret;
-
-	ret = util_getinfo(&udpx_util_prov, version, node, service, flags,
-			   hints, info);
-	if (ret)
-		return ret;
-
-	if (!(*info)->src_addr && !(*info)->dest_addr)
-		udpx_getinfo_ifs(info);
-
-	return 0;
+	return ofi_ip_getinfo(&udpx_util_prov, version, node, service, flags,
+			      hints, info);
 }
 
 static void udpx_fini(void)
diff --git a/prov/util/src/util_attr.c b/prov/util/src/util_attr.c
index 5bee103..a43bb42 100644
--- a/prov/util/src/util_attr.c
+++ b/prov/util/src/util_attr.c
@@ -36,8 +36,9 @@
 #include <shared/ofi_str.h>
 #include <ofi_util.h>
 
-#define OFI_MSG_CAPS	(FI_SEND | FI_RECV)
-#define OFI_RMA_CAPS	(FI_READ | FI_WRITE | FI_REMOTE_READ | FI_REMOTE_WRITE)
+#define OFI_MSG_DIRECTION_CAPS	(FI_SEND | FI_RECV)
+#define OFI_RMA_DIRECTION_CAPS	(FI_READ | FI_WRITE | \
+				 FI_REMOTE_READ | FI_REMOTE_WRITE)
 
 static int fi_valid_addr_format(uint32_t prov_format, uint32_t user_format)
 {
@@ -520,13 +521,6 @@ int ofi_check_domain_attr(const struct fi_provider *prov, uint32_t api_version,
 {
 	const struct fi_domain_attr *user_attr = user_info->domain_attr;
 
-	if (prov_attr->name && user_attr->name &&
-	    strcasecmp(user_attr->name, prov_attr->name)) {
-		FI_INFO(prov, FI_LOG_CORE, "Unknown domain name\n");
-		FI_INFO_NAME(prov, prov_attr, user_attr);
-		return -FI_ENODATA;
-	}
-
 	if (fi_thread_level(user_attr->threading) <
 	    fi_thread_level(prov_attr->threading)) {
 		FI_INFO(prov, FI_LOG_CORE, "Invalid threading model\n");
@@ -810,34 +804,26 @@ int ofi_check_rx_attr(const struct fi_provider *prov,
 	return 0;
 }
 
-static uint64_t ofi_expand_caps(uint64_t base_caps)
-{
-	uint64_t expanded_caps = base_caps;
-	uint64_t msg_caps = FI_SEND | FI_RECV;
-	uint64_t rma_caps = FI_WRITE | FI_READ | FI_REMOTE_WRITE | FI_REMOTE_READ;
-
-	if (base_caps & (FI_MSG | FI_TAGGED))
-		if (!(base_caps & msg_caps))
-			expanded_caps |= msg_caps;
-
-	if (base_caps & (FI_RMA | FI_ATOMIC))
-		if (!(base_caps & rma_caps))
-			expanded_caps |= rma_caps;
-
-	return expanded_caps;
-}
-
 int ofi_check_attr_subset(const struct fi_provider *prov,
 		uint64_t base_caps, uint64_t requested_caps)
 {
-	uint64_t expanded_base_caps;
+	uint64_t expanded_caps;
 
-	expanded_base_caps = ofi_expand_caps(base_caps);
+	expanded_caps = base_caps;
+	if (base_caps & (FI_MSG | FI_TAGGED)) {
+		if (!(base_caps & OFI_MSG_DIRECTION_CAPS))
+			expanded_caps |= OFI_MSG_DIRECTION_CAPS;
+	}
+	if (base_caps & (FI_RMA | FI_ATOMIC)) {
+		if (!(base_caps & OFI_RMA_DIRECTION_CAPS))
+			expanded_caps |= OFI_RMA_DIRECTION_CAPS;
+	}
 
-	if (~expanded_base_caps & requested_caps) {
-		FI_INFO(prov, FI_LOG_CORE, "requested caps not subset of base endpoint caps\n");
-		FI_INFO_FIELD(prov, expanded_base_caps, requested_caps, "Supported",
-				"Requested", FI_TYPE_CAPS);
+	if (~expanded_caps & requested_caps) {
+		FI_INFO(prov, FI_LOG_CORE,
+			"requested caps not subset of base endpoint caps\n");
+		FI_INFO_FIELD(prov, expanded_caps, requested_caps,
+			"Supported", "Requested", FI_TYPE_CAPS);
 		return -FI_ENODATA;
 	}
 
@@ -909,7 +895,7 @@ int ofi_check_tx_attr(const struct fi_provider *prov,
 	return 0;
 }
 
-/* if there are multiple fi_info in the provider:
+/* Use if there are multiple fi_info in the provider:
  * check provider's info */
 int ofi_prov_check_info(const struct util_prov *util_prov,
 			uint32_t api_version,
@@ -932,7 +918,7 @@ int ofi_prov_check_info(const struct util_prov *util_prov,
 	return (!success_info ? -FI_ENODATA : FI_SUCCESS);
 }
 
-/* if there are multiple fi_info in the provider:
+/* Use if there are multiple fi_info in the provider:
  * check and duplicate provider's info */
 int ofi_prov_check_dup_info(const struct util_prov *util_prov,
 			    uint32_t api_version,
@@ -965,7 +951,7 @@ int ofi_prov_check_dup_info(const struct util_prov *util_prov,
 		tail = fi;
 	}
 
-	return (!*info ? -FI_ENODATA : FI_SUCCESS);
+	return !*info ? -FI_ENODATA : FI_SUCCESS;
 err:
 	fi_freeinfo(*info);
 	FI_INFO(prov, FI_LOG_CORE,
@@ -973,7 +959,7 @@ err:
 	return ret;
 }
 
-/* if there is only single fi_info in the provider */
+/* Use if there is only single fi_info in the provider */
 int ofi_check_info(const struct util_prov *util_prov,
 		   const struct fi_info *prov_info, uint32_t api_version,
 		   const struct fi_info *user_info)
@@ -1067,10 +1053,10 @@ static uint64_t ofi_get_caps(uint64_t info_caps, uint64_t hint_caps,
 		       (attr_caps & FI_SECONDARY_CAPS);
 	}
 
-	if (caps & (FI_MSG | FI_TAGGED) && !(caps & OFI_MSG_CAPS))
-		caps |= OFI_MSG_CAPS;
-	if (caps & (FI_RMA | FI_ATOMICS) && !(caps & OFI_RMA_CAPS))
-		caps |= OFI_RMA_CAPS;
+	if (caps & (FI_MSG | FI_TAGGED) && !(caps & OFI_MSG_DIRECTION_CAPS))
+		caps |= (attr_caps & OFI_MSG_DIRECTION_CAPS);
+	if (caps & (FI_RMA | FI_ATOMICS) && !(caps & OFI_RMA_DIRECTION_CAPS))
+		caps |= (attr_caps & OFI_RMA_DIRECTION_CAPS);
 
 	return caps;
 }
diff --git a/prov/util/src/util_ep.c b/prov/util/src/util_ep.c
index 9163513..fe2be36 100644
--- a/prov/util/src/util_ep.c
+++ b/prov/util/src/util_ep.c
@@ -174,18 +174,18 @@ int ofi_ep_bind(struct util_ep *util_ep, struct fid *fid, uint64_t flags)
 		return ret;
 
 	switch (fid->fclass) {
-		case FI_CLASS_CQ:
-			cq = container_of(fid, struct util_cq, cq_fid.fid);
-			return ofi_ep_bind_cq(util_ep, cq, flags);
-		case FI_CLASS_EQ:
-			eq = container_of(fid, struct util_eq, eq_fid.fid);
-			return ofi_ep_bind_eq(util_ep, eq);
-		case FI_CLASS_AV:
-			av = container_of(fid, struct util_av, av_fid.fid);
-			return ofi_ep_bind_av(util_ep, av);
-		case FI_CLASS_CNTR:
-			cntr = container_of(fid, struct util_cntr, cntr_fid.fid);
-			return ofi_ep_bind_cntr(util_ep, cntr, flags);
+	case FI_CLASS_CQ:
+		cq = container_of(fid, struct util_cq, cq_fid.fid);
+		return ofi_ep_bind_cq(util_ep, cq, flags);
+	case FI_CLASS_EQ:
+		eq = container_of(fid, struct util_eq, eq_fid.fid);
+		return ofi_ep_bind_eq(util_ep, eq);
+	case FI_CLASS_AV:
+		av = container_of(fid, struct util_av, av_fid.fid);
+		return ofi_ep_bind_av(util_ep, av);
+	case FI_CLASS_CNTR:
+		cntr = container_of(fid, struct util_cntr, cntr_fid.fid);
+		return ofi_ep_bind_cntr(util_ep, cntr, flags);
 	}
 
 	return -FI_EINVAL;
diff --git a/prov/util/src/util_main.c b/prov/util/src/util_main.c
index 504a522..345c74d 100644
--- a/prov/util/src/util_main.c
+++ b/prov/util/src/util_main.c
@@ -127,6 +127,11 @@ static int util_find_domain(struct dlist_entry *item, const void *arg)
 		 ((info->domain_attr->mr_mode & domain->mr_mode) == domain->mr_mode);
 }
 
+/*
+ * Produces 1 fi_info output for each fi_info entry in the provider's base
+ * list (stored with util_prov), subject to the base fi_info meeting the
+ * user's hints.
+ */
 int util_getinfo(const struct util_prov *util_prov, uint32_t version,
 		 const char *node, const char *service, uint64_t flags,
 		 const struct fi_info *hints, struct fi_info **info)
@@ -254,3 +259,148 @@ err:
 	fi_freeinfo(*info);
 	return ret;
 }
+
+static void util_set_netif_names(struct fi_info *info,
+				 struct ofi_addr_list_entry *addr_entry)
+{
+	char *name;
+
+	name = strdup(addr_entry->net_name);
+	if (name) {
+		free(info->fabric_attr->name);
+		info->fabric_attr->name = name;
+	}
+
+	name = strdup(addr_entry->ifa_name);
+	if (name) {
+		free(info->domain_attr->name);
+		info->domain_attr->name = name;
+	}
+}
+
+/*
+ * Produces 1 fi_info output for each usable IP address in the system for the
+ * given fi_info input.
+ */
+#if HAVE_GETIFADDRS
+static void util_getinfo_ifs(const struct util_prov *prov, struct fi_info *src_info,
+			     struct fi_info **head, struct fi_info **tail)
+{
+	struct fi_info *cur;
+	struct slist addr_list;
+	size_t addrlen;
+	uint32_t addr_format;
+	struct slist_entry *entry, *prev;
+	struct ofi_addr_list_entry *addr_entry;
+
+	*head = *tail = NULL;
+	slist_init(&addr_list);
+
+	ofi_get_list_of_addr(prov->prov, "iface", &addr_list);
+
+	(void) prev; /* Makes compiler happy */
+	slist_foreach(&addr_list, entry, prev) {
+		addr_entry = container_of(entry, struct ofi_addr_list_entry, entry);
+
+		cur = fi_dupinfo(src_info);
+		if (!cur)
+			break;
+
+		if (!*head) {
+			*head = cur;
+			FI_INFO(prov->prov, FI_LOG_CORE, "Chosen addr for using: %s,"
+				" speed %zu\n", addr_entry->ipstr, addr_entry->speed);
+		} else {
+			(*tail)->next = cur;
+		}
+		*tail = cur;
+
+		switch (addr_entry->ipaddr.sin.sin_family) {
+		case AF_INET:
+			addrlen = sizeof(struct sockaddr_in);
+			addr_format = FI_SOCKADDR_IN;
+			break;
+		case AF_INET6:
+			addrlen = sizeof(struct sockaddr_in6);
+			addr_format = FI_SOCKADDR_IN6;
+			break;
+		default:
+			continue;
+		}
+
+		cur->src_addr = mem_dup(&addr_entry->ipaddr, addrlen);
+		if (cur->src_addr) {
+			cur->src_addrlen = addrlen;
+			cur->addr_format = addr_format;
+		}
+		util_set_netif_names(cur, addr_entry);
+	}
+
+	ofi_free_list_of_addr(&addr_list);
+	if (!*head) {
+		*head = src_info;
+		*tail = src_info;
+	}
+}
+#else
+static void util_getinfo_ifs(const struct util_prov *prov, struct fi_info *src_info,
+			     struct fi_info **head, struct fi_info **tail)
+{
+	*head = src_info;
+	*tail = src_info;
+}
+#endif
+
+static int util_match_addr(struct slist_entry *entry, const void *addr)
+{
+	struct ofi_addr_list_entry *addr_entry;
+
+	addr_entry = container_of(entry, struct ofi_addr_list_entry, entry);
+	return ofi_equals_ipaddr(&addr_entry->ipaddr.sa, addr);
+}
+
+int ofi_ip_getinfo(const struct util_prov *prov, uint32_t version,
+		   const char *node, const char *service, uint64_t flags,
+		   const struct fi_info *hints, struct fi_info **info)
+{
+	struct fi_info *head, *tail, *cur, **prev;
+	struct ofi_addr_list_entry *addr_entry;
+	struct slist addr_list;
+	struct slist_entry *entry;
+	int ret;
+
+	ret = util_getinfo(prov, version, node, service, flags,
+			   hints, info);
+	if (ret)
+		return ret;
+
+	prev = info;
+	for (cur = *info; cur; cur = cur->next) {
+		if (!cur->src_addr && !cur->dest_addr) {
+			util_getinfo_ifs(prov, cur, &head, &tail);
+			if (head != cur) {
+				tail->next = (*prev)->next;
+				*prev = head;
+
+				cur->next = NULL;
+				fi_freeinfo(cur);
+				cur = tail;
+			}
+		} else if (cur->src_addr) {
+			slist_init(&addr_list);
+			ofi_get_list_of_addr(prov->prov, "iface", &addr_list);
+
+			entry = slist_find_first_match(&addr_list, util_match_addr,
+						(*info)->src_addr);
+			if (entry) {
+				addr_entry = container_of(entry,
+						struct ofi_addr_list_entry, entry);
+				util_set_netif_names(cur, addr_entry);
+			}
+			ofi_free_list_of_addr(&addr_list);
+		}
+		prev = &cur->next;
+	}
+
+	return 0;
+}
diff --git a/prov/util/src/util_mem_monitor.c b/prov/util/src/util_mem_monitor.c
index c69c342..741acff 100644
--- a/prov/util/src/util_mem_monitor.c
+++ b/prov/util/src/util_mem_monitor.c
@@ -68,10 +68,10 @@ void ofi_monitor_init(void)
 	pthread_mutex_init(&memhooks_monitor->lock, NULL);
 	dlist_init(&memhooks_monitor->list);
 
-#if HAVE_UFFD_UNMAP
-        default_monitor = uffd_monitor;
-#elif defined(HAVE_ELF_H) && defined(HAVE_SYS_AUXV_H)
+#if defined(HAVE_ELF_H) && defined(HAVE_SYS_AUXV_H)
         default_monitor = memhooks_monitor;
+#elif HAVE_UFFD_UNMAP
+        default_monitor = uffd_monitor;
 #else
         default_monitor = NULL;
 #endif
diff --git a/prov/util/src/util_shm.c b/prov/util/src/util_shm.c
index 5cd7d0e..91b4f61 100644
--- a/prov/util/src/util_shm.c
+++ b/prov/util/src/util_shm.c
@@ -185,7 +185,7 @@ int smr_map_to_region(const struct fi_provider *prov, struct smr_peer *peer_buf)
 
 	fd = shm_open(peer_buf->peer.name, O_RDWR, S_IRUSR | S_IWUSR);
 	if (fd < 0) {
-		FI_WARN(prov, FI_LOG_AV, "shm_open error\n");
+		FI_WARN_ONCE(prov, FI_LOG_AV, "shm_open error\n");
 		return -errno;
 	}
 
diff --git a/prov/util/src/util_wait.c b/prov/util/src/util_wait.c
index 01df8c6..a90e35a 100644
--- a/prov/util/src/util_wait.c
+++ b/prov/util/src/util_wait.c
@@ -178,7 +178,7 @@ int ofi_wait_fd_del(struct util_wait *wait, int fd)
 	if (ofi_atomic_dec32(&fd_entry->ref))
 		goto out;
 	dlist_remove(&fd_entry->entry);
-	fi_epoll_del(wait_fd->epoll_fd, fd_entry->fd);
+	ofi_epoll_del(wait_fd->epoll_fd, fd_entry->fd);
 	free(fd_entry);
 out:
 	fastlock_release(&wait_fd->lock);
@@ -205,7 +205,7 @@ int ofi_wait_fd_add(struct util_wait *wait, int fd, uint32_t events,
 		goto out;
 	}
 
-	ret = fi_epoll_add(wait_fd->epoll_fd, fd, events, context);
+	ret = ofi_epoll_add(wait_fd->epoll_fd, fd, events, context);
 	if (ret) {
 		FI_WARN(wait->prov, FI_LOG_FABRIC, "Unable to add fd to epoll\n");
 		goto out;
@@ -214,7 +214,7 @@ int ofi_wait_fd_add(struct util_wait *wait, int fd, uint32_t events,
 	fd_entry = calloc(1, sizeof *fd_entry);
 	if (!fd_entry) {
 		ret = -FI_ENOMEM;
-		fi_epoll_del(wait_fd->epoll_fd, fd);
+		ofi_epoll_del(wait_fd->epoll_fd, fd);
 		goto out;
 	}
 	fd_entry->fd = fd;
@@ -276,7 +276,7 @@ static int util_wait_fd_run(struct fid_wait *wait_fid, int timeout)
 		if (ofi_adjust_timeout(endtime, &timeout))
 			return -FI_ETIMEDOUT;
 
-		ret = fi_epoll_wait(wait->epoll_fd, ep_context, 1, timeout);
+		ret = ofi_epoll_wait(wait->epoll_fd, ep_context, 1, timeout);
 		if (ret > 0)
 			return FI_SUCCESS;
 
@@ -327,14 +327,14 @@ static int util_wait_fd_close(struct fid *fid)
 	while (!dlist_empty(&wait->fd_list)) {
 		dlist_pop_front(&wait->fd_list, struct ofi_wait_fd_entry,
 				fd_entry, entry);
-		fi_epoll_del(wait->epoll_fd, fd_entry->fd);
+		ofi_epoll_del(wait->epoll_fd, fd_entry->fd);
 		free(fd_entry);
 	}
 	fastlock_release(&wait->lock);
 
-	fi_epoll_del(wait->epoll_fd, wait->signal.fd[FI_READ_FD]);
+	ofi_epoll_del(wait->epoll_fd, wait->signal.fd[FI_READ_FD]);
 	fd_signal_free(&wait->signal);
-	fi_epoll_close(wait->epoll_fd);
+	ofi_epoll_close(wait->epoll_fd);
 	fastlock_destroy(&wait->lock);
 	free(wait);
 	return 0;
@@ -400,12 +400,12 @@ int ofi_wait_fd_open(struct fid_fabric *fabric_fid, struct fi_wait_attr *attr,
 	if (ret)
 		goto err2;
 
-	ret = fi_epoll_create(&wait->epoll_fd);
+	ret = ofi_epoll_create(&wait->epoll_fd);
 	if (ret)
 		goto err3;
 
-	ret = fi_epoll_add(wait->epoll_fd, wait->signal.fd[FI_READ_FD],
-	                   FI_EPOLL_IN, &wait->util_wait.wait_fid.fid);
+	ret = ofi_epoll_add(wait->epoll_fd, wait->signal.fd[FI_READ_FD],
+	                   OFI_EPOLL_IN, &wait->util_wait.wait_fid.fid);
 	if (ret)
 		goto err4;
 
@@ -419,7 +419,7 @@ int ofi_wait_fd_open(struct fid_fabric *fabric_fid, struct fi_wait_attr *attr,
 	return 0;
 
 err4:
-	fi_epoll_close(wait->epoll_fd);
+	ofi_epoll_close(wait->epoll_fd);
 err3:
 	fd_signal_free(&wait->signal);
 err2:
diff --git a/prov/verbs/src/fi_verbs.c b/prov/verbs/src/fi_verbs.c
index 7b0def8..adfb38a 100644
--- a/prov/verbs/src/fi_verbs.c
+++ b/prov/verbs/src/fi_verbs.c
@@ -595,6 +595,25 @@ static int vrb_read_params(void)
 			   "Invalid value of cqread_bunch_size\n");
 		return -FI_EINVAL;
 	}
+	if (vrb_get_param_int("gid_idx", "Set which gid index to use "
+				 "attribute (0 - 255)",
+				 &vrb_gl_data.gid_idx) ||
+	    (vrb_gl_data.gid_idx < 0 ||
+	     vrb_gl_data.gid_idx > 255)) {
+		VERBS_WARN(FI_LOG_CORE,
+			   "Invalid value of gid index\n");
+		return -FI_EINVAL;
+	}
+
+	if (vrb_get_param_str("device_name", "The prefix or the full name of the "
+			      "verbs device to use",
+			      &vrb_gl_data.device_name)) {
+		VERBS_WARN(FI_LOG_CORE,
+			   "Invalid value of device_name\n");
+		return -FI_EINVAL;
+	}
+
+	/* MSG-specific parameter */
 	if (vrb_get_param_str("iface", "The prefix or the full name of the "
 				 "network interface associated with the verbs device",
 				 &vrb_gl_data.iface)) {
@@ -624,15 +643,6 @@ static int vrb_read_params(void)
 			   "Invalid value of dgram_name_server_port\n");
 		return -FI_EINVAL;
 	}
-	if (vrb_get_param_int("gid_idx", "Set which gid index to use "
-				 "attribute (0 - 255)",
-				 &vrb_gl_data.gid_idx) ||
-	    (vrb_gl_data.gid_idx < 0 ||
-	     vrb_gl_data.gid_idx > 255)) {
-		VERBS_WARN(FI_LOG_CORE,
-			   "Invalid value of gid index\n");
-		return -FI_EINVAL;
-	}
 
 	return FI_SUCCESS;
 }
diff --git a/prov/verbs/src/fi_verbs.h b/prov/verbs/src/fi_verbs.h
index b322fbf..3fe48ba 100644
--- a/prov/verbs/src/fi_verbs.h
+++ b/prov/verbs/src/fi_verbs.h
@@ -48,7 +48,7 @@
 #include <unistd.h>
 #include <assert.h>
 #include <pthread.h>
-#include <sys/epoll.h>
+#include <ofi_epoll.h>
 
 #include <infiniband/ib.h>
 #include <infiniband/verbs.h>
@@ -150,6 +150,7 @@ extern struct vrb_gl_data {
 	int	use_odp;
 	char	*iface;
 	int	gid_idx;
+	char	*device_name;
 
 	struct {
 		int	buffer_num;
@@ -274,7 +275,7 @@ struct vrb_eq {
 	struct rdma_event_channel *channel;
 	uint64_t		flags;
 	struct fi_eq_err_entry	err;
-	int			epfd;
+	ofi_epoll_t		epollfd;
 
 	struct {
 		/* The connection key map is used during the XRC connection
@@ -443,6 +444,8 @@ struct vrb_srq_ep {
 	struct fid_ep		ep_fid;
 	struct ibv_srq		*srq;
 	struct vrb_domain	*domain;
+	struct ofi_bufpool	*ctx_pool;
+	fastlock_t		ctx_lock;
 
 	/* For XRC SRQ only */
 	struct {
@@ -456,7 +459,7 @@ struct vrb_srq_ep {
 		/* The RX CQ associated with this XRC SRQ. This field
 		 * and the srq_entry should only be modified while holding
 		 * the associted cq::xrc.srq_list_lock. */
-		struct vrb_cq	*cq;
+		struct vrb_cq		*cq;
 
 		/* The CQ maintains a list of XRC SRQ associated with it */
 		struct dlist_entry	srq_entry;
@@ -580,8 +583,10 @@ struct vrb_ep {
 
 /* Must be cast-able to struct fi_context */
 struct vrb_context {
-	struct vrb_ep		*ep;
+	struct vrb_ep			*ep;
+	struct vrb_srq_ep		*srx;
 	void				*user_ctx;
+	uint32_t			flags;
 };
 
 
@@ -756,7 +761,6 @@ struct verbs_ep_domain {
 	char			*suffix;
 	enum fi_ep_type		type;
 	uint32_t		protocol;
-	uint64_t		caps;
 };
 
 extern const struct verbs_ep_domain verbs_dgram_domain;
diff --git a/prov/verbs/src/verbs_cq.c b/prov/verbs/src/verbs_cq.c
index 723bee7..166edf9 100644
--- a/prov/verbs/src/verbs_cq.c
+++ b/prov/verbs/src/verbs_cq.c
@@ -234,14 +234,29 @@ int vrb_poll_cq(struct vrb_cq *cq, struct ibv_wc *wc)
 
 	do {
 		ret = ibv_poll_cq(cq->cq, 1, wc);
-		if (ret <= 0 || (wc->opcode & IBV_WC_RECV))
+		if (ret <= 0)
 			break;
 
 		ctx = (struct vrb_context *) (uintptr_t) wc->wr_id;
-		cq->credits++;
-		ctx->ep->tx_credits++;
 		wc->wr_id = (uintptr_t) ctx->user_ctx;
-		ofi_buf_free(ctx);
+		if (ctx->flags & FI_TRANSMIT) {
+			cq->credits++;
+			ctx->ep->tx_credits++;
+		}
+
+		if (wc->status) {
+			if (ctx->flags & FI_RECV)
+				wc->opcode |= IBV_WC_RECV;
+			else
+				wc->opcode &= ~IBV_WC_RECV;
+		}
+		if (ctx->srx) {
+			fastlock_acquire(&ctx->srx->ctx_lock);
+			ofi_buf_free(ctx);
+			fastlock_release(&ctx->srx->ctx_lock);
+		} else {
+			ofi_buf_free(ctx);
+		}
 
 	} while (wc->wr_id == VERBS_NO_COMP_FLAG);
 
@@ -640,8 +655,7 @@ int vrb_cq_open(struct fid_domain *domain_fid, struct fi_cq_attr *attr,
 	}
 
 	ret = ofi_bufpool_create(&cq->ctx_pool, sizeof(struct fi_context),
-				 16, size, vrb_gl_data.def_tx_size,
-				 OFI_BUFPOOL_NO_TRACK);
+				 16, 0, size, OFI_BUFPOOL_NO_TRACK);
 	if (ret)
 		goto err6;
 
diff --git a/prov/verbs/src/verbs_ep.c b/prov/verbs/src/verbs_ep.c
index c74e063..a2df0a2 100644
--- a/prov/verbs/src/verbs_ep.c
+++ b/prov/verbs/src/verbs_ep.c
@@ -40,12 +40,34 @@ static struct fi_ops_msg vrb_srq_msg_ops;
 /* Receive CQ credits are pre-allocated */
 ssize_t vrb_post_recv(struct vrb_ep *ep, struct ibv_recv_wr *wr)
 {
+	struct vrb_context *ctx;
+	struct vrb_cq *cq;
 	struct ibv_recv_wr *bad_wr;
 	int ret;
 
-	assert(ep->util_ep.rx_cq);
+	cq = container_of(ep->util_ep.rx_cq, struct vrb_cq, util_cq);
+	cq->util_cq.cq_fastlock_acquire(&cq->util_cq.cq_lock);
+	ctx = ofi_buf_alloc(cq->ctx_pool);
+	if (!ctx)
+		goto unlock;
+
+	ctx->ep = ep;
+	ctx->user_ctx = (void *) (uintptr_t) wr->wr_id;
+	ctx->flags = FI_RECV;
+	wr->wr_id = (uintptr_t) ctx;
+
 	ret = ibv_post_recv(ep->ibv_qp, wr, &bad_wr);
-	return vrb_convert_ret(ret);
+	wr->wr_id = (uintptr_t) ctx->user_ctx;
+	if (ret)
+		goto freebuf;
+	cq->util_cq.cq_fastlock_release(&cq->util_cq.cq_lock);
+	return 0;
+
+freebuf:
+	ofi_buf_free(ctx);
+unlock:
+	cq->util_cq.cq_fastlock_release(&cq->util_cq.cq_lock);
+	return -FI_EAGAIN;
 }
 
 ssize_t vrb_post_send(struct vrb_ep *ep, struct ibv_send_wr *wr)
@@ -76,6 +98,7 @@ ssize_t vrb_post_send(struct vrb_ep *ep, struct ibv_send_wr *wr)
 
 	ctx->ep = ep;
 	ctx->user_ctx = (void *) (uintptr_t) wr->wr_id;
+	ctx->flags = FI_TRANSMIT;
 	wr->wr_id = (uintptr_t) ctx;
 
 	ret = ibv_post_send(ep->ibv_qp, wr, &bad_wr);
@@ -1267,41 +1290,65 @@ static struct fi_ops_atomic vrb_srq_atomic_ops = {
 	.compwritevalid = fi_no_atomic_compwritevalid,
 };
 
+/* Receive CQ credits are pre-allocated */
+ssize_t vrb_post_srq(struct vrb_srq_ep *ep, struct ibv_recv_wr *wr)
+{
+	struct vrb_context *ctx;
+	struct ibv_recv_wr *bad_wr;
+	int ret;
+
+	fastlock_acquire(&ep->ctx_lock);
+	ctx = ofi_buf_alloc(ep->ctx_pool);
+	if (!ctx)
+		goto unlock;
+
+	ctx->srx = ep;
+	ctx->user_ctx = (void *) (uintptr_t) wr->wr_id;
+	ctx->flags = FI_RECV;
+	wr->wr_id = (uintptr_t) ctx;
+
+	ret = ibv_post_srq_recv(ep->srq, wr, &bad_wr);
+	wr->wr_id = (uintptr_t) ctx->user_ctx;
+	if (ret)
+		goto freebuf;
+	fastlock_release(&ep->ctx_lock);
+	return 0;
+
+freebuf:
+	ofi_buf_free(ctx);
+unlock:
+	fastlock_release(&ep->ctx_lock);
+	return -FI_EAGAIN;
+}
+
 static inline ssize_t
 vrb_srq_ep_recvmsg(struct fid_ep *ep_fid, const struct fi_msg *msg, uint64_t flags)
 {
-	struct vrb_srq_ep *ep =
-		container_of(ep_fid, struct vrb_srq_ep, ep_fid);
+	struct vrb_srq_ep *ep = container_of(ep_fid, struct vrb_srq_ep, ep_fid);
 	struct ibv_recv_wr wr = {
-		.wr_id = (uintptr_t)msg->context,
+		.wr_id = (uintptr_t )msg->context,
 		.num_sge = msg->iov_count,
 		.next = NULL,
 	};
-	struct ibv_recv_wr *bad_wr;
-
-	assert(ep->srq);
 
 	vrb_set_sge_iov(wr.sg_list, msg->msg_iov, msg->iov_count, msg->desc);
-
-	return vrb_convert_ret(ibv_post_srq_recv(ep->srq, &wr, &bad_wr));
+	return vrb_post_srq(ep, &wr);
 }
 
 static ssize_t
 vrb_srq_ep_recv(struct fid_ep *ep_fid, void *buf, size_t len,
 		void *desc, fi_addr_t src_addr, void *context)
 {
-	struct vrb_srq_ep *ep =
-		container_of(ep_fid, struct vrb_srq_ep, ep_fid);
+	struct vrb_srq_ep *ep = container_of(ep_fid, struct vrb_srq_ep, ep_fid);
 	struct ibv_sge sge = vrb_init_sge(buf, len, desc);
 	struct ibv_recv_wr wr = {
-		.wr_id = (uintptr_t)context,
+		.wr_id = (uintptr_t) context,
 		.num_sge = 1,
 		.sg_list = &sge,
 		.next = NULL,
 	};
-	struct ibv_recv_wr *bad_wr;
 
-	return vrb_convert_ret(ibv_post_srq_recv(ep->srq, &wr, &bad_wr));
+	return vrb_post_srq(ep, &wr);
 }
 
 static ssize_t
@@ -1434,7 +1481,7 @@ int vrb_xrc_close_srq(struct vrb_srq_ep *srq_ep)
 static int vrb_srq_close(fid_t fid)
 {
 	struct vrb_srq_ep *srq_ep = container_of(fid, struct vrb_srq_ep,
-						    ep_fid.fid);
+						 ep_fid.fid);
 	int ret;
 
 	if (srq_ep->domain->flags & VRB_USE_XRC) {
@@ -1451,6 +1498,9 @@ static int vrb_srq_close(fid_t fid)
 		if (ret)
 			goto err;
 	}
+
+	ofi_bufpool_destroy(srq_ep->ctx_pool);
+	fastlock_destroy(&srq_ep->ctx_lock);
 	free(srq_ep);
 	return FI_SUCCESS;
 
@@ -1479,10 +1529,14 @@ int vrb_srq_context(struct fid_domain *domain, struct fi_rx_attr *attr,
 		return -FI_EINVAL;
 
 	srq_ep = calloc(1, sizeof(*srq_ep));
-	if (!srq_ep) {
-		ret = -FI_ENOMEM;
-		goto err1;
-	}
+	if (!srq_ep)
+		return -FI_ENOMEM;
+
+	fastlock_init(&srq_ep->ctx_lock);
+	ret = ofi_bufpool_create(&srq_ep->ctx_pool, sizeof(struct fi_context),
+				 16, attr->size, 1024, OFI_BUFPOOL_NO_TRACK);
+	if (ret)
+		goto free_ep;
 
 	dom = container_of(domain, struct vrb_domain,
 			   util_domain.domain_fid);
@@ -1516,18 +1570,18 @@ int vrb_srq_context(struct fid_domain *domain, struct fi_rx_attr *attr,
 	if (!srq_ep->srq) {
 		VERBS_INFO_ERRNO(FI_LOG_DOMAIN, "ibv_create_srq", errno);
 		ret = -errno;
-		goto err2;
+		goto free_bufs;
 	}
 
 done:
 	*srq_ep_fid = &srq_ep->ep_fid;
-
 	return FI_SUCCESS;
 
-err2:
-	/* Only basic SRQ can take this path */
+free_bufs:
+	ofi_bufpool_destroy(srq_ep->ctx_pool);
+free_ep:
+	fastlock_destroy(&srq_ep->ctx_lock);
 	free(srq_ep);
-err1:
 	return ret;
 }
 
diff --git a/prov/verbs/src/verbs_eq.c b/prov/verbs/src/verbs_eq.c
index 845adb5..07255ca 100644
--- a/prov/verbs/src/verbs_eq.c
+++ b/prov/verbs/src/verbs_eq.c
@@ -1141,7 +1141,7 @@ vrb_eq_sread(struct fid_eq *eq_fid, uint32_t *event,
 		void *buf, size_t len, int timeout, uint64_t flags)
 {
 	struct vrb_eq *eq;
-	struct epoll_event events[2];
+	void *contexts;
 	ssize_t ret;
 
 	eq = container_of(eq_fid, struct vrb_eq, eq_fid.fid);
@@ -1151,7 +1151,7 @@ vrb_eq_sread(struct fid_eq *eq_fid, uint32_t *event,
 		if (ret && (ret != -FI_EAGAIN))
 			return ret;
 
-		ret = epoll_wait(eq->epfd, events, 2, timeout);
+		ret = ofi_epoll_wait(eq->epollfd, &contexts, 1, timeout);
 		if (ret == 0)
 			return -FI_EAGAIN;
 		else if (ret < 0)
@@ -1180,16 +1180,17 @@ static struct fi_ops_eq vrb_eq_ops = {
 static int vrb_eq_control(fid_t fid, int command, void *arg)
 {
 	struct vrb_eq *eq;
-	int ret = 0;
+	int ret;
 
 	eq = container_of(fid, struct vrb_eq, eq_fid.fid);
 	switch (command) {
 	case FI_GETWAIT:
-		if (!eq->epfd) {
-			ret = -FI_ENODATA;
-			break;
-		}
-		*(int *) arg = eq->epfd;
+#ifdef HAVE_EPOLL
+		*(int *) arg = eq->epollfd;
+		ret = 0;
+#else
+		ret = -FI_ENOSYS;
+#endif
 		break;
 	default:
 		ret = -FI_ENOSYS;
@@ -1215,7 +1216,7 @@ static int vrb_eq_close(fid_t fid)
 	if (eq->channel)
 		rdma_destroy_event_channel(eq->channel);
 
-	close(eq->epfd);
+	ofi_epoll_close(eq->epollfd);
 
 	while (!dlistfd_empty(&eq->list_head)) {
 		entry = container_of(eq->list_head.list.next,
@@ -1247,7 +1248,6 @@ int vrb_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
 		   struct fid_eq **eq, void *context)
 {
 	struct vrb_eq *_eq;
-	struct epoll_event event;
 	int ret;
 
 	_eq = calloc(1, sizeof *_eq);
@@ -1272,17 +1272,12 @@ int vrb_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
 		goto err1;
 	}
 
-	_eq->epfd = epoll_create1(0);
-	if (_eq->epfd < 0) {
-		ret = -errno;
+	ret = ofi_epoll_create(&_eq->epollfd);
+	if (ret)
 		goto err2;
-	}
-
-	memset(&event, 0, sizeof(event));
-	event.events = EPOLLIN;
 
-	if (epoll_ctl(_eq->epfd, EPOLL_CTL_ADD,
-		      _eq->list_head.signal.fd[FI_READ_FD], &event)) {
+	if (ofi_epoll_add(_eq->epollfd, _eq->list_head.signal.fd[FI_READ_FD],
+			  OFI_EPOLL_IN, NULL)) {
 		ret = -errno;
 		goto err3;
 	}
@@ -1301,7 +1296,8 @@ int vrb_eq_open(struct fid_fabric *fabric, struct fi_eq_attr *attr,
 		if (ret)
 			goto err4;
 
-		if (epoll_ctl(_eq->epfd, EPOLL_CTL_ADD, _eq->channel->fd, &event)) {
+		if (ofi_epoll_add(_eq->epollfd, _eq->channel->fd, OFI_EPOLL_IN,
+				  NULL)) {
 			ret = -errno;
 			goto err4;
 		}
@@ -1324,7 +1320,7 @@ err4:
 	if (_eq->channel)
 		rdma_destroy_event_channel(_eq->channel);
 err3:
-	close(_eq->epfd);
+	ofi_epoll_close(_eq->epollfd);
 err2:
 	dlistfd_head_free(&_eq->list_head);
 err1:
diff --git a/prov/verbs/src/verbs_info.c b/prov/verbs/src/verbs_info.c
index 4482c12..8fb2f2d 100644
--- a/prov/verbs/src/verbs_info.c
+++ b/prov/verbs/src/verbs_info.c
@@ -44,10 +44,13 @@
 
 #define VERBS_DOMAIN_CAPS (FI_LOCAL_COMM | FI_REMOTE_COMM)
 
-#define VERBS_MSG_CAPS (FI_MSG | FI_RMA | FI_ATOMICS | FI_READ | FI_WRITE |	\
-			FI_SEND | FI_RECV | FI_REMOTE_READ | FI_REMOTE_WRITE |	\
-			VERBS_DOMAIN_CAPS)
-#define VERBS_DGRAM_CAPS (FI_MSG | FI_RECV | FI_SEND | VERBS_DOMAIN_CAPS)
+#define VERBS_MSG_TX_CAPS (OFI_TX_MSG_CAPS | OFI_TX_RMA_CAPS | FI_ATOMICS)
+#define VERBS_MSG_RX_CAPS (OFI_RX_MSG_CAPS | OFI_RX_RMA_CAPS | FI_ATOMICS)
+#define VERBS_MSG_CAPS (VERBS_MSG_TX_CAPS | VERBS_MSG_RX_CAPS | VERBS_DOMAIN_CAPS)
+#define VERBS_DGRAM_TX_CAPS (OFI_TX_MSG_CAPS)
+#define VERBS_DGRAM_RX_CAPS (OFI_RX_MSG_CAPS)
+#define VERBS_DGRAM_CAPS (VERBS_DGRAM_TX_CAPS | VERBS_DGRAM_RX_CAPS | \
+			  VERBS_DOMAIN_CAPS)
 
 #define VERBS_DGRAM_RX_MODE (FI_MSG_PREFIX)
 
@@ -99,6 +102,7 @@ const struct fi_ep_attr verbs_ep_attr = {
 };
 
 const struct fi_rx_attr verbs_rx_attr = {
+	.caps			= VERBS_MSG_RX_CAPS,
 	.mode			= VERBS_RX_MODE,
 	.op_flags		= FI_COMPLETION,
 	.msg_order		= VERBS_MSG_ORDER,
@@ -107,6 +111,7 @@ const struct fi_rx_attr verbs_rx_attr = {
 };
 
 const struct fi_rx_attr verbs_dgram_rx_attr = {
+	.caps			= VERBS_DGRAM_RX_CAPS,
 	.mode			= VERBS_DGRAM_RX_MODE | VERBS_RX_MODE,
 	.op_flags		= FI_COMPLETION,
 	.msg_order		= VERBS_MSG_ORDER,
@@ -115,6 +120,7 @@ const struct fi_rx_attr verbs_dgram_rx_attr = {
 };
 
 const struct fi_tx_attr verbs_tx_attr = {
+	.caps			= VERBS_MSG_TX_CAPS,
 	.mode			= 0,
 	.op_flags		= VERBS_TX_OP_FLAGS,
 	.msg_order		= VERBS_MSG_ORDER,
@@ -124,6 +130,7 @@ const struct fi_tx_attr verbs_tx_attr = {
 };
 
 const struct fi_tx_attr verbs_dgram_tx_attr = {
+	.caps			= VERBS_DGRAM_TX_CAPS,
 	.mode			= 0,
 	.op_flags		= VERBS_TX_OP_FLAGS,
 	.msg_order		= VERBS_MSG_ORDER,
@@ -136,21 +143,18 @@ const struct verbs_ep_domain verbs_msg_domain = {
 	.suffix			= "",
 	.type			= FI_EP_MSG,
 	.protocol		= FI_PROTO_UNSPEC,
-	.caps			= VERBS_MSG_CAPS,
 };
 
 const struct verbs_ep_domain verbs_msg_xrc_domain = {
 	.suffix			= "-xrc",
 	.type			= FI_EP_MSG,
 	.protocol		= FI_PROTO_RDMA_CM_IB_XRC,
-	.caps			= VERBS_MSG_CAPS,
 };
 
 const struct verbs_ep_domain verbs_dgram_domain = {
 	.suffix			= "-dgram",
 	.type			= FI_EP_DGRAM,
 	.protocol		= FI_PROTO_UNSPEC,
-	.caps			= VERBS_DGRAM_CAPS,
 };
 
 /* The list (not thread safe) is populated once when the provider is initialized */
@@ -768,17 +772,18 @@ static int vrb_alloc_info(struct ibv_context *ctx, struct fi_info **info,
 	if (!fi)
 		return -FI_ENOMEM;
 
-	fi->caps = ep_dom->caps;
 	fi->handle = NULL;
 	*(fi->ep_attr) = verbs_ep_attr;
 	*(fi->domain_attr) = verbs_domain_attr;
 
 	switch (ep_dom->type) {
 	case FI_EP_MSG:
+		fi->caps = VERBS_MSG_CAPS;
 		*(fi->tx_attr) = verbs_tx_attr;
 		*(fi->rx_attr) = verbs_rx_attr;
 		break;
 	case FI_EP_DGRAM:
+		fi->caps = VERBS_DGRAM_CAPS;
 		fi->mode = VERBS_DGRAM_RX_MODE;
 		*(fi->tx_attr) = verbs_dgram_tx_attr;
 		*(fi->rx_attr) = verbs_dgram_rx_attr;
@@ -793,8 +798,6 @@ static int vrb_alloc_info(struct ibv_context *ctx, struct fi_info **info,
 	*(fi->fabric_attr) = verbs_fabric_attr;
 
 	fi->ep_attr->type = ep_dom->type;
-	fi->tx_attr->caps = ep_dom->caps;
-	fi->rx_attr->caps = ep_dom->caps;
 
 	fi->nic = ofi_nic_dup(NULL);
 	if (!fi->nic) {
@@ -1296,6 +1299,11 @@ int vrb_init_info(const struct fi_info **all_infos)
 					ctx_list[i]->device->name);
 				continue;
 			}
+			if (vrb_gl_data.device_name &&
+			    strncasecmp(ctx_list[i]->device->name,
+					vrb_gl_data.device_name,
+					strlen(vrb_gl_data.device_name)))
+				continue;
 
 			ret = vrb_alloc_info(ctx_list[i], &fi, ep_type[j]);
 			if (!ret) {
@@ -1517,6 +1525,31 @@ static int vrb_resolve_ib_ud_dest_addr(const char *node, const char *service,
 	return 0;
 }
 
+static void vrb_delete_dgram_infos(struct fi_info **info)
+{
+	struct fi_info *check_info = *info;
+	struct fi_info *cur, *prev = NULL;
+
+	*info = NULL;
+
+	while (check_info) {
+		if (check_info->ep_attr->type == FI_EP_DGRAM) {
+			cur = check_info;
+			if (prev)
+				prev->next = check_info->next;
+			check_info = check_info->next;
+
+			cur->next = NULL;
+			fi_freeinfo(cur);
+		} else {
+			prev = check_info;
+			if (!*info)
+				*info = check_info;
+			check_info = check_info->next;
+		}
+	}
+}
+
 static int vrb_handle_ib_ud_addr(const char *node, const char *service,
 				    uint64_t flags, struct fi_info **info)
 {
@@ -1546,7 +1579,8 @@ static int vrb_handle_ib_ud_addr(const char *node, const char *service,
 		if (!src_addr) {
 			VERBS_INFO(FI_LOG_CORE,
 			           "failed to allocate src addr.\n");
-			return -FI_ENODATA;
+			ret = -FI_ENODATA;
+			goto err;
 		}
 
 		if (flags & FI_SOURCE) {
@@ -1555,7 +1589,7 @@ static int vrb_handle_ib_ud_addr(const char *node, const char *service,
 					     &src_addr->service);
 				if (ret != 1) {
 					ret = -errno;
-					goto fn2;
+					goto err;
 				}
 			}
 
@@ -1568,16 +1602,17 @@ static int vrb_handle_ib_ud_addr(const char *node, const char *service,
 	if (!dest_addr && node && !(flags & FI_SOURCE)) {
 		ret = vrb_resolve_ib_ud_dest_addr(node, service, &dest_addr);
 		if (ret)
-			goto fn2; /* Here possible that `src_addr` isn't a NULL */
+			goto err; /* Here possible that `src_addr` isn't a NULL */
 	}
 
 	ret = vrb_set_info_addrs(*info, NULL, fmt, src_addr, dest_addr);
-	if  (ret)
-		goto fn2;
-
+	if  (!ret)
+		goto out;
+err:
+	vrb_delete_dgram_infos(info);
 	/* `fi_info::src_addr` and `fi_info::dest_addr` is freed
 	 * in the `fi_freeinfo` function in case of failure */
-fn2:
+out:
 	if (src_addr)
 		free(src_addr);
 	if (dest_addr)
diff --git a/src/common.c b/src/common.c
index df4ae37..29bb0db 100644
--- a/src/common.c
+++ b/src/common.c
@@ -228,12 +228,12 @@ uint64_t ofi_gettime_ns(void)
 
 uint64_t ofi_gettime_us(void)
 {
-	return ofi_gettime_ns() / 1000000;
+	return ofi_gettime_ns() / 1000;
 }
 
 uint64_t ofi_gettime_ms(void)
 {
-	return ofi_gettime_ns() / 1000;
+	return ofi_gettime_ns() / 1000000;
 }
 
 uint16_t ofi_get_sa_family(const struct fi_info *info)
@@ -846,7 +846,7 @@ int ofi_discard_socket(SOCKET sock, size_t len)
 
 #ifndef HAVE_EPOLL
 
-int fi_epoll_create(struct fi_epoll **ep)
+int ofi_epoll_create(struct fi_epoll **ep)
 {
 	int ret;
 
@@ -868,7 +868,7 @@ int fi_epoll_create(struct fi_epoll **ep)
 		goto err2;
 
 	(*ep)->fds[(*ep)->nfds].fd = (*ep)->signal.fd[FI_READ_FD];
-	(*ep)->fds[(*ep)->nfds].events = FI_EPOLL_IN;
+	(*ep)->fds[(*ep)->nfds].events = OFI_EPOLL_IN;
 	(*ep)->context[(*ep)->nfds++] = NULL;
 	slist_init(&(*ep)->work_item_list);
 	fastlock_init(&(*ep)->lock);
@@ -881,10 +881,10 @@ err1:
 }
 
 
-static int fi_epoll_ctl(struct fi_epoll *ep, enum fi_epoll_ctl op,
+static int ofi_epoll_ctl(struct fi_epoll *ep, enum ofi_epoll_ctl op,
 			int fd, uint32_t events, void *context)
 {
-	struct fi_epoll_work_item *item;
+	struct ofi_epoll_work_item *item;
 
 	item = calloc(1,sizeof(*item));
 	if (!item)
@@ -901,22 +901,22 @@ static int fi_epoll_ctl(struct fi_epoll *ep, enum fi_epoll_ctl op,
 	return 0;
 }
 
-int fi_epoll_add(struct fi_epoll *ep, int fd, uint32_t events, void *context)
+int ofi_epoll_add(struct fi_epoll *ep, int fd, uint32_t events, void *context)
 {
-	return fi_epoll_ctl(ep, EPOLL_CTL_ADD, fd, events, context);
+	return ofi_epoll_ctl(ep, EPOLL_CTL_ADD, fd, events, context);
 }
 
-int fi_epoll_mod(struct fi_epoll *ep, int fd, uint32_t events, void *context)
+int ofi_epoll_mod(struct fi_epoll *ep, int fd, uint32_t events, void *context)
 {
-	return fi_epoll_ctl(ep, EPOLL_CTL_MOD, fd, events, context);
+	return ofi_epoll_ctl(ep, EPOLL_CTL_MOD, fd, events, context);
 }
 
-int fi_epoll_del(struct fi_epoll *ep, int fd)
+int ofi_epoll_del(struct fi_epoll *ep, int fd)
 {
-	return fi_epoll_ctl(ep, EPOLL_CTL_DEL, fd, 0, NULL);
+	return ofi_epoll_ctl(ep, EPOLL_CTL_DEL, fd, 0, NULL);
 }
 
-static int fi_epoll_fd_array_grow(struct fi_epoll *ep)
+static int ofi_epoll_fd_array_grow(struct fi_epoll *ep)
 {
 	struct pollfd *fds;
 	void *contexts;
@@ -937,7 +937,7 @@ static int fi_epoll_fd_array_grow(struct fi_epoll *ep)
 	return FI_SUCCESS;
 }
 
-static void fi_epoll_cleanup_array(struct fi_epoll *ep)
+static void ofi_epoll_cleanup_array(struct fi_epoll *ep)
 {
 	int i;
 
@@ -954,19 +954,19 @@ static void fi_epoll_cleanup_array(struct fi_epoll *ep)
 	}
 }
 
-static void fi_epoll_process_work_item_list(struct fi_epoll *ep)
+static void ofi_epoll_process_work_item_list(struct fi_epoll *ep)
 {
 	struct slist_entry *entry;
-	struct fi_epoll_work_item *item;
+	struct ofi_epoll_work_item *item;
 	int i;
 
 	while (!slist_empty(&ep->work_item_list)) {
 		if ((ep->nfds == ep->size) &&
-		    fi_epoll_fd_array_grow(ep))
+		    ofi_epoll_fd_array_grow(ep))
 			continue;
 
 		entry = slist_remove_head(&ep->work_item_list);
-		item = container_of(entry, struct fi_epoll_work_item, entry);
+		item = container_of(entry, struct ofi_epoll_work_item, entry);
 
 		switch (item->type) {
 		case EPOLL_CTL_ADD:
@@ -987,10 +987,9 @@ static void fi_epoll_process_work_item_list(struct fi_epoll *ep)
 		case EPOLL_CTL_MOD:
 			for (i = 0; i < ep->nfds; i++) {
 				if (ep->fds[i].fd == item->fd) {
-
 					ep->fds[i].events = item->events;
 					ep->fds[i].revents &= item->events;
-					ep->context = item->context;
+					ep->context[i] = item->context;
 					break;
 				}
 			}
@@ -1002,10 +1001,10 @@ static void fi_epoll_process_work_item_list(struct fi_epoll *ep)
 		free(item);
 	}
 out:
-	fi_epoll_cleanup_array(ep);
+	ofi_epoll_cleanup_array(ep);
 }
 
-int fi_epoll_wait(struct fi_epoll *ep, void **contexts, int max_contexts,
+int ofi_epoll_wait(struct fi_epoll *ep, void **contexts, int max_contexts,
                   int timeout)
 {
 	int i, ret;
@@ -1024,10 +1023,11 @@ int fi_epoll_wait(struct fi_epoll *ep, void **contexts, int max_contexts,
 
 		fastlock_acquire(&ep->lock);
 		if (!slist_empty(&ep->work_item_list))
-			fi_epoll_process_work_item_list(ep);
+			ofi_epoll_process_work_item_list(ep);
 
 		fastlock_release(&ep->lock);
 
+		/* Index 0 is the internal signaling fd, skip it */
 		for (i = ep->index; i < ep->nfds && found < max_contexts; i++) {
 			if (ep->fds[i].revents && i) {
 				contexts[found++] = ep->context[i];
@@ -1049,15 +1049,15 @@ int fi_epoll_wait(struct fi_epoll *ep, void **contexts, int max_contexts,
 	return found;
 }
 
-void fi_epoll_close(struct fi_epoll *ep)
+void ofi_epoll_close(struct fi_epoll *ep)
 {
-	struct fi_epoll_work_item *item;
+	struct ofi_epoll_work_item *item;
 	struct slist_entry *entry;
 	if (ep) {
 		while (!slist_empty(&ep->work_item_list)) {
 			entry = slist_remove_head(&ep->work_item_list);
 			item = container_of(entry,
-					    struct fi_epoll_work_item,
+					    struct ofi_epoll_work_item,
 					    entry);
 			free(item);
 		}
@@ -1083,7 +1083,7 @@ void ofi_free_list_of_addr(struct slist *addr_list)
 }
 
 static inline
-void ofi_insert_loopback_addr(struct fi_provider *prov, struct slist *addr_list)
+void ofi_insert_loopback_addr(const struct fi_provider *prov, struct slist *addr_list)
 {
 	struct ofi_addr_list_entry *addr_entry;
 
@@ -1097,6 +1097,8 @@ void ofi_insert_loopback_addr(struct fi_provider *prov, struct slist *addr_list)
 			"available addr: ", &addr_entry->ipaddr);
 
 	strncpy(addr_entry->ipstr, "127.0.0.1", sizeof(addr_entry->ipstr));
+	strncpy(addr_entry->net_name, "127.0.0.1/32", sizeof(addr_entry->net_name));
+	strncpy(addr_entry->ifa_name, "lo", sizeof(addr_entry->ifa_name));
 	slist_insert_tail(&addr_entry->entry, addr_list);
 
 	addr_entry = calloc(1, sizeof(struct ofi_addr_list_entry));
@@ -1109,6 +1111,8 @@ void ofi_insert_loopback_addr(struct fi_provider *prov, struct slist *addr_list)
 			"available addr: ", &addr_entry->ipaddr);
 
 	strncpy(addr_entry->ipstr, "::1", sizeof(addr_entry->ipstr));
+	strncpy(addr_entry->net_name, "::1/128", sizeof(addr_entry->net_name));
+	strncpy(addr_entry->ifa_name, "lo", sizeof(addr_entry->ifa_name));
 	slist_insert_tail(&addr_entry->entry, addr_list);
 }
 
@@ -1154,7 +1158,33 @@ ofi_addr_list_entry_comp_speed(struct slist_entry *cur, const void *insert)
 	return (cur_addr->speed < insert_addr->speed);
 }
 
-void ofi_get_list_of_addr(struct fi_provider *prov, const char *env_name,
+void ofi_set_netmask_str(char *netstr, size_t len, struct ifaddrs *ifa)
+{
+	union ofi_sock_ip addr;
+	size_t prefix_len;
+
+	netstr[0] = '\0';
+	prefix_len = ofi_mask_addr(&addr.sa, ifa->ifa_addr, ifa->ifa_netmask);
+
+	switch (addr.sa.sa_family) {
+	case AF_INET:
+		inet_ntop(AF_INET, &addr.sin.sin_addr, netstr, len);
+		break;
+	case AF_INET6:
+		inet_ntop(AF_INET6, &addr.sin6.sin6_addr, netstr, len);
+		break;
+	default:
+		snprintf(netstr, len, "%s", "<unknown>");
+		netstr[len - 1] = '\0';
+		break;
+	}
+
+	snprintf(netstr + strlen(netstr), len - strlen(netstr),
+		 "%s%d", "/", (int) prefix_len);
+	netstr[len - 1] = '\0';
+}
+
+void ofi_get_list_of_addr(const struct fi_provider *prov, const char *env_name,
 			  struct slist *addr_list)
 {
 	int ret;
@@ -1162,72 +1192,79 @@ void ofi_get_list_of_addr(struct fi_provider *prov, const char *env_name,
 	struct ofi_addr_list_entry *addr_entry;
 	struct ifaddrs *ifaddrs, *ifa;
 
-	fi_param_get_str(prov, env_name, &iface);
+	fi_param_get_str((struct fi_provider *) prov, env_name, &iface);
 
 	ret = ofi_getifaddrs(&ifaddrs);
-	if (!ret) {
-		if (iface) {
-			for (ifa = ifaddrs; ifa != NULL; ifa = ifa->ifa_next) {
-				if (strncmp(iface, ifa->ifa_name,
-					    strlen(iface)) == 0) {
-					break;
-				}
-			}
-			if (ifa == NULL) {
-				FI_INFO(prov, FI_LOG_CORE,
-					"Can't set filter to unknown interface: (%s)\n",
-					iface);
-				iface = NULL;
-			}
-		}
-		for (ifa = ifaddrs; ifa != NULL; ifa = ifa->ifa_next) {
-			if (ifa->ifa_addr == NULL ||
-			    !(ifa->ifa_flags & IFF_UP) ||
-			    (ifa->ifa_flags & IFF_LOOPBACK) ||
-			    ((ifa->ifa_addr->sa_family != AF_INET) &&
-			     (ifa->ifa_addr->sa_family != AF_INET6)))
-				continue;
-			if (iface && strncmp(iface, ifa->ifa_name, strlen(iface)) != 0) {
-				FI_DBG(prov, FI_LOG_CORE,
-				       "Skip (%s) interface\n", ifa->ifa_name);
-				continue;
-			}
+	if (ret)
+		goto insert_lo;
 
-			addr_entry = calloc(1, sizeof(struct ofi_addr_list_entry));
-			if (!addr_entry)
-				continue;
-
-			memcpy(&addr_entry->ipaddr, ifa->ifa_addr,
-			       ofi_sizeofaddr(ifa->ifa_addr));
-			ofi_straddr_log(prov, FI_LOG_INFO, FI_LOG_CORE,
-					"available addr: ", ifa->ifa_addr);
-
-			if (!inet_ntop(ifa->ifa_addr->sa_family,
-					ofi_get_ipaddr(ifa->ifa_addr),
-					addr_entry->ipstr,
-					sizeof(addr_entry->ipstr))) {
-				FI_DBG(prov, FI_LOG_CORE,
-				       "inet_ntop failed: %d\n", errno);
-				free(addr_entry);
-				continue;
+	if (iface) {
+		for (ifa = ifaddrs; ifa != NULL; ifa = ifa->ifa_next) {
+			if (strncmp(iface, ifa->ifa_name,
+					strlen(iface)) == 0) {
+				break;
 			}
+		}
+		if (ifa == NULL) {
+			FI_INFO(prov, FI_LOG_CORE,
+				"Can't set filter to unknown interface: (%s)\n",
+				iface);
+			iface = NULL;
+		}
+	}
+	for (ifa = ifaddrs; ifa != NULL; ifa = ifa->ifa_next) {
+		if (ifa->ifa_addr == NULL ||
+			!(ifa->ifa_flags & IFF_UP) ||
+			(ifa->ifa_flags & IFF_LOOPBACK) ||
+			((ifa->ifa_addr->sa_family != AF_INET) &&
+			(ifa->ifa_addr->sa_family != AF_INET6)))
+			continue;
+		if (iface && strncmp(iface, ifa->ifa_name, strlen(iface)) != 0) {
+			FI_DBG(prov, FI_LOG_CORE,
+				"Skip (%s) interface\n", ifa->ifa_name);
+			continue;
+		}
 
-			addr_entry->speed = ofi_ifaddr_get_speed(ifa);
+		addr_entry = calloc(1, sizeof(*addr_entry));
+		if (!addr_entry)
+			continue;
 
-			slist_insert_before_first_match(addr_list, ofi_addr_list_entry_comp_speed,
-							&addr_entry->entry);
+		memcpy(&addr_entry->ipaddr, ifa->ifa_addr,
+			ofi_sizeofaddr(ifa->ifa_addr));
+		strncpy(addr_entry->ifa_name, ifa->ifa_name,
+			sizeof(addr_entry->ifa_name));
+		ofi_set_netmask_str(addr_entry->net_name,
+				    sizeof(addr_entry->net_name), ifa);
+
+		if (!inet_ntop(ifa->ifa_addr->sa_family,
+				ofi_get_ipaddr(ifa->ifa_addr),
+				addr_entry->ipstr,
+				sizeof(addr_entry->ipstr))) {
+			FI_DBG(prov, FI_LOG_CORE,
+				"inet_ntop failed: %d\n", errno);
+			free(addr_entry);
+			continue;
 		}
 
-		freeifaddrs(ifaddrs);
+		addr_entry->speed = ofi_ifaddr_get_speed(ifa);
+		FI_INFO(prov, FI_LOG_CORE, "Available addr: %s, "
+			"iface name: %s, speed: %zu\n",
+			addr_entry->ipstr, ifa->ifa_name, addr_entry->speed);
+
+		slist_insert_before_first_match(addr_list, ofi_addr_list_entry_comp_speed,
+						&addr_entry->entry);
 	}
 
+	freeifaddrs(ifaddrs);
+
+insert_lo:
 	/* Always add loopback address at the end */
 	ofi_insert_loopback_addr(prov, addr_list);
 }
 
 #elif defined HAVE_MIB_IPADDRTABLE
 
-void ofi_get_list_of_addr(struct fi_provider *prov, const char *env_name,
+void ofi_get_list_of_addr(const struct fi_provider *prov, const char *env_name,
 			  struct slist *addr_list)
 {
 	struct ofi_addr_list_entry *addr_entry;
@@ -1276,7 +1313,7 @@ out:
 
 #else /* !HAVE_MIB_IPADDRTABLE && !HAVE_MIB_IPADDRTABLE */
 
-void ofi_get_list_of_addr(struct fi_provider *prov, const char *env_name,
+void ofi_get_list_of_addr(const struct fi_provider *prov, const char *env_name,
 			  struct slist *addr_list)
 {
 	ofi_insert_loopback_addr(prov, addr_list);
diff --git a/src/fabric.c b/src/fabric.c
index e13a0ab..01b8d71 100644
--- a/src/fabric.c
+++ b/src/fabric.c
@@ -385,17 +385,15 @@ static void ofi_set_prov_type(struct fi_prov_context *ctx,
 		ctx->type = OFI_PROV_CORE;
 }
 
-static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
+static void ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 {
 	struct fi_prov_context *ctx;
 	struct ofi_prov *prov = NULL;
 	bool hidden = false;
-	int ret;
 
 	if (!provider || !provider->name) {
 		FI_DBG(&core_prov, FI_LOG_CORE,
 		       "no provider structure or name\n");
-		ret = -FI_EINVAL;
 		goto cleanup;
 	}
 
@@ -406,7 +404,6 @@ static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 	if (!provider->fabric) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
 			"provider missing mandatory entry points\n");
-		ret = -FI_EINVAL;
 		goto cleanup;
 	}
 
@@ -421,8 +418,6 @@ static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 			FI_MAJOR(provider->fi_version),
 			FI_MINOR(provider->fi_version), FI_MAJOR_VERSION,
 			FI_MINOR_VERSION);
-
-		ret = -FI_ENOSYS;
 		goto cleanup;
 	}
 
@@ -433,7 +428,6 @@ static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 		FI_INFO(&core_prov, FI_LOG_CORE,
 			"\"%s\" filtered by provider include/exclude "
 			"list, skipping\n", provider->name);
-		ret = -FI_ENODEV;
 		hidden = true;
 	}
 
@@ -455,7 +449,6 @@ static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 			FI_INFO(&core_prov, FI_LOG_CORE,
 				"a newer %s provider was already loaded; "
 				"ignoring this one\n", provider->name);
-			ret = -FI_EALREADY;
 			goto cleanup;
 		}
 
@@ -470,10 +463,8 @@ static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 		cleanup_provider(prov->provider, prov->dlhandle);
 	} else {
 		prov = ofi_create_prov_entry(provider->name);
-		if (!prov) {
-			ret = -FI_EOTHER;
+		if (!prov)
 			goto cleanup;
-		}
 	}
 
 	if (hidden)
@@ -482,11 +473,10 @@ static int ofi_register_provider(struct fi_provider *provider, void *dlhandle)
 update_prov_registry:
 	prov->dlhandle = dlhandle;
 	prov->provider = provider;
-	return 0;
+	return;
 
 cleanup:
 	cleanup_provider(provider, dlhandle);
-	return ret;
 }
 
 #ifdef HAVE_LIBDL
diff --git a/src/fi_tostr.c b/src/fi_tostr.c
index e9d9182..344ee6a 100644
--- a/src/fi_tostr.c
+++ b/src/fi_tostr.c
@@ -104,7 +104,7 @@ static void ofi_tostr_opflags(char *buf, uint64_t flags)
 	ofi_remove_comma(buf);
 }
 
-static void oofi_tostr_addr_format(char *buf, uint32_t addr_format)
+static void ofi_tostr_addr_format(char *buf, uint32_t addr_format)
 {
 	switch (addr_format) {
 	CASEENUMSTR(FI_FORMAT_UNSPEC);
@@ -551,7 +551,7 @@ static void ofi_tostr_info(char *buf, const struct fi_info *info)
 	ofi_strcatf(buf, " ]\n");
 
 	ofi_strcatf(buf, "%saddr_format: ", TAB);
-	oofi_tostr_addr_format(buf, info->addr_format);
+	ofi_tostr_addr_format(buf, info->addr_format);
 	ofi_strcatf(buf, "\n");
 
 	ofi_strcatf(buf, "%ssrc_addrlen: %zu\n", TAB, info->src_addrlen);
@@ -718,7 +718,7 @@ char *DEFAULT_SYMVER_PRE(fi_tostr)(const void *data, enum fi_type datatype)
 		ofi_tostr_opflags(buf, *val64);
 		break;
 	case FI_TYPE_ADDR_FORMAT:
-		oofi_tostr_addr_format(buf, *val32);
+		ofi_tostr_addr_format(buf, *val32);
 		break;
 	case FI_TYPE_TX_ATTR:
 		ofi_tostr_tx_attr(buf, data, "");
diff --git a/src/tree.c b/src/tree.c
index 0d1bf44..087826c 100644
--- a/src/tree.c
+++ b/src/tree.c
@@ -110,7 +110,14 @@ void ofi_rbmap_cleanup(struct ofi_rbmap *map)
 
 void ofi_rbmap_destroy(struct ofi_rbmap *map)
 {
+	struct ofi_rbnode *node;
+
 	ofi_rbmap_cleanup(map);
+	while (map->free_list) {
+		node = map->free_list;
+		map->free_list = node->right;
+		free(node);
+	}
 	free(map);
 }
 
