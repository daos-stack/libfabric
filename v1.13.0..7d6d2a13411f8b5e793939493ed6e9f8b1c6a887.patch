diff --git a/.github/workflows/pr-ci.yml b/.github/workflows/pr-ci.yml
index 970e920..7e66800 100644
--- a/.github/workflows/pr-ci.yml
+++ b/.github/workflows/pr-ci.yml
@@ -1,5 +1,41 @@
-name: OS Matrix Build Checks
+name: Build Checks
 on: [push, pull_request]
+env:
+  APT_PACKAGES: >-
+    abi-compliance-checker
+    abi-dumper
+    build-essential
+    debhelper
+    dh-systemd
+    fakeroot
+    gcc
+    git
+    libnl-3-200 libnl-3-dev libnl-route-3-200 libnl-route-3-dev
+    libnuma-dev
+    libudev-dev
+    uuid-dev
+    make
+    ninja-build
+    pandoc
+    pkg-config
+    python
+    rpm
+    sparse
+    valgrind
+    wget
+  OFI_PROVIDER_FLAGS: >-
+    --enable-efa=rdma-core/build
+    --enable-mrail
+    --enable-psm3=rdma-core/build
+    --enable-rxd
+    --enable-rxm
+    --enable-shm
+    --enable-tcp
+    --enable-udp
+    --enable-usnic
+    --enable-verbs=rdma-core/build
+  RDMA_CORE_PATH: 'rdma-core/build'
+  RDMA_CORE_VERSION: v34.1
 jobs:
   linux:
     runs-on: '${{ matrix.os }}'
@@ -13,51 +49,71 @@ jobs:
           - clang
       fail-fast: false
     steps:
-      - name: Install dependencies (Linux OS)
+      - name: Install dependencies (Linux)
         run: |
           sudo apt-get update
-          sudo apt-get install -y abi-compliance-checker \
-                                  abi-dumper \
-                                  build-essential \
-                                  debhelper \
-                                  dh-systemd \
-                                  fakeroot \
-                                  gcc \
-                                  git \
-                                  libnl-3-200 libnl-3-dev libnl-route-3-200 libnl-route-3-dev \
-                                  libnuma-dev \
-                                  libudev-dev \
-                                  uuid-dev \
-                                  make \
-                                  ninja-build \
-                                  pandoc \
-                                  pkg-config \
-                                  python \
-                                  rpm \
-                                  sparse \
-                                  valgrind \
-                                  wget
+          sudo apt-get install -y ${{ env.APT_PACKAGES }}
       - uses: actions/checkout@v2
       - name: Build Check
         run: |
-          git clone --depth 1 -b v34.1 https://github.com/linux-rdma/rdma-core.git
+          set -x
+          git clone --depth 1 -b ${{ env.RDMA_CORE_VERSION }} https://github.com/linux-rdma/rdma-core.git
           pushd rdma-core; bash build.sh; popd
-          export RDMA_CORE_PATH="rdma-core/build"
-          export LD_LIBRARY_PATH="$RDMA_CORE_PATH/lib:$LD_LIBRARY_PATH"
+          export LD_LIBRARY_PATH="${{ env.RDMA_CORE_PATH }}/lib:$LD_LIBRARY_PATH"
           ./autogen.sh
-          ./configure --prefix=$PWD/install --enable-efa=${RDMA_CORE_PATH} \
-                                            --enable-mrail \
-                                            --enable-psm3=${RDMA_CORE_PATH} \
-                                            --enable-rxd \
-                                            --enable-rxm \
-                                            --enable-shm \
-                                            --enable-tcp \
-                                            --enable-udp \
-                                            --enable-usnic \
-                                            --enable-verbs=${RDMA_CORE_PATH} \
+          ./configure --prefix=$PWD/install ${{ env.OFI_PROVIDER_FLAGS }} CC=${{ matrix.cc }}
+          make -j 2; make install
+          $PWD/install/bin/fi_info -l
+      - name: Upload build logs
+        if: failure()
+        uses: actions/upload-artifact@v2
+        with:
+          name: config.log
+          path: config.log
+  hmem:
+    runs-on: ubuntu-20.04
+    steps:
+      - name: Install dependencies (Linux)
+        run: |
+          sudo apt-get update
+          sudo apt-get install -y ${{ env.APT_PACKAGES }}
+      - name: Install CUDA
+        run: |
+          sudo apt-get install -y nvidia-cuda-toolkit
+      - name: Install ROCm
+        run: |
+          echo "Installing ROCm SDK"
+          # TODO: Install ROCm dependencies and add --with-rocm to build in next step
+      - name: Install Ze
+        run: |
+          echo "Installing Ze SDK"
+          sudo apt-get install -y gpg-agent wget
+          wget -qO - https://repositories.intel.com/graphics/intel-graphics.key | sudo apt-key add -
+          sudo apt-add-repository 'deb [arch=amd64] https://repositories.intel.com/graphics/ubuntu focal main'
+          sudo apt-get update
+          sudo apt-get install -y level-zero level-zero-dev
+      - uses: actions/checkout@v2
+      - name: HMEM Checks
+        run: |
+          set -x
+          # We could use 'upload-artifact' and persist the rdma-core build
+          # across jobs, but this is just as quick.
+          git clone --depth 1 -b ${{ env.RDMA_CORE_VERSION }} https://github.com/linux-rdma/rdma-core.git
+          pushd rdma-core; bash build.sh; popd
+          export LD_LIBRARY_PATH="${{ env.RDMA_CORE_PATH }}/lib:$LD_LIBRARY_PATH"
+          ./autogen.sh
+          ./configure --prefix=$PWD/install ${{ env.OFI_PROVIDER_FLAGS }} \
+                                            --with-cuda=/usr/local/cuda --with-ze \
                                             CC=${{ matrix.cc }}
-          make -j 4; make install
+          make -j 2; make install
           $PWD/install/bin/fi_info -l
+          $PWD/install/bin/fi_info -c FI_HMEM
+      - name: Upload build logs
+        if: failure()
+        uses: actions/upload-artifact@v2
+        with:
+          name: config.log
+          path: config.log
   macos:
     runs-on: macos-10.15
     steps:
@@ -70,5 +126,11 @@ jobs:
         run: |
           ./autogen.sh
           ./configure --prefix=$PWD/install
-          make -j 4; make install
+          make -j 2; make install
           $PWD/install/bin/fi_info -l
+      - name: Upload build logs
+        if: failure()
+        uses: actions/upload-artifact@v2
+        with:
+          name: config.log
+          path: config.log
diff --git a/Makefile.am b/Makefile.am
index e0eb6e2..d9e4268 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -76,6 +76,7 @@ common_srcs =				\
 	prov/util/src/util_mr_cache.c	\
 	prov/util/src/cuda_mem_monitor.c \
 	prov/util/src/rocr_mem_monitor.c \
+	prov/util/src/ze_mem_monitor.c \
 	prov/util/src/util_coll.c
 
 
diff --git a/configure.ac b/configure.ac
index 4fc4ea5..5f95b29 100644
--- a/configure.ac
+++ b/configure.ac
@@ -7,7 +7,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ([2.60])
-AC_INIT([libfabric], [1.13.0], [ofiwg@lists.openfabrics.org])
+AC_INIT([libfabric], [1.14.0a1], [ofiwg@lists.openfabrics.org])
 AC_CONFIG_SRCDIR([src/fabric.c])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
@@ -550,10 +550,7 @@ AS_IF([test x"$with_ze" != x"no"],
 			[$with_ze],
 			[],
 			[have_ze=1],
-			[], [])
-       CPPFLAGS="$CPPFLAGS $ze_CPPFLAGS"
-       LDFLAGS="$LDFLAGS $ze_LDFLAGS"
-       LIBS="$LIBS $ze_LIBS"],
+			[], [])],
       [])
 
 have_drm=0
@@ -568,6 +565,24 @@ AS_IF([test x"$with_ze" != x"no" && test -n "$with_ze" && test "$have_ze" = "0"
 AC_DEFINE_UNQUOTED([HAVE_LIBZE], [$have_ze], [ZE support])
 AC_DEFINE_UNQUOTED([HAVE_DRM], [$have_drm], [i915 DRM header])
 
+AC_ARG_ENABLE([ze-dlopen],
+    [AS_HELP_STRING([--enable-ze-dlopen],
+        [Enable dlopen of ZE libraries @<:@default=no@:>@])
+    ],
+    [
+        AS_IF([test "$freebsd" = "0"], [
+            AC_CHECK_LIB(dl, dlopen, [],
+                [AC_MSG_ERROR([dlopen not found.  libfabric requires libdl.])])
+        ])
+        AC_DEFINE([ENABLE_ZE_DLOPEN], [1], [dlopen ZE libraries])
+    ],
+    [enable_ze_dlopen=no])
+
+AS_IF([test x"$enable_ze_dlopen" != x"yes"], [LIBS="$LIBS $ze_LIBS"])
+AS_IF([test "$have_ze" = "1" && test x"$with_ze" != x"yes"],
+      [CPPFLAGS="$CPPFLAGS $ze_CPPFLAGS"
+       LDFLAGS="$LDFLAGS $ze_LDFLAGS"])
+
 enable_memhooks=1
 AC_ARG_ENABLE([memhooks-monitor],
               [AC_HELP_STRING([--disable-memhooks-monitor],
diff --git a/fabtests/Makefile.am b/fabtests/Makefile.am
index 5382f87..bdd71f5 100644
--- a/fabtests/Makefile.am
+++ b/fabtests/Makefile.am
@@ -43,6 +43,8 @@ bin_PROGRAMS = \
 	functional/fi_rdm_atomic \
 	functional/fi_multi_recv \
 	functional/fi_bw \
+	functional/fi_rdm_multi_client \
+	functional/fi_loopback \
 	benchmarks/fi_msg_pingpong \
 	benchmarks/fi_msg_bw \
 	benchmarks/fi_rma_bw \
@@ -113,7 +115,6 @@ noinst_LTLIBRARIES = libfabtests.la
 
 libfabtests_la_SOURCES = \
 	common/shared.c \
-	common/jsmn.c \
 	common/hmem.c \
 	common/hmem_cuda.c \
 	common/hmem_rocr.c \
@@ -258,6 +259,14 @@ functional_fi_bw_SOURCES = \
 	functional/bw.c
 functional_fi_bw_LDADD = libfabtests.la
 
+functional_fi_rdm_multi_client_SOURCES = \
+	functional/rdm_multi_client.c
+functional_fi_rdm_multi_client_LDADD = libfabtests.la
+
+functional_fi_loopback_SOURCES = \
+	functional/loopback.c
+functional_fi_loopback_LDADD = libfabtests.la
+
 benchmarks_fi_msg_pingpong_SOURCES = \
 	benchmarks/msg_pingpong.c \
 	$(benchmarks_srcs)
@@ -427,6 +436,7 @@ dummy_man_pages = \
 	man/man1/fi_getinfo_test.1 \
 	man/man1/fi_mr_test.1 \
 	man/man1/fi_bw.1 \
+	man/man1/fi_rdm_multi_client.1 \
 	man/man1/fi_ubertest.1
 
 nroff:
diff --git a/fabtests/Makefile.win b/fabtests/Makefile.win
index 2c416de..0ec7f4b 100644
--- a/fabtests/Makefile.win
+++ b/fabtests/Makefile.win
@@ -35,7 +35,7 @@ outdir = $(output_root)$(arch)\release-v142
 CFLAGS = $(CFLAGS) /O2 /MT
 !endif
 
-basedeps = common\hmem.c common\shared.c common\jsmn.c \
+basedeps = common\hmem.c common\shared.c \
 	common\windows\getopt.c common\windows\osd.c \
 	common\hmem_cuda.c common\hmem_rocr.c common\hmem_ze.c
 
diff --git a/fabtests/benchmarks/dgram_pingpong.c b/fabtests/benchmarks/dgram_pingpong.c
index 74920a7..191f0da 100644
--- a/fabtests/benchmarks/dgram_pingpong.c
+++ b/fabtests/benchmarks/dgram_pingpong.c
@@ -119,6 +119,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_cntr_pingpong.c b/fabtests/benchmarks/rdm_cntr_pingpong.c
index 61c64a6..a4a46fa 100644
--- a/fabtests/benchmarks/rdm_cntr_pingpong.c
+++ b/fabtests/benchmarks/rdm_cntr_pingpong.c
@@ -101,6 +101,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_pingpong.c b/fabtests/benchmarks/rdm_pingpong.c
index bd2dc40..96cc725 100644
--- a/fabtests/benchmarks/rdm_pingpong.c
+++ b/fabtests/benchmarks/rdm_pingpong.c
@@ -102,6 +102,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_tagged_bw.c b/fabtests/benchmarks/rdm_tagged_bw.c
index 323a66d..66ba7d8 100644
--- a/fabtests/benchmarks/rdm_tagged_bw.c
+++ b/fabtests/benchmarks/rdm_tagged_bw.c
@@ -105,6 +105,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_BULK_DATA;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/benchmarks/rdm_tagged_pingpong.c b/fabtests/benchmarks/rdm_tagged_pingpong.c
index e0ca5f2..9a623af 100644
--- a/fabtests/benchmarks/rdm_tagged_pingpong.c
+++ b/fabtests/benchmarks/rdm_tagged_pingpong.c
@@ -103,6 +103,7 @@ int main(int argc, char **argv)
 	hints->domain_attr->mr_mode = opts.mr_mode;
 	hints->domain_attr->threading = FI_THREAD_DOMAIN;
 	hints->tx_attr->tclass = FI_TC_LOW_LATENCY;
+	hints->addr_format = opts.address_format;
 
 	ret = run();
 
diff --git a/fabtests/common/jsmn.c b/fabtests/common/jsmn.c
deleted file mode 100644
index 9fe0fb4..0000000
--- a/fabtests/common/jsmn.c
+++ /dev/null
@@ -1,333 +0,0 @@
-/*
- * Copyright (c) 2010 Serge A. Zaitsev
- *
- * Permission is hereby granted, free of charge, to any person obtaining a copy
- * of this software and associated documentation files (the "Software"), to deal
- * in the Software without restriction, including without limitation the rights
- * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
- * copies of the Software, and to permit persons to whom the Software is
- * furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
- * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
- * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
- * THE SOFTWARE.
- */
-
-#include <stdlib.h>
-
-#include "jsmn.h"
-
-/**
- * Allocates a fresh unused token from the token pull.
- */
-static jsmntok_t *jsmn_alloc_token(jsmn_parser *parser,
-		jsmntok_t *tokens, size_t num_tokens) {
-	jsmntok_t *tok;
-	if (parser->toknext >= num_tokens) {
-		return NULL;
-	}
-	tok = &tokens[parser->toknext++];
-	tok->start = tok->end = -1;
-	tok->size = 0;
-#ifdef JSMN_PARENT_LINKS
-	tok->parent = -1;
-#endif
-	return tok;
-}
-
-/**
- * Fills token type and boundaries.
- */
-static void jsmn_fill_token(jsmntok_t *token, jsmntype_t type,
-                            int start, int end) {
-	token->type = type;
-	token->start = start;
-	token->end = end;
-	token->size = 0;
-}
-
-/**
- * Fills next available token with JSON primitive.
- */
-static jsmnerr_t jsmn_parse_primitive(jsmn_parser *parser, const char *js,
-		size_t len, jsmntok_t *tokens, size_t num_tokens) {
-	jsmntok_t *token;
-	int start;
-
-	start = parser->pos;
-
-	for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
-		switch (js[parser->pos]) {
-#ifndef JSMN_STRICT
-			/* In strict mode primitive must be followed by "," or "}" or "]" */
-			case ':':
-#endif
-			case '\t' : case '\r' : case '\n' : case ' ' :
-			case ','  : case ']'  : case '}' :
-				goto found;
-		}
-		if (js[parser->pos] < 32 || js[parser->pos] >= 127) {
-			parser->pos = start;
-			return JSMN_ERROR_INVAL;
-		}
-	}
-#ifdef JSMN_STRICT
-	/* In strict mode primitive must be followed by a comma/object/array */
-	parser->pos = start;
-	return JSMN_ERROR_PART;
-#endif
-
-found:
-	if (tokens == NULL) {
-		parser->pos--;
-		return 0;
-	}
-	token = jsmn_alloc_token(parser, tokens, num_tokens);
-	if (token == NULL) {
-		parser->pos = start;
-		return JSMN_ERROR_NOMEM;
-	}
-	jsmn_fill_token(token, JSMN_PRIMITIVE, start, parser->pos);
-#ifdef JSMN_PARENT_LINKS
-	token->parent = parser->toksuper;
-#endif
-	parser->pos--;
-	return 0;
-}
-
-/**
- * Filsl next token with JSON string.
- */
-static jsmnerr_t jsmn_parse_string(jsmn_parser *parser, const char *js,
-		size_t len, jsmntok_t *tokens, size_t num_tokens) {
-	jsmntok_t *token;
-
-	int start = parser->pos;
-
-	parser->pos++;
-
-	/* Skip starting quote */
-	for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
-		char c = js[parser->pos];
-
-		/* Quote: end of string */
-		if (c == '\"') {
-			if (tokens == NULL) {
-				return 0;
-			}
-			token = jsmn_alloc_token(parser, tokens, num_tokens);
-			if (token == NULL) {
-				parser->pos = start;
-				return JSMN_ERROR_NOMEM;
-			}
-			jsmn_fill_token(token, JSMN_STRING, start+1, parser->pos);
-#ifdef JSMN_PARENT_LINKS
-			token->parent = parser->toksuper;
-#endif
-			return 0;
-		}
-
-		/* Backslash: Quoted symbol expected */
-		if (c == '\\' && parser->pos + 1 < len) {
-			int i;
-			parser->pos++;
-			switch (js[parser->pos]) {
-				/* Allowed escaped symbols */
-				case '\"': case '/' : case '\\' : case 'b' :
-				case 'f' : case 'r' : case 'n'  : case 't' :
-					break;
-				/* Allows escaped symbol \uXXXX */
-				case 'u':
-					parser->pos++;
-					for(i = 0; i < 4 && parser->pos < len && js[parser->pos] != '\0'; i++) {
-						/* If it isn't a hex character we have an error */
-						if(!((js[parser->pos] >= 48 && js[parser->pos] <= 57) || /* 0-9 */
-									(js[parser->pos] >= 65 && js[parser->pos] <= 70) || /* A-F */
-									(js[parser->pos] >= 97 && js[parser->pos] <= 102))) { /* a-f */
-							parser->pos = start;
-							return JSMN_ERROR_INVAL;
-						}
-						parser->pos++;
-					}
-					parser->pos--;
-					break;
-				/* Unexpected symbol */
-				default:
-					parser->pos = start;
-					return JSMN_ERROR_INVAL;
-			}
-		}
-	}
-	parser->pos = start;
-	return JSMN_ERROR_PART;
-}
-
-/**
- * Parse JSON string and fill tokens.
- */
-jsmnerr_t jsmn_parse(jsmn_parser *parser, const char *js, size_t len,
-		jsmntok_t *tokens, unsigned int num_tokens) {
-	jsmnerr_t r;
-	int i;
-	jsmntok_t *token;
-	int count = 0;
-
-	for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
-		char c;
-		jsmntype_t type;
-
-		c = js[parser->pos];
-		switch (c) {
-			case '{': case '[':
-				count++;
-				if (tokens == NULL) {
-					break;
-				}
-				token = jsmn_alloc_token(parser, tokens, num_tokens);
-				if (token == NULL)
-					return JSMN_ERROR_NOMEM;
-				if (parser->toksuper != -1) {
-					tokens[parser->toksuper].size++;
-#ifdef JSMN_PARENT_LINKS
-					token->parent = parser->toksuper;
-#endif
-				}
-				token->type = (c == '{' ? JSMN_OBJECT : JSMN_ARRAY);
-				token->start = parser->pos;
-				parser->toksuper = parser->toknext - 1;
-				break;
-			case '}': case ']':
-				if (tokens == NULL)
-					break;
-				type = (c == '}' ? JSMN_OBJECT : JSMN_ARRAY);
-#ifdef JSMN_PARENT_LINKS
-				if (parser->toknext < 1) {
-					return JSMN_ERROR_INVAL;
-				}
-				token = &tokens[parser->toknext - 1];
-				for (;;) {
-					if (token->start != -1 && token->end == -1) {
-						if (token->type != type) {
-							return JSMN_ERROR_INVAL;
-						}
-						token->end = parser->pos + 1;
-						parser->toksuper = token->parent;
-						break;
-					}
-					if (token->parent == -1) {
-						break;
-					}
-					token = &tokens[token->parent];
-				}
-#else
-				for (i = parser->toknext - 1; i >= 0; i--) {
-					token = &tokens[i];
-					if (token->start != -1 && token->end == -1) {
-						if (token->type != type) {
-							return JSMN_ERROR_INVAL;
-						}
-						parser->toksuper = -1;
-						token->end = parser->pos + 1;
-						break;
-					}
-				}
-				/* Error if unmatched closing bracket */
-				if (i == -1) return JSMN_ERROR_INVAL;
-				for (; i >= 0; i--) {
-					token = &tokens[i];
-					if (token->start != -1 && token->end == -1) {
-						parser->toksuper = i;
-						break;
-					}
-				}
-#endif
-				break;
-			case '\"':
-				r = jsmn_parse_string(parser, js, len, tokens, num_tokens);
-				if (r < 0) return r;
-				count++;
-				if (parser->toksuper != -1 && tokens != NULL)
-					tokens[parser->toksuper].size++;
-				break;
-			case '\t' : case '\r' : case '\n' : case ' ':
-				break;
-			case ':':
-				parser->toksuper = parser->toknext - 1;
-				break;
-			case ',':
-				if (tokens != NULL &&
-						tokens[parser->toksuper].type != JSMN_ARRAY &&
-						tokens[parser->toksuper].type != JSMN_OBJECT) {
-#ifdef JSMN_PARENT_LINKS
-					parser->toksuper = tokens[parser->toksuper].parent;
-#else
-					for (i = parser->toknext - 1; i >= 0; i--) {
-						if (tokens[i].type == JSMN_ARRAY || tokens[i].type == JSMN_OBJECT) {
-							if (tokens[i].start != -1 && tokens[i].end == -1) {
-								parser->toksuper = i;
-								break;
-							}
-						}
-					}
-#endif
-				}
-				break;
-#ifdef JSMN_STRICT
-			/* In strict mode primitives are: numbers and booleans */
-			case '-': case '0': case '1' : case '2': case '3' : case '4':
-			case '5': case '6': case '7' : case '8': case '9':
-			case 't': case 'f': case 'n' :
-				/* And they must not be keys of the object */
-				if (tokens != NULL) {
-					jsmntok_t *t = &tokens[parser->toksuper];
-					if (t->type == JSMN_OBJECT ||
-							(t->type == JSMN_STRING && t->size != 0)) {
-						return JSMN_ERROR_INVAL;
-					}
-				}
-#else
-			/* In non-strict mode every unquoted value is a primitive */
-			default:
-#endif
-				r = jsmn_parse_primitive(parser, js, len, tokens, num_tokens);
-				if (r < 0) return r;
-				count++;
-				if (parser->toksuper != -1 && tokens != NULL)
-					tokens[parser->toksuper].size++;
-				break;
-
-#ifdef JSMN_STRICT
-			/* Unexpected char in strict mode */
-			default:
-				return JSMN_ERROR_INVAL;
-#endif
-		}
-	}
-
-	for (i = parser->toknext - 1; i >= 0; i--) {
-		/* Unmatched opened object or array */
-		if (tokens && tokens[i].start != -1 && tokens[i].end == -1) {
-			return JSMN_ERROR_PART;
-		}
-	}
-
-	return count;
-}
-
-/**
- * Creates a new parser based over a given  buffer with an array of tokens
- * available.
- */
-void jsmn_init(jsmn_parser *parser) {
-	parser->pos = 0;
-	parser->toknext = 0;
-	parser->toksuper = -1;
-}
-
diff --git a/fabtests/common/shared.c b/fabtests/common/shared.c
index 77a7ead..583ee00 100644
--- a/fabtests/common/shared.c
+++ b/fabtests/common/shared.c
@@ -2,6 +2,7 @@
  * Copyright (c) 2013-2018 Intel Corporation.  All rights reserved.
  * Copyright (c) 2016 Cray Inc.  All rights reserved.
  * Copyright (c) 2014-2017, Cisco Systems, Inc. All rights reserved.
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under the BSD license below:
  *
@@ -772,6 +773,10 @@ free:
 	return ret;
 }
 
+/*
+ * Handles a persistent server communicating with multiple clients,
+ * one at a time, in sequence.
+ */
 int ft_accept_next_client() {
 	int ret;
 
@@ -782,9 +787,48 @@ int ft_accept_next_client() {
 			return ret;
 	}
 
+	/* Clients may be separate processes, so re-initialize any OOB setup. */
+	if (opts.options & FT_OPT_OOB_ADDR_EXCH) {
+		ret = ft_reset_oob();
+		if (ret)
+			return ret;
+	}
 	return ft_init_av();
 }
 
+/*
+ * Re-initialize the OOB setup.
+ */
+int ft_reset_oob()
+{
+	int ret;
+	ret = ft_close_oob();
+	if (ret) {
+		FT_PRINTERR("ft_close_oob", ret);
+		return ret;
+	}
+	ret = ft_init_oob();
+	if (ret) {
+		FT_PRINTERR("ft_init_oob", ret);
+		return ret;
+	}
+	return 0;
+}
+
+int ft_close_oob()
+{
+	int ret;
+	if (oob_sock == -1)
+		return 0;
+	ret = close(oob_sock);
+	if (ret) {
+		FT_PRINTERR("close", errno);
+		return ret;
+	}
+	oob_sock = -1;
+	return 0;
+}
+
 int ft_getinfo(struct fi_info *hints, struct fi_info **info)
 {
 	char *node, *service;
@@ -1892,8 +1936,11 @@ ssize_t ft_tx(struct fid_ep *ep, fi_addr_t fi_addr, size_t size, void *ctx)
 {
 	ssize_t ret;
 
-	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
-		ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE)) {
+		ret = ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+		if (ret)
+			return ret;
+	}
 
 	ret = ft_post_tx(ep, fi_addr, size, NO_CQ_DATA, ctx);
 	if (ret)
@@ -1923,8 +1970,11 @@ ssize_t ft_inject(struct fid_ep *ep, fi_addr_t fi_addr, size_t size)
 {
 	ssize_t ret;
 
-	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
-		ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE)) {
+		ret = ft_fill_buf((char *) tx_buf + ft_tx_prefix_size(), size);
+		if (ret)
+			return ret;
+	}
 
 	ret = ft_post_inject(ep, fi_addr, size);
 	if (ret)
@@ -3020,10 +3070,10 @@ void ft_parse_addr_opts(int op, char *optarg, struct ft_opts *opts)
 			opts->oob_port = default_oob_port;
 		break;
 	case 'F':
-		if (!strncasecmp("fi_sockaddr_in", optarg, 14))
-			opts->address_format = FI_SOCKADDR_IN;
-		else if (!strncasecmp("fi_sockaddr_in6", optarg, 15))
+		if (!strncasecmp("fi_sockaddr_in6", optarg, 15))
 			opts->address_format = FI_SOCKADDR_IN6;
+		else if (!strncasecmp("fi_sockaddr_in", optarg, 14))
+			opts->address_format = FI_SOCKADDR_IN;
 		else if (!strncasecmp("fi_sockaddr_ib", optarg, 14))
 			opts->address_format = FI_SOCKADDR_IB;
 		else if (!strncasecmp("fi_sockaddr", optarg, 11)) /* keep me last */
@@ -3121,18 +3171,36 @@ int ft_parse_rma_opts(int op, char *optarg, struct fi_info *hints,
 	return 0;
 }
 
-void ft_fill_buf(void *buf, size_t size)
+int ft_fill_buf(void *buf, size_t size)
 {
 	char *msg_buf;
 	int msg_index = 0;
 	size_t i;
+	int ret = 0;
+
+	if (opts.iface != FI_HMEM_SYSTEM) {
+		msg_buf = malloc(size);
+		if (!msg_buf)
+			return -FI_ENOMEM;
+	} else {
+		msg_buf = (char *) buf;
+	}
 
-	msg_buf = (char *) buf;
 	for (i = 0; i < size; i++) {
 		msg_buf[i] = integ_alphabet[msg_index];
 		if (++msg_index >= integ_alphabet_length)
 			msg_index = 0;
 	}
+
+	if (opts.iface != FI_HMEM_SYSTEM) {
+		ret = ft_hmem_copy_to(opts.iface, opts.device, buf, msg_buf, size);
+		if (ret)
+			goto out;
+	}
+out:
+	if (opts.iface != FI_HMEM_SYSTEM)
+		free(msg_buf);
+	return ret;
 }
 
 int ft_check_buf(void *buf, size_t size)
@@ -3141,8 +3209,20 @@ int ft_check_buf(void *buf, size_t size)
 	char c;
 	int msg_index = 0;
 	size_t i;
+	int ret = 0;
 
-	recv_data = (char *)buf;
+	if (opts.iface != FI_HMEM_SYSTEM) {
+		recv_data = malloc(size);
+		if (!recv_data)
+			return -FI_ENOMEM;
+	
+		ret = ft_hmem_copy_from(opts.iface, opts.device,
+					recv_data, buf, size);
+		if (ret)
+			goto out;
+	} else {
+		recv_data = (char *)buf;
+	}
 
 	for (i = 0; i < size; i++) {
 		c = integ_alphabet[msg_index];
@@ -3154,10 +3234,13 @@ int ft_check_buf(void *buf, size_t size)
 	if (i != size) {
 		printf("Data check error (%c!=%c) at byte %zu for "
 		       "buffer size %zu\n", c, recv_data[i], i, size);
-		return -FI_EIO;
+		ret = -FI_EIO;
 	}
 
-	return 0;
+out:
+	if (opts.iface != FI_HMEM_SYSTEM)
+		free(recv_data);
+	return ret;
 }
 
 uint64_t ft_init_cq_data(struct fi_info *info)
diff --git a/fabtests/configure.ac b/fabtests/configure.ac
index d20acb8..586d9d3 100644
--- a/fabtests/configure.ac
+++ b/fabtests/configure.ac
@@ -5,7 +5,7 @@ dnl
 dnl Process this file with autoconf to produce a configure script.
 
 AC_PREREQ(2.57)
-AC_INIT([fabtests], [1.13.0], [ofiwg@lists.openfabrics.org])
+AC_INIT([fabtests], [1.14.0a1], [ofiwg@lists.openfabrics.org])
 AC_CONFIG_AUX_DIR(config)
 AC_CONFIG_MACRO_DIR(config)
 AC_CONFIG_HEADERS(config.h)
diff --git a/fabtests/fabtests.vcxproj b/fabtests/fabtests.vcxproj
index 5242eb8..d7bd0fe 100644
--- a/fabtests/fabtests.vcxproj
+++ b/fabtests/fabtests.vcxproj
@@ -1,4 +1,4 @@
-﻿<?xml version="1.0" encoding="utf-8"?>
+<?xml version="1.0" encoding="utf-8"?>
 <Project DefaultTargets="Build" ToolsVersion="14.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
   <ItemGroup Label="ProjectConfigurations">
     <ProjectConfiguration Include="Debug-v140|x64">
@@ -149,7 +149,6 @@
     <ClCompile Include="benchmarks\rdm_tagged_bw.c" />
     <ClCompile Include="benchmarks\rdm_tagged_pingpong.c" />
     <ClCompile Include="benchmarks\rma_bw.c" />
-    <ClCompile Include="common\jsmn.c" />
     <ClCompile Include="common\hmem.c" />
     <ClCompile Include="common\hmem_cuda.c" />
     <ClCompile Include="common\hmem_rocr.c" />
diff --git a/fabtests/functional/bw.c b/fabtests/functional/bw.c
index ff48be6..7be849e 100644
--- a/fabtests/functional/bw.c
+++ b/fabtests/functional/bw.c
@@ -38,9 +38,14 @@ int sleep_time = 0;
 
 static ssize_t post_one_tx(struct ft_context *msg)
 {
-	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE))
-		ft_fill_buf(msg->buf + ft_tx_prefix_size(),
-			    opts.transfer_size);
+	ssize_t ret;
+
+	if (ft_check_opts(FT_OPT_VERIFY_DATA | FT_OPT_ACTIVE)) {
+		ret = ft_fill_buf(msg->buf + ft_tx_prefix_size(),
+				  opts.transfer_size);
+		if (ret)
+			return ret;
+	}
 
 	return ft_post_tx_buf(ep, remote_fi_addr, opts.transfer_size,
 			      NO_CQ_DATA, &msg->context, msg->buf,
diff --git a/fabtests/functional/cm_data.c b/fabtests/functional/cm_data.c
index 74fec72..68b91a5 100644
--- a/fabtests/functional/cm_data.c
+++ b/fabtests/functional/cm_data.c
@@ -150,7 +150,10 @@ static int server_reject(size_t paramlen)
 		return ret;
 
 	/* Data will appear in error event generated on remote end. */
-	ft_fill_buf(cm_data, paramlen);
+	ret = ft_fill_buf(cm_data, paramlen);
+	if (ret)
+		return ret;
+
 	ret = fi_reject(pep, fi->handle, cm_data, paramlen);
 	if (ret)
 		FT_PRINTERR("fi_reject", ret);
@@ -187,7 +190,9 @@ static int server_accept(size_t paramlen)
 		goto err;
 	}
 	/* Data will appear on accept event on remote end. */
-	ft_fill_buf(cm_data, paramlen);
+	ret = ft_fill_buf(cm_data, paramlen);
+	if (ret)
+		return ret;
 
 	/* Accept the incoming connection. Also transitions endpoint to active
 	 * state.
@@ -254,7 +259,11 @@ static int server(size_t paramlen)
 
 static int client_connect(size_t paramlen)
 {
-	ft_fill_buf(cm_data, paramlen);
+	int ret;
+
+	ret = ft_fill_buf(cm_data, paramlen);
+	if (ret)
+		return ret;
 
 	/* Connect to server */
 	return fi_connect(ep, fi->dest_addr, cm_data, paramlen);
diff --git a/fabtests/functional/inj_complete.c b/fabtests/functional/inj_complete.c
index 0980508..ce27ec4 100644
--- a/fabtests/functional/inj_complete.c
+++ b/fabtests/functional/inj_complete.c
@@ -44,8 +44,11 @@ static int send_msg(int sendmsg, size_t size)
 	int ret;
 	ft_tag = 0xabcd;
 
-	if (ft_check_opts(FT_OPT_VERIFY_DATA))
-		ft_fill_buf(tx_buf, size);
+	if (ft_check_opts(FT_OPT_VERIFY_DATA)) {
+		ret = ft_fill_buf(tx_buf, size);
+		if (ret)
+			return ret;
+	}
 
 	if (sendmsg) {
 		ret = ft_sendmsg(ep, remote_fi_addr, size,
diff --git a/fabtests/functional/loopback.c b/fabtests/functional/loopback.c
new file mode 100644
index 0000000..e15e4e3
--- /dev/null
+++ b/fabtests/functional/loopback.c
@@ -0,0 +1,113 @@
+/*
+ * Copyright (c) 2021 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under the BSD license
+ * below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <getopt.h>
+#include <unistd.h>
+
+#include <shared.h>
+
+
+static int run(void)
+{
+	int ret;
+
+	ret = ft_getinfo(hints, &fi);
+	if (ret)
+		return ret;
+
+	ret = ft_open_fabric_res();
+	if (ret)
+		return ret;
+
+	ret = ft_alloc_active_res(fi);
+	if (ret)
+		return ret;
+
+	ret = ft_enable_ep_recv();
+	if (ret)
+		return ret;
+
+	opts.dst_addr = fi->src_addr;
+	fi->dest_addr = fi->src_addr;
+	fi->dest_addrlen = fi->src_addrlen;
+
+	ret = ft_init_av();
+	if (ret)
+		goto out;
+
+	ret = ft_send_greeting(ep);
+	if (ret)
+		goto out;
+
+	ret = ft_recv_greeting(ep);
+	if (ret)
+		goto out;
+
+out:
+	fi->dest_addr = NULL;
+	fi->dest_addrlen = 0;
+	return ret;
+}
+
+int main(int argc, char **argv)
+{
+	int op, ret;
+
+	opts = INIT_OPTS;
+
+	hints = fi_allocinfo();
+	if (!hints)
+		return EXIT_FAILURE;
+
+	hints->caps |= FI_LOCAL_COMM;
+	hints->ep_attr->type = FI_EP_RDM;
+
+	while ((op = getopt(argc, argv, "h" INFO_OPTS)) != -1) {
+		switch (op) {
+		default:
+			ft_parseinfo(op, optarg, hints, &opts);
+			break;
+		case '?':
+		case 'h':
+			ft_usage(argv[0], "A loopback communication test.");
+			return EXIT_FAILURE;
+		}
+	}
+
+	hints->caps = FI_MSG;
+	hints->mode = FI_CONTEXT;
+	hints->domain_attr->mr_mode = opts.mr_mode;
+
+	ret = run();
+
+	ft_free_res();
+	return ft_exit_code(ret);
+}
diff --git a/fabtests/functional/multi_ep.c b/fabtests/functional/multi_ep.c
index 3e122ca..c233d14 100644
--- a/fabtests/functional/multi_ep.c
+++ b/fabtests/functional/multi_ep.c
@@ -111,8 +111,11 @@ static int do_transfers(void)
 	}
 
 	for (i = 0; i < num_eps; i++) {
-		if (ft_check_opts(FT_OPT_VERIFY_DATA))
-			ft_fill_buf(send_bufs[i], opts.transfer_size);
+		if (ft_check_opts(FT_OPT_VERIFY_DATA)) {
+			ret = ft_fill_buf(send_bufs[i], opts.transfer_size);
+			if (ret)
+				return ret;
+		}
 
 		tx_buf = send_bufs[i];
 		ret = ft_post_tx(eps[i], remote_addr[i], opts.transfer_size, NO_CQ_DATA, &send_ctx[i]);
diff --git a/fabtests/functional/multi_mr.c b/fabtests/functional/multi_mr.c
index 6a814fc..500e25f 100644
--- a/fabtests/functional/multi_mr.c
+++ b/fabtests/functional/multi_mr.c
@@ -184,8 +184,10 @@ static int mr_key_test()
 		tx_buf = (char *)mr_res_array[i].buf;
 
 		if (opts.dst_addr) {
-			ft_fill_buf(mr_res_array[i].buf,
-					opts.transfer_size);
+			ret = ft_fill_buf(mr_res_array[i].buf,
+					  opts.transfer_size);
+			if (ret)
+				return ret;
 
 			if (verbose)
 				printf("write to host's key %lx\n",
@@ -231,8 +233,10 @@ static int mr_key_test()
 					return ret;
 			}
 
-			ft_fill_buf(mr_res_array[i].buf,
-					opts.transfer_size);
+			ret = ft_fill_buf(mr_res_array[i].buf,
+					  opts.transfer_size);
+			if (ret)
+				return ret;
 
 			if (verbose)
 				printf("write to client's key %lx\n",
diff --git a/fabtests/functional/rdm_multi_client.c b/fabtests/functional/rdm_multi_client.c
new file mode 100644
index 0000000..5f4ed68
--- /dev/null
+++ b/fabtests/functional/rdm_multi_client.c
@@ -0,0 +1,241 @@
+/*
+ * Copyright (c) 2021, Amazon.com, Inc.  All rights reserved.
+ *
+ * This software is available to you under the BSD license
+ * below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * This program tests the functionality of RDM endpoint in the case
+ * that a persistent server does ping-pong with multiple clients that
+ * come and leave in sequence. The client connects to a server, sends
+ * ping-pong, disconnects with the server by cleaning all fabric
+ * resources, and repeats.
+ * If the `-R` option is specified, it will re-use the first client's 
+ * address for the subsequent clients by setting the src_addr for
+ * endpoints 2..n to the output of fi_getname() of the first client.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <getopt.h>
+
+#include <shared.h>
+#include <rdma/fi_cm.h>
+
+static int run_pingpong(void)
+{
+	int ret, i;
+
+	fprintf(stdout, "Start ping-pong.\n");
+	for (i = 0; i < opts.iterations; i++) {
+		if (opts.dst_addr) {
+			ret = ft_tx(ep, remote_fi_addr, opts.transfer_size, &tx_ctx);
+			if (ret) {
+				FT_PRINTERR("ft_tx", -ret);
+				return ret;
+			}
+			ret = ft_rx(ep, opts.transfer_size);
+			if (ret) {
+				FT_PRINTERR("ft_rx", -ret);
+				return ret;
+			}
+		} else {
+			ret = ft_rx(ep, opts.transfer_size);
+			if (ret) {
+				FT_PRINTERR("ft_rx", -ret);
+				return ret;
+			}
+			ret = ft_tx(ep, remote_fi_addr, opts.transfer_size, &tx_ctx);
+			if (ret) {
+				FT_PRINTERR("ft_tx", -ret);
+				return ret;
+			}
+		}
+	}
+
+	fprintf(stdout, "Ping-pong succeeds.\n");
+	return 0;
+}
+
+static int run_server(void)
+{
+	int nconn, ret;
+
+	ret = ft_init_fabric();
+	if (ret) {
+		FT_PRINTERR("ft_init_fabric", -ret);
+		return ret;
+	}
+
+	nconn = opts.num_connections;
+
+	while (nconn) {
+		ret = run_pingpong();
+		if (ret) {
+			FT_PRINTERR("run_pingpong", -ret);
+			return ret;
+		}
+		if (--nconn) {
+			ret = ft_init_av();
+			if (ret) {
+				FT_PRINTERR("ft_init_av", -ret);
+				return ret;
+			}
+		}
+	}
+	return 0;
+}
+
+static int run_client(int client_id, bool address_reuse)
+{
+	static char name[256];
+	static size_t size = sizeof(name);
+	int ret;
+
+	tx_seq = 0;
+	rx_seq = 0;
+	tx_cq_cntr = 0;
+	rx_cq_cntr = 0;
+
+	ret = ft_init_oob();
+	if (ret) {
+		FT_PRINTERR("ft_init_oob", -ret);
+		return ret;
+	}
+
+	ret = ft_getinfo(hints, &fi);
+	if (ret) {
+		FT_PRINTERR("ft_getinfo", -ret);
+		return ret;
+	}
+
+	ret = ft_open_fabric_res();
+	if (ret) {
+		FT_PRINTERR("ft_open_fabric_res", -ret);
+		return ret;
+	}
+
+	if (client_id > 0 && address_reuse) {
+		memcpy(fi->src_addr, name, size);
+		fi->src_addrlen = size;
+	}
+
+	ret = ft_alloc_active_res(fi);
+	if (ret) {
+		FT_PRINTERR("ft_alloc_active_res", -ret);
+		return ret;
+	}
+
+	ret = ft_enable_ep_recv();
+	if (ret) {
+		FT_PRINTERR("ft_enable_ep_recv", -ret);
+		return ret;
+	}
+
+	ret = ft_init_av();
+	if (ret) {
+		FT_PRINTERR("ft_init_av", -ret);
+		return ret;
+	}
+
+	if (client_id == 0) {
+		ret = fi_getname(&ep->fid, name, &size);
+		if (ret) {
+			FT_PRINTERR("fi_getname", -ret);
+			return ret;
+		}
+	}
+
+	return run_pingpong();
+}
+
+static void print_opts_usage(char *name, char *desc)
+{
+	ft_usage(name, desc);
+	/* rdm_multi_client test op type */
+	FT_PRINT_OPTS_USAGE("-R", "Reuse the address of the first client for subsequent clients");
+}
+
+int main(int argc, char **argv)
+{
+	int op, ret, i;
+	struct fi_info *save;
+	bool address_reuse = false;
+
+	opts = INIT_OPTS;
+	opts.options |= FT_OPT_SIZE;
+
+	hints = fi_allocinfo();
+	if (!hints)
+		return EXIT_FAILURE;
+
+	while ((op = getopt(argc, argv, "URh" ADDR_OPTS INFO_OPTS CS_OPTS)) != -1) {
+		switch (op) {
+		default:
+			ft_parse_addr_opts(op, optarg, &opts);
+			ft_parseinfo(op, optarg, hints, &opts);
+			ft_parsecsopts(op, optarg, &opts);
+			break;
+		case 'U':
+			hints->tx_attr->op_flags |= FI_DELIVERY_COMPLETE;
+			break;
+		case 'R':
+			address_reuse = true;
+			break;
+		case '?':
+		case 'h':
+			print_opts_usage(argv[0], "RDM multi-client test");
+			return EXIT_FAILURE;
+		}
+	}
+
+	if (optind < argc)
+		opts.dst_addr = argv[optind];
+
+	hints->ep_attr->type = FI_EP_RDM;
+	hints->caps = FI_MSG;
+	hints->mode = FI_CONTEXT;
+	hints->domain_attr->mr_mode = opts.mr_mode;
+
+	if (opts.dst_addr) {
+		for (i = 0; i < opts.num_connections; i++) {
+			save = fi_dupinfo(hints);
+			printf("Starting client: %d\n", i);
+			ret = run_client(i, address_reuse);
+			if (ret) {
+				FT_PRINTERR("run_client", -ret);
+				goto out;
+			}
+			ft_free_res();
+			hints = save;
+		}
+	} else {
+		ret = run_server();
+		if (ret)
+			FT_PRINTERR("run_server", -ret);
+	}
+out:
+	ft_free_res();
+	return ft_exit_code(ret);
+}
diff --git a/fabtests/functional/unexpected_msg.c b/fabtests/functional/unexpected_msg.c
index 1c9ef71..c180fc0 100644
--- a/fabtests/functional/unexpected_msg.c
+++ b/fabtests/functional/unexpected_msg.c
@@ -132,9 +132,12 @@ static int run_test_loop(void)
 	for (i = 0; i < num_iters; i++) {
 		for (j = 0; j < concurrent_msgs; j++) {
 			op_buf = get_tx_buf(j);
-			if (ft_check_opts(FT_OPT_VERIFY_DATA))
-				ft_fill_buf(op_buf + ft_tx_prefix_size(),
-					    opts.transfer_size);
+			if (ft_check_opts(FT_OPT_VERIFY_DATA)) {
+				ret = ft_fill_buf(op_buf + ft_tx_prefix_size(),
+						  opts.transfer_size);
+				if (ret)
+					return ret;
+			}
 
 			ret = ft_post_tx_buf(ep, remote_fi_addr,
 					     opts.transfer_size,
diff --git a/fabtests/include/jsmn.h b/fabtests/include/jsmn.h
index 48a07c1..b95368a 100644
--- a/fabtests/include/jsmn.h
+++ b/fabtests/include/jsmn.h
@@ -1,5 +1,7 @@
 /*
- * Copyright (c) 2010 Serge A. Zaitsev
+ * MIT License
+ *
+ * Copyright (c) 2010 Serge Zaitsev
  *
  * Permission is hereby granted, free of charge, to any person obtaining a copy
  * of this software and associated documentation files (the "Software"), to deal
@@ -16,12 +18,11 @@
  * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
- * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
- * THE SOFTWARE.
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
  */
-
-#ifndef __JSMN_H_
-#define __JSMN_H_
+#ifndef JSMN_H
+#define JSMN_H
 
 #include <stddef.h>
 
@@ -29,6 +30,12 @@
 extern "C" {
 #endif
 
+#ifdef JSMN_STATIC
+#define JSMN_API static
+#else
+#define JSMN_API extern
+#endif
+
 /**
  * JSON type identifier. Basic types are:
  * 	o Object
@@ -37,61 +44,425 @@ extern "C" {
  * 	o Other primitive: number, boolean (true/false) or null
  */
 typedef enum {
-	JSMN_PRIMITIVE = 0,
-	JSMN_OBJECT = 1,
-	JSMN_ARRAY = 2,
-	JSMN_STRING = 3
+  JSMN_UNDEFINED = 0,
+  JSMN_OBJECT = 1,
+  JSMN_ARRAY = 2,
+  JSMN_STRING = 3,
+  JSMN_PRIMITIVE = 4
 } jsmntype_t;
 
-typedef enum {
-	/* Not enough tokens were provided */
-	JSMN_ERROR_NOMEM = -1,
-	/* Invalid character inside JSON string */
-	JSMN_ERROR_INVAL = -2,
-	/* The string is not a full JSON packet, more bytes expected */
-	JSMN_ERROR_PART = -3
-} jsmnerr_t;
+enum jsmnerr {
+  /* Not enough tokens were provided */
+  JSMN_ERROR_NOMEM = -1,
+  /* Invalid character inside JSON string */
+  JSMN_ERROR_INVAL = -2,
+  /* The string is not a full JSON packet, more bytes expected */
+  JSMN_ERROR_PART = -3
+};
 
 /**
  * JSON token description.
- * @param		type	type (object, array, string etc.)
- * @param		start	start position in JSON data string
- * @param		end		end position in JSON data string
+ * type		type (object, array, string etc.)
+ * start	start position in JSON data string
+ * end		end position in JSON data string
  */
 typedef struct {
-	jsmntype_t type;
-	int start;
-	int end;
-	int size;
+  jsmntype_t type;
+  int start;
+  int end;
+  int size;
 #ifdef JSMN_PARENT_LINKS
-	int parent;
+  int parent;
 #endif
 } jsmntok_t;
 
 /**
  * JSON parser. Contains an array of token blocks available. Also stores
- * the string being parsed now and current position in that string
+ * the string being parsed now and current position in that string.
  */
 typedef struct {
-	unsigned int pos; /* offset in the JSON string */
-	unsigned int toknext; /* next token to allocate */
-	int toksuper; /* superior token node, e.g parent object or array */
+  unsigned int pos;     /* offset in the JSON string */
+  unsigned int toknext; /* next token to allocate */
+  int toksuper;         /* superior token node, e.g. parent object or array */
 } jsmn_parser;
 
 /**
  * Create JSON parser over an array of tokens
  */
-void jsmn_init(jsmn_parser *parser);
+JSMN_API void jsmn_init(jsmn_parser *parser);
 
 /**
- * Run JSON parser. It parses a JSON data string into and array of tokens, each describing
+ * Run JSON parser. It parses a JSON data string into and array of tokens, each
+ * describing
  * a single JSON object.
  */
-jsmnerr_t jsmn_parse(jsmn_parser *parser, const char *js, size_t len,
-		jsmntok_t *tokens, unsigned int num_tokens);
+JSMN_API int jsmn_parse(jsmn_parser *parser, const char *js, const size_t len,
+                        jsmntok_t *tokens, const unsigned int num_tokens);
+
+#ifndef JSMN_HEADER
+/**
+ * Allocates a fresh unused token from the token pool.
+ */
+static jsmntok_t *jsmn_alloc_token(jsmn_parser *parser, jsmntok_t *tokens,
+                                   const size_t num_tokens) {
+  jsmntok_t *tok;
+  if (parser->toknext >= num_tokens) {
+    return NULL;
+  }
+  tok = &tokens[parser->toknext++];
+  tok->start = tok->end = -1;
+  tok->size = 0;
+#ifdef JSMN_PARENT_LINKS
+  tok->parent = -1;
+#endif
+  return tok;
+}
+
+/**
+ * Fills token type and boundaries.
+ */
+static void jsmn_fill_token(jsmntok_t *token, const jsmntype_t type,
+                            const int start, const int end) {
+  token->type = type;
+  token->start = start;
+  token->end = end;
+  token->size = 0;
+}
+
+/**
+ * Fills next available token with JSON primitive.
+ */
+static int jsmn_parse_primitive(jsmn_parser *parser, const char *js,
+                                const size_t len, jsmntok_t *tokens,
+                                const size_t num_tokens) {
+  jsmntok_t *token;
+  int start;
+
+  start = parser->pos;
+
+  for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
+    switch (js[parser->pos]) {
+#ifndef JSMN_STRICT
+    /* In strict mode primitive must be followed by "," or "}" or "]" */
+    case ':':
+#endif
+    case '\t':
+    case '\r':
+    case '\n':
+    case ' ':
+    case ',':
+    case ']':
+    case '}':
+      goto found;
+    }
+    if (js[parser->pos] < 32 || js[parser->pos] >= 127) {
+      parser->pos = start;
+      return JSMN_ERROR_INVAL;
+    }
+  }
+#ifdef JSMN_STRICT
+  /* In strict mode primitive must be followed by a comma/object/array */
+  parser->pos = start;
+  return JSMN_ERROR_PART;
+#endif
+
+found:
+  if (tokens == NULL) {
+    parser->pos--;
+    return 0;
+  }
+  token = jsmn_alloc_token(parser, tokens, num_tokens);
+  if (token == NULL) {
+    parser->pos = start;
+    return JSMN_ERROR_NOMEM;
+  }
+  jsmn_fill_token(token, JSMN_PRIMITIVE, start, parser->pos);
+#ifdef JSMN_PARENT_LINKS
+  token->parent = parser->toksuper;
+#endif
+  parser->pos--;
+  return 0;
+}
+
+/**
+ * Fills next token with JSON string.
+ */
+static int jsmn_parse_string(jsmn_parser *parser, const char *js,
+                             const size_t len, jsmntok_t *tokens,
+                             const size_t num_tokens) {
+  jsmntok_t *token;
+
+  int start = parser->pos;
+
+  parser->pos++;
+
+  /* Skip starting quote */
+  for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
+    char c = js[parser->pos];
+
+    /* Quote: end of string */
+    if (c == '\"') {
+      if (tokens == NULL) {
+        return 0;
+      }
+      token = jsmn_alloc_token(parser, tokens, num_tokens);
+      if (token == NULL) {
+        parser->pos = start;
+        return JSMN_ERROR_NOMEM;
+      }
+      jsmn_fill_token(token, JSMN_STRING, start + 1, parser->pos);
+#ifdef JSMN_PARENT_LINKS
+      token->parent = parser->toksuper;
+#endif
+      return 0;
+    }
+
+    /* Backslash: Quoted symbol expected */
+    if (c == '\\' && parser->pos + 1 < len) {
+      int i;
+      parser->pos++;
+      switch (js[parser->pos]) {
+      /* Allowed escaped symbols */
+      case '\"':
+      case '/':
+      case '\\':
+      case 'b':
+      case 'f':
+      case 'r':
+      case 'n':
+      case 't':
+        break;
+      /* Allows escaped symbol \uXXXX */
+      case 'u':
+        parser->pos++;
+        for (i = 0; i < 4 && parser->pos < len && js[parser->pos] != '\0';
+             i++) {
+          /* If it isn't a hex character we have an error */
+          if (!((js[parser->pos] >= 48 && js[parser->pos] <= 57) ||   /* 0-9 */
+                (js[parser->pos] >= 65 && js[parser->pos] <= 70) ||   /* A-F */
+                (js[parser->pos] >= 97 && js[parser->pos] <= 102))) { /* a-f */
+            parser->pos = start;
+            return JSMN_ERROR_INVAL;
+          }
+          parser->pos++;
+        }
+        parser->pos--;
+        break;
+      /* Unexpected symbol */
+      default:
+        parser->pos = start;
+        return JSMN_ERROR_INVAL;
+      }
+    }
+  }
+  parser->pos = start;
+  return JSMN_ERROR_PART;
+}
+
+/**
+ * Parse JSON string and fill tokens.
+ */
+JSMN_API int jsmn_parse(jsmn_parser *parser, const char *js, const size_t len,
+                        jsmntok_t *tokens, const unsigned int num_tokens) {
+  int r;
+  int i;
+  jsmntok_t *token;
+  int count = parser->toknext;
+
+  for (; parser->pos < len && js[parser->pos] != '\0'; parser->pos++) {
+    char c;
+    jsmntype_t type;
+
+    c = js[parser->pos];
+    switch (c) {
+    case '{':
+    case '[':
+      count++;
+      if (tokens == NULL) {
+        break;
+      }
+      token = jsmn_alloc_token(parser, tokens, num_tokens);
+      if (token == NULL) {
+        return JSMN_ERROR_NOMEM;
+      }
+      if (parser->toksuper != -1) {
+        jsmntok_t *t = &tokens[parser->toksuper];
+#ifdef JSMN_STRICT
+        /* In strict mode an object or array can't become a key */
+        if (t->type == JSMN_OBJECT) {
+          return JSMN_ERROR_INVAL;
+        }
+#endif
+        t->size++;
+#ifdef JSMN_PARENT_LINKS
+        token->parent = parser->toksuper;
+#endif
+      }
+      token->type = (c == '{' ? JSMN_OBJECT : JSMN_ARRAY);
+      token->start = parser->pos;
+      parser->toksuper = parser->toknext - 1;
+      break;
+    case '}':
+    case ']':
+      if (tokens == NULL) {
+        break;
+      }
+      type = (c == '}' ? JSMN_OBJECT : JSMN_ARRAY);
+#ifdef JSMN_PARENT_LINKS
+      if (parser->toknext < 1) {
+        return JSMN_ERROR_INVAL;
+      }
+      token = &tokens[parser->toknext - 1];
+      for (;;) {
+        if (token->start != -1 && token->end == -1) {
+          if (token->type != type) {
+            return JSMN_ERROR_INVAL;
+          }
+          token->end = parser->pos + 1;
+          parser->toksuper = token->parent;
+          break;
+        }
+        if (token->parent == -1) {
+          if (token->type != type || parser->toksuper == -1) {
+            return JSMN_ERROR_INVAL;
+          }
+          break;
+        }
+        token = &tokens[token->parent];
+      }
+#else
+      for (i = parser->toknext - 1; i >= 0; i--) {
+        token = &tokens[i];
+        if (token->start != -1 && token->end == -1) {
+          if (token->type != type) {
+            return JSMN_ERROR_INVAL;
+          }
+          parser->toksuper = -1;
+          token->end = parser->pos + 1;
+          break;
+        }
+      }
+      /* Error if unmatched closing bracket */
+      if (i == -1) {
+        return JSMN_ERROR_INVAL;
+      }
+      for (; i >= 0; i--) {
+        token = &tokens[i];
+        if (token->start != -1 && token->end == -1) {
+          parser->toksuper = i;
+          break;
+        }
+      }
+#endif
+      break;
+    case '\"':
+      r = jsmn_parse_string(parser, js, len, tokens, num_tokens);
+      if (r < 0) {
+        return r;
+      }
+      count++;
+      if (parser->toksuper != -1 && tokens != NULL) {
+        tokens[parser->toksuper].size++;
+      }
+      break;
+    case '\t':
+    case '\r':
+    case '\n':
+    case ' ':
+      break;
+    case ':':
+      parser->toksuper = parser->toknext - 1;
+      break;
+    case ',':
+      if (tokens != NULL && parser->toksuper != -1 &&
+          tokens[parser->toksuper].type != JSMN_ARRAY &&
+          tokens[parser->toksuper].type != JSMN_OBJECT) {
+#ifdef JSMN_PARENT_LINKS
+        parser->toksuper = tokens[parser->toksuper].parent;
+#else
+        for (i = parser->toknext - 1; i >= 0; i--) {
+          if (tokens[i].type == JSMN_ARRAY || tokens[i].type == JSMN_OBJECT) {
+            if (tokens[i].start != -1 && tokens[i].end == -1) {
+              parser->toksuper = i;
+              break;
+            }
+          }
+        }
+#endif
+      }
+      break;
+#ifdef JSMN_STRICT
+    /* In strict mode primitives are: numbers and booleans */
+    case '-':
+    case '0':
+    case '1':
+    case '2':
+    case '3':
+    case '4':
+    case '5':
+    case '6':
+    case '7':
+    case '8':
+    case '9':
+    case 't':
+    case 'f':
+    case 'n':
+      /* And they must not be keys of the object */
+      if (tokens != NULL && parser->toksuper != -1) {
+        const jsmntok_t *t = &tokens[parser->toksuper];
+        if (t->type == JSMN_OBJECT ||
+            (t->type == JSMN_STRING && t->size != 0)) {
+          return JSMN_ERROR_INVAL;
+        }
+      }
+#else
+    /* In non-strict mode every unquoted value is a primitive */
+    default:
+#endif
+      r = jsmn_parse_primitive(parser, js, len, tokens, num_tokens);
+      if (r < 0) {
+        return r;
+      }
+      count++;
+      if (parser->toksuper != -1 && tokens != NULL) {
+        tokens[parser->toksuper].size++;
+      }
+      break;
+
+#ifdef JSMN_STRICT
+    /* Unexpected char in strict mode */
+    default:
+      return JSMN_ERROR_INVAL;
+#endif
+    }
+  }
+
+  if (tokens != NULL) {
+    for (i = parser->toknext - 1; i >= 0; i--) {
+      /* Unmatched opened object or array */
+      if (tokens[i].start != -1 && tokens[i].end == -1) {
+        return JSMN_ERROR_PART;
+      }
+    }
+  }
+
+  return count;
+}
+
+/**
+ * Creates a new parser based over a given buffer with an array of tokens
+ * available.
+ */
+JSMN_API void jsmn_init(jsmn_parser *parser) {
+  parser->pos = 0;
+  parser->toknext = 0;
+  parser->toksuper = -1;
+}
+
+#endif /* JSMN_HEADER */
 
 #ifdef __cplusplus
 }
 #endif
 
-#endif /* __JSMN_H_ */
+#endif /* JSMN_H */
diff --git a/fabtests/include/shared.h b/fabtests/include/shared.h
index 7e29d54..dbbae68 100644
--- a/fabtests/include/shared.h
+++ b/fabtests/include/shared.h
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2013-2017 Intel Corporation.  All rights reserved.
  * Copyright (c) 2014-2017, Cisco Systems, Inc. All rights reserved.
+ * Copyright (c) 2021 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under the BSD license below:
  *
@@ -231,7 +232,7 @@ void ft_usage(char *name, char *desc);
 void ft_mcusage(char *name, char *desc);
 void ft_csusage(char *name, char *desc);
 
-void ft_fill_buf(void *buf, size_t size);
+int ft_fill_buf(void *buf, size_t size);
 int ft_check_buf(void *buf, size_t size);
 int ft_check_opts(uint64_t flags);
 uint64_t ft_init_cq_data(struct fi_info *info);
@@ -260,6 +261,7 @@ extern char default_port[8];
 	{	.options = FT_OPT_RX_CQ | FT_OPT_TX_CQ, \
 		.iterations = 1000, \
 		.warmup_iterations = 10, \
+		.num_connections = 1, \
 		.transfer_size = 1024, \
 		.window_size = 64, \
 		.av_size = 1, \
@@ -369,6 +371,8 @@ int ft_open_fabric_res();
 int ft_getinfo(struct fi_info *hints, struct fi_info **info);
 int ft_init_fabric();
 int ft_init_oob();
+int ft_close_oob();
+int ft_reset_oob();
 int ft_start_server();
 int ft_server_connect();
 int ft_client_connect();
diff --git a/fabtests/man/fabtests.7.md b/fabtests/man/fabtests.7.md
index ac171d3..725dea4 100644
--- a/fabtests/man/fabtests.7.md
+++ b/fabtests/man/fabtests.7.md
@@ -148,6 +148,10 @@ features of libfabric.
   A sleep time on the receiving side can be enabled in order to allow
   the sender to get ahead of the receiver.
 
+*fi_rdm_multi_client*
+: Tests a persistent server communicating with multiple clients, one at a
+  time, in sequence.
+
 # Benchmarks
 
 The client and the server exchange messages in either a ping-pong manner,
diff --git a/fabtests/man/man1/fi_rdm_multi_client.1 b/fabtests/man/man1/fi_rdm_multi_client.1
new file mode 100644
index 0000000..3f6ccf9
--- /dev/null
+++ b/fabtests/man/man1/fi_rdm_multi_client.1
@@ -0,0 +1 @@
+.so man7/fabtests.7
diff --git a/fabtests/scripts/runfabtests.sh b/fabtests/scripts/runfabtests.sh
index 03e55f9..6be927b 100755
--- a/fabtests/scripts/runfabtests.sh
+++ b/fabtests/scripts/runfabtests.sh
@@ -138,6 +138,8 @@ functional_tests=(
 	"fi_bw -e rdm -v -T 1"
 	"fi_bw -e rdm -v -T 1 -U"
 	"fi_bw -e msg -v -T 1"
+	"fi_rdm_multi_client -C 10 -I 5"
+	"fi_rdm_multi_client -C 10 -I 5 -U"
 )
 
 short_tests=(
@@ -476,7 +478,12 @@ function cs_test {
 	wait $c_pid
 	c_ret=$?
 
-	[[ c_ret -ne 0 ]] && kill -9 $s_pid 2> /dev/null
+	if [[ $c_ret -ne 0 ]] && ps -p $s_pid > /dev/null; then
+	    if [[ $STRICT_MODE -eq 0 ]]; then
+	        sleep 2
+	    fi
+	    kill -9 $s_pid 2> /dev/null
+	fi
 
 	wait $s_pid
 	s_ret=$?
@@ -636,7 +643,12 @@ function multinode_test {
 		c_ret=($?)||$c_ret
 	done
 
-	[[ c_ret -ne 0 ]] && kill -9 $s_pid 2> /dev/null
+	if [[ $c_ret -ne 0 ]] && ps -p $s_pid > /dev/null; then
+	    if [[ $STRICT_MODE -eq 0 ]]; then
+	        sleep 2
+	    fi
+	    kill -9 $s_pid 2> /dev/null
+	fi
 
 	wait $s_pid
 	s_ret=$?
diff --git a/fabtests/test_configs/efa/efa.exclude b/fabtests/test_configs/efa/efa.exclude
index 8ec42b4..8b13f4a 100644
--- a/fabtests/test_configs/efa/efa.exclude
+++ b/fabtests/test_configs/efa/efa.exclude
@@ -61,9 +61,6 @@ trigger
 #rdm_cntr_pingpong
 
 
-# This test requires ENA IPs for the OOB sync
-av_xfer
-
 # Connection manager isn't supported
 cm_data
 
@@ -96,3 +93,6 @@ dgram_bw
 
 # Multinode tests failing with an unsupported address format
 multinode
+
+# Remove this once the av bug fix is merged.
+rdm_multi_client
diff --git a/fabtests/test_configs/psm2/psm2.exclude b/fabtests/test_configs/psm2/psm2.exclude
index 30303e0..36ec149 100644
--- a/fabtests/test_configs/psm2/psm2.exclude
+++ b/fabtests/test_configs/psm2/psm2.exclude
@@ -15,3 +15,4 @@ scalable_ep
 shared_av
 rdm_cntr_pingpong
 multi_recv
+rdm_multi_client
diff --git a/fabtests/ubertest/verify.c b/fabtests/ubertest/verify.c
index c732220..1569b31 100644
--- a/fabtests/ubertest/verify.c
+++ b/fabtests/ubertest/verify.c
@@ -84,6 +84,7 @@ static const int integ_alphabet_length = (sizeof(integ_alphabet)/sizeof(*integ_a
 
 int ft_sync_fill_bufs(size_t size)
 {
+	int ret;
 	ft_sock_sync(0);
 
 	if (test_info.caps & FI_ATOMIC) {
@@ -94,9 +95,14 @@ int ft_sync_fill_bufs(size_t size)
 		memcpy(ft_atom_ctrl.orig_buf, ft_mr_ctrl.buf, size);
 		memcpy(ft_tx_ctrl.cpy_buf, ft_tx_ctrl.buf, size);
 	} else if (is_read_func(test_info.class_function)) {
-		ft_fill_buf(ft_mr_ctrl.buf, size);
+		ret = ft_fill_buf(ft_mr_ctrl.buf, size);
+		if (ret)
+			return ret;
 	} else {
-		ft_fill_buf(ft_tx_ctrl.buf, size);
+		ret = ft_fill_buf(ft_tx_ctrl.buf, size);
+		if (ret)
+			return ret;
+
 		memcpy(ft_tx_ctrl.cpy_buf, ft_tx_ctrl.buf, size);
 	}
 
diff --git a/include/ofi_hmem.h b/include/ofi_hmem.h
index ff67b0b..86eb4b0 100644
--- a/include/ofi_hmem.h
+++ b/include/ofi_hmem.h
@@ -150,6 +150,7 @@ int ze_hmem_open_shared_handle(int dev_fd, void **handle, int *ze_fd,
 int ze_hmem_close_handle(void *ipc_ptr);
 bool ze_hmem_p2p_enabled(void);
 int ze_hmem_get_base_addr(const void *ptr, void **base);
+int ze_hmem_get_id(const void *ptr, uint64_t *id);
 int *ze_hmem_get_dev_fds(int *nfds);
 
 static inline int ofi_memcpy(uint64_t device, void *dest, const void *src,
diff --git a/include/ofi_list.h b/include/ofi_list.h
index 1b79d0a..5112000 100644
--- a/include/ofi_list.h
+++ b/include/ofi_list.h
@@ -45,6 +45,12 @@
 #include <ofi_signal.h>
 #include <ofi_lock.h>
 
+
+enum ofi_list_end {
+	OFI_LIST_TAIL,
+	OFI_LIST_HEAD
+};
+
 /*
  * Double-linked list
  */
diff --git a/include/ofi_mr.h b/include/ofi_mr.h
index 991dcbc..a9fb920 100644
--- a/include/ofi_mr.h
+++ b/include/ofi_mr.h
@@ -124,6 +124,7 @@ struct ofi_mr_cache;
 
 union ofi_mr_hmem_info {
 	uint64_t cuda_id;
+	uint64_t ze_id;
 };
 
 struct ofi_mem_monitor {
@@ -174,6 +175,7 @@ void ofi_monitor_unsubscribe(struct ofi_mem_monitor *monitor,
 extern struct ofi_mem_monitor *default_monitor;
 extern struct ofi_mem_monitor *default_cuda_monitor;
 extern struct ofi_mem_monitor *default_rocr_monitor;
+extern struct ofi_mem_monitor *default_ze_monitor;
 
 /*
  * Userfault fd memory monitor
@@ -198,6 +200,7 @@ extern struct ofi_mem_monitor *memhooks_monitor;
 
 extern struct ofi_mem_monitor *cuda_monitor;
 extern struct ofi_mem_monitor *rocr_monitor;
+extern struct ofi_mem_monitor *ze_monitor;
 extern struct ofi_mem_monitor *import_monitor;
 
 /*
@@ -268,6 +271,7 @@ struct ofi_mr_cache_params {
 	char *				monitor;
 	int				cuda_monitor_enabled;
 	int				rocr_monitor_enabled;
+	int				ze_monitor_enabled;
 };
 
 extern struct ofi_mr_cache_params	cache_params;
diff --git a/include/ofi_net.h b/include/ofi_net.h
index 53b3ad2..df36b9e 100644
--- a/include/ofi_net.h
+++ b/include/ofi_net.h
@@ -401,6 +401,21 @@ static inline int ofi_translate_addr_format(int family)
 	}
 }
 
+static inline size_t ofi_sizeof_addr_format(int format)
+{
+	switch (format) {
+	case FI_SOCKADDR_IN:
+		return sizeof(struct sockaddr_in);
+	case FI_SOCKADDR_IN6:
+		return sizeof(struct sockaddr_in6);
+	case FI_SOCKADDR_IB:
+		return sizeof(struct ofi_sockaddr_ib);
+	default:
+		FI_WARN(&core_prov, FI_LOG_CORE, "Unsupported address format\n");
+		return 0;
+	}
+}
+
 uint16_t ofi_get_sa_family(const struct fi_info *info);
 
 static inline bool ofi_sin_is_any_addr(const struct sockaddr *sa)
diff --git a/include/ofi_recvwin.h b/include/ofi_recvwin.h
index 468c657..2d1073a 100644
--- a/include/ofi_recvwin.h
+++ b/include/ofi_recvwin.h
@@ -80,6 +80,12 @@ ofi_recvwin_id_valid(struct name *recvq, id_type id)			\
 }									\
 									\
 static inline int							\
+ofi_recvwin_id_processed(struct name *recvq, id_type id)		\
+{									\
+	return ofi_recvwin_id_processed_ ## id_type (recvq, id);	\
+}									\
+									\
+static inline int							\
 ofi_recvwin_queue_msg(struct name *recvq, entrytype * msg, id_type id)	\
 {									\
 	size_t write_idx;						\
@@ -139,4 +145,9 @@ ofi_recvwin_slide(struct name *recvq)					\
 #define ofi_recvwin_id_valid_uint64_t(rq, id) \
 	ofi_val64_inrange(rq->exp_msg_id, rq->win_size, id)
 
+#define ofi_recvwin_id_processed_uint32_t(rq, id) \
+	ofi_val32_gt(rq->exp_msg_id, id)
+#define ofi_recvwin_id_processed_uint64_t(rq, id) \
+	ofi_val64_gt(rq->exp_msg_id, id)
+
 #endif /* FI_RECVWIN_H */
diff --git a/include/ofi_util.h b/include/ofi_util.h
index 598d51d..6f1add8 100644
--- a/include/ofi_util.h
+++ b/include/ofi_util.h
@@ -766,6 +766,8 @@ int ofi_ip_av_create_flags(struct fid_domain *domain_fid, struct fi_av_attr *att
 
 void *ofi_av_get_addr(struct util_av *av, fi_addr_t fi_addr);
 #define ofi_ip_av_get_addr ofi_av_get_addr
+void *ofi_av_addr_context(struct util_av *av, fi_addr_t fi_addr);
+
 fi_addr_t ofi_ip_av_get_fi_addr(struct util_av *av, const void *addr);
 
 int ofi_get_addr(uint32_t *addr_format, uint64_t flags,
diff --git a/include/windows/config.h b/include/windows/config.h
index 589dbf9..8a986e5 100644
--- a/include/windows/config.h
+++ b/include/windows/config.h
@@ -165,7 +165,7 @@
 #define PACKAGE_TARNAME PACKAGE
 
 /* Define to the version of this package. */
-#define PACKAGE_VERSION "1.13.0"
+#define PACKAGE_VERSION "1.14.0a1"
 
 /* Define to the full name and version of this package. */
 #define PACKAGE_STRING PACKAGE_NAME " " PACKAGE_VERSION
diff --git a/libfabric.vcxproj b/libfabric.vcxproj
index bc24ab0..c1e1792 100644
--- a/libfabric.vcxproj
+++ b/libfabric.vcxproj
@@ -687,6 +687,7 @@
     <ClCompile Include="prov\util\src\util_mr_cache.c" />
     <ClCompile Include="prov\util\src\cuda_mem_monitor.c" />
     <ClCompile Include="prov\util\src\rocr_mem_monitor.c" />
+    <ClCompile Include="prov\util\src\ze_mem_monitor.c" />
     <ClCompile Include="src\common.c" />
     <ClCompile Include="src\enosys.c">
       <DisableSpecificWarnings Condition="'$(Configuration)|$(Platform)'=='Debug-ICC|x64'">4127;869</DisableSpecificWarnings>
diff --git a/libfabric.vcxproj.filters b/libfabric.vcxproj.filters
index 4d39e90..385816e 100644
--- a/libfabric.vcxproj.filters
+++ b/libfabric.vcxproj.filters
@@ -201,6 +201,9 @@
     <ClCompile Include="prov\util\src\rocr_mem_monitor.c">
       <Filter>Source Files\prov\util</Filter>
     </ClCompile>
+    <ClCompile Include="prov\util\src\ze_mem_monitor.c">
+      <Filter>Source Files\prov\util</Filter>
+    </ClCompile>
     <ClCompile Include="src\windows\osd.c">
       <Filter>Source Files\src\windows</Filter>
     </ClCompile>
diff --git a/man/fi_mr.3.md b/man/fi_mr.3.md
index 08ecd8c..decda08 100644
--- a/man/fi_mr.3.md
+++ b/man/fi_mr.3.md
@@ -800,6 +800,12 @@ configure registration caches.
   are: 0 or 1. Note that the ROCR memory monitor requires a ROCR version with
   unified virtual addressing enabled.
 
+*FI_MR_ZE_CACHE_MONITOR_ENABLED*
+: The ZE cache monitor is responsible for detecting ZE device memory
+  (FI_HMEM_ZE) changes made between the device virtual addresses used by an
+  application and the underlying device physical pages. Valid monitor options
+  are: 0 or 1.
+
 More direct access to the internal registration cache is possible through the
 fi_open() call, using the "mr_cache" service name.  Once opened, custom
 memory monitors may be installed.  A memory monitor is a component of the cache
diff --git a/man/man3/fi_mr.3 b/man/man3/fi_mr.3
index be59220..e4ac603 100644
--- a/man/man3/fi_mr.3
+++ b/man/man3/fi_mr.3
@@ -1,6 +1,6 @@
 .\" Automatically generated by Pandoc 2.5
 .\"
-.TH "fi_mr" "3" "2021\-06\-15" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
+.TH "fi_mr" "3" "2021\-07\-07" "Libfabric Programmer\[cq]s Manual" "#VERSION#"
 .hy
 .SH NAME
 .PP
@@ -848,6 +848,12 @@ an application and the underlying device physical pages.
 Valid monitor options are: 0 or 1.
 Note that the ROCR memory monitor requires a ROCR version with unified
 virtual addressing enabled.
+.TP
+.B \f[I]FI_MR_ZE_CACHE_MONITOR_ENABLED\f[R]
+The ZE cache monitor is responsible for detecting ZE device memory
+(FI_HMEM_ZE) changes made between the device virtual addresses used by
+an application and the underlying device physical pages.
+Valid monitor options are: 0 or 1.
 .PP
 More direct access to the internal registration cache is possible
 through the fi_open() call, using the \[lq]mr_cache\[rq] service name.
diff --git a/prov/efa/configure.m4 b/prov/efa/configure.m4
index b6f8055..6f309f8 100644
--- a/prov/efa/configure.m4
+++ b/prov/efa/configure.m4
@@ -84,6 +84,21 @@ AC_DEFUN([FI_EFA_CONFIGURE],[
 	      ])
 	CPPFLAGS=$save_CPPFLAGS
 
+	dnl Check for ibv_is_fork_initialized() in libibverbs
+	have_ibv_is_fork_initialized=0
+	AS_IF([test $efa_happy -eq 1],
+		[AC_CHECK_DECL([ibv_is_fork_initialized],
+			[have_ibv_is_fork_initialized=1],
+			[],
+			[[#include <infiniband/verbs.h>]])
+		])
+
+	AC_DEFINE_UNQUOTED([HAVE_IBV_IS_FORK_INITIALIZED],
+		[$have_ibv_is_fork_initialized],
+		[Define to 1 if libibverbs has ibv_is_fork_initialized])
+
+	AS_IF([test "$enable_efa" = "no"], [efa_happy=0])
+
 	AS_IF([test $efa_happy -eq 1 ], [$1], [$2])
 
 	efa_CPPFLAGS="$efa_ibverbs_CPPFLAGS $efadv_CPPFLAGS"
diff --git a/prov/efa/src/efa.h b/prov/efa/src/efa.h
index aecc91e..fb4f37e 100644
--- a/prov/efa/src/efa.h
+++ b/prov/efa/src/efa.h
@@ -142,7 +142,7 @@ struct efa_ah {
 
 struct efa_conn {
 	struct efa_ah		*ah;
-	struct efa_ep_addr	ep_addr;
+	struct efa_ep_addr	*ep_addr;
 	/* for FI_AV_TABLE, fi_addr is same as util_av_fi_addr,
 	 * for FI_AV_MAP, fi_addr is pointer to efa_conn; */
 	fi_addr_t		fi_addr;
@@ -451,12 +451,6 @@ ssize_t efa_cq_readerr(struct fid_cq *cq_fid, struct fi_cq_err_entry *entry, uin
 /*
  * ON will avoid using huge pages for bounce buffers, so that the libibverbs
  * fork support can be used safely.
- *
- * UNNEEDED is currently not used but will be set when rdma-core adds a verb to
- * check this state. Fork support will become irrelevant once the kernel copies
- * pages into the fork, leaving the pinned pages intact.
- *
- * See https://github.com/linux-rdma/rdma-core/pull/883 for more information.
  */
 enum efa_fork_support_status {
 	EFA_FORK_SUPPORT_OFF = 0,
@@ -594,40 +588,7 @@ struct rdm_peer *rxr_ep_get_peer(struct rxr_ep *ep, fi_addr_t addr)
 	util_av_entry = ofi_bufpool_get_ibuf(ep->util_ep.av->av_entry_pool,
 	                                     addr);
 	av_entry = (struct efa_av_entry *)util_av_entry->data;
-	return &av_entry->conn.rdm_peer;
-}
-
-static inline
-int efa_peer_in_use(struct rdm_peer *peer)
-{
-	struct rxr_pkt_entry *pending_pkt;
-
-	if (ofi_atomic_get32(&peer->use_cnt) > 1)
-		return -FI_EBUSY;
-	if ((peer->tx_pending) || (peer->flags & RXR_PEER_IN_BACKOFF))
-		return -FI_EBUSY;
-
-	pending_pkt = *ofi_recvwin_peek((&peer->robuf));
-	if (pending_pkt)
-		return -FI_EBUSY;
-
-	return 0;
-}
-
-static inline
-void efa_rdm_peer_reset(struct rdm_peer *peer)
-{
-	if (peer->robuf.pending)
-		ofi_recvwin_free(&peer->robuf);
-
-	if (peer->flags & RXR_PEER_HANDSHAKE_QUEUED)
-		dlist_remove(&peer->handshake_queued_entry);
-
-	memset(peer, 0, sizeof(struct rdm_peer));
-#ifdef ENABLE_EFA_POISONING
-	rxr_poison_mem_region((uint32_t *)peer, sizeof(struct rdm_peer));
-#endif
-	dlist_init(&peer->rnr_entry);
+	return av_entry->conn.ep_addr ? &av_entry->conn.rdm_peer : NULL;
 }
 
 static inline bool efa_ep_is_cuda_mr(struct efa_mr *efa_mr)
diff --git a/prov/efa/src/efa_av.c b/prov/efa/src/efa_av.c
index b892f34..a6c9740 100644
--- a/prov/efa/src/efa_av.c
+++ b/prov/efa/src/efa_av.c
@@ -75,6 +75,100 @@ static bool efa_is_same_addr(struct efa_ep_addr *lhs, struct efa_ep_addr *rhs)
 }
 
 /**
+ * @brief initialize a rdm peer
+ *
+ * @param[in,out]	peer	rdm peer
+ * @param[in]		ep	rdm endpoint
+ * @param[in]		conn	efa conn object
+ */
+static inline
+void efa_rdm_peer_init(struct rdm_peer *peer, struct rxr_ep *ep, struct efa_conn *conn)
+{
+	memset(peer, 0, sizeof(struct rdm_peer));
+
+	peer->efa_fiaddr = conn->fi_addr;
+	peer->is_self = efa_is_same_addr((struct efa_ep_addr *)ep->core_addr,
+					 conn->ep_addr);
+
+	ofi_recvwin_buf_alloc(&peer->robuf, rxr_env.recvwin_size);
+	peer->rx_credits = rxr_env.rx_window_size;
+	peer->tx_credits = rxr_env.tx_max_credits;
+	dlist_init(&peer->outstanding_tx_pkts);
+	dlist_init(&peer->rx_unexp_list);
+	dlist_init(&peer->rx_unexp_tagged_list);
+	dlist_init(&peer->tx_entry_list);
+	dlist_init(&peer->rx_entry_list);
+}
+
+/**
+ * @brief clear resources accociated with a peer
+ *
+ * release reorder buffer, tx_entry list and rx_entry list of a peer
+ *
+ * @param[in,out]	peer 	rdm peer
+ */
+void efa_rdm_peer_clear(struct rxr_ep *ep, struct rdm_peer *peer)
+{
+	struct dlist_entry *tmp;
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_rx_entry *rx_entry;
+	struct rxr_pkt_entry *pkt_entry;
+	/*
+	 * TODO: Add support for wait/signal until all pending messages have
+	 * been sent/received so we do not attempt to complete a data transfer
+	 * or internal transfer after the EP is shutdown.
+	 */
+	if ((peer->flags & RXR_PEER_REQ_SENT) &&
+	    !(peer->flags & RXR_PEER_HANDSHAKE_RECEIVED))
+		FI_WARN_ONCE(&rxr_prov, FI_LOG_EP_CTRL, "Closing EP with unacked CONNREQs in flight\n");
+
+	if (peer->robuf.pending)
+		ofi_recvwin_free(&peer->robuf);
+
+	if (!ep) {
+		/* ep is NULL means the endpoint has been closed.
+		 * In this case there is no need to proceed because
+		 * all the tx_entry, rx_entry, pkt_entry has been released.
+		 */
+		return;
+	}
+
+	/* we cannot release outstanding TX packets because device
+	 * will report completion of these packets later. Setting
+	 * the address to FI_ADDR_NOTAVAIL, so rxr_ep_get_peer()
+	 * will return NULL for the address, so the completion will
+	 * be ignored.
+	 */
+	dlist_foreach_container(&peer->outstanding_tx_pkts,
+				struct rxr_pkt_entry,
+				pkt_entry, entry) {
+		pkt_entry->addr = FI_ADDR_NOTAVAIL;
+	}
+
+	dlist_foreach_container_safe(&peer->tx_entry_list,
+				     struct rxr_tx_entry,
+				     tx_entry, peer_entry, tmp) {
+		rxr_release_tx_entry(ep, tx_entry);
+	}
+
+	dlist_foreach_container_safe(&peer->rx_entry_list,
+				     struct rxr_rx_entry,
+				     rx_entry, peer_entry, tmp) {
+		rxr_release_rx_entry(ep, rx_entry);
+	}
+
+	if (peer->flags & RXR_PEER_HANDSHAKE_QUEUED)
+		dlist_remove(&peer->handshake_queued_entry);
+
+	if (peer->flags & RXR_PEER_IN_BACKOFF)
+		dlist_remove(&peer->rnr_backoff_entry);
+
+#ifdef ENABLE_EFA_POISONING
+	rxr_poison_mem_region((uint32_t *)peer, sizeof(struct rdm_peer));
+#endif
+}
+
+/**
  * @brief find efa_conn struct using fi_addr
  *
  * @param[in]	av	efa av
@@ -100,7 +194,7 @@ struct efa_conn *efa_av_addr_to_conn(struct efa_av *av, fi_addr_t fi_addr)
 		return NULL;
 
 	efa_av_entry = (struct efa_av_entry *)util_av_entry->data;
-	return &efa_av_entry->conn;
+	return efa_av_entry->conn.ep_addr ? &efa_av_entry->conn : NULL;
 }
 
 fi_addr_t efa_ahn_qpn_to_addr(struct efa_av *av, uint16_t ahn, uint16_t qpn)
@@ -257,25 +351,17 @@ int efa_conn_rdm_init(struct efa_av *av, struct efa_conn *conn)
 	struct rdm_peer *peer;
 
 	assert(av->ep_type == FI_EP_RDM);
+	assert(conn->ep_addr);
 
 	/* currently multiple EP bind to same av is not supported */
+	assert(!dlist_empty(&av->util_av.ep_list));
 	rxr_ep = container_of(av->util_av.ep_list.next, struct rxr_ep, util_ep.av_entry);
 
 	peer = &conn->rdm_peer;
-	memset(peer, 0, sizeof(struct rdm_peer));
-	ofi_atomic_initialize32(&peer->use_cnt, 1);
-	peer->efa_fiaddr = conn->fi_addr;
-	peer->is_self = efa_is_same_addr((struct efa_ep_addr *)rxr_ep->core_addr,
-					 &conn->ep_addr);
-
-	ofi_recvwin_buf_alloc(&peer->robuf, rxr_env.recvwin_size);
-	peer->rx_credits = rxr_env.rx_window_size;
-	peer->tx_credits = rxr_env.tx_max_credits;
-	dlist_init(&peer->rx_unexp_list);
-	dlist_init(&peer->rx_unexp_tagged_list);
+	efa_rdm_peer_init(peer, rxr_ep, conn);
 
 	/* If peer is local, insert the address into shm provider's av */
-	if (rxr_ep->use_shm && efa_is_local_peer(av, &conn->ep_addr)) {
+	if (rxr_ep->use_shm && efa_is_local_peer(av, conn->ep_addr)) {
 		if (av->shm_used >= rxr_env.shm_av_size) {
 			EFA_WARN(FI_LOG_AV,
 				 "Max number of shm AV entry (%d) has been reached.\n",
@@ -283,7 +369,7 @@ int efa_conn_rdm_init(struct efa_av *av, struct efa_conn *conn)
 			return -FI_ENOMEM;
 		}
 
-		err = rxr_ep_efa_addr_to_str(&conn->ep_addr, smr_name);
+		err = rxr_ep_efa_addr_to_str(conn->ep_addr, smr_name);
 		if (err != FI_SUCCESS) {
 			EFA_WARN(FI_LOG_AV,
 				 "rxr_ep_efa_addr_to_str() failed! err=%d\n", err);
@@ -324,6 +410,7 @@ void efa_conn_rdm_deinit(struct efa_av *av, struct efa_conn *conn)
 {
 	int err;
 	struct rdm_peer *peer;
+	struct rxr_ep *ep;
 
 	assert(av->ep_type == FI_EP_RDM);
 
@@ -341,9 +428,10 @@ void efa_conn_rdm_deinit(struct efa_av *av, struct efa_conn *conn)
 
 	/*
 	 * We need peer->shm_fiaddr to remove shm address from shm av table,
-	 * so efa_rdm_peer_reset must be after removing shm av table.
+	 * so efa_rdm_peer_clear must be after removing shm av table.
 	 */
-	efa_rdm_peer_reset(peer);
+	ep = dlist_empty(&av->util_av.ep_list) ? NULL : container_of(av->util_av.ep_list.next, struct rxr_ep, util_ep.av_entry);
+	efa_rdm_peer_clear(ep, peer);
 }
 
 /**
@@ -388,10 +476,11 @@ struct efa_conn *efa_conn_alloc(struct efa_av *av, struct efa_ep_addr *raw_addr,
 	util_av_entry = ofi_bufpool_get_ibuf(av->util_av.av_entry_pool,
 					     util_av_fi_addr);
 	efa_av_entry = (struct efa_av_entry *)util_av_entry->data;
+	assert(efa_is_same_addr(raw_addr, (struct efa_ep_addr *)efa_av_entry->ep_addr));
+
 	conn = &efa_av_entry->conn;
 	memset(conn, 0, sizeof(*conn));
-
-	memcpy(&conn->ep_addr, raw_addr, sizeof(*raw_addr));
+	conn->ep_addr = (struct efa_ep_addr *)efa_av_entry->ep_addr;
 	assert(av->type == FI_AV_MAP || av->type == FI_AV_TABLE);
 	conn->fi_addr = (av->type == FI_AV_MAP) ? (uintptr_t)(void *)conn : util_av_fi_addr;
 	conn->util_av_fi_addr = util_av_fi_addr;
@@ -422,12 +511,12 @@ struct efa_conn *efa_conn_alloc(struct efa_av *av, struct efa_ep_addr *raw_addr,
 		 */
 		prev_conn = reverse_av_entry->conn;
 		assert(prev_conn);
-		assert(memcmp(prev_conn->ep_addr.raw, conn->ep_addr.raw, EFA_GID_LEN)==0);
-		assert(prev_conn->ep_addr.qpn == conn->ep_addr.qpn);
-		assert(prev_conn->ep_addr.qkey != conn->ep_addr.qkey);
+		assert(memcmp(prev_conn->ep_addr->raw, conn->ep_addr->raw, EFA_GID_LEN)==0);
+		assert(prev_conn->ep_addr->qpn == conn->ep_addr->qpn);
+		assert(prev_conn->ep_addr->qkey != conn->ep_addr->qkey);
 		EFA_WARN(FI_LOG_AV, "QP reuse detected! Previous qkey: %d Current qkey: %d\n",
-			 prev_conn->ep_addr.qkey, conn->ep_addr.qkey);
-		conn->rdm_peer.prev_qkey = prev_conn->ep_addr.qkey;
+			 prev_conn->ep_addr->qkey, conn->ep_addr->qkey);
+		conn->rdm_peer.prev_qkey = prev_conn->ep_addr->qkey;
 		efa_conn_release(av, prev_conn);
 	}
 
@@ -451,10 +540,12 @@ err_release:
 	if (conn->ah)
 		efa_ah_release(av, conn->ah);
 
+	conn->ep_addr = NULL;
 	err = ofi_av_remove_addr(&av->util_av, util_av_fi_addr);
 	if (err)
 		EFA_WARN(FI_LOG_AV, "While processing previous failure, ofi_av_remove_addr failed! err=%d\n",
 			 err);
+
 	return NULL;
 }
 
@@ -469,6 +560,8 @@ static
 void efa_conn_release(struct efa_av *av, struct efa_conn *conn)
 {
 	struct efa_reverse_av *reverse_av_entry;
+	struct util_av_entry *util_av_entry;
+	struct efa_av_entry *efa_av_entry;
 	struct efa_ah_qpn key;
 	char gidstr[INET6_ADDRSTRLEN];
 
@@ -477,7 +570,7 @@ void efa_conn_release(struct efa_av *av, struct efa_conn *conn)
 
 	memset(&key, 0, sizeof(key));
 	key.ahn = conn->ah->ahn;
-	key.qpn = conn->ep_addr.qpn;
+	key.qpn = conn->ep_addr->qpn;
 	HASH_FIND(hh, av->reverse_av, &key, sizeof(key), reverse_av_entry);
 	assert(reverse_av_entry);
 	HASH_DEL(av->reverse_av, reverse_av_entry);
@@ -485,11 +578,19 @@ void efa_conn_release(struct efa_av *av, struct efa_conn *conn)
 
 	efa_ah_release(av, conn->ah);
 
+	util_av_entry = ofi_bufpool_get_ibuf(av->util_av.av_entry_pool, conn->util_av_fi_addr);
+	assert(util_av_entry);
+	efa_av_entry = (struct efa_av_entry *)util_av_entry->data;
+
 	ofi_av_remove_addr(&av->util_av, conn->util_av_fi_addr);
 
-	inet_ntop(AF_INET6, conn->ep_addr.raw, gidstr, INET6_ADDRSTRLEN);
+	inet_ntop(AF_INET6, conn->ep_addr->raw, gidstr, INET6_ADDRSTRLEN);
 	EFA_INFO(FI_LOG_AV, "efa_conn released! conn[%p] GID[%s] QP[%u]\n",
-		 conn, gidstr, conn->ep_addr.qpn);
+		 conn, gidstr, conn->ep_addr->qpn);
+
+	conn->ep_addr = NULL;
+	memset(efa_av_entry->ep_addr, 0, EFA_EP_ADDR_LEN);
+
 	av->used--;
 }
 
@@ -621,12 +722,37 @@ static int efa_av_lookup(struct fid_av *av_fid, fi_addr_t fi_addr,
 	if (!conn)
 		return -FI_EINVAL;
 
-	memcpy(addr, (void *)&conn->ep_addr, MIN(sizeof(conn->ep_addr), *addrlen));
-	if (*addrlen > sizeof(conn->ep_addr))
-		*addrlen = sizeof(conn->ep_addr);
+	memcpy(addr, (void *)conn->ep_addr, MIN(EFA_EP_ADDR_LEN, *addrlen));
+	if (*addrlen > EFA_EP_ADDR_LEN)
+		*addrlen = EFA_EP_ADDR_LEN;
 	return 0;
 }
 
+/*
+ * @brief remove a set of addresses from AV and release its resources
+ *
+ * This function implements fi_av_remove() for EFA provider.
+ *
+ * Note that even after an address was removed from AV, it is still
+ * possible to get TX and RX completion for the address. Per libfabric
+ * standard, these completions should be ignored.
+ *
+ * To help TX completion handler to identify such a TX completion,
+ * when removing an address, all its outstanding TX packet's addr
+ * was set to FI_ADDR_NOTAVAIL. The TX completion handler will
+ * ignore TX packet whose address is FI_ADDR_NOTAVAIL.
+ *
+ * Meanwhile, lower provider  will set a packet's address to
+ * FI_ADDR_NOTAVAIL from it is from a removed address. RX completion
+ * handler will ignore such packets.
+ *
+ * @param[in]	av_fid	fid of AV (address vector)
+ * @param[in]	fi_addr pointer to an array of libfabric addresses
+ * @param[in]	counter	number of libfabric addresses in the array
+ * @param[in]	flags	flags
+ * @return	0 if all addresses have been removed successfully,
+ * 		negative libfabric error code if error was encoutnered.
+ */
 static int efa_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
 			 size_t count, uint64_t flags)
 {
@@ -650,11 +776,6 @@ static int efa_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
 			break;
 		}
 
-		if (av->ep_type == FI_EP_RDM && efa_peer_in_use(&conn->rdm_peer)) {
-			err = -FI_EBUSY;
-			break;
-		}
-
 		efa_conn_release(av, conn);
 	}
 
diff --git a/prov/efa/src/efa_domain.c b/prov/efa/src/efa_domain.c
index 52817fe..572e21c 100644
--- a/prov/efa/src/efa_domain.c
+++ b/prov/efa/src/efa_domain.c
@@ -141,15 +141,17 @@ static int efa_open_device_by_name(struct efa_domain *domain, const char *name)
 	return ret;
 }
 
-/* @brief Check if rdma-core fork support is enabled
+/* @brief Check if rdma-core fork support is enabled and prevent fork
+ * support from being enabled later.
  *
- * Register a temporary buffer and call ibv_fork_init() to determine if fork
- * support is enabled.
+ * Register a temporary buffer and call ibv_fork_init() to determine
+ * if fork support is enabled. Registering a buffer prevents future
+ * calls to ibv_fork_init() from completing successfully.
  *
  * This relies on internal behavior in rdma-core and is a temporary workaround.
  *
  * @param domain_fid domain fid so we can register memory
- * @return 0 if fork support is not enabled, 1 if it is.
+ * @return 1 if fork support is enabled, 0 otherwise
  */
 static int efa_check_fork_enabled(struct fid_domain *domain_fid)
 {
@@ -157,6 +159,18 @@ static int efa_check_fork_enabled(struct fid_domain *domain_fid)
 	char *buf;
 	int ret;
 
+	/* If ibv_is_fork_initialized is availble, check if the function
+	 * can exit early.
+	 */
+#if HAVE_IBV_IS_FORK_INITIALIZED == 1
+	enum ibv_fork_status fork_status = ibv_is_fork_initialized();
+
+	/* If fork support is enabled or unneeded, return. */
+	if (fork_status != IBV_FORK_DISABLED)
+		return fork_status == IBV_FORK_ENABLED;
+
+#endif /* HAVE_IBV_IS_FORK_INITIALIZED */
+
 	buf = malloc(ofi_get_page_size());
 	if (!buf)
 		return -FI_ENOMEM;
@@ -237,10 +251,14 @@ void efa_atfork_callback()
 		"other system errors.\n"
 		"\n"
 		"For the Libfabric EFA provider to work safely when fork()\n"
-		"is called please set the following environment variable:\n"
+		"is called please do one of the following:\n"
+		"1) Set the environment variable:\n"
 		"          FI_EFA_FORK_SAFE=1\n"
 		"and verify you are using rdma-core v31.1 or later.\n"
 		"\n"
+		"OR\n"
+		"2) Use Linux Kernel 5.13+ with rdma-core v35.0+\n"
+		"\n"
 		"Please note that enabling fork support may cause a\n"
 		"small performance impact.\n"
 		"\n"
diff --git a/prov/efa/src/efa_msg.c b/prov/efa/src/efa_msg.c
index 6eb37c1..047aa20 100644
--- a/prov/efa/src/efa_msg.c
+++ b/prov/efa/src/efa_msg.c
@@ -332,6 +332,7 @@ static ssize_t efa_post_send(struct efa_ep *ep, const struct fi_msg *msg, uint64
 	memset(ewr, 0, sizeof(*ewr) + sizeof(*ewr->sge) * msg->iov_count);
 	wr = &ewr->wr;
 	conn = efa_av_addr_to_conn(ep->av, msg->addr);
+	assert(conn && conn->ep_addr);
 
 	ret = efa_post_send_validate(ep, msg, conn, flags, &len);
 	if (OFI_UNLIKELY(ret)) {
@@ -341,14 +342,14 @@ static ssize_t efa_post_send(struct efa_ep *ep, const struct fi_msg *msg, uint64
 
 	efa_post_send_sgl(ep, msg, ewr);
 
-	if (flags & FI_INJECT)
+	if (len <= ep->domain->ctx->inline_buf_size)
 		wr->send_flags |= IBV_SEND_INLINE;
 
 	wr->opcode = IBV_WR_SEND;
 	wr->wr_id = (uintptr_t)msg->context;
 	wr->wr.ud.ah = conn->ah->ibv_ah;
-	wr->wr.ud.remote_qpn = conn->ep_addr.qpn;
-	wr->wr.ud.remote_qkey = conn->ep_addr.qkey;
+	wr->wr.ud.remote_qpn = conn->ep_addr->qpn;
+	wr->wr.ud.remote_qkey = conn->ep_addr->qkey;
 
 	ep->xmit_more_wr_tail->next = wr;
 	ep->xmit_more_wr_tail = wr;
diff --git a/prov/efa/src/efa_rma.c b/prov/efa/src/efa_rma.c
index 0478def..32ee572 100644
--- a/prov/efa/src/efa_rma.c
+++ b/prov/efa/src/efa_rma.c
@@ -102,8 +102,9 @@ ssize_t efa_rma_post_read(struct efa_ep *ep, const struct fi_msg_rma *msg,
 				   qp->qp_num, qp->qkey);
 	} else {
 		conn = efa_av_addr_to_conn(ep->av, msg->addr);
+		assert(conn && conn->ep_addr);
 		ibv_wr_set_ud_addr(qp->ibv_qp_ex, conn->ah->ibv_ah,
-				   conn->ep_addr.qpn, conn->ep_addr.qkey);
+				   conn->ep_addr->qpn, conn->ep_addr->qkey);
 	}
 
 	return ibv_wr_complete(qp->ibv_qp_ex);
diff --git a/prov/efa/src/rxr/rxr.h b/prov/efa/src/rxr/rxr.h
index cca407d..c209dcc 100644
--- a/prov/efa/src/rxr/rxr.h
+++ b/prov/efa/src/rxr/rxr.h
@@ -102,8 +102,17 @@ static inline void rxr_poison_mem_region(uint32_t *ptr, size_t size)
 #define RXR_DEF_CQ_SIZE			(8192)
 #define RXR_REMOTE_CQ_DATA_LEN		(8)
 
-/* maximum timeout for RNR backoff (microseconds) */
-#define RXR_DEF_RNR_MAX_TIMEOUT		(1000000)
+/* the default value for rxr_env.rnr_backoff_wait_time_cap */
+#define RXR_DEFAULT_RNR_BACKOFF_WAIT_TIME_CAP	(1000000)
+
+/*
+ * the maximum value for rxr_env.rnr_backoff_wait_time_cap
+ * Because the backoff wait time is multiplied by 2 when
+ * RNR is encountered, its value must be < INT_MAX/2.
+ * Therefore, its cap must be < INT_MAX/2 too.
+ */
+#define RXR_MAX_RNR_BACKOFF_WAIT_TIME_CAP	(INT_MAX/2 - 1)
+
 /* bounds for random RNR backoff timeout */
 #define RXR_RAND_MIN_TIMEOUT		(40)
 #define RXR_RAND_MAX_TIMEOUT		(120)
@@ -229,8 +238,8 @@ struct rxr_env {
 	size_t rx_iov_limit;
 	int rx_copy_unexp;
 	int rx_copy_ooo;
-	int max_timeout;
-	int timeout_interval;
+	int rnr_backoff_wait_time_cap; /* unit is us */
+	int rnr_backoff_initial_wait_time; /* unit is us */
 	size_t efa_cq_read_size;
 	size_t shm_cq_read_size;
 	size_t efa_max_medium_msg_size;
@@ -288,7 +297,6 @@ enum rxr_rx_comm_type {
 #define RXR_PEER_HANDSHAKE_SENT BIT_ULL(1) /* a handshake packet has been sent to a peer */
 #define RXR_PEER_HANDSHAKE_RECEIVED BIT_ULL(2)
 #define RXR_PEER_IN_BACKOFF BIT_ULL(3) /* peer is in backoff, not allowed to send */
-#define RXR_PEER_BACKED_OFF BIT_ULL(4) /* peer backoff was increased during this loop of the progress engine */
 /*
  * FI_EAGAIN error was encountered when sending handsahke to this peer,
  * the peer was put in rxr_ep->handshake_queued_peer_list.
@@ -318,18 +326,20 @@ struct rdm_peer {
 	uint32_t flags;
 	uint32_t maxproto;		/* maximum supported protocol version by this peer */
 	uint64_t features[RXR_MAX_NUM_PROTOCOLS]; /* the feature flag for each version */
-	size_t tx_pending;		/* tracks pending tx ops to this peer */
+	size_t efa_outstanding_tx_ops;	/* tracks outstanding tx ops to this peer on EFA device */
+	size_t shm_outstanding_tx_ops;  /* tracks outstanding tx ops to this peer on SHM */
+	struct dlist_entry outstanding_tx_pkts; /* a list of outstanding tx pkts to the peer */
 	uint16_t tx_credits;		/* available send credits */
 	uint16_t rx_credits;		/* available credits to allocate */
-	uint64_t rnr_ts;		/* timestamp for RNR backoff tracking */
+	uint64_t rnr_backoff_begin_ts;	/* timestamp for RNR backoff period begin */
+	uint64_t rnr_backoff_wait_time;	/* how long the RNR backoff period last */
 	int rnr_queued_pkt_cnt;		/* queued RNR packet count */
-	int timeout_interval;		/* initial RNR timeout value */
-	int rnr_timeout_exp;		/* RNR timeout exponentation calc val */
-	struct dlist_entry rnr_entry;	/* linked to rxr_ep peer_backoff_list */
+	struct dlist_entry rnr_backoff_entry;	/* linked to rxr_ep peer_backoff_list */
 	struct dlist_entry handshake_queued_entry; /* linked with rxr_ep->handshake_queued_peer_list */
 	struct dlist_entry rx_unexp_list; /* a list of unexpected untagged rx_entry for this peer */
 	struct dlist_entry rx_unexp_tagged_list; /* a list of unexpected tagged rx_entry for this peer */
-	ofi_atomic32_t use_cnt;		/* refcount */
+	struct dlist_entry tx_entry_list; /* a list of tx_entry related to this peer */
+	struct dlist_entry rx_entry_list; /* a list of rx_entry relased to this peer */
 };
 
 struct rxr_queued_ctrl_info {
@@ -434,11 +444,14 @@ struct rxr_rx_entry {
 	struct rxr_pkt_entry *unexp_pkt;
 	char *atomrsp_data;
 
+	/* linked with rx_entry_list in rdm_peer */
+	struct dlist_entry peer_entry;
+
+	/* linked with rx_entry_list in rxr_ep */
+	struct dlist_entry ep_entry;
 #if ENABLE_DEBUG
 	/* linked with rx_pending_list in rxr_ep */
 	struct dlist_entry rx_pending_entry;
-	/* linked with rx_entry_list in rxr_ep */
-	struct dlist_entry rx_entry_entry;
 #endif
 };
 
@@ -507,10 +520,11 @@ struct rxr_tx_entry {
 	/* Queued packets due to TX queue full or RNR backoff */
 	struct dlist_entry queued_pkts;
 
-#if ENABLE_DEBUG
+	/* peer_entry is linked with tx_entry_list in rdm_peer */
+	struct dlist_entry peer_entry;
+
 	/* linked with tx_entry_list in rxr_ep */
-	struct dlist_entry tx_entry_entry;
-#endif
+	struct dlist_entry ep_entry;
 };
 
 #define RXR_GET_X_ENTRY_TYPE(pkt_entry)	\
@@ -575,7 +589,7 @@ struct rxr_ep {
 
 	/* rx/tx queue size of core provider */
 	size_t core_rx_size;
-	size_t max_outstanding_tx;
+	size_t efa_max_outstanding_tx_ops;
 	size_t core_inject_size;
 	size_t max_data_payload_size;
 
@@ -603,8 +617,8 @@ struct rxr_ep {
 	size_t min_multi_recv_size;
 
 	/* buffer pool for send & recv */
-	struct ofi_bufpool *tx_pkt_efa_pool;
-	struct ofi_bufpool *rx_pkt_efa_pool;
+	struct ofi_bufpool *efa_tx_pkt_pool;
+	struct ofi_bufpool *efa_rx_pkt_pool;
 
 	/*
 	 * buffer pool for rxr_pkt_sendv struct, which is used
@@ -616,8 +630,8 @@ struct rxr_ep {
 	 * buffer pool for send & recv for shm as mtu size is different from
 	 * the one of efa, and do not require local memory registration
 	 */
-	struct ofi_bufpool *tx_pkt_shm_pool;
-	struct ofi_bufpool *rx_pkt_shm_pool;
+	struct ofi_bufpool *shm_tx_pkt_pool;
+	struct ofi_bufpool *shm_rx_pkt_pool;
 
 	/* staging area for unexpected and out-of-order packets */
 	struct ofi_bufpool *rx_unexp_pkt_pool;
@@ -686,15 +700,16 @@ struct rxr_ep {
 	/* tx packets waiting for send completion */
 	struct dlist_entry tx_pkt_list;
 
-	/* track allocated rx_entries and tx_entries for endpoint cleanup */
-	struct dlist_entry rx_entry_list;
-	struct dlist_entry tx_entry_list;
-
-	size_t sends;
+	size_t efa_total_posted_tx_ops;
+	size_t shm_total_posted_tx_ops;
 	size_t send_comps;
 	size_t failed_send_comps;
 	size_t recv_comps;
 #endif
+	/* track allocated rx_entries and tx_entries for endpoint cleanup */
+	struct dlist_entry rx_entry_list;
+	struct dlist_entry tx_entry_list;
+
 	/* number of posted buffer for shm */
 	size_t posted_bufs_shm;
 	size_t rx_bufs_shm_to_post;
@@ -707,8 +722,10 @@ struct rxr_ep {
 	/* Timestamp of when available_data_bufs was exhausted. */
 	uint64_t available_data_bufs_ts;
 
-	/* number of outstanding sends */
-	size_t tx_pending;
+	/* number of outstanding tx ops on efa device */
+	size_t efa_outstanding_tx_ops;
+	/* number of outstanding tx ops on shm */
+	size_t shm_outstanding_tx_ops;
 };
 
 #define rxr_rx_flags(rxr_ep) ((rxr_ep)->util_ep.rx_op_flags)
@@ -786,11 +803,10 @@ static inline void rxr_release_rx_entry(struct rxr_ep *ep,
 	struct dlist_entry *tmp;
 
 	if (rx_entry->peer)
-		ofi_atomic_dec32(&rx_entry->peer->use_cnt);
+		dlist_remove(&rx_entry->peer_entry);
+
+	dlist_remove(&rx_entry->ep_entry);
 
-#if ENABLE_DEBUG
-	dlist_remove(&rx_entry->rx_entry_entry);
-#endif
 	if (!dlist_empty(&rx_entry->queued_pkts)) {
 		dlist_foreach_container_safe(&rx_entry->queued_pkts,
 					     struct rxr_pkt_entry,
@@ -821,27 +837,9 @@ static inline int rxr_match_tag(uint64_t tag, uint64_t ignore,
 	return ((tag | ignore) == (match_tag | ignore));
 }
 
-static inline void rxr_ep_inc_tx_pending(struct rxr_ep *ep,
-					 struct rdm_peer *peer)
-{
-	ep->tx_pending++;
-	peer->tx_pending++;
-#if ENABLE_DEBUG
-	ep->sends++;
-#endif
-}
+void rxr_ep_record_tx_op_submitted(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
-static inline void rxr_ep_dec_tx_pending(struct rxr_ep *ep,
-					 struct rdm_peer *peer,
-					 int failed)
-{
-	ep->tx_pending--;
-	peer->tx_pending--;
-#if ENABLE_DEBUG
-	if (failed)
-		ep->failed_send_comps++;
-#endif
-}
+void rxr_ep_record_tx_op_completed(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
 static inline size_t rxr_get_rx_pool_chunk_cnt(struct rxr_ep *ep)
 {
@@ -850,7 +848,7 @@ static inline size_t rxr_get_rx_pool_chunk_cnt(struct rxr_ep *ep)
 
 static inline size_t rxr_get_tx_pool_chunk_cnt(struct rxr_ep *ep)
 {
-	return MIN(ep->max_outstanding_tx, ep->tx_size);
+	return MIN(ep->efa_max_outstanding_tx_ops, ep->tx_size);
 }
 
 static inline int rxr_need_sas_ordering(struct rxr_ep *ep)
@@ -924,12 +922,15 @@ struct rxr_rx_entry *rxr_ep_split_rx_entry(struct rxr_ep *ep,
 int rxr_ep_efa_addr_to_str(const void *addr, char *temp_name);
 
 /* CQ sub-functions */
-int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-			   ssize_t prov_errno);
-int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
-			   ssize_t prov_errno);
-int rxr_cq_handle_error(struct rxr_ep *ep, ssize_t prov_errno,
-			struct rxr_pkt_entry *pkt_entry);
+void rxr_cq_write_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+			   int err, int prov_errno);
+
+void rxr_cq_write_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
+			   int err, int prov_errno);
+
+void rxr_cq_queue_rnr_pkt(struct rxr_ep *ep,
+			  struct dlist_entry *list,
+			  struct rxr_pkt_entry *pkt_entry);
 
 void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 				struct rxr_rx_entry *rx_entry);
@@ -1030,15 +1031,6 @@ static inline void rxr_rm_tx_cq_check(struct rxr_ep *ep, struct util_cq *tx_cq)
 	fastlock_release(&tx_cq->cq_lock);
 }
 
-static inline bool rxr_peer_timeout_expired(struct rxr_ep *ep,
-					    struct rdm_peer *peer,
-					    uint64_t ts)
-{
-	return (ts >= (peer->rnr_ts + MIN(rxr_env.max_timeout,
-					  peer->timeout_interval *
-					  (1 << peer->rnr_timeout_exp))));
-}
-
 /* Performance counter declarations */
 #ifdef RXR_PERF_ENABLED
 #define RXR_PERF_FOREACH(DECL)	\
diff --git a/prov/efa/src/rxr/rxr_atomic.c b/prov/efa/src/rxr/rxr_atomic.c
index cf87f1d..0c1af60 100644
--- a/prov/efa/src/rxr/rxr_atomic.c
+++ b/prov/efa/src/rxr/rxr_atomic.c
@@ -83,9 +83,8 @@ rxr_atomic_alloc_tx_entry(struct rxr_ep *rxr_ep,
 		return NULL;
 	}
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
+
 	ofi_ioc_to_iov(msg_atomic->msg_iov, iov, msg_atomic->iov_count, datatype_size);
 	msg.addr = msg_atomic->addr;
 	msg.msg_iov = iov;
diff --git a/prov/efa/src/rxr/rxr_cq.c b/prov/efa/src/rxr/rxr_cq.c
index f21249a..2d6d5c0 100644
--- a/prov/efa/src/rxr/rxr_cq.c
+++ b/prov/efa/src/rxr/rxr_cq.c
@@ -67,32 +67,42 @@ static const char *rxr_cq_strerror(struct fid_cq *cq_fid, int prov_errno,
 	return str;
 }
 
-/*
- * Teardown rx_entry and write an error cq entry. With our current protocol we
- * will only encounter an RX error when sending a queued REQ or CTS packet or
- * if we are sending a CTS message. Because of this, the sender will not send
- * any additional data packets if the receiver encounters an error. If there is
- * a scenario in the future where the sender will continue to send data packets
- * we need to prevent rx_id mismatch. Ideally, we should add a NACK message and
- * tear down both RX and TX entires although whatever caused the error may
- * prevent that.
+/**
+ * @brief handle error happened to an RX (receive) operation
+ *
+ * This function will write an error cq entry to notify application the rx
+ * operation failed. If write failed, it will write an eq entry.
+ *
+ * It will also release resources owned by the RX entry, such as unexpected
+ * packet entry, because the RX operation is aborted.
+ *
+ * It will remove the rx_entry from queued rx_entry list for the same reason.
+ *
+ * It will NOT release the rx_entry because it is still possible to receive
+ * packet for this rx_entry.
  *
  * TODO: add a NACK message to tear down state on sender side
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	rx_entry	rx_entry that contains information of the tx operation
+ * @param[in]	err		positive libfabric error code
+ * @param[in]	prov_errno	positive provider specific error code
  */
-int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
-			   ssize_t prov_errno)
+void rxr_cq_write_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
+			   int err, int prov_errno)
 {
 	struct fi_cq_err_entry err_entry;
 	struct util_cq *util_cq;
 	struct dlist_entry *tmp;
 	struct rxr_pkt_entry *pkt_entry;
+	int write_cq_err;
 
 	memset(&err_entry, 0, sizeof(err_entry));
 
 	util_cq = ep->util_ep.rx_cq;
 
-	err_entry.err = FI_EIO;
-	err_entry.prov_errno = (int)prov_errno;
+	err_entry.err = err;
+	err_entry.prov_errno = prov_errno;
 
 	switch (rx_entry->state) {
 	case RXR_RX_INIT:
@@ -138,7 +148,7 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 	rxr_msg_multi_recv_free_posted_entry(ep, rx_entry);
 
         FI_WARN(&rxr_prov, FI_LOG_CQ,
-		"rxr_cq_handle_rx_error: err: %d, prov_err: %s (%d)\n",
+		"rxr_cq_write_rx_error: err: %d, prov_err: %s (%d)\n",
 		err_entry.err, fi_strerror(-err_entry.prov_errno),
 		err_entry.prov_errno);
 
@@ -150,35 +160,50 @@ int rxr_cq_handle_rx_error(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry,
 	//rxr_release_rx_entry(ep, rx_entry);
 
 	efa_cntr_report_error(&ep->util_ep, err_entry.flags);
-	return ofi_cq_write_error(util_cq, &err_entry);
+	write_cq_err = ofi_cq_write_error(util_cq, &err_entry);
+	if (write_cq_err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Error writing error cq entry when handling RX error");
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	}
 }
 
-/*
- * Teardown tx_entry and write an error cq entry. With our current protocol the
- * receiver will only send a CTS once the window is exhausted, meaning that all
- * data packets for that window will have been received successfully. This
- * means that the receiver will not send any CTS packets if the sender
- * encounters and error sending data packets. If that changes in the future we
- * will need to be careful to prevent tx_id mismatch.
+/**
+ * @brief write error CQ entry for a TX operation.
+ *
+ * This function write an error cq entry for a TX operation, if writing
+ * CQ error entry failed, it will write eq entry.
+ *
+ * If also remote the TX entry from ep->tx_queued_list and ep->tx_pending_list
+ * if the tx_entry is on it.
+ *
+ * It does NOT release tx entry because it is still possible to receive
+ * send completion for this TX entry
  *
  * TODO: add NACK message to tear down receive side state
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	tx_entry	tx_entry that contains information of the tx operation
+ * @param[in]	err		positive libfabric error code
+ * @param[in]	prov_errno	positive EFA provider specific error code
  */
-int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
-			   ssize_t prov_errno)
+void rxr_cq_write_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
+			   int err, int prov_errno)
 {
 	struct fi_cq_err_entry err_entry;
 	struct util_cq *util_cq;
 	uint32_t api_version;
 	struct dlist_entry *tmp;
 	struct rxr_pkt_entry *pkt_entry;
+	int write_cq_err;
 
 	memset(&err_entry, 0, sizeof(err_entry));
 
 	util_cq = ep->util_ep.tx_cq;
 	api_version = util_cq->domain->fabric->fabric_fid.api_version;
 
-	err_entry.err = FI_EIO;
-	err_entry.prov_errno = (int)prov_errno;
+	err_entry.err = err;
+	err_entry.prov_errno = prov_errno;
 
 	switch (tx_entry->state) {
 	case RXR_TX_REQ:
@@ -212,7 +237,7 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 		err_entry.err_data_size = 0;
 
 	FI_WARN(&rxr_prov, FI_LOG_CQ,
-		"rxr_cq_handle_tx_error: err: %d, prov_err: %s (%d)\n",
+		"rxr_cq_write_tx_error: err: %d, prov_err: %s (%d)\n",
 		err_entry.err, fi_strerror(-err_entry.prov_errno),
 		err_entry.prov_errno);
 
@@ -224,191 +249,122 @@ int rxr_cq_handle_tx_error(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	//rxr_release_tx_entry(ep, tx_entry);
 
 	efa_cntr_report_error(&ep->util_ep, tx_entry->cq_entry.flags);
-	return ofi_cq_write_error(util_cq, &err_entry);
+	write_cq_err = ofi_cq_write_error(util_cq, &err_entry);
+	if (write_cq_err) {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+			"Error writing error cq entry when handling TX error");
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	}
 }
 
-/*
- * Queue a packet on the appropriate list when an RNR error is received.
+/* @brief Queue a packet that encountered RNR error and setup RNR backoff
+ *
+ * We uses an exponential backoff strategy to handle RNR errors.
+ *
+ * `Backoff` means if a peer encountered RNR, an endpoint will
+ * wait a period of time before sending packets to the peer again
+ *
+ * `Exponential` means the more RNR encountered, the longer the
+ * backoff wait time will be.
+ *
+ * To quantify how long a peer stay in backoff mode, two parameters
+ * are defined:
+ *
+ *    rnr_backoff_begin_ts (ts is timestamp) and rnr_backoff_wait_time.
+ *
+ * A peer stays in backoff mode until:
+ *
+ * current_timestamp >= (rnr_backoff_begin_ts + rnr_backoff_wait_time),
+ *
+ * with one exception: a peer can got out of backoff mode early if a
+ * packet's send completion to this peer was reported by the device.
+ *
+ * Specifically, the implementation of RNR backoff is:
+ *
+ * For a peer, the first time RNR is encountered, the packet will
+ * be resent immediately.
+ *
+ * The second time RNR is encountered, the endpoint will put the
+ * peer in backoff mode, and initialize rnr_backoff_begin_timestamp
+ * and rnr_backoff_wait_time.
+ *
+ * The 3rd and following time RNR is encounter, the RNR will be handled
+ * like this:
+ *
+ *     If peer is already in backoff mode, rnr_backoff_begin_ts
+ *     will be updated
+ *
+ *     Otherwise, peer will be put in backoff mode again,
+ *     rnr_backoff_begin_ts will be updated and rnr_backoff_wait_time
+ *     will be doubled until it reached maximum wait time.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	list		queued RNR packet list
+ * @param[in]	pkt_entry	packet entry that encounter RNR
  */
-static inline void rxr_cq_queue_pkt(struct rxr_ep *ep,
-				    struct dlist_entry *list,
-				    struct rxr_pkt_entry *pkt_entry)
+void rxr_cq_queue_rnr_pkt(struct rxr_ep *ep,
+			  struct dlist_entry *list,
+			  struct rxr_pkt_entry *pkt_entry)
 {
 	struct rdm_peer *peer;
 
+#if ENABLE_DEBUG
+	dlist_remove(&pkt_entry->dbg_entry);
+#endif
+	dlist_insert_tail(&pkt_entry->entry, list);
+
 	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
 	assert(peer);
-
-	/*
-	 * Queue the packet if it has not been retransmitted yet.
-	 */
 	if (pkt_entry->state != RXR_PKT_ENTRY_RNR_RETRANSMIT) {
+		/* This is the first time this packet encountered RNR,
+		 * we are NOT going to put the peer in backoff mode just yet.
+		 */
 		pkt_entry->state = RXR_PKT_ENTRY_RNR_RETRANSMIT;
 		peer->rnr_queued_pkt_cnt++;
-		goto queue_pkt;
+		return;
 	}
 
-	/*
-	 * Otherwise, increase the backoff if the peer is already not in
-	 * backoff. Reset the timer when starting backoff or if another RNR for
-	 * a retransmitted packet is received while waiting for the timer to
-	 * expire.
+	/* This packet has encountered RNR multiple times, therefore the peer
+	 * need to be in backoff mode.
+	 *
+	 * If the peer is already in backoff mode, we just need to update the
+	 * RNR backoff begin time.
+	 *
+	 * Otherwise, we need to put the peer in backoff mode and set up backoff
+	 * begin time and wait time.
 	 */
-	peer->rnr_ts = ofi_gettime_us();
-	if (peer->flags & RXR_PEER_IN_BACKOFF)
-		goto queue_pkt;
+	if (peer->flags & RXR_PEER_IN_BACKOFF) {
+		peer->rnr_backoff_begin_ts = ofi_gettime_us();
+		return;
+	}
 
 	peer->flags |= RXR_PEER_IN_BACKOFF;
+	dlist_insert_tail(&peer->rnr_backoff_entry,
+			  &ep->peer_backoff_list);
 
-	if (!peer->timeout_interval) {
-		if (rxr_env.timeout_interval)
-			peer->timeout_interval = rxr_env.timeout_interval;
+	peer->rnr_backoff_begin_ts = ofi_gettime_us();
+	if (peer->rnr_backoff_wait_time == 0) {
+		if (rxr_env.rnr_backoff_initial_wait_time > 0)
+			peer->rnr_backoff_wait_time = rxr_env.rnr_backoff_initial_wait_time;
 		else
-			peer->timeout_interval = MAX(RXR_RAND_MIN_TIMEOUT,
-						     rand() %
-						     RXR_RAND_MAX_TIMEOUT);
+			peer->rnr_backoff_wait_time = MAX(RXR_RAND_MIN_TIMEOUT,
+							  rand() %
+							  RXR_RAND_MAX_TIMEOUT);
 
-		peer->rnr_timeout_exp = 1;
 		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 		       "initializing backoff timeout for peer: %" PRIu64
-		       " timeout: %d rnr_queued_pkts: %d\n",
-		       pkt_entry->addr, peer->timeout_interval,
+		       " timeout: %ld rnr_queued_pkts: %d\n",
+		       pkt_entry->addr, peer->rnr_backoff_wait_time,
 		       peer->rnr_queued_pkt_cnt);
 	} else {
-		/* Only backoff once per peer per progress thread loop. */
-		if (!(peer->flags & RXR_PEER_BACKED_OFF)) {
-			peer->flags |= RXR_PEER_BACKED_OFF;
-			peer->rnr_timeout_exp++;
-			FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-			       "increasing backoff for peer: %" PRIu64
-			       " rnr_timeout_exp: %d rnr_queued_pkts: %d\n",
-			       pkt_entry->addr, peer->rnr_timeout_exp,
-			       peer->rnr_queued_pkt_cnt);
-		}
-	}
-	dlist_insert_tail(&peer->rnr_entry,
-			  &ep->peer_backoff_list);
-
-queue_pkt:
-#if ENABLE_DEBUG
-	dlist_remove(&pkt_entry->dbg_entry);
-#endif
-	dlist_insert_tail(&pkt_entry->entry, list);
-}
-
-int rxr_cq_handle_error(struct rxr_ep *ep, ssize_t prov_errno, struct rxr_pkt_entry *pkt_entry)
-{
-	struct rxr_rx_entry *rx_entry;
-	struct rxr_tx_entry *tx_entry;
-	struct rxr_read_entry *read_entry;
-	struct rdm_peer *peer;
-	ssize_t ret;
-
-	if (!pkt_entry)
-		goto write_eq_err;
-
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(peer);
-	if (rxr_get_base_hdr(pkt_entry->pkt)->type == RXR_HANDSHAKE_PKT) {
-		rxr_ep_dec_tx_pending(ep, peer, 1);
-		rxr_pkt_entry_release_tx(ep, pkt_entry);
-		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR) {
-			/* Add peer to handshake_queued_peer_list for retry later
-			 * in progress engine.
-			 */
-			assert(!(peer->flags & RXR_PEER_HANDSHAKE_QUEUED));
-			peer->flags |= RXR_PEER_HANDSHAKE_QUEUED;
-			dlist_insert_tail(&peer->handshake_queued_entry,
-					  &ep->handshake_queued_peer_list);
-		} else if (prov_errno != IBV_WC_REM_INV_RD_REQ_ERR) {
-			/* If prov_errno is IBV_WC_REM_INV_RD_REQ_ERR, the peer has been destroyed.
-			 * Which is normal, as peer does not always need a handshake packet perform
-			 * its duty. (For example, if a peer just want to sent 1 message to the ep, it
-			 * does not need handshake.)
-			 * In this case, it is safe to ignore this error completion. In all other cases,
-			 * we write an eq entry because there is no application operation associated
-			 * with handshake.
-			 */
-			efa_eq_write_error(&ep->util_ep, FI_EIO, prov_errno);
-		}
-		return 0;
-	}
-
-	if (!pkt_entry->x_entry) {
-		/*
-		 * A NULL x_entry means this is a recv posted buf pkt_entry.
-		 * Since we don't have any context besides the error code,
-		 * we will write to the eq instead.
-		 */
-		rxr_pkt_entry_release_rx(ep, pkt_entry);
-		goto write_eq_err;
-	}
-
-	/*
-	 * If x_entry is set this rx or tx entry error is for a sent
-	 * packet. Decrement the tx_pending counter and fall through to
-	 * the rx or tx entry handlers.
-	 */
-	if (!peer->is_local)
-		rxr_ep_dec_tx_pending(ep, peer, 1);
-	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
-		tx_entry = (struct rxr_tx_entry *)pkt_entry->x_entry;
-		if (prov_errno != IBV_WC_RNR_RETRY_EXC_ERR ||
-		    ep->handle_resource_management != FI_RM_ENABLED) {
-			ret = rxr_cq_handle_tx_error(ep, tx_entry, prov_errno);
-			rxr_pkt_entry_release_tx(ep, pkt_entry);
-			return ret;
-		}
-
-		rxr_cq_queue_pkt(ep, &tx_entry->queued_pkts, pkt_entry);
-		if (tx_entry->state == RXR_TX_SEND) {
-			dlist_remove(&tx_entry->entry);
-			tx_entry->state = RXR_TX_QUEUED_DATA_RNR;
-			dlist_insert_tail(&tx_entry->queued_entry,
-					  &ep->tx_entry_queued_list);
-		} else if (tx_entry->state == RXR_TX_REQ) {
-			tx_entry->state = RXR_TX_QUEUED_REQ_RNR;
-			dlist_insert_tail(&tx_entry->queued_entry,
-					  &ep->tx_entry_queued_list);
-		}
-		return 0;
-	} else if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_RX_ENTRY) {
-		rx_entry = (struct rxr_rx_entry *)pkt_entry->x_entry;
-		if (prov_errno != IBV_WC_RNR_RETRY_EXC_ERR ||
-		    ep->handle_resource_management != FI_RM_ENABLED) {
-			ret = rxr_cq_handle_rx_error(ep, rx_entry, prov_errno);
-			rxr_pkt_entry_release_tx(ep, pkt_entry);
-			return ret;
-		}
-		rxr_cq_queue_pkt(ep, &rx_entry->queued_pkts, pkt_entry);
-		/*
-		 * rx_entry send one ctrl packet at a time, so if we
-		 * received RNR for the packet, the rx_entry must not
-		 * be in ep's rx_queued_entry_list, thus cannot
-		 * be in QUEUED_CTRL state
-		 */
-		assert(rx_entry->state != RXR_RX_QUEUED_CTRL);
-		rx_entry->state = RXR_RX_QUEUED_CTRL;
-		dlist_insert_tail(&rx_entry->queued_entry,
-				  &ep->rx_entry_queued_list);
-		return 0;
-	} else if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_READ_ENTRY) {
-		read_entry = (struct rxr_read_entry *)pkt_entry->x_entry;
-		/* read requests is not expected to get RNR, so we call
-		 * rxr_read_handle_error() to handle general error here.
-		 */
-		ret = rxr_read_handle_error(ep, read_entry, prov_errno);
-		rxr_pkt_entry_release_tx(ep, pkt_entry);
-		return ret;
+		peer->rnr_backoff_wait_time = MIN(peer->rnr_backoff_wait_time * 2,
+						  rxr_env.rnr_backoff_wait_time_cap);
+		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
+		       "increasing backoff timeout for peer: %" PRIu64
+		       "to %ld rnr_queued_pkts: %d\n",
+		       pkt_entry->addr, peer->rnr_backoff_wait_time,
+		       peer->rnr_queued_pkt_cnt);
 	}
-
-	FI_WARN(&rxr_prov, FI_LOG_CQ,
-		"%s unknown x_entry state %d\n",
-		__func__, RXR_GET_X_ENTRY_TYPE(pkt_entry));
-	assert(0 && "unknown x_entry state");
-write_eq_err:
-	efa_eq_write_error(&ep->util_ep, prov_errno, prov_errno);
-	return 0;
 }
 
 void rxr_cq_write_rx_completion(struct rxr_ep *ep,
@@ -480,8 +436,7 @@ void rxr_cq_write_rx_completion(struct rxr_ep *ep,
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Unable to write recv completion: %s\n",
 				fi_strerror(-ret));
-			if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-				assert(0 && "failed to write err cq entry");
+			rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
 			return;
 		}
 
@@ -583,12 +538,27 @@ int rxr_cq_reorder_msg(struct rxr_ep *ep,
 #endif
 	if (ofi_recvwin_is_exp(robuf, msg_id))
 		return 0;
-	else if (!ofi_recvwin_id_valid(robuf, msg_id))
-		return -FI_EALREADY;
+	else if (!ofi_recvwin_id_valid(robuf, msg_id)) {
+		if (ofi_recvwin_id_processed(robuf, msg_id)) {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			       "Error: message id has already been processed. received: %" PRIu32 " expected: %"
+			       PRIu32 "\n", msg_id, ofi_recvwin_next_exp_id(robuf));
+			return -FI_EALREADY;
+		} else {
+			fprintf(stderr,
+				"Current receive window size (%d) is too small to hold incoming messages.\n"
+				"As a result, you application cannot proceed.\n"
+				"Receive window size can be increased by setting the environment variable:\n"
+				"              FI_EFA_RECVWIN_SIZE\n"
+				"\n"
+				"Your job will now abort.\n\n", rxr_env.recvwin_size);
+			abort();
+		}
+	}
 
 	if (OFI_LIKELY(rxr_env.rx_copy_ooo)) {
-		assert(pkt_entry->type == RXR_PKT_ENTRY_POSTED);
-		ooo_entry = rxr_pkt_entry_clone(ep, ep->rx_ooo_pkt_pool, pkt_entry, RXR_PKT_ENTRY_OOO);
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_EFA_RX_POOL);
+		ooo_entry = rxr_pkt_entry_clone(ep, ep->rx_ooo_pkt_pool, RXR_PKT_FROM_OOO_POOL, pkt_entry);
 		if (OFI_UNLIKELY(!ooo_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unable to allocate rx_pkt_entry for OOO msg\n");
@@ -762,8 +732,7 @@ void rxr_cq_write_tx_completion(struct rxr_ep *ep,
 			FI_WARN(&rxr_prov, FI_LOG_CQ,
 				"Unable to write send completion: %s\n",
 				fi_strerror(-ret));
-			if (rxr_cq_handle_tx_error(ep, tx_entry, ret))
-				assert(0 && "failed to write err cq entry");
+			rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
 			return;
 		}
 	}
diff --git a/prov/efa/src/rxr/rxr_ep.c b/prov/efa/src/rxr/rxr_ep.c
index 197a50b..47dc153 100644
--- a/prov/efa/src/rxr/rxr_ep.c
+++ b/prov/efa/src/rxr/rxr_ep.c
@@ -65,8 +65,7 @@ struct efa_ep_addr *rxr_peer_raw_addr(struct rxr_ep *ep, fi_addr_t addr)
 	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
 	efa_av = efa_ep->av;
 	efa_conn = efa_av_addr_to_conn(efa_av, addr);
-
-	return &efa_conn->ep_addr;
+	return efa_conn ? efa_conn->ep_addr : NULL;
 }
 
 const char *rxr_peer_raw_addr_str(struct rxr_ep *ep, fi_addr_t addr, char *buf, size_t *buflen)
@@ -94,9 +93,7 @@ struct rxr_rx_entry *rxr_ep_alloc_rx_entry(struct rxr_ep *ep, fi_addr_t addr, ui
 	}
 	memset(rx_entry, 0, sizeof(struct rxr_rx_entry));
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&rx_entry->rx_entry_entry, &ep->rx_entry_list);
-#endif
+	dlist_insert_tail(&rx_entry->ep_entry, &ep->rx_entry_list);
 	rx_entry->type = RXR_RX_ENTRY;
 	rx_entry->rx_id = ofi_buf_index(rx_entry);
 	dlist_init(&rx_entry->queued_pkts);
@@ -106,7 +103,7 @@ struct rxr_rx_entry *rxr_ep_alloc_rx_entry(struct rxr_ep *ep, fi_addr_t addr, ui
 	if (addr != FI_ADDR_UNSPEC) {
 		rx_entry->peer = rxr_ep_get_peer(ep, addr);
 		assert(rx_entry->peer);
-		ofi_atomic_inc32(&rx_entry->peer->use_cnt);
+		dlist_insert_tail(&rx_entry->peer_entry, &rx_entry->peer->rx_entry_list);
 	} else {
 		/*
 		 * If msg->addr is not provided, rx_entry->peer will be set
@@ -175,7 +172,7 @@ int rxr_ep_post_user_buf(struct rxr_ep *ep, struct rxr_rx_entry *rx_entry, uint6
 	dlist_init(&pkt_entry->entry);
 	mr = (struct efa_mr *)rx_entry->desc[0];
 	pkt_entry->mr = &mr->mr_fid;
-	pkt_entry->type = RXR_PKT_ENTRY_USER;
+	pkt_entry->alloc_type = RXR_PKT_FROM_USER_BUFFER;
 	pkt_entry->state = RXR_PKT_ENTRY_IN_USE;
 	pkt_entry->next = NULL;
 	/*
@@ -235,10 +232,10 @@ int rxr_ep_post_prov_buf(struct rxr_ep *ep, uint64_t flags, enum rxr_lower_ep_ty
 
 	switch (lower_ep_type) {
 	case SHM_EP:
-		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->rx_pkt_shm_pool);
+		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->shm_rx_pkt_pool, RXR_PKT_FROM_SHM_RX_POOL);
 		break;
 	case EFA_EP:
-		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->rx_pkt_efa_pool);
+		rx_pkt_entry = rxr_pkt_entry_alloc(ep, ep->efa_rx_pkt_pool, RXR_PKT_FROM_EFA_RX_POOL);
 		break;
 	default:
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
@@ -348,7 +345,7 @@ void rxr_tx_entry_init(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry,
 	tx_entry->addr = msg->addr;
 	tx_entry->peer = rxr_ep_get_peer(ep, tx_entry->addr);
 	assert(tx_entry->peer);
-	ofi_atomic_inc32(&tx_entry->peer->use_cnt);
+	dlist_insert_tail(&tx_entry->peer_entry, &tx_entry->peer->tx_entry_list);
 
 	tx_entry->send_flags = 0;
 	tx_entry->rxr_flags = 0;
@@ -441,18 +438,18 @@ struct rxr_tx_entry *rxr_ep_alloc_tx_entry(struct rxr_ep *rxr_ep,
 		tx_entry->tag = tag;
 	}
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
 	return tx_entry;
 }
 
 void rxr_release_tx_entry(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 {
 	int i, err = 0;
+	struct dlist_entry *tmp;
+	struct rxr_pkt_entry *pkt_entry;
 
 	assert(tx_entry->peer);
-	ofi_atomic_dec32(&tx_entry->peer->use_cnt);
+	dlist_remove(&tx_entry->peer_entry);
 
 	for (i = 0; i < tx_entry->iov_count; i++) {
 		if (tx_entry->mr[i]) {
@@ -466,10 +463,14 @@ void rxr_release_tx_entry(struct rxr_ep *ep, struct rxr_tx_entry *tx_entry)
 		}
 	}
 
-#if ENABLE_DEBUG
-	dlist_remove(&tx_entry->tx_entry_entry);
-#endif
-	assert(dlist_empty(&tx_entry->queued_pkts));
+	dlist_remove(&tx_entry->ep_entry);
+
+	dlist_foreach_container_safe(&tx_entry->queued_pkts,
+				     struct rxr_pkt_entry,
+				     pkt_entry, entry, tmp) {
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+	}
+
 #ifdef ENABLE_EFA_POISONING
 	rxr_poison_mem_region((uint32_t *)tx_entry,
 			      sizeof(struct rxr_tx_entry));
@@ -571,7 +572,7 @@ void rxr_prepare_desc_send(struct rxr_domain *rxr_domain,
 int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_entry)
 {
 	struct rdm_peer *peer;
-	int pending;
+	int outstanding;
 
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
 	assert(peer);
@@ -581,8 +582,8 @@ int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_
 	 * minimum of that and the amount required to finish the current long
 	 * message.
 	 */
-	pending = peer->tx_pending + 1;
-	tx_entry->credit_request = MIN(ofi_div_ceil(peer->tx_credits, pending),
+	outstanding = peer->efa_outstanding_tx_ops + 1;
+	tx_entry->credit_request = MIN(ofi_div_ceil(peer->tx_credits, outstanding),
 				       ofi_div_ceil(tx_entry->total_len,
 						    rxr_ep->max_data_payload_size));
 	tx_entry->credit_request = MAX(tx_entry->credit_request,
@@ -599,75 +600,96 @@ int rxr_ep_set_tx_credit_request(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_
 
 static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 {
-#if ENABLE_DEBUG
-	struct dlist_entry *tmp;
-	struct dlist_entry *entry;
+	struct dlist_entry *entry, *tmp;
 	struct rxr_rx_entry *rx_entry;
 	struct rxr_tx_entry *tx_entry;
+#if ENABLE_DEBUG
 	struct rxr_pkt_entry *pkt;
 #endif
 
-#if ENABLE_DEBUG
-	dlist_foreach(&rxr_ep->rx_unexp_list, entry) {
+	dlist_foreach_safe(&rxr_ep->rx_unexp_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unmatched unexpected rx_entry: %p pkt_entry %p\n",
+			rx_entry, rx_entry->unexp_pkt);
 		rxr_pkt_entry_release_rx(rxr_ep, rx_entry->unexp_pkt);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
 
-	dlist_foreach(&rxr_ep->rx_unexp_tagged_list, entry) {
+	dlist_foreach_safe(&rxr_ep->rx_unexp_tagged_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry, entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unmatched unexpected tagged rx_entry: %p pkt_entry %p\n",
+			rx_entry, rx_entry->unexp_pkt);
 		rxr_pkt_entry_release_rx(rxr_ep, rx_entry->unexp_pkt);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
 
-	dlist_foreach(&rxr_ep->rx_entry_queued_list, entry) {
+	dlist_foreach_safe(&rxr_ep->rx_entry_queued_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry,
 					queued_entry);
-		dlist_foreach_container_safe(&rx_entry->queued_pkts,
-					     struct rxr_pkt_entry,
-					     pkt, entry, tmp)
-			rxr_pkt_entry_release_tx(rxr_ep, pkt);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with queued rx_entry: %p\n",
+			rx_entry);
+		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
 
-	dlist_foreach(&rxr_ep->tx_entry_queued_list, entry) {
+	dlist_foreach_safe(&rxr_ep->tx_entry_queued_list, entry, tmp) {
 		tx_entry = container_of(entry, struct rxr_tx_entry,
 					queued_entry);
-		dlist_foreach_container_safe(&tx_entry->queued_pkts,
-					     struct rxr_pkt_entry,
-					     pkt, entry, tmp)
-			rxr_pkt_entry_release_tx(rxr_ep, pkt);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with queued tx_entry: %p\n",
+			tx_entry);
+		rxr_release_tx_entry(rxr_ep, tx_entry);
 	}
 
-	dlist_foreach_safe(&rxr_ep->rx_pkt_list, entry, tmp) {
+#if ENABLE_DEBUG
+	dlist_foreach_safe(&rxr_ep->rx_posted_buf_list, entry, tmp) {
 		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-		rxr_pkt_entry_release_rx(rxr_ep, pkt);
+		ofi_buf_free(pkt);
 	}
 
-	dlist_foreach_safe(&rxr_ep->rx_posted_buf_list, entry, tmp) {
+	if (rxr_ep->use_shm) {
+		dlist_foreach_safe(&rxr_ep->rx_posted_buf_shm_list, entry, tmp) {
+			pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
+			ofi_buf_free(pkt);
+		}
+	}
+
+	dlist_foreach_safe(&rxr_ep->rx_pkt_list, entry, tmp) {
 		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-		ofi_buf_free(pkt);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased RX pkt_entry: %p\n",
+			pkt);
+		rxr_pkt_entry_release_rx(rxr_ep, pkt);
 	}
 
 	dlist_foreach_safe(&rxr_ep->tx_pkt_list, entry, tmp) {
 		pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased TX pkt_entry: %p\n",
+			pkt);
 		rxr_pkt_entry_release_tx(rxr_ep, pkt);
 	}
+#endif
 
 	dlist_foreach_safe(&rxr_ep->rx_entry_list, entry, tmp) {
 		rx_entry = container_of(entry, struct rxr_rx_entry,
-					rx_entry_entry);
+					ep_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased rx_entry: %p\n",
+			rx_entry);
 		rxr_release_rx_entry(rxr_ep, rx_entry);
 	}
+
 	dlist_foreach_safe(&rxr_ep->tx_entry_list, entry, tmp) {
 		tx_entry = container_of(entry, struct rxr_tx_entry,
-					tx_entry_entry);
+					ep_entry);
+		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+			"Closing ep with unreleased tx_entry: %p\n",
+			tx_entry);
 		rxr_release_tx_entry(rxr_ep, tx_entry);
 	}
-	if (rxr_ep->use_shm) {
-		dlist_foreach_safe(&rxr_ep->rx_posted_buf_shm_list, entry, tmp) {
-			pkt = container_of(entry, struct rxr_pkt_entry, dbg_entry);
-			ofi_buf_free(pkt);
-		}
-	}
-#endif
 
 	if (rxr_ep->rx_entry_pool)
 		ofi_bufpool_destroy(rxr_ep->rx_entry_pool);
@@ -699,21 +721,21 @@ static void rxr_ep_free_res(struct rxr_ep *rxr_ep)
 	if (rxr_ep->rx_unexp_pkt_pool)
 		ofi_bufpool_destroy(rxr_ep->rx_unexp_pkt_pool);
 
-	if (rxr_ep->rx_pkt_efa_pool)
-		ofi_bufpool_destroy(rxr_ep->rx_pkt_efa_pool);
+	if (rxr_ep->efa_rx_pkt_pool)
+		ofi_bufpool_destroy(rxr_ep->efa_rx_pkt_pool);
 
-	if (rxr_ep->tx_pkt_efa_pool)
-		ofi_bufpool_destroy(rxr_ep->tx_pkt_efa_pool);
+	if (rxr_ep->efa_tx_pkt_pool)
+		ofi_bufpool_destroy(rxr_ep->efa_tx_pkt_pool);
 
 	if (rxr_ep->pkt_sendv_pool)
 		ofi_bufpool_destroy(rxr_ep->pkt_sendv_pool);
 
 	if (rxr_ep->use_shm) {
-		if (rxr_ep->rx_pkt_shm_pool)
-			ofi_bufpool_destroy(rxr_ep->rx_pkt_shm_pool);
+		if (rxr_ep->shm_rx_pkt_pool)
+			ofi_bufpool_destroy(rxr_ep->shm_rx_pkt_pool);
 
-		if (rxr_ep->tx_pkt_shm_pool)
-			ofi_bufpool_destroy(rxr_ep->tx_pkt_shm_pool);
+		if (rxr_ep->shm_tx_pkt_pool)
+			ofi_bufpool_destroy(rxr_ep->shm_tx_pkt_pool);
 	}
 }
 
@@ -1114,7 +1136,7 @@ static int rxr_create_pkt_pool(struct rxr_ep *ep, size_t size,
  */
 int rxr_ep_init(struct rxr_ep *ep)
 {
-	size_t entry_sz;
+	size_t entry_sz, sendv_pool_size;
 	int hp_pool_flag;
 	int ret;
 
@@ -1131,13 +1153,13 @@ int rxr_ep_init(struct rxr_ep *ep)
 
 	ret = rxr_create_pkt_pool(ep, entry_sz, rxr_get_tx_pool_chunk_cnt(ep),
 				  hp_pool_flag,
-				  &ep->tx_pkt_efa_pool);
+				  &ep->efa_tx_pkt_pool);
 	if (ret)
 		goto err_free;
 
 	ret = rxr_create_pkt_pool(ep, entry_sz, rxr_get_rx_pool_chunk_cnt(ep),
 				  hp_pool_flag,
-				  &ep->rx_pkt_efa_pool);
+				  &ep->efa_rx_pkt_pool);
 	if (ret)
 		goto err_free;
 
@@ -1247,17 +1269,20 @@ int rxr_ep_init(struct rxr_ep *ep)
 	if (ret)
 		goto err_free;
 
+	sendv_pool_size = rxr_get_tx_pool_chunk_cnt(ep);
+	if (ep->use_shm)
+		sendv_pool_size += shm_info->tx_attr->size;
 	ret = ofi_bufpool_create(&ep->pkt_sendv_pool,
 				 sizeof(struct rxr_pkt_sendv),
 				 RXR_BUF_POOL_ALIGNMENT,
-				 rxr_get_tx_pool_chunk_cnt(ep),
-				 rxr_get_tx_pool_chunk_cnt(ep), 0);
+				 sendv_pool_size,
+				 sendv_pool_size, 0);
 	if (ret)
 		goto err_free;
 
 	/* create pkt pool for shm */
 	if (ep->use_shm) {
-		ret = ofi_bufpool_create(&ep->tx_pkt_shm_pool,
+		ret = ofi_bufpool_create(&ep->shm_tx_pkt_pool,
 					 entry_sz,
 					 RXR_BUF_POOL_ALIGNMENT,
 					 shm_info->tx_attr->size,
@@ -1265,7 +1290,7 @@ int rxr_ep_init(struct rxr_ep *ep)
 		if (ret)
 			goto err_free;
 
-		ret = ofi_bufpool_create(&ep->rx_pkt_shm_pool,
+		ret = ofi_bufpool_create(&ep->shm_rx_pkt_pool,
 					 entry_sz,
 					 RXR_BUF_POOL_ALIGNMENT,
 					 shm_info->rx_attr->size,
@@ -1292,16 +1317,17 @@ int rxr_ep_init(struct rxr_ep *ep)
 	dlist_init(&ep->rx_pending_list);
 	dlist_init(&ep->rx_pkt_list);
 	dlist_init(&ep->tx_pkt_list);
+#endif
 	dlist_init(&ep->rx_entry_list);
 	dlist_init(&ep->tx_entry_list);
-#endif
+
 	/* Initialize pkt to rx map */
 	ep->pkt_rx_map = NULL;
 	return 0;
 
 err_free:
-	if (ep->tx_pkt_shm_pool)
-		ofi_bufpool_destroy(ep->tx_pkt_shm_pool);
+	if (ep->shm_tx_pkt_pool)
+		ofi_bufpool_destroy(ep->shm_tx_pkt_pool);
 
 	if (ep->pkt_sendv_pool)
 		ofi_bufpool_destroy(ep->pkt_sendv_pool);
@@ -1333,11 +1359,11 @@ err_free:
 	if (rxr_env.rx_copy_unexp && ep->rx_unexp_pkt_pool)
 		ofi_bufpool_destroy(ep->rx_unexp_pkt_pool);
 
-	if (ep->rx_pkt_efa_pool)
-		ofi_bufpool_destroy(ep->rx_pkt_efa_pool);
+	if (ep->efa_rx_pkt_pool)
+		ofi_bufpool_destroy(ep->efa_rx_pkt_pool);
 
-	if (ep->tx_pkt_efa_pool)
-		ofi_bufpool_destroy(ep->tx_pkt_efa_pool);
+	if (ep->efa_tx_pkt_pool)
+		ofi_bufpool_destroy(ep->efa_tx_pkt_pool);
 
 	return ret;
 }
@@ -1438,10 +1464,7 @@ void rxr_ep_progress_post_prov_buf(struct rxr_ep *ep)
 
 err_exit:
 
-	if (rxr_cq_handle_error(ep, err, NULL)) {
-		assert(0 &&
-		       "error writing error cq entry after failed post recv");
-	}
+	efa_eq_write_error(&ep->util_ep, err, err);
 }
 
 static inline int rxr_ep_send_queued_pkts(struct rxr_ep *ep,
@@ -1457,10 +1480,22 @@ static inline int rxr_ep_send_queued_pkts(struct rxr_ep *ep,
 			dlist_remove(&pkt_entry->entry);
 			continue;
 		}
+
+		/* If send succeeded, pkt_entry->entry will be added
+		 * to peer->outstanding_tx_pkts. Therefore, it must
+		 * be removed from the list before send.
+		 */
+		dlist_remove(&pkt_entry->entry);
+
 		ret = rxr_pkt_entry_send(ep, pkt_entry, 0);
-		if (ret)
+		if (ret) {
+			if (ret == -FI_EAGAIN) {
+				/* add the pkt back to pkts, so it can be resent again */
+				dlist_insert_tail(&pkt_entry->entry, pkts);
+			}
+
 			return ret;
-		dlist_remove(&pkt_entry->entry);
+		}
 	}
 	return 0;
 }
@@ -1488,12 +1523,12 @@ static inline void rxr_ep_check_peer_backoff_timer(struct rxr_ep *ep)
 		return;
 
 	dlist_foreach_container_safe(&ep->peer_backoff_list, struct rdm_peer,
-				     peer, rnr_entry, tmp) {
-		peer->flags &= ~RXR_PEER_BACKED_OFF;
-		if (!rxr_peer_timeout_expired(ep, peer, ofi_gettime_us()))
-			continue;
-		peer->flags &= ~RXR_PEER_IN_BACKOFF;
-		dlist_remove(&peer->rnr_entry);
+				     peer, rnr_backoff_entry, tmp) {
+		if (ofi_gettime_us() >= peer->rnr_backoff_begin_ts +
+					peer->rnr_backoff_wait_time) {
+			peer->flags &= ~RXR_PEER_IN_BACKOFF;
+			dlist_remove(&peer->rnr_backoff_entry);
+		}
 	}
 }
 
@@ -1513,7 +1548,7 @@ static inline void rdm_ep_poll_ibv_cq(struct rxr_ep *ep,
 	struct rdm_peer *peer;
 	struct rxr_pkt_entry *pkt_entry;
 	ssize_t ret;
-	int i;
+	int i, err, prov_errno;
 
 	efa_ep = container_of(ep->rdm_ep, struct efa_ep, util_ep.ep_fid);
 	efa_av = efa_ep->av;
@@ -1526,10 +1561,21 @@ static inline void rdm_ep_poll_ibv_cq(struct rxr_ep *ep,
 
 		if (OFI_UNLIKELY(ret < 0 || ibv_wc.status)) {
 			if (ret < 0) {
-				rxr_cq_handle_error(ep, -ret, NULL);
+				efa_eq_write_error(&ep->util_ep, -ret, -ret);
+				return;
+			}
+
+			pkt_entry = (void *)(uintptr_t)ibv_wc.wr_id;
+			err = ibv_wc.status;
+			prov_errno = ibv_wc.status;
+			if (ibv_wc.opcode == IBV_WC_SEND) {
+#if ENABLE_DEBUG
+				ep->failed_send_comps++;
+#endif
+				rxr_pkt_handle_send_error(ep, pkt_entry, err, prov_errno);
 			} else {
-				pkt_entry = (void *)(uintptr_t)ibv_wc.wr_id;
-				rxr_cq_handle_error(ep, ibv_wc.status, pkt_entry);
+				assert(ibv_wc.opcode == IBV_WC_RECV);
+				rxr_pkt_handle_recv_error(ep, pkt_entry, err, prov_errno);
 			}
 
 			return;
@@ -1607,11 +1653,20 @@ static inline void rdm_ep_poll_shm_cq(struct rxr_ep *ep,
 			return;
 
 		if (OFI_UNLIKELY(ret < 0)) {
-			if (ret == -FI_EAVAIL) {
-				rdm_ep_poll_shm_err_cq(ep->shm_cq, &cq_err_entry);
-				rxr_cq_handle_error(ep, cq_err_entry.prov_errno, cq_err_entry.op_context);
+			if (ret != -FI_EAVAIL) {
+				efa_eq_write_error(&ep->util_ep, -ret, -ret);
+				return;
+			}
+
+			rdm_ep_poll_shm_err_cq(ep->shm_cq, &cq_err_entry);
+			if (cq_err_entry.flags & (FI_SEND | FI_READ | FI_WRITE)) {
+				assert(cq_entry.op_context);
+				rxr_pkt_handle_send_error(ep, cq_entry.op_context, cq_err_entry.err, cq_err_entry.prov_errno);
+			} else if (cq_err_entry.flags & FI_RECV) {
+				assert(cq_entry.op_context);
+				rxr_pkt_handle_recv_error(ep, cq_entry.op_context, cq_err_entry.err, cq_err_entry.prov_errno);
 			} else {
-				rxr_cq_handle_error(ep, -ret, NULL);
+				efa_eq_write_error(&ep->util_ep, cq_err_entry.err, cq_err_entry.prov_errno);
 			}
 
 			return;
@@ -1683,8 +1738,13 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 		if (ret == -FI_EAGAIN)
 			break;
 
-		if (OFI_UNLIKELY(ret))
-			goto handshake_err;
+		if (OFI_UNLIKELY(ret)) {
+			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
+				"Failed to post HANDSHAKE to peer %ld: %s\n",
+				peer->efa_fiaddr, fi_strerror(-ret));
+			efa_eq_write_error(&ep->util_ep, FI_EIO, -ret);
+			return;
+		}
 
 		dlist_remove(&peer->handshake_queued_entry);
 		peer->flags &= ~RXR_PEER_HANDSHAKE_QUEUED;
@@ -1724,8 +1784,11 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 
 		if (ret == -FI_EAGAIN)
 			break;
-		if (OFI_UNLIKELY(ret))
-			goto rx_err;
+
+		if (OFI_UNLIKELY(ret)) {
+			rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
+			return;
+		}
 
 		/* it can happen that rxr_pkt_post_ctrl() released rx_entry
 		 * (if the packet type is EOR and inject is used). In
@@ -1764,8 +1827,10 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 		ret = rxr_ep_send_queued_pkts(ep, &tx_entry->queued_pkts);
 		if (ret == -FI_EAGAIN)
 			break;
-		if (OFI_UNLIKELY(ret))
-			goto tx_err;
+		if (OFI_UNLIKELY(ret)) {
+			rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
+			return;
+		}
 
 		if (tx_entry->state == RXR_TX_QUEUED_CTRL) {
 			ret = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry,
@@ -1773,8 +1838,11 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 						tx_entry->queued_ctrl.inject);
 			if (ret == -FI_EAGAIN)
 				break;
-			if (OFI_UNLIKELY(ret))
-				goto tx_err;
+
+			if (OFI_UNLIKELY(ret)) {
+				rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
+				return;
+			}
 		}
 
 		dlist_remove(&tx_entry->queued_entry);
@@ -1806,14 +1874,14 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 			continue;
 
 		while (tx_entry->window > 0) {
-			if (ep->max_outstanding_tx - ep->tx_pending <= 1 ||
+			if (ep->efa_max_outstanding_tx_ops - ep->efa_outstanding_tx_ops <= 1 ||
 			    tx_entry->window <= ep->max_data_payload_size)
 				tx_entry->send_flags &= ~FI_MORE;
 			/*
 			 * The core's TX queue is full so we can't do any
 			 * additional work.
 			 */
-			if (ep->tx_pending == ep->max_outstanding_tx)
+			if (ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 				goto out;
 
 			if (peer->flags & RXR_PEER_IN_BACKOFF)
@@ -1824,7 +1892,9 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 				tx_entry->send_flags &= ~FI_MORE;
 				if (ret == -FI_EAGAIN)
 					goto out;
-				goto tx_err;
+
+				rxr_cq_write_tx_error(ep, tx_entry, -ret, -ret);
+				return;
 			}
 		}
 	}
@@ -1844,15 +1914,17 @@ void rxr_ep_progress_internal(struct rxr_ep *ep)
 		 * The core's TX queue is full so we can't do any
 		 * additional work.
 		 */
-		if (ep->tx_pending == ep->max_outstanding_tx)
+		if (ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 			goto out;
 
 		ret = rxr_read_post(ep, read_entry);
 		if (ret == -FI_EAGAIN)
 			break;
 
-		if (OFI_UNLIKELY(ret))
-			goto read_err;
+		if (OFI_UNLIKELY(ret)) {
+			rxr_read_write_error(ep, read_entry, -ret, -ret);
+			return;
+		}
 
 		read_entry->state = RXR_RDMA_ENTRY_SUBMITTED;
 		dlist_remove(&read_entry->pending_entry);
@@ -1863,34 +1935,10 @@ out:
 	if (efa_ep->xmit_more_wr_tail != &efa_ep->xmit_more_wr_head) {
 		ret = efa_post_flush(efa_ep, &bad_wr);
 		if (OFI_UNLIKELY(ret))
-			goto tx_err;
+			efa_eq_write_error(&ep->util_ep, -ret, -ret);
 	}
 
 	return;
-rx_err:
-	if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-		assert(0 &&
-		       "error writing error cq entry when handling RX error");
-	return;
-tx_err:
-	if (rxr_cq_handle_tx_error(ep, tx_entry, ret))
-		assert(0 &&
-		       "error writing error cq entry when handling TX error");
-	return;
-
-read_err:
-	if (rxr_read_handle_error(ep, read_entry, ret))
-		assert(0 &&
-		       "error writing err cq entry while handling RDMA error");
-	return;
-
-handshake_err:
-	FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
-		"Failed to post HANDSHAKE to peer %ld: %s\n",
-		peer->efa_fiaddr, fi_strerror(-ret));
-	assert(0 && "Failed to post HANDSHAKE to peer");
-	efa_eq_write_error(&ep->util_ep, FI_EIO, -ret);
-	return;
 }
 
 void rxr_ep_progress(struct util_ep *util_ep)
@@ -1999,7 +2047,7 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	rxr_ep->rx_iov_limit = info->rx_attr->iov_limit;
 	rxr_ep->tx_iov_limit = info->tx_attr->iov_limit;
 	rxr_ep->inject_size = info->tx_attr->inject_size;
-	rxr_ep->max_outstanding_tx = rdm_info->tx_attr->size;
+	rxr_ep->efa_max_outstanding_tx_ops = rdm_info->tx_attr->size;
 	rxr_ep->core_rx_size = rdm_info->rx_attr->size;
 	rxr_ep->core_iov_limit = rdm_info->tx_attr->iov_limit;
 	rxr_ep->core_caps = rdm_info->caps;
@@ -2030,8 +2078,8 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	rxr_ep->min_multi_recv_size = rxr_ep->mtu_size - rxr_ep->max_proto_hdr_size;
 
 	if (rxr_env.tx_queue_size > 0 &&
-	    rxr_env.tx_queue_size < rxr_ep->max_outstanding_tx)
-		rxr_ep->max_outstanding_tx = rxr_env.tx_queue_size;
+	    rxr_env.tx_queue_size < rxr_ep->efa_max_outstanding_tx_ops)
+		rxr_ep->efa_max_outstanding_tx_ops = rxr_env.tx_queue_size;
 
 
 	rxr_ep->use_zcpy_rx = rxr_ep_use_zcpy_rx(rxr_ep, info);
@@ -2043,7 +2091,8 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 		rxr_ep->handle_resource_management);
 
 #if ENABLE_DEBUG
-	rxr_ep->sends = 0;
+	rxr_ep->efa_total_posted_tx_ops = 0;
+	rxr_ep->shm_total_posted_tx_ops = 0;
 	rxr_ep->send_comps = 0;
 	rxr_ep->failed_send_comps = 0;
 	rxr_ep->recv_comps = 0;
@@ -2053,7 +2102,8 @@ int rxr_endpoint(struct fid_domain *domain, struct fi_info *info,
 	rxr_ep->rx_bufs_shm_to_post = 0;
 	rxr_ep->posted_bufs_efa = 0;
 	rxr_ep->rx_bufs_efa_to_post = 0;
-	rxr_ep->tx_pending = 0;
+	rxr_ep->efa_outstanding_tx_ops = 0;
+	rxr_ep->shm_outstanding_tx_ops = 0;
 	rxr_ep->available_data_bufs_ts = 0;
 
 	ret = fi_cq_open(rxr_domain->rdm_domain, &cq_attr,
@@ -2129,3 +2179,120 @@ err_free_ep:
 	free(rxr_ep);
 	return ret;
 }
+
+/**
+ * @brief record the event that a TX op has been submitted
+ *
+ * This function is called after a TX operation has been posted
+ * successfully. It will:
+ *
+ *  1. increase the outstanding tx_op counter in endpoint and
+ *     in the peer structure.
+ *
+ *  2. add the TX packet to peer's outstanding TX packet list.
+ *
+ * Both send and read are considered TX operation.
+ *
+ * The tx_op counters used to prevent over posting the device
+ * and used in flow control. They are also usefull for debugging.
+ *
+ * Peer's outstanding TX packet list is used when removing a peer
+ * to invalidate address of these packets, so that the completion
+ * of these packet is ignored.
+ *
+ * @param[in,out]	ep		endpoint
+ * @param[in]		pkt_entry	TX pkt_entry, which contains
+ * 					the info of the TX op.
+ */
+void rxr_ep_record_tx_op_submitted(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rdm_peer *peer;
+
+	/*
+	 * peer can be NULL when the pkt_entry is a RMA_CONTEXT_PKT,
+	 * and the RMA is a local read toward the endpoint itself
+	 */
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (peer)
+		dlist_insert_tail(&pkt_entry->entry, &peer->outstanding_tx_pkts);
+
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL) {
+		ep->efa_outstanding_tx_ops++;
+		if (peer)
+			peer->efa_outstanding_tx_ops++;
+#if ENABLE_DEBUG
+		ep->efa_total_posted_tx_ops++;
+#endif
+	} else {
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_SHM_TX_POOL);
+		ep->shm_outstanding_tx_ops++;
+		if (peer)
+			peer->shm_outstanding_tx_ops++;
+#if ENABLE_DEBUG
+		ep->shm_total_posted_tx_ops++;
+#endif
+	}
+}
+
+/**
+ * @brief record the event that an TX op is completed
+ *
+ * This function is called when the completion of
+ * a TX operation is received. It will
+ *
+ * 1. decrease the outstanding tx_op counter in the endpoint
+ *    and in the peer.
+ *
+ * 2. remove the TX packet from peer's outstanding
+ *    TX packet list.
+ *
+ * Both send and read are considered TX operation.
+ *
+ * One may ask why this function is not integrated
+ * into rxr_pkt_entry_relase_tx()?
+ *
+ * The reason is the action of decrease tx_op counter
+ * is not tied to releasing a TX pkt_entry.
+ *
+ * Sometimes we need to decreate the tx_op counter
+ * without releasing a TX pkt_entry. For example,
+ * we handle a TX pkt_entry encountered RNR. We need
+ * to decrease the tx_op counter and queue the packet.
+ *
+ * Sometimes we need release TX pkt_entry without
+ * decreasing the tx_op counter. For example, when
+ * rxr_pkt_post_ctrl() failed to post a pkt entry.
+ *
+ * @param[in,out]	ep		endpoint
+ * @param[in]		pkt_entry	TX pkt_entry, which contains
+ * 					the info of the TX op
+ */
+void rxr_ep_record_tx_op_completed(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
+{
+	struct rdm_peer *peer;
+
+	/*
+	 * peer can be NULL when:
+	 *
+	 * 1. the pkt_entry is a RMA_CONTEXT_PKT, and the RMA op is a local read
+	 *    toward the endpoint itself.
+	 * 2. peer's address has been removed from address vector. Either because
+	 *    a new peer has the same GID+QPN was inserted to address, or because
+	 *    application removed the peer from address vector.
+	 */
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (peer)
+		dlist_remove(&pkt_entry->entry);
+
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL) {
+		ep->efa_outstanding_tx_ops--;
+		if (peer)
+			peer->efa_outstanding_tx_ops--;
+	} else {
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_SHM_TX_POOL);
+		ep->shm_outstanding_tx_ops--;
+		if (peer)
+			peer->shm_outstanding_tx_ops--;
+	}
+}
+
diff --git a/prov/efa/src/rxr/rxr_init.c b/prov/efa/src/rxr/rxr_init.c
index ff24efb..98ac4b2 100644
--- a/prov/efa/src/rxr/rxr_init.c
+++ b/prov/efa/src/rxr/rxr_init.c
@@ -68,20 +68,15 @@ struct rxr_env rxr_env = {
 	.rx_iov_limit = 0,
 	.rx_copy_unexp = 1,
 	.rx_copy_ooo = 1,
-	.max_timeout = RXR_DEF_RNR_MAX_TIMEOUT,
-	.timeout_interval = 0, /* 0 is random timeout */
+	.rnr_backoff_wait_time_cap = RXR_DEFAULT_RNR_BACKOFF_WAIT_TIME_CAP,
+	.rnr_backoff_initial_wait_time = 0, /* 0 is random wait time  */
 	.efa_cq_read_size = 50,
 	.shm_cq_read_size = 50,
 	.efa_max_medium_msg_size = 65536,
 	.efa_min_read_msg_size = 1048576,
 	.efa_min_read_write_size = 65536,
 	.efa_read_segment_size = 1073741824,
-	/*
-	 * There's still hanging issue when disabling RNR firmware infinite
-	 * retry. Just reset RNR firmware retry to EFA_RNR_INFINITE_RETRY
-	 * to be safe, while we are working on a fix.
-	 */
-	.rnr_retry = EFA_RNR_INFINITE_RETRY,
+	.rnr_retry = 3, /* Setting this value to EFA_RNR_INFINITE_RETRY makes the firmware retry indefinitey */
 };
 
 /* @brief Read and store the FI_EFA_* environment variables.
@@ -121,9 +116,13 @@ static void rxr_init_env(void)
 			  &rxr_env.rx_copy_unexp);
 	fi_param_get_bool(&rxr_prov, "rx_copy_ooo",
 			  &rxr_env.rx_copy_ooo);
-	fi_param_get_int(&rxr_prov, "max_timeout", &rxr_env.max_timeout);
+
+	fi_param_get_int(&rxr_prov, "max_timeout", &rxr_env.rnr_backoff_wait_time_cap);
+	if (rxr_env.rnr_backoff_wait_time_cap > RXR_MAX_RNR_BACKOFF_WAIT_TIME_CAP)
+		rxr_env.rnr_backoff_wait_time_cap = RXR_MAX_RNR_BACKOFF_WAIT_TIME_CAP;
+
 	fi_param_get_int(&rxr_prov, "timeout_interval",
-			 &rxr_env.timeout_interval);
+			 &rxr_env.rnr_backoff_initial_wait_time);
 	fi_param_get_size_t(&rxr_prov, "efa_cq_read_size",
 			 &rxr_env.efa_cq_read_size);
 	fi_param_get_size_t(&rxr_prov, "shm_cq_read_size",
@@ -136,10 +135,27 @@ static void rxr_init_env(void)
 			    &rxr_env.efa_min_read_write_size);
 	fi_param_get_size_t(&rxr_prov, "inter_read_segment_size",
 			    &rxr_env.efa_read_segment_size);
-	fi_param_get_bool(&rxr_prov, "fork_safe", &fork_safe);
 
-	if (fork_safe || getenv("RDMAV_FORK_SAFE") || getenv("IBV_FORK_SAFE"))
-		efa_fork_status = EFA_FORK_SUPPORT_ON;
+	/* Initialize EFA's fork support flag based on the environment and
+	 * system support. */
+	efa_fork_status = EFA_FORK_SUPPORT_OFF;
+
+#if HAVE_IBV_IS_FORK_INITIALIZED == 1
+	if (ibv_is_fork_initialized() == IBV_FORK_UNNEEDED)
+		efa_fork_status = EFA_FORK_SUPPORT_UNNEEDED;
+#endif
+
+	if (efa_fork_status != EFA_FORK_SUPPORT_UNNEEDED) {
+		fi_param_get_bool(&rxr_prov, "fork_safe", &fork_safe);
+
+		/*
+		 * Check if any environment variables which would trigger
+		 * libibverbs' fork support are set. These variables are
+		 * defined by ibv_fork_init(3).
+		 */
+		if (fork_safe || getenv("RDMAV_FORK_SAFE") || getenv("IBV_FORK_SAFE"))
+			efa_fork_status = EFA_FORK_SUPPORT_ON;
+	}
 }
 
 /*
@@ -823,7 +839,8 @@ EFA_INI
 	fi_param_define(&rxr_prov, "inter_read_segment_size", FI_PARAM_INT,
 			"Calls to RDMA read is segmented using this value.");
 	fi_param_define(&rxr_prov, "fork_safe", FI_PARAM_BOOL,
-			"Enables fork support and disables internal usage of huge pages. (Default: false)");
+			"Enables fork support and disables internal usage of huge pages. Has no effect on kernels which set copy-on-fork for registered pages, generally 5.13 and later. (Default: false)");
+
 	rxr_init_env();
 
 #if HAVE_EFA_DL
diff --git a/prov/efa/src/rxr/rxr_pkt_cmd.c b/prov/efa/src/rxr/rxr_pkt_cmd.c
index 3dd4631..c543018 100644
--- a/prov/efa/src/rxr/rxr_pkt_cmd.c
+++ b/prov/efa/src/rxr/rxr_pkt_cmd.c
@@ -58,11 +58,11 @@ ssize_t rxr_pkt_post_data(struct rxr_ep *rxr_ep,
 	struct rxr_data_pkt *data_pkt;
 	ssize_t ret;
 
-	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_efa_pool);
+	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 	if (OFI_UNLIKELY(!pkt_entry)) {
 		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
-		       "TX packets exhausted, current packets in flight %lu",
-		       rxr_ep->tx_pending);
+		       "TX packets exhausted, current tx ops in flight %lu",
+		       rxr_ep->efa_outstanding_tx_ops);
 		return -FI_EAGAIN;
 	}
 
@@ -305,9 +305,9 @@ ssize_t rxr_pkt_post_ctrl_once(struct rxr_ep *rxr_ep, int entry_type, void *x_en
 	assert(peer);
 	if (peer->is_local) {
 		assert(rxr_ep->use_shm);
-		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_shm_pool);
+		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 	} else {
-		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_efa_pool);
+		pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 	}
 
 	if (!pkt_entry)
@@ -504,7 +504,7 @@ ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
 	tx_entry->addr = addr;
 	tx_entry->peer = rxr_ep_get_peer(ep, tx_entry->addr);
 	assert(tx_entry->peer);
-	ofi_atomic_inc32(&tx_entry->peer->use_cnt);
+	dlist_insert_tail(&tx_entry->peer_entry, &tx_entry->peer->tx_entry_list);
 	tx_entry->msg_id = -1;
 	tx_entry->cq_entry.flags = FI_RMA | FI_WRITE;
 	tx_entry->cq_entry.buf = NULL;
@@ -525,9 +525,7 @@ ssize_t rxr_pkt_trigger_handshake(struct rxr_ep *ep,
 	tx_entry->iov_offset = 0;
 	tx_entry->fi_flags = RXR_NO_COMPLETION | RXR_NO_COUNTER;
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &ep->tx_entry_list);
 
 	err = rxr_pkt_post_ctrl(ep, RXR_TX_ENTRY, tx_entry, RXR_EAGER_RTW_PKT, 0);
 
@@ -671,13 +669,184 @@ void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 	}
 }
 
-/*
- *   Functions used to handle packet send completion
+/**
+ * @brief handle the a packet that encountered error completion while sending
+ *
+ * Depend on the packet type and error type, the error are handled differently.
+ *
+ * If the packet is associated with an user initialized TX operation:
+ * (TX means send,read or write; such packets include all REQ packets and DATA):
+ *
+ *    If the error is Receiver Not Ready (RNR). there are two cases:
+ *
+ *         If user wants to manager RNR by itself (FI_RM_DISABLED),
+ *         an error CQ entry will be written.
+ *
+ *         Otherwise, the packet will be queued and resnt by progress engine.
+ *
+ *    For other type of error, an error CQ entry is written.
+ *
+ * If the packet is associated with an user initialized recv operiaton,
+ * (such packets include EOR, CTS):
+ *
+ *      If the error is RNR, the packet is queued and resent by progress
+ *      engine. No CQ entry is written.
+ *
+ *      For other types of error, an error CQ entry is written.
+ *
+ * If the packet is not associated with an user operation (such packet include
+ * HANDSHAKE):
+ *
+ *      If the error is RNR, the packet is queued and resent by progress engine.
+ *
+ *      For othre types of error, an error EQ entry is written.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	pkt_entry	pkt entry
+ * @param[in]	err		libfabric error code
+ * @param[in]	prov_errno	provider specific error code
  */
+void rxr_pkt_handle_send_error(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry, int err, int prov_errno)
+{
+	struct rdm_peer *peer;
+	struct rxr_tx_entry *tx_entry;
+	struct rxr_rx_entry *rx_entry;
+
+	assert(pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL ||
+	       pkt_entry->alloc_type == RXR_PKT_FROM_SHM_TX_POOL);
+
+	rxr_ep_record_tx_op_completed(ep, pkt_entry);
+
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (!peer) {
+		/*
+		 * If peer is NULL, it means the peer has been removed from AV.
+		 * In this case, ignore this error completion.
+		 */
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "ignoring send error completion of a packet to a removed peer.\n");
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
+	if (!pkt_entry->x_entry) {
+		/* only handshake packet is not associated with any TX/RX operation */
+		assert(rxr_get_base_hdr(pkt_entry->pkt)->type == RXR_HANDSHAKE_PKT);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR) {
+			/*
+			 * handshake should always be queued for RNR
+			 */
+			assert(!(peer->flags & RXR_PEER_HANDSHAKE_QUEUED));
+			peer->flags |= RXR_PEER_HANDSHAKE_QUEUED;
+			dlist_insert_tail(&peer->handshake_queued_entry,
+					  &ep->handshake_queued_peer_list);
+		} else if (prov_errno != IBV_WC_REM_INV_RD_REQ_ERR) {
+			/* If prov_errno is IBV_WC_REM_INV_RD_REQ_ERR, the peer has been destroyed.
+			 * Which is normal, as peer does not always need a handshake packet perform
+			 * its duty. (For example, if a peer just want to sent 1 message to the ep, it
+			 * does not need handshake.)
+			 * In this case, it is safe to ignore this error completion.
+			 * In all other cases, we write an eq entry because there is no application
+			 * operation associated with handshake.
+			 */
+			efa_eq_write_error(&ep->util_ep, err, prov_errno);
+		}
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
+		tx_entry = pkt_entry->x_entry;
+		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR &&
+		    ep->handle_resource_management == FI_RM_ENABLED) {
+			/*
+			 * This packet is assoiciated with a send operation,
+			 * (such packets include all REQ, DATA)
+			 * thus shoud be queued for RNR only if
+			 * application want EFA to manager resource.
+			 */
+			rxr_cq_queue_rnr_pkt(ep, &tx_entry->queued_pkts, pkt_entry);
+			if (tx_entry->state == RXR_TX_SEND) {
+				dlist_remove(&tx_entry->entry);
+				tx_entry->state = RXR_TX_QUEUED_DATA_RNR;
+				dlist_insert_tail(&tx_entry->queued_entry,
+						  &ep->tx_entry_queued_list);
+			} else if (tx_entry->state == RXR_TX_REQ) {
+				tx_entry->state = RXR_TX_QUEUED_REQ_RNR;
+				dlist_insert_tail(&tx_entry->queued_entry,
+						  &ep->tx_entry_queued_list);
+			}
+		} else {
+			rxr_cq_write_tx_error(ep, pkt_entry->x_entry, err, prov_errno);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		}
+
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_RX_ENTRY) {
+		rx_entry = pkt_entry->x_entry;
+		if (prov_errno == IBV_WC_RNR_RETRY_EXC_ERR) {
+			/*
+			 * This packet is associated with a recv operation,
+			 * (such packets include CTS and EOR)
+			 * thus should always be queued for RNR.
+			 * This is regardless value of ep->handle_resource_management,
+			 * because resource management is only applied to send operation.
+			 */
+			rxr_cq_queue_rnr_pkt(ep, &rx_entry->queued_pkts, pkt_entry);
+			/*
+			 * rx_entry send one ctrl packet at a time, so if we
+			 * received RNR for the packet, the rx_entry must not
+			 * be in ep's rx_queued_entry_list, thus cannot
+			 * be in QUEUED_CTRL state
+			 */
+			assert(rx_entry->state != RXR_RX_QUEUED_CTRL);
+			rx_entry->state = RXR_RX_QUEUED_CTRL;
+			dlist_insert_tail(&rx_entry->queued_entry,
+					  &ep->rx_entry_queued_list);
+
+		} else {
+			rxr_cq_write_rx_error(ep, pkt_entry->x_entry, err, prov_errno);
+			rxr_pkt_entry_release_tx(ep, pkt_entry);
+		}
+
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_READ_ENTRY) {
+		/* read will not encounter RNR */
+		assert(prov_errno != IBV_WC_RNR_RETRY_EXC_ERR);
+		rxr_read_write_error(ep, pkt_entry->x_entry, err, prov_errno);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
+	FI_WARN(&rxr_prov, FI_LOG_CQ,
+		"%s unknown x_entry type %d\n",
+		__func__, RXR_GET_X_ENTRY_TYPE(pkt_entry));
+	assert(0 && "unknown x_entry state");
+	efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	rxr_pkt_entry_release_tx(ep, pkt_entry);
+}
+
 void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 {
 	struct rdm_peer *peer;
 
+	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
+	if (!peer) {
+		/*
+		 * peer could be NULL in the following 2 scenarios:
+		 * 1. a new peer with same gid+qpn was inserted to av, thus the peer was remove.
+		 * 2. application removed the peer's address from av.
+		 * Either way, we need to ignore this error completion.
+		 */
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "ignoring send completion of a packet to a removed peer.\n");
+		rxr_ep_record_tx_op_completed(ep, pkt_entry);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
 	switch (rxr_get_base_hdr(pkt_entry->pkt)->type) {
 	case RXR_HANDSHAKE_PKT:
 		break;
@@ -768,20 +937,47 @@ void rxr_pkt_handle_send_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt
 			"invalid control pkt type %d\n",
 			rxr_get_base_hdr(pkt_entry->pkt)->type);
 		assert(0 && "invalid control pkt type");
-		rxr_cq_handle_error(ep, FI_EIO, NULL);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
 		return;
 	}
 
-	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-	assert(peer);
-	if (!peer->is_local)
-		rxr_ep_dec_tx_pending(ep, peer, 0);
+	rxr_ep_record_tx_op_completed(ep, pkt_entry);
 	rxr_pkt_entry_release_tx(ep, pkt_entry);
 }
 
-/*
- *  Functions used to handle packet receive completion
+/**
+ * @brief handle the a packet that encountered error completion while receiving
+ *
+ * This function will write error cq or eq entry, then release the packet entry.
+ *
+ * @param[in]	ep		endpoint
+ * @param[in]	pkt_entry	pkt entry
+ * @param[in]	err		libfabric error code
+ * @param[in]	prov_errno	provider specific error code
  */
+void rxr_pkt_handle_recv_error(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry, int err, int prov_errno)
+{
+	if (!pkt_entry->x_entry) {
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+		rxr_pkt_entry_release_tx(ep, pkt_entry);
+		return;
+	}
+
+	if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_TX_ENTRY) {
+		rxr_cq_write_tx_error(ep, pkt_entry->x_entry, err, prov_errno);
+	} else if (RXR_GET_X_ENTRY_TYPE(pkt_entry) == RXR_RX_ENTRY) {
+		rxr_cq_write_rx_error(ep, pkt_entry->x_entry, err, prov_errno);
+	} else {
+		FI_WARN(&rxr_prov, FI_LOG_CQ,
+		"%s unknown x_entry type %d\n",
+			__func__, RXR_GET_X_ENTRY_TYPE(pkt_entry));
+		assert(0 && "unknown x_entry state");
+		efa_eq_write_error(&ep->util_ep, err, prov_errno);
+	}
+
+	rxr_pkt_entry_release_rx(ep, pkt_entry);
+}
+
 static
 fi_addr_t rxr_pkt_insert_addr(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry, void *raw_addr)
 {
@@ -838,14 +1034,14 @@ void rxr_pkt_proc_received(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Received a RTS packet, which has been retired since protocol version 4\n");
 		assert(0 && "deprecated RTS pakcet received");
-		rxr_cq_handle_error(ep, FI_EIO, NULL);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	case RXR_RETIRED_CONNACK_PKT:
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Received a CONNACK packet, which has been retired since protocol version 4\n");
 		assert(0 && "deprecated CONNACK pakcet received");
-		rxr_cq_handle_error(ep, FI_EIO, NULL);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	case RXR_EOR_PKT:
@@ -911,7 +1107,7 @@ void rxr_pkt_proc_received(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 			"invalid control pkt type %d\n",
 			rxr_get_base_hdr(pkt_entry->pkt)->type);
 		assert(0 && "invalid control pkt type");
-		rxr_cq_handle_error(ep, FI_EIO, NULL);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
@@ -933,22 +1129,41 @@ void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 			(int)pkt_entry->addr, base_hdr->type);
 
 		assert(0 && "invalid REQ packe type");
-		rxr_cq_handle_error(ep, FI_EIO, NULL);
+		efa_eq_write_error(&ep->util_ep, FI_EIO, FI_EIO);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
 		return;
 	}
 
-	if (pkt_type >= RXR_REQ_PKT_BEGIN) {
+	if (pkt_type >= RXR_REQ_PKT_BEGIN && rxr_pkt_req_raw_addr(pkt_entry)) {
 		/*
-		 * as long as the REQ packet contain raw address
-		 * we will need to call insert because it might be a new
-		 * EP with new Q-Key.
+		 * A REQ packet with raw address in its header could always
+		 * be the 1st packet we receive from a peer, even if we already
+		 * have the address in AV.
+		 *
+		 * This is because the peer might be a newly created one,
+		 * with the same GID+QPN as an old peer (though a different Q-Key),
+		 * therefore lower provider thinks it is the older peer.
+		 *
+		 * Therefore, we alwyas need to call rxr_pkt_insert_addr() for
+		 * such a packet. rxr_pkt_insert_addr() will insert the address
+		 * to AV if it is indeed new.
 		 */
 		void *raw_addr;
 
 		raw_addr = rxr_pkt_req_raw_addr(pkt_entry);
-		if (OFI_UNLIKELY(raw_addr != NULL))
-			pkt_entry->addr = rxr_pkt_insert_addr(ep, pkt_entry, raw_addr);
+		assert(raw_addr);
+		pkt_entry->addr = rxr_pkt_insert_addr(ep, pkt_entry, raw_addr);
+	} else if (pkt_entry->addr == FI_ADDR_NOTAVAIL) {
+		/*
+		 * Receiving a non-REQ packet or a REQ packet without raw address means
+		 * we had prior communication with the peer. For such a packet,
+		 * the only possiblity for its pkt_entry->addr to be FI_ADDR_NOTAVAIL
+		 * is application called fi_av_remove() to remove the address
+		 * from AV. In this case, this packet should be ignored.
+		 */
+		FI_WARN(&rxr_prov, FI_LOG_CQ, "Warning: ignoring a received packet from a removed address\n");
+		rxr_pkt_entry_release_rx(ep, pkt_entry);
+		return;
 	}
 
 	assert(pkt_entry->addr != FI_ADDR_NOTAVAIL);
@@ -973,7 +1188,7 @@ void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 		ep->posted_bufs_efa--;
 	}
 
-	if (pkt_entry->type == RXR_PKT_ENTRY_USER) {
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_USER_BUFFER) {
 		assert(pkt_entry->x_entry);
 		zcpy_rx_entry = pkt_entry->x_entry;
 	}
diff --git a/prov/efa/src/rxr/rxr_pkt_cmd.h b/prov/efa/src/rxr/rxr_pkt_cmd.h
index 935287b..66ebb9a 100644
--- a/prov/efa/src/rxr/rxr_pkt_cmd.h
+++ b/prov/efa/src/rxr/rxr_pkt_cmd.h
@@ -56,9 +56,17 @@ void rxr_pkt_handle_data_copied(struct rxr_ep *ep,
 				struct rxr_pkt_entry *pkt_entry,
 				size_t data_size);
 
+void rxr_pkt_handle_send_error(struct rxr_ep *ep,
+			       struct rxr_pkt_entry *pkt_entry,
+			       int err, int prov_errno);
+
 void rxr_pkt_handle_send_completion(struct rxr_ep *ep,
 				    struct rxr_pkt_entry *pkt_entry);
 
+void rxr_pkt_handle_recv_error(struct rxr_ep *ep,
+			       struct rxr_pkt_entry *pkt_entry,
+			       int err, int prov_errno);
+
 void rxr_pkt_handle_recv_completion(struct rxr_ep *ep,
 				    struct rxr_pkt_entry *pkt_entry);
 
diff --git a/prov/efa/src/rxr/rxr_pkt_entry.c b/prov/efa/src/rxr/rxr_pkt_entry.c
index b126b90..998ed87 100644
--- a/prov/efa/src/rxr/rxr_pkt_entry.c
+++ b/prov/efa/src/rxr/rxr_pkt_entry.c
@@ -48,7 +48,8 @@
  *   General purpose utility functions
  */
 struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
-					  struct ofi_bufpool *pkt_pool)
+					  struct ofi_bufpool *pkt_pool,
+					  enum rxr_pkt_entry_alloc_type alloc_type)
 {
 	struct rxr_pkt_entry *pkt_entry;
 	void *mr = NULL;
@@ -68,10 +69,10 @@ struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
 #ifdef ENABLE_EFA_POISONING
 	memset(pkt_entry->pkt, 0, ep->mtu_size);
 #endif
-	pkt_entry->type = RXR_PKT_ENTRY_POSTED;
+	pkt_entry->alloc_type = alloc_type;
 	pkt_entry->state = RXR_PKT_ENTRY_IN_USE;
 	pkt_entry->next = NULL;
-
+	pkt_entry->x_entry = NULL;
 	return pkt_entry;
 }
 
@@ -97,11 +98,11 @@ void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
 		peer = rxr_ep_get_peer(ep, pkt->addr);
 		assert(peer);
 		peer->rnr_queued_pkt_cnt--;
-		peer->timeout_interval = 0;
-		peer->rnr_timeout_exp = 0;
-		if (peer->flags & RXR_PEER_IN_BACKOFF)
-			dlist_remove(&peer->rnr_entry);
-		peer->flags &= ~RXR_PEER_IN_BACKOFF;
+		peer->rnr_backoff_wait_time = 0;
+		if (peer->flags & RXR_PEER_IN_BACKOFF) {
+			dlist_remove(&peer->rnr_backoff_entry);
+			peer->flags &= ~RXR_PEER_IN_BACKOFF;
+		}
 		FI_DBG(&rxr_prov, FI_LOG_EP_DATA,
 		       "reset backoff timer for peer: %" PRIu64 "\n",
 		       pkt->addr);
@@ -132,22 +133,14 @@ void rxr_pkt_entry_release_rx(struct rxr_ep *ep,
 {
 	assert(pkt_entry->next == NULL);
 
-	if (ep->use_zcpy_rx && pkt_entry->type == RXR_PKT_ENTRY_USER)
+	if (ep->use_zcpy_rx && pkt_entry->alloc_type == RXR_PKT_FROM_USER_BUFFER)
 		return;
 
-	if (pkt_entry->type == RXR_PKT_ENTRY_POSTED) {
-		struct rdm_peer *peer;
-
-		peer = rxr_ep_get_peer(ep, pkt_entry->addr);
-		assert(peer);
-
-		if (peer->is_local)
-			ep->rx_bufs_shm_to_post++;
-		else
-			ep->rx_bufs_efa_to_post++;
-	}
-
-	if (pkt_entry->type == RXR_PKT_ENTRY_READ_COPY) {
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_RX_POOL) {
+		ep->rx_bufs_efa_to_post++;
+	} else if (pkt_entry->alloc_type == RXR_PKT_FROM_SHM_RX_POOL) {
+		ep->rx_bufs_shm_to_post++;
+	} else if (pkt_entry->alloc_type == RXR_PKT_FROM_READ_COPY_POOL) {
 		assert(ep->rx_readcopy_pkt_pool_used > 0);
 		ep->rx_readcopy_pkt_pool_used--;
 	}
@@ -165,12 +158,11 @@ void rxr_pkt_entry_release_rx(struct rxr_ep *ep,
 
 void rxr_pkt_entry_copy(struct rxr_ep *ep,
 			struct rxr_pkt_entry *dest,
-			struct rxr_pkt_entry *src,
-			int new_entry_type)
+			struct rxr_pkt_entry *src)
 {
 	FI_DBG(&rxr_prov, FI_LOG_EP_CTRL,
-	       "Copying packet out of posted buffer! src_entry_type: %d new_entry_type: %d\n",
-		src->type, new_entry_type);
+	       "Copying packet out of posted buffer! src_entry_alloc_type: %d desc_entry_alloc_type: %d\n",
+		src->alloc_type, dest->alloc_type);
 	dlist_init(&dest->entry);
 #if ENABLE_DEBUG
 	dlist_init(&dest->dbg_entry);
@@ -182,7 +174,6 @@ void rxr_pkt_entry_copy(struct rxr_ep *ep,
 	dest->x_entry = src->x_entry;
 	dest->pkt_size = src->pkt_size;
 	dest->addr = src->addr;
-	dest->type = new_entry_type;
 	dest->state = RXR_PKT_ENTRY_IN_USE;
 	dest->next = NULL;
 	memcpy(dest->pkt, src->pkt, ep->mtu_size);
@@ -197,8 +188,8 @@ struct rxr_pkt_entry *rxr_pkt_get_unexp(struct rxr_ep *ep,
 {
 	struct rxr_pkt_entry *unexp_pkt_entry;
 
-	if (rxr_env.rx_copy_unexp && (*pkt_entry_ptr)->type == RXR_PKT_ENTRY_POSTED) {
-		unexp_pkt_entry = rxr_pkt_entry_clone(ep, ep->rx_unexp_pkt_pool, *pkt_entry_ptr, RXR_PKT_ENTRY_UNEXP);
+	if (rxr_env.rx_copy_unexp && (*pkt_entry_ptr)->alloc_type == RXR_PKT_FROM_EFA_RX_POOL) {
+		unexp_pkt_entry = rxr_pkt_entry_clone(ep, ep->rx_unexp_pkt_pool, RXR_PKT_FROM_UNEXP_POOL, *pkt_entry_ptr);
 		if (OFI_UNLIKELY(!unexp_pkt_entry)) {
 			FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 				"Unable to allocate rx_pkt_entry for unexp msg\n");
@@ -218,8 +209,8 @@ void rxr_pkt_entry_release_cloned(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_e
 	struct rxr_pkt_entry *next;
 
 	while (pkt_entry) {
-		assert(pkt_entry->type == RXR_PKT_ENTRY_OOO  ||
-		       pkt_entry->type == RXR_PKT_ENTRY_UNEXP);
+		assert(pkt_entry->alloc_type == RXR_PKT_FROM_OOO_POOL ||
+		       pkt_entry->alloc_type == RXR_PKT_FROM_UNEXP_POOL);
 #ifdef ENABLE_EFA_POISONING
 		rxr_poison_mem_region((uint32_t *)pkt_entry, ep->tx_pkt_pool_entry_sz);
 #endif
@@ -232,38 +223,38 @@ void rxr_pkt_entry_release_cloned(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_e
 
 struct rxr_pkt_entry *rxr_pkt_entry_clone(struct rxr_ep *ep,
 					  struct ofi_bufpool *pkt_pool,
-					  struct rxr_pkt_entry *src,
-					  int new_entry_type)
+					  enum rxr_pkt_entry_alloc_type alloc_type,
+					  struct rxr_pkt_entry *src)
 {
 	struct rxr_pkt_entry *root = NULL;
 	struct rxr_pkt_entry *dst;
 
 	assert(src);
-	assert(new_entry_type == RXR_PKT_ENTRY_OOO ||
-	       new_entry_type == RXR_PKT_ENTRY_UNEXP ||
-	       new_entry_type == RXR_PKT_ENTRY_READ_COPY);
+	assert(alloc_type == RXR_PKT_FROM_OOO_POOL ||
+	       alloc_type == RXR_PKT_FROM_UNEXP_POOL ||
+	       alloc_type == RXR_PKT_FROM_READ_COPY_POOL);
 
-	dst = rxr_pkt_entry_alloc(ep, pkt_pool);
+	dst = rxr_pkt_entry_alloc(ep, pkt_pool, alloc_type);
 	if (!dst)
 		return NULL;
 
-	if (new_entry_type == RXR_PKT_ENTRY_READ_COPY) {
+	if (alloc_type == RXR_PKT_FROM_READ_COPY_POOL) {
 		assert(pkt_pool == ep->rx_readcopy_pkt_pool);
 		ep->rx_readcopy_pkt_pool_used++;
 		ep->rx_readcopy_pkt_pool_max_used = MAX(ep->rx_readcopy_pkt_pool_used,
 							ep->rx_readcopy_pkt_pool_max_used);
 	}
 
-	rxr_pkt_entry_copy(ep, dst, src, new_entry_type);
+	rxr_pkt_entry_copy(ep, dst, src);
 	root = dst;
 	while (src->next) {
-		dst->next = rxr_pkt_entry_alloc(ep, pkt_pool);
+		dst->next = rxr_pkt_entry_alloc(ep, pkt_pool, alloc_type);
 		if (!dst->next) {
 			rxr_pkt_entry_release_cloned(ep, root);
 			return NULL;
 		}
 
-		rxr_pkt_entry_copy(ep, dst->next, src->next, new_entry_type);
+		rxr_pkt_entry_copy(ep, dst->next, src->next);
 		src = src->next;
 		dst = dst->next;
 	}
@@ -301,9 +292,8 @@ ssize_t rxr_pkt_entry_sendmsg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry
 	struct rdm_peer *peer;
 	size_t ret;
 
-	assert(ep->tx_pending <= ep->max_outstanding_tx);
-
-	if (ep->tx_pending == ep->max_outstanding_tx)
+	if (pkt_entry->alloc_type == RXR_PKT_FROM_EFA_TX_POOL &&
+	    ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 		return -FI_EAGAIN;
 
 	peer = rxr_ep_get_peer(ep, pkt_entry->addr);
@@ -323,11 +313,13 @@ ssize_t rxr_pkt_entry_sendmsg(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry
 		ret = fi_sendmsg(ep->shm_ep, msg, flags);
 	} else {
 		ret = fi_sendmsg(ep->rdm_ep, msg, flags);
-		if (OFI_LIKELY(!ret))
-			rxr_ep_inc_tx_pending(ep, peer);
 	}
 
-	return ret;
+	if (OFI_UNLIKELY(ret))
+		return ret;
+
+	rxr_ep_record_tx_op_submitted(ep, pkt_entry);
+	return 0;
 }
 
 /**
diff --git a/prov/efa/src/rxr/rxr_pkt_entry.h b/prov/efa/src/rxr/rxr_pkt_entry.h
index 7262022..a21e32b 100644
--- a/prov/efa/src/rxr/rxr_pkt_entry.h
+++ b/prov/efa/src/rxr/rxr_pkt_entry.h
@@ -43,13 +43,16 @@ enum rxr_pkt_entry_state {
 	RXR_PKT_ENTRY_RNR_RETRANSMIT,
 };
 
-/* pkt_entry types for rx pkts */
-enum rxr_pkt_entry_type {
-	RXR_PKT_ENTRY_POSTED = 1,   /* entries that are posted to the device from the RX bufpool */
-	RXR_PKT_ENTRY_UNEXP,        /* entries used to stage unexpected msgs */
-	RXR_PKT_ENTRY_OOO,	    /* entries used to stage out-of-order RTM or RTA */
-	RXR_PKT_ENTRY_USER,	    /* entries backed by user-provided msg prefix (FI_MSG_PREFIX)*/
-	RXR_PKT_ENTRY_READ_COPY,    /* entries used to stage copy by read */
+/* pkt_entry_alloc_type indicate where the packet entry is allocated from */
+enum rxr_pkt_entry_alloc_type {
+	RXR_PKT_FROM_EFA_TX_POOL = 1, /* packet is allcoated from ep->efa_tx_pkt_pool */
+	RXR_PKT_FROM_EFA_RX_POOL,     /* packet is allocated from ep->efa_rx_pkt_pool */
+	RXR_PKT_FROM_SHM_TX_POOL,     /* packet is allocated from ep->shm_tx_pkt_pool */
+	RXR_PKT_FROM_SHM_RX_POOL,     /* packet is allocated from ep->shm_rx_pkt_pool */
+	RXR_PKT_FROM_UNEXP_POOL,      /* packet is allocated from ep->rx_unexp_pkt_pool */
+	RXR_PKT_FROM_OOO_POOL,	      /* packet is allocated from ep->rx_ooo_pkt_pool */
+	RXR_PKT_FROM_USER_BUFFER,     /* packet is from user proivded buffer */
+	RXR_PKT_FROM_READ_COPY_POOL,  /* packet is allocated from ep->rx_readcopy_pkt_pool */
 };
 
 struct rxr_pkt_sendv {
@@ -64,8 +67,13 @@ struct rxr_pkt_sendv {
 	void *desc[2];
 };
 
+/* rxr_pkt_entry is used both for sending data to a peer and for receiving dat from a peer.
+ */
 struct rxr_pkt_entry {
-	/* for rx/tx_entry queued_pkts list */
+	/* entry is used for sending only.
+	 * It is either linked peer->outstanding_tx_pkts (after a packet has been successfully sent, but it get a completion),
+	 * or linked to tx_rx_entry->queued_pkts (after it encountered RNR error completion).
+	 */
 	struct dlist_entry entry;
 #if ENABLE_DEBUG
 	/* for tx/rx debug list or posted buf list */
@@ -75,8 +83,26 @@ struct rxr_pkt_entry {
 	size_t pkt_size;
 
 	struct fid_mr *mr;
+	/* `addr` is used for both sending data and receiving data.
+	 *
+	 * When sending a packet, `addr` will be provided by application and it cannot be FI_ADDR_NOTAVAIL.
+	 * However, after a packet is sent, application can remove a peer by calling fi_av_remove().
+	 * When removing the peering, `addr` will be set to FI_ADDR_NOTAVAIL. Later, when device report
+	 * completion for such a TX packet, the TX completion will be ignored.
+	 *
+	 * When receiving a packet, lower device will set `addr`. If the sender's address is not in
+	 * address vector (AV), `lower device will set `addr` to FI_ADDR_NOTAVAIL. This can happen in
+	 * two scenarios:
+	 *
+	 * 1. there has been no prior communication with the peer. In this case, the packet should have
+	 *    peer's raw address in the header, and progress engien will insert the raw address into
+	 *    addres vector, and update `addr`.
+	 *
+	 * 2. this packet is from a peer whose address has been removed from AV. In this case, the
+	 *    recived packet will be ignored because all resources associated with peer has been released.
+	 */
 	fi_addr_t addr;
-	enum rxr_pkt_entry_type type;
+	enum rxr_pkt_entry_alloc_type alloc_type; /* where the memory of this packet entry reside */
 	enum rxr_pkt_entry_state state;
 
 	/*
@@ -120,7 +146,8 @@ struct rxr_pkt_entry *rxr_pkt_entry_init_prefix(struct rxr_ep *ep,
 						struct ofi_bufpool *pkt_pool);
 
 struct rxr_pkt_entry *rxr_pkt_entry_alloc(struct rxr_ep *ep,
-					  struct ofi_bufpool *pkt_pool);
+					  struct ofi_bufpool *pkt_pool,
+					  enum rxr_pkt_entry_alloc_type alloc_type);
 
 void rxr_pkt_entry_release_tx(struct rxr_ep *ep,
 			      struct rxr_pkt_entry *pkt_entry);
@@ -133,8 +160,8 @@ void rxr_pkt_entry_append(struct rxr_pkt_entry *dst,
 
 struct rxr_pkt_entry *rxr_pkt_entry_clone(struct rxr_ep *ep,
 					  struct ofi_bufpool *pkt_pool,
-					  struct rxr_pkt_entry *src,
-					  int new_entry_type);
+					  enum rxr_pkt_entry_alloc_type alloc_type,
+					  struct rxr_pkt_entry *src);
 
 struct rxr_pkt_entry *rxr_pkt_get_unexp(struct rxr_ep *ep,
 					struct rxr_pkt_entry **pkt_entry_ptr);
diff --git a/prov/efa/src/rxr/rxr_pkt_type_data.c b/prov/efa/src/rxr/rxr_pkt_type_data.c
index c7a6b2b..847676d 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_data.c
+++ b/prov/efa/src/rxr/rxr_pkt_type_data.c
@@ -317,7 +317,7 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 				 pkt_entry, data, seg_size);
 	if (err) {
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
-		rxr_cq_handle_rx_error(ep, rx_entry, err);
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 	}
 
 	if (all_received)
@@ -328,7 +328,7 @@ void rxr_pkt_proc_data(struct rxr_ep *ep,
 		err = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
 		if (err) {
 			FI_WARN(&rxr_prov, FI_LOG_CQ, "post CTS packet failed!\n");
-			rxr_cq_handle_rx_error(ep, rx_entry, err);
+			rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 		}
 	}
 }
diff --git a/prov/efa/src/rxr/rxr_pkt_type_misc.c b/prov/efa/src/rxr/rxr_pkt_type_misc.c
index c145d5f..b0c1aac 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_misc.c
+++ b/prov/efa/src/rxr/rxr_pkt_type_misc.c
@@ -81,9 +81,9 @@ ssize_t rxr_pkt_post_handshake(struct rxr_ep *ep, struct rdm_peer *peer)
 
 	addr = peer->efa_fiaddr;
 	if (peer->is_local)
-		pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_shm_pool);
+		pkt_entry = rxr_pkt_entry_alloc(ep, ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 	else
-		pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_efa_pool);
+		pkt_entry = rxr_pkt_entry_alloc(ep, ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 	if (OFI_UNLIKELY(!pkt_entry))
 		return -FI_EAGAIN;
 
@@ -420,8 +420,6 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 	struct rxr_pkt_entry *pkt_entry;
 	struct rxr_read_entry *read_entry;
 	struct rxr_rma_context_pkt *rma_context_pkt;
-	enum rxr_read_context_type read_context_type;
-	struct rdm_peer *peer;
 	int inject;
 	size_t data_size;
 	ssize_t ret;
@@ -433,7 +431,6 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 	read_entry = (struct rxr_read_entry *)context_pkt_entry->x_entry;
 	read_entry->bytes_finished += rma_context_pkt->seg_size;
 	assert(read_entry->bytes_finished <= read_entry->total_len);
-	read_context_type = read_entry->context_type;
 
 	if (read_entry->bytes_finished == read_entry->total_len) {
 		if (read_entry->context_type == RXR_READ_CONTEXT_TX_ENTRY) {
@@ -454,8 +451,7 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 			inject = (read_entry->lower_ep_type == SHM_EP);
 			ret = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_EOR_PKT, inject);
 			if (OFI_UNLIKELY(ret)) {
-				if (rxr_cq_handle_rx_error(ep, rx_entry, ret))
-					assert(0 && "failed to write err cq entry");
+				rxr_cq_write_rx_error(ep, rx_entry, -ret, -ret);
 				rxr_release_rx_entry(ep, rx_entry);
 			}
 		} else {
@@ -469,15 +465,7 @@ void rxr_pkt_handle_rma_read_completion(struct rxr_ep *ep,
 		rxr_read_release_entry(ep, read_entry);
 	}
 
-	if (read_context_type == RXR_READ_CONTEXT_PKT_ENTRY) {
-		assert(context_pkt_entry->addr == FI_ADDR_NOTAVAIL);
-		ep->tx_pending--;
-	} else {
-		peer = rxr_ep_get_peer(ep, context_pkt_entry->addr);
-		assert(peer);
-		if (!peer->is_local)
-			rxr_ep_dec_tx_pending(ep, peer, 0);
-	}
+	rxr_ep_record_tx_op_completed(ep, context_pkt_entry);
 }
 
 void rxr_pkt_handle_rma_completion(struct rxr_ep *ep,
diff --git a/prov/efa/src/rxr/rxr_pkt_type_req.c b/prov/efa/src/rxr/rxr_pkt_type_req.c
index fcf1091..2a5c83d 100644
--- a/prov/efa/src/rxr/rxr_pkt_type_req.c
+++ b/prov/efa/src/rxr/rxr_pkt_type_req.c
@@ -240,7 +240,7 @@ int64_t rxr_pkt_req_cq_data(struct rxr_pkt_entry *pkt_entry)
 size_t rxr_pkt_req_max_header_size(int pkt_type)
 {
 	int max_hdr_size = REQ_INF_LIST[pkt_type].base_hdr_size
-		+ sizeof(struct rxr_req_opt_raw_addr_hdr) + RXR_MAX_NAME_LENGTH
+		+ RXR_REQ_OPT_RAW_ADDR_HDR_SIZE
 		+ sizeof(struct rxr_req_opt_cq_data_hdr);
 
 	if (pkt_type == RXR_EAGER_RTW_PKT ||
@@ -844,7 +844,7 @@ struct rxr_rx_entry *rxr_pkt_get_msgrtm_rx_entry(struct rxr_ep *ep,
 	dlist_func_t *match_func;
 	int pkt_type;
 
-	if ((*pkt_entry_ptr)->type == RXR_PKT_ENTRY_USER) {
+	if ((*pkt_entry_ptr)->alloc_type == RXR_PKT_FROM_USER_BUFFER) {
 		/* If a pkt_entry is constructred from user supplied buffer,
 		 * the endpoint must be in zero copy receive mode.
 		 */
@@ -1019,7 +1019,7 @@ ssize_t rxr_pkt_proc_matched_eager_rtm(struct rxr_ep *ep,
 
 	hdr_size = rxr_pkt_req_hdr_size(pkt_entry);
 
-	if (pkt_entry->type != RXR_PKT_ENTRY_USER) {
+	if (pkt_entry->alloc_type != RXR_PKT_FROM_USER_BUFFER) {
 		data = (char *)pkt_entry->pkt + hdr_size;
 		data_size = pkt_entry->pkt_size - hdr_size;
 
@@ -1077,7 +1077,7 @@ ssize_t rxr_pkt_proc_matched_rtm(struct rxr_ep *ep,
 		rx_entry->addr = pkt_entry->addr;
 		rx_entry->peer = rxr_ep_get_peer(ep, rx_entry->addr);
 		assert(rx_entry->peer);
-		ofi_atomic_inc32(&rx_entry->peer->use_cnt);
+		dlist_insert_tail(&rx_entry->peer_entry, &rx_entry->peer->rx_entry_list);
 	}
 
 	/* Adjust rx_entry->cq_entry.len as needed.
@@ -1161,10 +1161,7 @@ ssize_t rxr_pkt_proc_msgrtm(struct rxr_ep *ep,
 	if (rx_entry->state == RXR_RX_MATCHED) {
 		err = rxr_pkt_proc_matched_rtm(ep, rx_entry, pkt_entry);
 		if (OFI_UNLIKELY(err)) {
-			if (rxr_cq_handle_rx_error(ep, rx_entry, err)) {
-				assert(0 && "cannot write cq error entry");
-				efa_eq_write_error(&ep->util_ep, -err, err);
-			}
+			rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 			rxr_pkt_entry_release_rx(ep, pkt_entry);
 			rxr_release_rx_entry(ep, rx_entry);
 			return err;
@@ -1190,10 +1187,7 @@ ssize_t rxr_pkt_proc_tagrtm(struct rxr_ep *ep,
 	if (rx_entry->state == RXR_RX_MATCHED) {
 		err = rxr_pkt_proc_matched_rtm(ep, rx_entry, pkt_entry);
 		if (OFI_UNLIKELY(err)) {
-			if (rxr_cq_handle_rx_error(ep, rx_entry, err)) {
-				assert(0 && "cannot write error cq entry");
-				efa_eq_write_error(&ep->util_ep, -err, err);
-			}
+			rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 			rxr_pkt_entry_release_rx(ep, pkt_entry);
 			rxr_release_rx_entry(ep, rx_entry);
 			return err;
@@ -1243,7 +1237,7 @@ ssize_t rxr_pkt_proc_rtm_rta(struct rxr_ep *ep,
 		FI_WARN(&rxr_prov, FI_LOG_EP_CTRL,
 			"Unknown packet type ID: %d\n",
 		       base_hdr->type);
-		rxr_cq_handle_error(ep, -FI_EINVAL, NULL);
+		efa_eq_write_error(&ep->util_ep, FI_EINVAL, FI_EINVAL);
 		rxr_pkt_entry_release_rx(ep, pkt_entry);
 	}
 
@@ -1743,7 +1737,7 @@ void rxr_pkt_handle_long_rtw_recv(struct rxr_ep *ep,
 	err = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_CTS_PKT, 0);
 	if (OFI_UNLIKELY(err)) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ, "Cannot post CTS packet\n");
-		rxr_cq_handle_rx_error(ep, rx_entry, err);
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 		rxr_release_rx_entry(ep, rx_entry);
 	}
 }
@@ -2128,8 +2122,7 @@ int rxr_pkt_proc_dc_write_rta(struct rxr_ep *ep,
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"Posting of receipt packet failed! err=%s\n",
 			fi_strerror(err));
-		if (rxr_cq_handle_rx_error(ep, rx_entry, err))
-			assert(0 && "Cannot handle rx error");
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 		return err;
 	}
 
@@ -2166,10 +2159,8 @@ int rxr_pkt_proc_fetch_rta(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry)
 	}
 
 	err = rxr_pkt_post_ctrl_or_queue(ep, RXR_RX_ENTRY, rx_entry, RXR_ATOMRSP_PKT, 0);
-	if (OFI_UNLIKELY(err)) {
-		if (rxr_cq_handle_rx_error(ep, rx_entry, err))
-			assert(0 && "Cannot handle rx error");
-	}
+	if (OFI_UNLIKELY(err))
+		rxr_cq_write_rx_error(ep, rx_entry, -err, -err);
 
 	rxr_pkt_entry_release_rx(ep, pkt_entry);
 	return 0;
diff --git a/prov/efa/src/rxr/rxr_read.c b/prov/efa/src/rxr/rxr_read.c
index 2147e12..f9b3b44 100644
--- a/prov/efa/src/rxr/rxr_read.c
+++ b/prov/efa/src/rxr/rxr_read.c
@@ -110,14 +110,15 @@ ssize_t rxr_read_prepare_pkt_entry_mr(struct rxr_ep *ep, struct rxr_read_entry *
 	}
 
 	/* only ooo and unexp packet entry's memory is not registered with device */
-	assert(pkt_entry->type == RXR_PKT_ENTRY_OOO ||
-	       pkt_entry->type == RXR_PKT_ENTRY_UNEXP);
+	assert(pkt_entry->alloc_type == RXR_PKT_FROM_OOO_POOL ||
+	       pkt_entry->alloc_type == RXR_PKT_FROM_UNEXP_POOL);
 
 	pkt_offset = (char *)read_entry->rma_iov[0].addr - (char *)pkt_entry->pkt;
 	assert(pkt_offset > sizeof(struct rxr_base_hdr));
 
 	pkt_entry_copy = rxr_pkt_entry_clone(ep, ep->rx_readcopy_pkt_pool,
-					     pkt_entry, RXR_PKT_ENTRY_READ_COPY);
+					     RXR_PKT_FROM_READ_COPY_POOL,
+					     pkt_entry);
 	if (!pkt_entry_copy) {
 		FI_WARN(&rxr_prov, FI_LOG_CQ,
 			"readcopy pkt pool exhausted! Set FI_EFA_READCOPY_POOL_SIZE to a higher value!");
@@ -310,7 +311,7 @@ void rxr_read_release_entry(struct rxr_ep *ep, struct rxr_read_entry *read_entry
 			err = fi_close((struct fid *)read_entry->mr[i]);
 			if (err) {
 				FI_WARN(&rxr_prov, FI_LOG_MR, "Unable to close mr\n");
-				rxr_read_handle_error(ep, read_entry, err);
+				rxr_read_write_error(ep, read_entry, -err, -err);
 			}
 		}
 	}
@@ -538,7 +539,7 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 
 	while (read_entry->bytes_submitted < read_entry->total_len) {
 
-		if (ep->tx_pending == ep->max_outstanding_tx)
+		if (read_entry->lower_ep_type == EFA_EP && ep->efa_outstanding_tx_ops == ep->efa_max_outstanding_tx_ops)
 			return -FI_EAGAIN;
 
 		assert(iov_idx < read_entry->iov_count);
@@ -563,9 +564,9 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 		 * we had to use a pkt_entry as context too
 		 */
 		if (read_entry->lower_ep_type == SHM_EP)
-			pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_shm_pool);
+			pkt_entry = rxr_pkt_entry_alloc(ep, ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 		else
-			pkt_entry = rxr_pkt_entry_alloc(ep, ep->tx_pkt_efa_pool);
+			pkt_entry = rxr_pkt_entry_alloc(ep, ep->efa_tx_pkt_pool, RXR_PKT_FROM_EFA_TX_POOL);
 
 		if (OFI_UNLIKELY(!pkt_entry))
 			return -FI_EAGAIN;
@@ -595,15 +596,7 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 			return ret;
 		}
 
-		if (read_entry->context_type == RXR_READ_CONTEXT_PKT_ENTRY) {
-			assert(read_entry->lower_ep_type == EFA_EP);
-			/* read from self, no peer */
-			ep->tx_pending++;
-		} else if (read_entry->lower_ep_type == EFA_EP) {
-			peer = rxr_ep_get_peer(ep, read_entry->addr);
-			assert(peer);
-			rxr_ep_inc_tx_pending(ep, peer);
-		}
+		rxr_ep_record_tx_op_submitted(ep, pkt_entry);
 
 		read_entry->bytes_submitted += iov.iov_len;
 
@@ -635,22 +628,22 @@ int rxr_read_post(struct rxr_ep *ep, struct rxr_read_entry *read_entry)
 	return 0;
 }
 
-int rxr_read_handle_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry, int ret)
+void rxr_read_write_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry,
+			  int err, int prov_errno)
 {
 	struct rxr_tx_entry *tx_entry;
 	struct rxr_rx_entry *rx_entry;
 
 	if (read_entry->context_type == RXR_READ_CONTEXT_TX_ENTRY) {
 		tx_entry = read_entry->context;
-		ret = rxr_cq_handle_tx_error(ep, tx_entry, ret);
+		rxr_cq_write_tx_error(ep, tx_entry, err, prov_errno);
 	} else {
 		assert(read_entry->context_type == RXR_READ_CONTEXT_RX_ENTRY);
 		rx_entry = read_entry->context;
-		ret = rxr_cq_handle_rx_error(ep, rx_entry, ret);
+		rxr_cq_write_rx_error(ep, rx_entry, err, prov_errno);
 	}
 
 	if (read_entry->state == RXR_RDMA_ENTRY_PENDING)
 		dlist_remove(&read_entry->pending_entry);
-	return ret;
 }
 
diff --git a/prov/efa/src/rxr/rxr_read.h b/prov/efa/src/rxr/rxr_read.h
index 334648a..934eaba 100644
--- a/prov/efa/src/rxr/rxr_read.h
+++ b/prov/efa/src/rxr/rxr_read.h
@@ -127,7 +127,7 @@ int rxr_read_post_local_read_or_queue(struct rxr_ep *ep,
 
 void rxr_read_handle_read_completion(struct rxr_ep *ep, struct rxr_pkt_entry *pkt_entry);
 
-int rxr_read_handle_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry, int ret);
+void rxr_read_write_error(struct rxr_ep *ep, struct rxr_read_entry *read_entry, int err, int prov_errno);
 
 #endif
 
diff --git a/prov/efa/src/rxr/rxr_rma.c b/prov/efa/src/rxr/rxr_rma.c
index 690c32b..dc49322 100644
--- a/prov/efa/src/rxr/rxr_rma.c
+++ b/prov/efa/src/rxr/rxr_rma.c
@@ -92,9 +92,7 @@ rxr_rma_alloc_readrsp_tx_entry(struct rxr_ep *rxr_ep,
 	}
 
 	assert(tx_entry);
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
 
 	msg.msg_iov = rx_entry->iov;
 	msg.iov_count = rx_entry->iov_count;
@@ -155,9 +153,7 @@ rxr_rma_alloc_tx_entry(struct rxr_ep *rxr_ep,
 	memcpy(tx_entry->rma_iov, msg_rma->rma_iov,
 	       sizeof(struct fi_rma_iov) * msg_rma->rma_iov_count);
 
-#if ENABLE_DEBUG
-	dlist_insert_tail(&tx_entry->tx_entry_entry, &rxr_ep->tx_entry_list);
-#endif
+	dlist_insert_tail(&tx_entry->ep_entry, &rxr_ep->tx_entry_list);
 	return tx_entry;
 }
 
@@ -171,7 +167,7 @@ size_t rxr_rma_post_shm_write(struct rxr_ep *rxr_ep, struct rxr_tx_entry *tx_ent
 	assert(tx_entry->op == ofi_op_write);
 	peer = rxr_ep_get_peer(rxr_ep, tx_entry->addr);
 	assert(peer);
-	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->tx_pkt_shm_pool);
+	pkt_entry = rxr_pkt_entry_alloc(rxr_ep, rxr_ep->shm_tx_pkt_pool, RXR_PKT_FROM_SHM_TX_POOL);
 	if (OFI_UNLIKELY(!pkt_entry))
 		return -FI_EAGAIN;
 
diff --git a/prov/psm3/VERSION b/prov/psm3/VERSION
index 73363e5..b435351 100644
--- a/prov/psm3/VERSION
+++ b/prov/psm3/VERSION
@@ -1,2 +1 @@
 3_1_0_0
-
diff --git a/prov/psm3/configure.ac b/prov/psm3/configure.ac
index a88c22f..8ea1671 100644
--- a/prov/psm3/configure.ac
+++ b/prov/psm3/configure.ac
@@ -680,17 +680,17 @@ AM_COND_IF([HAVE_PSM3_SRC],
 			|| git describe --dirty --always --abbrev=8 --broken 2>/dev/null || echo 'unknown commit')}"
 		IFS_VERSION=${IFS_VERSION//./_}
 		GIT_HASH="$(git log --oneline --format='%H' -1)"
-		RPM_RELEASE=$(cut -d'_' -f5 <<<"${IFS_VERSION}")
-		RELEASE_VER=$(cut -d'_' -f1-4 <<<"${IFS_VERSION}" | sed 's/_/./g')
+		RPM_RELEASE=$(echo "${IFS_VERSION}" | cut -d'_' -f5)
+		RELEASE_VER=$(echo "${IFS_VERSION}" | cut -d'_' -f1-4 | sed 's/_/./g')
 		AS_IF([test x"${RELEASE_VER}" = x"${PACKAGE_VERSION}"], [], [
 			AC_MSG_NOTICE([Release Tag does not match VERSION file])
 			AC_MSG_NOTICE([${RELEASE_VER} != ${PACKAGE_VERSION}])
 			RPM_RELEASE=999
 		])
-		PSM3_PROV_VER_MAJOR=$(cut -d'.' -f1 <<<"${PACKAGE_VERSION}")
-		PSM3_PROV_VER_MINOR=$(cut -d'.' -f2 <<<"${PACKAGE_VERSION}")
-		PSM3_PROV_VER_MAINT=$(cut -d'.' -f3 <<<"${PACKAGE_VERSION}")
-		PSM3_PROV_VER_PATCH=$(cut -d'.' -f4 <<<"${PACKAGE_VERSION}")
+		PSM3_PROV_VER_MAJOR=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f1)
+		PSM3_PROV_VER_MINOR=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f2)
+		PSM3_PROV_VER_MAINT=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f3)
+		PSM3_PROV_VER_PATCH=$(echo "${PACKAGE_VERSION}" | cut -d'.' -f4)
 	   ])
 AS_IF([test $have_libcuda -eq 1], [RPM_RELEASE=${RPM_RELEASE}cuda])
 
diff --git a/prov/psm3/configure.m4 b/prov/psm3/configure.m4
index ece8a85..c78d035 100644
--- a/prov/psm3/configure.m4
+++ b/prov/psm3/configure.m4
@@ -227,10 +227,10 @@ ifelse('
 
 	 PSM3_IFS_VERSION=m4_normalize(m4_esyscmd([cat prov/psm3/VERSION]))
 	 AC_SUBST(PSM3_IFS_VERSION)
-	 PSM3_PROV_VER_MAJOR=$(cut -d'_' -f1 <<<"${PSM3_IFS_VERSION}")
-	 PSM3_PROV_VER_MINOR=$(cut -d'_' -f2 <<<"${PSM3_IFS_VERSION}")
-	 PSM3_PROV_VER_MAINT=$(cut -d'_' -f3 <<<"${PSM3_IFS_VERSION}")
-	 PSM3_PROV_VER_PATCH=$(cut -d'_' -f4 <<<"${PSM3_IFS_VERSION}")
+	 PSM3_PROV_VER_MAJOR=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f1)
+	 PSM3_PROV_VER_MINOR=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f2)
+	 PSM3_PROV_VER_MAINT=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f3)
+	 PSM3_PROV_VER_PATCH=$(echo "${PSM3_IFS_VERSION}" | cut -d'_' -f4)
 	 AC_SUBST(PSM3_PROV_VER_MAJOR)
 	 AC_SUBST(PSM3_PROV_VER_MINOR)
 	 AC_SUBST(PSM3_PROV_VER_MAINT)
diff --git a/prov/psm3/libpsm3-fi.spec.in b/prov/psm3/libpsm3-fi.spec.in
index 1d1cc24..eee516c 100644
--- a/prov/psm3/libpsm3-fi.spec.in
+++ b/prov/psm3/libpsm3-fi.spec.in
@@ -4,7 +4,7 @@
 
 Name: lib%{provider}-fi
 Version: @VERSION@
-Release: @RPM_RELEASE@
+Release: 179_@RPM_RELEASE@
 Summary: Dynamic %{provider_formal} provider for Libfabric
 
 Group: System Environment/Libraries
diff --git a/prov/psm3/psm3/psm.c b/prov/psm3/psm3/psm.c
index 9ed1964..7159722 100644
--- a/prov/psm3/psm3/psm.c
+++ b/prov/psm3/psm3/psm.c
@@ -356,6 +356,9 @@ int psmi_cuda_initialize()
 
 	PSMI_CUDA_CALL(cuInit, 0);
 
+#ifdef RNDV_MOD
+	psm2_get_gpu_bars();
+#endif
 	union psmi_envvar_val env_enable_gdr_copy;
 	psmi_getenv("PSM3_GDRCOPY",
 				"Enable (set envvar to 1) for gdr copy support in PSM (Enabled by default)",
diff --git a/prov/psm3/psm3/psm2_mq.h b/prov/psm3/psm3/psm2_mq.h
index 8aac128..7267b09 100644
--- a/prov/psm3/psm3/psm2_mq.h
+++ b/prov/psm3/psm3/psm2_mq.h
@@ -1572,6 +1572,7 @@ struct psm2_mq_stats {
 	/** Bytes received into an unmatched (or out of order) system buffer */
 	uint64_t rx_sys_bytes;
 	/** Messages received into an unmatched (or out of order) system buffer */
+	/** this count includes unexpected zero length eager recv */
 	uint64_t rx_sys_num;
 
 	/** Total Messages transmitted (shm and hfi) */
@@ -1580,18 +1581,23 @@ struct psm2_mq_stats {
 	uint64_t tx_eager_num;
 	/** Bytes transmitted eagerly */
 	uint64_t tx_eager_bytes;
-	/** Messages transmitted using expected TID mechanism */
+	/** Messages transmitted using any rendezvous mechanism */
 	uint64_t tx_rndv_num;
-	/** Bytes transmitted using expected TID mechanism */
+	/** Bytes transmitted using any rendezvous mechanism */
 	uint64_t tx_rndv_bytes;
 	/** Messages transmitted (shm only) */
 	uint64_t tx_shm_num;
+	/** Bytes transmitted (shm only) */
+	uint64_t tx_shm_bytes;
 	/** Messages received through shm */
 	uint64_t rx_shm_num;
+	/** Bytes received through shm */
+	uint64_t rx_shm_bytes;
 
-	/** Number of system buffers allocated  */
+	/** sysbufs are used for unexpected eager receive (and RTS payload) */
+	/** Number of messages using system buffers (not used for 0 byte msg) */
 	uint64_t rx_sysbuf_num;
-	/** Bytes allcoated for system buffers */
+	/** Bytes using system buffers */
 	uint64_t rx_sysbuf_bytes;
 
 	/** rank in MPI_COMM_WORLD, while unchanging, easiest to put here */
@@ -1608,6 +1614,7 @@ struct psm2_mq_stats {
 	uint64_t tx_eager_gpu_bytes;
 
 	/** Bytes copied from a system buffer into a matched CPU user buffer */
+	/** this count also includes unexpected zero length eager recv */
 	uint64_t rx_sysbuf_cpu_bytes;
 	/** Messages copied from a system buffer into a matched CPU user buffer */
 	uint64_t rx_sysbuf_cpu_num;
@@ -1621,7 +1628,7 @@ struct psm2_mq_stats {
 	uint64_t rx_sysbuf_cuCopy_num;
 
 	/** Internally reserved for future use */
-	uint64_t _reserved[5];
+	uint64_t _reserved[3];
 #else
 	uint64_t _reserved[15];
 #endif
diff --git a/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c b/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c
index 6d24dd8..398646d 100644
--- a/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c
+++ b/prov/psm3/psm3/psm_hal_gen1/psm_gdrcpy.c
@@ -90,11 +90,12 @@ gdr_convert_gpu_to_host_addr(int gdr_fd, unsigned long buf,
 	_HFI_VDBG("buf=%p size=%zu pageaddr=%p pagelen=%"PRIu64" flags=0x%x ep=%p\n",
 		(void *)buf, size, (void *)pageaddr, pagelen, flags, ep);
 #ifdef RNDV_MOD
-	host_addr_buf = __psm2_rv_pin_and_mmap(psmi_opened_endpoint->verbs_ep.rv, pageaddr, pagelen, IBV_ACCESS_IS_GPU_ADDR);
+	ep = ep->mctxt_master;
+	host_addr_buf = __psm2_rv_pin_and_mmap(ep->verbs_ep.rv, pageaddr, pagelen, IBV_ACCESS_IS_GPU_ADDR);
 	if_pf (! host_addr_buf) {
 		if (errno == ENOMEM) {
-			if (psm2_verbs_evict_some(psmi_opened_endpoint, pagelen, IBV_ACCESS_IS_GPU_ADDR) >= 0)
-				host_addr_buf = __psm2_rv_pin_and_mmap(psmi_opened_endpoint->verbs_ep.rv, pageaddr, pagelen, IBV_ACCESS_IS_GPU_ADDR);
+			if (psm2_verbs_evict_some(ep, pagelen, IBV_ACCESS_IS_GPU_ADDR) > 0)
+				host_addr_buf = __psm2_rv_pin_and_mmap(ep->verbs_ep.rv, pageaddr, pagelen, IBV_ACCESS_IS_GPU_ADDR);
 		}
 		if_pf (! host_addr_buf)
 			return NULL;
diff --git a/prov/psm3/psm3/psm_mq.c b/prov/psm3/psm3/psm_mq.c
index d358236..660f188 100644
--- a/prov/psm3/psm3/psm_mq.c
+++ b/prov/psm3/psm3/psm_mq.c
@@ -759,6 +759,10 @@ PSMI_API_DECL(psm2_mq_send)
  * that the provided request has been matched, and begins copying message data
  * that has already arrived to the user's buffer.  Any remaining data is copied
  * by PSM polling until the message is complete.
+ * Caller has initialized req->is_buf_gpu_mem and req->user_gpu_buffer
+ * consistently with buf/len which represent the application buffer
+ * but req->req_data.buf and req->req_data.len still point to the sysbuf
+ * where data was landed.
  */
 static psm2_error_t
 psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
@@ -774,7 +778,11 @@ psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
 	case MQ_STATE_COMPLETE:
 		if (req->req_data.buf != NULL) {	/* 0-byte messages don't alloc a sysbuf */
 			msglen = mq_set_msglen(req, len, req->req_data.send_msglen);
-			psmi_mq_recv_copy(mq, req, buf, len, msglen);
+			psmi_mq_recv_copy(mq, req,
+#ifdef PSM_CUDA
+					req->is_buf_gpu_mem,
+#endif
+					buf, len, msglen);
 			psmi_mq_sysbuf_free(mq, req->req_data.buf);
 #ifdef PSM_CUDA
 		} else {
@@ -792,7 +800,11 @@ psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
 		 * any more than copysz.  After that, swap system with user buffer
 		 */
 		req->recv_msgoff = min(req->recv_msgoff, msglen);
-		psmi_mq_recv_copy(mq, req, buf, len, req->recv_msgoff);
+		psmi_mq_recv_copy(mq, req,
+#ifdef PSM_CUDA
+				req->is_buf_gpu_mem,
+#endif
+				buf, len, req->recv_msgoff);
 		psmi_mq_sysbuf_free(mq, req->req_data.buf);
 
 		req->state = MQ_STATE_MATCHED;
@@ -807,7 +819,11 @@ psm2_mq_irecv_inner(psm2_mq_t mq, psm2_mq_req_t req, void *buf, uint32_t len)
 		 */
 		req->recv_msgoff = min(req->recv_msgoff, msglen);
 		if (req->send_msgoff) {	// only have sysbuf if RTS w/payload
-			psmi_mq_recv_copy(mq, req, buf, len, req->recv_msgoff);
+			psmi_mq_recv_copy(mq, req,
+#ifdef PSM_CUDA
+					req->is_buf_gpu_mem,
+#endif
+					buf, len, req->recv_msgoff);
 			psmi_mq_sysbuf_free(mq, req->req_data.buf);
 		}
 
@@ -1485,20 +1501,22 @@ psm2_error_t psmi_mq_initstats(psm2_mq_t mq, psm2_epid_t epid)
 		PSMI_STATS_DECLU64("Unexpected_count_recv", &mq->stats.rx_sys_num),
 		PSMI_STATS_DECLU64("Unexpected_bytes_recv", &mq->stats.rx_sys_bytes),
 		PSMI_STATS_DECLU64("shm_count_sent", &mq->stats.tx_shm_num),
+		PSMI_STATS_DECLU64("shm_bytes_sent", &mq->stats.tx_shm_bytes),
 		PSMI_STATS_DECLU64("shm_count_recv", &mq->stats.rx_shm_num),
-		PSMI_STATS_DECLU64("sysbuf_count", &mq->stats.rx_sysbuf_num),
-		PSMI_STATS_DECLU64("sysbuf_bytes", &mq->stats.rx_sysbuf_bytes),
+		PSMI_STATS_DECLU64("shm_bytes_recv", &mq->stats.rx_shm_bytes),
+		PSMI_STATS_DECLU64("sysbuf_count_recv", &mq->stats.rx_sysbuf_num),
+		PSMI_STATS_DECLU64("sysbuf_bytes_recv", &mq->stats.rx_sysbuf_bytes),
 #ifdef PSM_CUDA
 		PSMI_STATS_DECLU64("Eager_cpu_count_sent", &mq->stats.tx_eager_cpu_num),
 		PSMI_STATS_DECLU64("Eager_cpu_bytes_sent", &mq->stats.tx_eager_cpu_bytes),
 		PSMI_STATS_DECLU64("Eager_gpu_count_sent", &mq->stats.tx_eager_gpu_num),
 		PSMI_STATS_DECLU64("Eager_gpu_bytes_sent", &mq->stats.tx_eager_gpu_bytes),
-		PSMI_STATS_DECLU64("sysbuf_cpu_count", &mq->stats.rx_sysbuf_cpu_num),
-		PSMI_STATS_DECLU64("sysbuf_cpu_bytes", &mq->stats.rx_sysbuf_cpu_bytes),
-		PSMI_STATS_DECLU64("sysbuf_gdrcopy_count", &mq->stats.rx_sysbuf_gdrcopy_num),
-		PSMI_STATS_DECLU64("sysbuf_gdrcopy_bytes", &mq->stats.rx_sysbuf_gdrcopy_bytes),
-		PSMI_STATS_DECLU64("sysbuf_cuCopy_count", &mq->stats.rx_sysbuf_cuCopy_num),
-		PSMI_STATS_DECLU64("sysbuf_cuCopy_bytes", &mq->stats.rx_sysbuf_cuCopy_bytes),
+		PSMI_STATS_DECLU64("sysbuf_cpu_count_recv", &mq->stats.rx_sysbuf_cpu_num),
+		PSMI_STATS_DECLU64("sysbuf_cpu_bytes_recv", &mq->stats.rx_sysbuf_cpu_bytes),
+		PSMI_STATS_DECLU64("sysbuf_gdrcopy_count_recv", &mq->stats.rx_sysbuf_gdrcopy_num),
+		PSMI_STATS_DECLU64("sysbuf_gdrcopy_bytes_recv", &mq->stats.rx_sysbuf_gdrcopy_bytes),
+		PSMI_STATS_DECLU64("sysbuf_cuCopy_count_recv", &mq->stats.rx_sysbuf_cuCopy_num),
+		PSMI_STATS_DECLU64("sysbuf_cuCopy_bytes_recv", &mq->stats.rx_sysbuf_cuCopy_bytes),
 #endif
 	};
 
diff --git a/prov/psm3/psm3/psm_mq_internal.h b/prov/psm3/psm3/psm_mq_internal.h
index 48b76f0..e3d246b 100644
--- a/prov/psm3/psm3/psm_mq_internal.h
+++ b/prov/psm3/psm3/psm_mq_internal.h
@@ -556,6 +556,7 @@ int psmi_mq_handle_data(psm2_mq_t mq, psm2_mq_req_t req,
 #endif
 			);
 int psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
+		       struct ptl_strategy_stats *stats,
 		       uint32_t msglen, const void *payload, uint32_t paylen,
 		       int msgorder, mq_rts_callback_fn_t cb,
 		       psm2_mq_req_t *req_o);
@@ -568,13 +569,14 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t req);
 
 // perform the actual copy for a recv matching a sysbuf.  We copy from a sysbuf
 // (req->req_data.buf) to the actual user buffer (buf) and keep statistics.
+// is_buf_gpu_mem indicates if buf is a gpu buffer
 // len - recv buffer size posted, we use this for any GDR copy pinning so
 // 	can get future cache hits on other size messages in same buffer
 // not needed - msglen - negotiated total message size
 // copysz - actual amount to copy (<= msglen)
 #ifdef PSM_CUDA
-void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, void *buf,
-                                uint32_t len, uint32_t copysz);
+void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, uint8_t is_buf_gpu_mem,
+                                void *buf, uint32_t len, uint32_t copysz);
 #else
 PSMI_ALWAYS_INLINE(
 void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, void *buf,
@@ -632,18 +634,4 @@ psmi_mq_register_unexpected_callback(psm2_mq_t mq,
 				     psm_mq_unexpected_callback_fn_t fn);
 #endif
 
-PSMI_ALWAYS_INLINE(void psmi_mq_stats_rts_account(psm2_mq_req_t req))
-{
-	psm2_mq_t mq = req->mq;
-	if (MQE_TYPE_IS_SEND(req->type)) {
-		mq->stats.tx_num++;
-		mq->stats.tx_rndv_num++;
-		mq->stats.tx_rndv_bytes += req->req_data.send_msglen;
-	} else {
-		mq->stats.rx_user_num++;
-		mq->stats.rx_user_bytes += req->req_data.recv_msglen;
-	}
-	return;
-}
-
 #endif
diff --git a/prov/psm3/psm3/psm_mq_recv.c b/prov/psm3/psm3/psm_mq_recv.c
index 028bb24..0e93807 100644
--- a/prov/psm3/psm3/psm_mq_recv.c
+++ b/prov/psm3/psm3/psm_mq_recv.c
@@ -90,8 +90,6 @@ void psmi_mq_handle_rts_complete(psm2_mq_req_t req)
 		ips_tid_mravail_callback(req->rts_peer->proto);
 	}
 
-	/* Stats on rendez-vous messages */
-	psmi_mq_stats_rts_account(req);
 	req->state = MQ_STATE_COMPLETE;
 	ips_barrier();
 	if(!psmi_is_req_internal(req))
@@ -298,6 +296,7 @@ mq_req_match(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag, int remove)
  */
 int
 psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
+		   struct ptl_strategy_stats *stats,
 		   uint32_t send_msglen, const void *payload, uint32_t paylen,
 		   int msgorder, mq_rts_callback_fn_t cb, psm2_mq_req_t *req_o)
 {
@@ -322,6 +321,16 @@ psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 		if (paylen) {
 			// payload of RTS can contain a single packet synchronous MPI msg
 			psmi_mq_mtucpy(req->req_data.buf, payload, paylen);
+#ifdef PSM_CUDA
+			if (req->is_buf_gpu_mem) {
+				stats->rndv_rts_cuCopy_recv++;
+				stats->rndv_rts_cuCopy_recv_bytes += paylen;
+			} else
+#endif
+			{
+				stats->rndv_rts_cpu_recv++;
+				stats->rndv_rts_cpu_recv_bytes += paylen;
+			}
 		}
 		req->recv_msgoff = req->send_msgoff = paylen;
 		*req_o = req;	/* yes match */
@@ -375,6 +384,8 @@ psmi_mq_handle_rts(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 #else
 			psmi_mq_mtucpy(req->req_data.buf, payload, paylen);
 #endif
+			stats->rndv_rts_sysbuf_recv++;
+			stats->rndv_rts_sysbuf_recv_bytes += paylen;
 		}
 		req->recv_msgoff = req->send_msgoff = paylen;
 
@@ -519,23 +530,24 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 			STAILQ_INSERT_TAIL(&mq->eager_q, req, nextq);
 			_HFI_VDBG("exp MSG_EAGER of length %d bytes pay=%d\n",
 				  msglen, paylen);
+			// !offset -> only count recv msgs on 1st pkt in msg
 #ifdef PSM_CUDA
 			if (!req->is_buf_gpu_mem) {
-				stats->eager_cpu_recv++;
-				stats->eager_cpu_recv_bytes += msglen;
+				if (!offset) stats->eager_cpu_recv++;
+				stats->eager_cpu_recv_bytes += paylen;
 			} else if (PSMI_USE_GDR_COPY_RECV(paylen)) {
 				req->req_data.buf = req->user_gpu_buffer;
 				use_gdrcopy = 1;
-				stats->eager_gdrcopy_recv++;
-				stats->eager_gdrcopy_recv_bytes += msglen;
+				if (!offset) stats->eager_gdrcopy_recv++;
+				stats->eager_gdrcopy_recv_bytes += paylen;
 			} else {
 				req->req_data.buf = req->user_gpu_buffer;
-				stats->eager_cuCopy_recv++;
-				stats->eager_cuCopy_recv_bytes += msglen;
+				if (!offset) stats->eager_cuCopy_recv++;
+				stats->eager_cuCopy_recv_bytes += paylen;
 			}
 #else
-			stats->eager_cpu_recv++;
-			stats->eager_cpu_recv_bytes += msglen;
+			if (!offset) stats->eager_cpu_recv++;
+			stats->eager_cpu_recv_bytes += paylen;
 #endif
 			if (paylen > 0)
 				psmi_mq_handle_data(mq, req, offset, payload,
@@ -612,8 +624,12 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 			mq_copy_tiny((uint32_t *) req->req_data.buf,
 				     (uint32_t *) payload, msglen);
 #endif
-		} else
+			stats->tiny_sysbuf_recv++;
+			stats->tiny_sysbuf_recv_bytes += msglen;
+		} else {
 			req->req_data.buf = NULL;
+			stats->tiny_sysbuf_recv++;	// 0 length
+		}
 		req->state = MQ_STATE_COMPLETE;
 		break;
 
@@ -645,6 +661,8 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 				(uint32_t *)&offset, msglen & 0x3);
 #endif
 		}
+		stats->short_sysbuf_recv++;
+		stats->short_sysbuf_recv_bytes += msglen;
 		req->state = MQ_STATE_COMPLETE;
 		break;
 
@@ -663,6 +681,8 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 #else
 			psmi_mq_handle_data(mq, req, offset, payload, paylen);
 #endif
+		stats->eager_sysbuf_recv++;
+		stats->eager_sysbuf_recv_bytes += paylen;
 		break;
 
 	default:
@@ -685,21 +705,22 @@ psmi_mq_handle_envelope(psm2_mq_t mq, psm2_epaddr_t src, psm2_mq_tag_t *tag,
 #ifdef PSM_CUDA	// declared inline in psm_mq_internal.h for non-CUDA
 // perform the actual copy for an psmi_mq_irecv_inner.  We copy from a sysbuf
 // (req->req_data.buf) to the actual user buffer (buf) and keep statistics.
+// is_buf_gpu_mem indicates if buf is a gpu buffer
 // len - recv buffer size posted, we use this for any GDR copy pinning so
 // 	can get future cache hits on other size messages in same buffer
 // not needed - msglen - negotiated total message size
 // copysz - actual amount to copy (<= msglen)
-void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, void *buf,
-				uint32_t len, uint32_t copysz)
+void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, uint8_t is_buf_gpu_mem,
+				void *buf, uint32_t len, uint32_t copysz)
 {
 	psmi_mtucpy_fn_t psmi_mtucpy_fn = psmi_mq_mtucpy;
 	void *ubuf = buf;
 	if (! copysz) {
-		mq->stats.rx_sysbuf_cpu_num++;
-		mq->stats.rx_sysbuf_cpu_bytes += copysz;
+		mq->stats.rx_sysbuf_cpu_num++; // zero length
 		return;
 	}
-	if (!req->is_buf_gpu_mem) {
+	if (!is_buf_gpu_mem) {
+		psmi_assert(!PSMI_IS_CUDA_MEM(buf));
 		mq->stats.rx_sysbuf_cpu_num++;
 		mq->stats.rx_sysbuf_cpu_bytes += copysz;
 		psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
@@ -709,10 +730,12 @@ void psmi_mq_recv_copy(psm2_mq_t mq, psm2_mq_req_t req, void *buf,
 		NULL != (ubuf = gdr_convert_gpu_to_host_addr(GDR_FD, (unsigned long)buf,
 						    min(gdr_copy_limit_recv, len), 1,
 						    mq->ep))) {
+		psmi_assert(PSMI_IS_CUDA_MEM(buf));
 		psmi_mtucpy_fn = psmi_mq_mtucpy_host_mem;
 		mq->stats.rx_sysbuf_gdrcopy_num++;
 		mq->stats.rx_sysbuf_gdrcopy_bytes += copysz;
 	} else {
+		psmi_assert(PSMI_IS_CUDA_MEM(buf));
 		ubuf = buf;
 		mq->stats.rx_sysbuf_cuCopy_num++;
 		mq->stats.rx_sysbuf_cuCopy_bytes += copysz;
@@ -743,12 +766,16 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 	switch (ureq->state) {
 	case MQ_STATE_COMPLETE:
 		if (ureq->req_data.buf != NULL) {	/* 0-byte don't alloc a sysreq_data.buf */
-			psmi_mq_recv_copy(mq, ureq, ereq->req_data.buf,
+			psmi_mq_recv_copy(mq, ureq,
+#ifdef PSM_CUDA
+					ereq->is_buf_gpu_mem,
+#endif
+					ereq->req_data.buf,
 					ereq->req_data.buf_len, msglen);
 			psmi_mq_sysbuf_free(mq, ureq->req_data.buf);
 #ifdef PSM_CUDA
 		} else {
-			mq->stats.rx_sysbuf_cpu_num++;
+			mq->stats.rx_sysbuf_cpu_num++; // zero length
 #endif
 		}
 		ereq->state = MQ_STATE_COMPLETE;
@@ -761,7 +788,11 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 		ereq->ptl_req_ptr = ureq->ptl_req_ptr;
 		ereq->send_msgoff = ureq->send_msgoff;
 		ereq->recv_msgoff = min(ureq->recv_msgoff, msglen);
-		psmi_mq_recv_copy(mq, ureq, ereq->req_data.buf,
+		psmi_mq_recv_copy(mq, ureq,
+#ifdef PSM_CUDA
+				ereq->is_buf_gpu_mem,
+#endif
+				ereq->req_data.buf,
 			 	ereq->req_data.buf_len, ereq->recv_msgoff);
 		psmi_mq_sysbuf_free(mq, ureq->req_data.buf);
 		ereq->type = ureq->type;
@@ -775,7 +806,11 @@ int psmi_mq_handle_outoforder(psm2_mq_t mq, psm2_mq_req_t ureq)
 		ereq->send_msgoff = ureq->send_msgoff;
 		ereq->recv_msgoff = min(ureq->recv_msgoff, msglen);
 		if (ereq->send_msgoff) { // only have sysbuf if RTS w/payload
-			psmi_mq_recv_copy(mq, ureq, ereq->req_data.buf,
+			psmi_mq_recv_copy(mq, ureq,
+#ifdef PSM_CUDA
+					ereq->is_buf_gpu_mem,
+#endif
+					ereq->req_data.buf,
 			 		ereq->req_data.buf_len,
 					ereq->recv_msgoff);
 			psmi_mq_sysbuf_free(mq, ureq->req_data.buf);
diff --git a/prov/psm3/psm3/psm_rndv_mod.c b/prov/psm3/psm3/psm_rndv_mod.c
index c5db980..f21aa6c 100644
--- a/prov/psm3/psm3/psm_rndv_mod.c
+++ b/prov/psm3/psm3/psm_rndv_mod.c
@@ -98,6 +98,130 @@ struct irdma_mem_reg_req {
 //#define my_calloc(nmemb, size) (psmi_calloc(PSMI_EP_NONE, NETWORK_BUFFERS, (nmemb), (size)))
 #define my_free(p) (psmi_free(p))
 
+#ifdef PSM_CUDA
+static int gpu_pin_check;	// PSM3_GPU_PIN_CHECK
+static uint64_t *gpu_bars;
+static int num_gpu_bars = 0;
+static uint64_t min_gpu_bar_size;
+
+// The second BAR address is where the GPU will map GPUDirect memory.
+// The beginning of this BAR is reserved for non-GPUDirect uses.
+// However, it has been observed that in some multi-process
+// pinning failures, HED-2035, the nvidia_p2p_get_pages can foul up
+// it's IOMMU after which the next successful pin will incorrectly
+// return the 1st physical address of the BAR for the pinned pages.
+// In this case it will report this same physical address for other GPU virtual
+// addresses and cause RDMA to use the wrong memory.
+// As a workaround, we gather the Region 1 BAR address start for each
+// GPU and if we see this address returned as the phys_addr of a mmapped
+// GPUDirect Copy or the iova of a GPU MR we fail the job before it can
+// corrupt any more application data.
+static uint64_t get_nvidia_bar_addr(int domain, int bus, int slot)
+{
+	char sysfs[100];
+	int ret;
+	FILE *f;
+	unsigned long long start_addr, end_addr, bar_size;
+
+	ret = snprintf(sysfs, sizeof(sysfs),
+		"/sys/class/pci_bus/%04x:%02x/device/%04x:%02x:%02x.0/resource",
+		domain, bus, domain, bus, slot);
+	psmi_assert_always(ret < sizeof(sysfs));
+	f = fopen(sysfs, "r");
+	if (! f) {
+		if (gpu_pin_check) {
+			_HFI_ERROR("Unable to open %s for GPU BAR Address: %s\n",
+				sysfs, strerror(errno));
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
+				"Unable to get GPU BAR address\n");
+		}
+		return 0;
+	}
+	// for each BAR region, start, end and flags are listed in hex
+	// nVidia uses the 2nd BAR region (aka Region #1) to map peer to peer
+	// accesses into it's potentially larger GPU local memory space
+	ret = fscanf(f, "%*x %*x %*x %llx %llx", &start_addr, &end_addr);
+	if (ret != 2) {
+		if (gpu_pin_check) {
+			_HFI_ERROR("Unable to get GPU BAR Address from %s: %s\n",
+				sysfs, strerror(errno));
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
+				"Unable to get GPU BAR address\n");
+		}
+		fclose(f);
+		return 0;
+	}
+	fclose(f);
+
+	bar_size = (end_addr - start_addr) + 1;
+	_HFI_DBG("GPU BAR Addr from %s is 0x%llx - 0x%llx (size 0x%llx)\n", sysfs, start_addr, end_addr, bar_size);
+	if (! min_gpu_bar_size || bar_size < min_gpu_bar_size)
+		min_gpu_bar_size = bar_size;
+	return start_addr;
+}
+
+void psm2_get_gpu_bars(void)
+{
+	int num_devices, dev;
+	union psmi_envvar_val env;
+
+	psmi_getenv("PSM3_GPU_PIN_CHECK",
+			"Enable sanity check of physical addresses mapped into GPU BAR space (Enabled by default)",
+			PSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,
+			(union psmi_envvar_val)1, &env);
+	gpu_pin_check = env.e_int;
+
+	PSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);
+	gpu_bars = psmi_calloc(PSMI_EP_NONE, UNDEFINED, num_devices, sizeof(gpu_bars[0]));
+	if (! gpu_bars)
+		return;	// psmi_calloc will have exited for Out of Memory
+
+	if (gpu_pin_check)
+		num_gpu_bars = num_devices;
+
+	for (dev = 0; dev < num_devices; dev++) {
+		CUdevice device;
+		int domain, bus, slot;
+
+		PSMI_CUDA_CALL(cuDeviceGet, &device, dev);
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&domain,
+				CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID,
+				device);
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&bus,
+				CU_DEVICE_ATTRIBUTE_PCI_BUS_ID,
+				device);
+		PSMI_CUDA_CALL(cuDeviceGetAttribute,
+				&slot,
+				CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID,
+				device);
+		gpu_bars[dev] = get_nvidia_bar_addr(domain, bus, slot);
+	}
+}
+
+static psm2_error_t psm2_check_phys_addr(uint64_t phys_addr)
+{
+	int i;
+	for (i=0; i < num_gpu_bars; i++) {
+		if (phys_addr == gpu_bars[i]) {
+			_HFI_ERROR("Incorrect Physical Address (0x%"PRIx64") returned by nVidia driver.  PSM3 exiting to avoid data corruption.  Job may be rerun with PSM3_GPUDIRECT=0 to avoid this issue.\n",
+				phys_addr);
+			psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,
+				"Incorrect Physical Address returned by nVidia driver\n");
+			psmi_assert_always(0);
+			return PSM2_INTERNAL_ERR;
+		}
+	}
+	return PSM2_OK;
+}
+
+uint64_t __psm2_min_gpu_bar_size(void)
+{
+	return min_gpu_bar_size;
+}
+#endif
+
 static int rv_map_event_ring(psm2_rv_t rv, struct rv_event_ring* ring,
 				int entries, int offset)
 {
@@ -768,6 +892,14 @@ psm2_rv_mr_t __psm2_rv_reg_mem(psm2_rv_t rv, int cmd_fd_int, struct ibv_pd *pd,
 		save_errno = errno;
 		goto fail;
 	}
+#ifdef PSM_CUDA
+	if ((access & IBV_ACCESS_IS_GPU_ADDR)
+		&& PSM2_OK != psm2_check_phys_addr(mparams.out.iova)) {
+		(void)__psm2_rv_dereg_mem(rv, mr);
+		errno = EFAULT;
+		return NULL;
+	}
+#endif
 	mr->addr = (uint64_t)addr;
 	mr->length = length;
 	mr->access = access;
@@ -835,6 +967,11 @@ void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr,
 	if ((ret = ioctl(rv->fd, RV_IOCTL_GPU_PIN_MMAP, &params)) != 0)
 		return NULL;
 
+	if (PSM2_OK != psm2_check_phys_addr(params.out.phys_addr)) {
+		(void)__psm2_rv_evict_exact(rv, (void*)pageaddr, pagelen, access);
+		errno = EFAULT;
+		return NULL;
+	}
 	// return mapped host address or NULL with errno set
 	return (void*)(uintptr_t)params.out.host_buf_addr;
 }
@@ -845,9 +982,9 @@ void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr,
 // this will remove from the cache the matching entry if it's
 // refcount is 0.  In the case of reg_mem, a matching call
 // to dereg_mem is required for this to be able to evict the entry
-// return 0 on success or -1 with errno
+// return number of bytes evicted (> 0) on success or -1 with errno
 // Reports ENOENT if entry not found in cache (may already be evicted)
-int __psm2_rv_evict_exact(psm2_rv_t rv, void *addr, uint64_t length, int access)
+int64_t __psm2_rv_evict_exact(psm2_rv_t rv, void *addr, uint64_t length, int access)
 {
 #ifdef RV_IOCTL_EVICT
 	struct rv_evict_params params;
@@ -880,9 +1017,9 @@ int __psm2_rv_evict_exact(psm2_rv_t rv, void *addr, uint64_t length, int access)
 // addresses between addr and addr+length-1 inclusive if it's
 // refcount is 0.  In the case of reg_mem, a matching call
 // to dereg_mem is required for this to be able to evict the entry
-// return 0 on success or -1 with errno
+// return number of bytes evicted (> 0) on success or -1 with errno
 // Reports ENOENT if no matching entries found in cache (may already be evicted)
-int __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length)
+int64_t __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length)
 {
 #ifdef RV_IOCTL_EVICT
 	struct rv_evict_params params;
@@ -915,9 +1052,9 @@ int __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length)
 // addresses between addr and addr+length-1 inclusive if it's
 // refcount is 0.  In the case of reg_mem, a matching call
 // to dereg_mem is required for this to be able to evict the entry
-// return 0 on success or -1 with errno
+// return number of bytes evicted (> 0) on success or -1 with errno
 // Reports ENOENT if no matching entries found in cache (may already be evicted)
-int __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr, uint64_t length)
+int64_t __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr, uint64_t length)
 {
 #ifdef RV_IOCTL_EVICT
 	struct rv_evict_params params;
@@ -950,9 +1087,9 @@ int __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr, uint64_t length)
 // Only entries with a refcount of 0 are removed.
 // In the case of reg_mem, a matching call
 // to dereg_mem is required for this to be able to evict the entry
-// return 0 on success or -1 with errno
+// return number of bytes evicted (> 0) on success or -1 with errno
 // Reports ENOENT if no entries could be evicted
-int __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
+int64_t __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
 {
 #ifdef RV_IOCTL_EVICT
 	struct rv_evict_params params;
@@ -985,9 +1122,9 @@ int __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
 // Only entries with a refcount of 0 are removed.
 // In the case of reg_mem, a matching call
 // to dereg_mem is required for this to be able to evict the entry
-// return 0 on success or -1 with errno
+// return number of bytes evicted (> 0) on success or -1 with errno
 // Reports ENOENT if no entries could be evicted
-int __psm2_rv_evict_gpu_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
+int64_t __psm2_rv_evict_gpu_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count)
 {
 #ifdef RV_IOCTL_EVICT
 	struct rv_evict_params params;
diff --git a/prov/psm3/psm3/psm_rndv_mod.h b/prov/psm3/psm3/psm_rndv_mod.h
index cba91ea..f5fa322 100644
--- a/prov/psm3/psm3/psm_rndv_mod.h
+++ b/prov/psm3/psm3/psm_rndv_mod.h
@@ -170,6 +170,8 @@ static inline uint16_t psm2_rv_get_gpu_user_minor_bldtime_version(void)
 {
 	return RV_GPU_ABI_VER_MINOR;
 }
+
+extern uint64_t __psm2_min_gpu_bar_size(void);
 #endif
 
 extern psm2_rv_t __psm2_rv_open(const char *devname, struct local_info *loc_info);
@@ -214,18 +216,18 @@ extern int __psm2_rv_dereg_mem(psm2_rv_t rv, psm2_rv_mr_t mr);
 extern void * __psm2_rv_pin_and_mmap(psm2_rv_t rv, uintptr_t pageaddr,
 			uint64_t pagelen, int access);
 
-extern int __psm2_rv_evict_exact(psm2_rv_t rv, void *addr,
+extern int64_t __psm2_rv_evict_exact(psm2_rv_t rv, void *addr,
 			uint64_t length, int access);
 
-extern int __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length);
+extern int64_t __psm2_rv_evict_range(psm2_rv_t rv, void *addr, uint64_t length);
 
-extern int __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count);
+extern int64_t __psm2_rv_evict_amount(psm2_rv_t rv, uint64_t bytes, uint32_t count);
 
 #ifdef PSM_CUDA
-extern int __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr,
+extern int64_t __psm2_rv_evict_gpu_range(psm2_rv_t rv, uintptr_t addr,
 			uint64_t length);
 
-extern int __psm2_rv_evict_gpu_amount(psm2_rv_t rv, uint64_t bytes,
+extern int64_t __psm2_rv_evict_gpu_amount(psm2_rv_t rv, uint64_t bytes,
 			uint32_t count);
 #endif
 
diff --git a/prov/psm3/psm3/psm_user.h b/prov/psm3/psm3/psm_user.h
index 3372fce..d376515 100644
--- a/prov/psm3/psm3/psm_user.h
+++ b/prov/psm3/psm3/psm_user.h
@@ -646,6 +646,7 @@ _psmi_is_gdr_copy_enabled())
 #define PSMI_IS_GDR_COPY_ENABLED _psmi_is_gdr_copy_enabled()
 
 #define PSMI_IS_CUDA_MEM(p) _psmi_is_cuda_mem(p)
+extern void psm2_get_gpu_bars(void);
 
 struct ips_cuda_hostbuf {
 	STAILQ_ENTRY(ips_cuda_hostbuf) req_next;
diff --git a/prov/psm3/psm3/psm_verbs_mr.c b/prov/psm3/psm3/psm_verbs_mr.c
index a5be642..bb77cbb 100644
--- a/prov/psm3/psm3/psm_verbs_mr.c
+++ b/prov/psm3/psm3/psm_verbs_mr.c
@@ -353,9 +353,9 @@ static uint64_t mr_cache_rv_gpu_size(void *context)
 		return cache->rv_gpu_stats.stat; \
     }
 
-CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_reg, max_cache_size_reg/MEGABYTE)
-CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_mmap, max_cache_size_mmap/MEGABYTE)
-CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_both, max_cache_size_both/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_reg, cache_size_reg/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_mmap, cache_size_mmap/MEGABYTE)
+CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_size_both, cache_size_both/MEGABYTE)
 CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size, max_cache_size/MEGABYTE)
 CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size_reg, max_cache_size_reg/MEGABYTE)
 CACHE_RV_GPU_STAT_FUNC(mr_cache_rv_gpu_max_size_mmap, max_cache_size_mmap/MEGABYTE)
@@ -514,12 +514,12 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 		// TBD - could make this a warning and set limit_inuse_bytes=0
 		// then depend on transfers queuing and retrying until
 		// reg_mr cache space is available
-		if (ep->rv_mr_cache_size*MEGABYTE < pri_size) {
+		if ((uint64_t)ep->rv_mr_cache_size*MEGABYTE < pri_size) {
 			_HFI_ERROR("PSM3_RV_MR_CACHE_SIZE=%u too small, require >= %"PRIu64"\n",
 				ep->rv_mr_cache_size, (pri_size + MEGABYTE-1)/MEGABYTE);
 			return NULL;
 		}
-		cache->limit_inuse_bytes = ep->rv_mr_cache_size*MEGABYTE - pri_size;
+		cache->limit_inuse_bytes = (uint64_t)ep->rv_mr_cache_size*MEGABYTE - pri_size;
 #ifdef PSM_CUDA
 		if (PSMI_IS_CUDA_ENABLED) {
 			// For GPU, due to GdrCopy, we can't undersize cache.
@@ -528,12 +528,21 @@ psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 			// retrying indefinitely.  If we want to allow undersize
 			// GPU cache, we need to have gdrcopy pin/mmap failures
 			// also invoke progress functions to release MRs
-			if (ep->rv_gpu_cache_size*MEGABYTE < gpu_pri_size) {
+			if (__psm2_min_gpu_bar_size()) {
+				uint64_t max_recommend = __psm2_min_gpu_bar_size() - 32*MEGABYTE;
+				if ((uint64_t)ep->rv_gpu_cache_size*MEGABYTE >= max_recommend) {
+					_HFI_INFO("Warning: PSM3_RV_GPU_CACHE_SIZE=%u too large for smallest GPU's BAR size of %"PRIu64" (< %"PRIu64" total of endpoint-rail-qp recommended)\n",
+						ep->rv_gpu_cache_size,
+						(__psm2_min_gpu_bar_size() + MEGABYTE-1)/MEGABYTE,
+						max_recommend/MEGABYTE);
+				}
+			}
+			if ((uint64_t)ep->rv_gpu_cache_size*MEGABYTE < gpu_pri_size) {
 				_HFI_ERROR("PSM3_RV_GPU_CACHE_SIZE=%u too small, require >= %"PRIu64"\n",
 					ep->rv_gpu_cache_size, (gpu_pri_size + MEGABYTE-1)/MEGABYTE);
 				return NULL;
 			}
-			cache->limit_gpu_inuse_bytes = ep->rv_gpu_cache_size*MEGABYTE - gpu_pri_size;
+			cache->limit_gpu_inuse_bytes = (uint64_t)ep->rv_gpu_cache_size*MEGABYTE - gpu_pri_size;
 		}
 		_HFI_MMDBG("CPU cache %u GPU cache %u\n", ep->rv_mr_cache_size, ep->rv_gpu_cache_size);
 #endif
@@ -796,51 +805,129 @@ static inline int have_space(psm2_mr_cache_t cache, uint64_t length, int access)
 // it's multi-rail/mult-QP EPs.
 // When it hits the last rail of the last user opened EP, it goes back to
 // the 1st rail of the 1st user opened EP.
+// caller must hold creation_lock
 static psm2_ep_t next_ep(psm2_ep_t ep)
 {
-	//mctxt_next is the circular list of rails/QPs in a given user EP
-	//mctxt_master is the 1st in the list, when we get back to the 1st
-	//go to the next user EP
-	ep = ep->mctxt_next;
-	if (ep->mctxt_master != ep)
-		return ep;
-	//user_ep_next is a linked list of user opened EPs.  End of list is NULL
-	//when hit end of list, go back to 1st (psmi_opened_endpoint)
-	//for each user opened EP, the entry on this list is the 1st rail within
-	//the EP
-	ep = ep->user_ep_next;
-	if (ep)
-		return ep;
+       //mctxt_next is the circular list of rails/QPs in a given user EP
+       //mctxt_master is the 1st in the list, when we get back to the 1st
+       //go to the next user EP
+       ep = ep->mctxt_next;
+       if (ep->mctxt_master != ep)
+               return ep;
+       //user_ep_next is a linked list of user opened EPs.  End of list is NULL
+       //when hit end of list, go back to 1st (psmi_opened_endpoint)
+       //for each user opened EP, the entry on this list is the 1st rail within
+       //the EP
+       ep = ep->user_ep_next;
+       if (ep)
+               return ep;
+       else
+               return psmi_opened_endpoint;
+}
+
+// determine if ep is still valid (can't dereference or trust ep given)
+// caller must hold creation_lock
+static int valid_ep(psm2_ep_t ep)
+{
+	psm2_ep_t e1 = psmi_opened_endpoint;
+
+	while (e1) {	// user opened ep's - linear list ending in NULL
+		psm2_ep_t e2 = e1;
+		//check mtcxt list (multi-rail within user opened ep)
+		do {
+			if (e2 == ep)
+				return 1;
+			e2 = e2->mctxt_next;
+		} while (e2 != e1);	// circular list
+		e1 = e1->user_ep_next;
+	}
+	return 0;	// not found
+}
+
+// advance ep to the next.  However it's possible ep is stale and
+// now closed/freed, so make sure it's good.  good_ep is at least one
+// known good_ep and lets us avoid search some of the time (or if only 1 EP)
+// caller must hold creation_lock
+static psm2_ep_t next_valid_ep(psm2_ep_t ep, psm2_ep_t good_ep)
+{
+	if (ep == good_ep || valid_ep(ep))
+		return next_ep(ep);
 	else
-		return psmi_opened_endpoint;
+		return good_ep;
 }
 
 /*
  * Evict some space in given cache (only GPU needs this)
- * We evict first from the ep we failed to pin memory in, but then we
- * rotate among all eps for eviction if additional attempts fail.
  * If nvidia_p2p_get_pages reports out of BAR space (perhaps prematurely),
- * we need to evict from other EPs too
- * length - amount attempted in pin which just failed
+ * we need to evict from other EPs too.
+ * So we rotate among all eps (rails or QPs) in our user opened EP for eviction.
+ * length - amount attempted in pin/register which just failed
  * access - indicates if IS_GPU_ADDR or not (rest ignored)
  * returns:
- * 	0 if some evicted
+ * 	>0 bytes evicted if some evicted
  * 	-1 if nothing evicted (errno == ENOENT means nothing evictable found)
  * 	ENOENT also used when access is not for GPU
+ * The caller will have the progress_lock, we need the creation_lock
+ * to walk the list of EPs outside our own MQ.  However creation_lock
+ * is above the progress_lock in lock heirarchy, so we use a LOCK_TRY
+ * to avoid deadlock in the rare case where another thread
+ * has creation_lock and is trying to get progress_lock (such as during
+ * open_ep, close_ep or rcvthread).
  */
-int psm2_verbs_evict_some(psm2_ep_t ep, uint64_t length, int access)
+int64_t psm2_verbs_evict_some(psm2_ep_t ep, uint64_t length, int access)
 {
-	static psm2_ep_t last_evict_ep;
+	static __thread psm2_ep_t last_evict_ep;	// among all eps
+	static __thread psm2_ep_t last_evict_myuser_ep;	// in my user ep
+	int64_t evicted = 0;
+	int ret;
 
 	if (! (access & IBV_ACCESS_IS_GPU_ADDR)) {
 		errno = ENOENT;
 		return -1;	// only need evictions on GPU addresses
 	}
-	if (ep == last_evict_ep)
-		ep = next_ep(ep);
-	else
+	if (! last_evict_ep) {	// first call only
 		last_evict_ep = ep;
-	return __psm2_rv_evict_gpu_amount(ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+		last_evict_myuser_ep = ep;
+	}
+	// 1st try to evict from 1st rail/QP in our opened EP (gdrcopy and MRs)
+	ret = __psm2_rv_evict_gpu_amount(ep->mctxt_master->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+	if (ret > 0)
+		evicted = ret;
+
+	// next rotate among other rails/QPs in our opened ep (MRs)
+	last_evict_myuser_ep = last_evict_myuser_ep->mctxt_next;
+	if (last_evict_myuser_ep != ep->mctxt_master) {
+		ret = __psm2_rv_evict_gpu_amount(last_evict_myuser_ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+		if (ret > 0)
+			evicted += ret;
+	}
+	if (evicted >= length)
+		return evicted;
+
+	// now try other opened EPs
+	if (PSMI_LOCK_TRY(psmi_creation_lock))
+		goto done;
+	// last_evict_ep could point to an ep which has since been closed/freed
+ 	last_evict_ep = next_valid_ep(last_evict_ep, ep);
+	if (last_evict_ep->mctxt_master != ep->mctxt_master) {
+		if (!PSMI_LOCK_TRY(last_evict_ep->mq->progress_lock)) {
+			ret = __psm2_rv_evict_gpu_amount(last_evict_ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+			PSMI_UNLOCK(last_evict_ep->mq->progress_lock);
+			if (ret > 0)
+				evicted += ret;
+		}
+	} else {
+		ret = __psm2_rv_evict_gpu_amount(last_evict_ep->verbs_ep.rv, max(gpu_cache_evict, length), 0);
+		if (ret > 0 )
+			evicted += ret;
+	}
+	PSMI_UNLOCK(psmi_creation_lock);
+done:
+	if (! evicted) {
+		errno = ENOENT;
+		return -1;
+	}
+	return evicted;
 }
 #endif
 #endif
@@ -1146,6 +1233,7 @@ void psm2_verbs_free_mr_cache(psm2_mr_cache_t cache)
 			if (mrc->refcount) {
 				_HFI_ERROR("unreleased MR in psm2_verbs_free_mr_cache addr %p len %"PRIu64" access 0x%x\n",
 					mrc->addr, mrc->length, mrc->access);
+				return; // leak the rest, let process exit cleanup
 			}
 			mrc->refcount = 0;
 			cl_map_item_t *p_item = container_of(mrc, cl_map_item_t, payload);
diff --git a/prov/psm3/psm3/psm_verbs_mr.h b/prov/psm3/psm3/psm_verbs_mr.h
index 739a752..98f4ada 100644
--- a/prov/psm3/psm3/psm_verbs_mr.h
+++ b/prov/psm3/psm3/psm_verbs_mr.h
@@ -154,7 +154,7 @@ extern psm2_mr_cache_t psm2_verbs_alloc_mr_cache(psm2_ep_t ep,
 extern int psm2_verbs_mr_cache_allows_user_mr(psm2_mr_cache_t cache);
 
 #ifdef PSM_CUDA
-extern int psm2_verbs_evict_some(psm2_ep_t ep, uint64_t length, int access);
+extern int64_t psm2_verbs_evict_some(psm2_ep_t ep, uint64_t length, int access);
 #endif
 
 // pd can be the verbs_ep.pd or NULL to use the RV module's kernel pd
diff --git a/prov/psm3/psm3/ptl.h b/prov/psm3/psm3/ptl.h
index d056aaf..0c2f148 100644
--- a/prov/psm3/psm3/ptl.h
+++ b/prov/psm3/psm3/ptl.h
@@ -158,6 +158,8 @@ struct ptl_strategy_stats {
 
 	uint64_t tiny_cpu_recv;
 	uint64_t tiny_cpu_recv_bytes;
+	uint64_t tiny_sysbuf_recv;	/* to unexpected Q sysbuf */ /* incl 0 byte */
+	uint64_t tiny_sysbuf_recv_bytes;
 #ifdef PSM_CUDA
 	uint64_t tiny_gdrcopy_recv;
 	uint64_t tiny_gdrcopy_recv_bytes;
@@ -193,6 +195,8 @@ struct ptl_strategy_stats {
 
 	uint64_t short_cpu_recv;
 	uint64_t short_cpu_recv_bytes;
+	uint64_t short_sysbuf_recv;	/* to unexpected Q sysbuf */
+	uint64_t short_sysbuf_recv_bytes;
 #ifdef PSM_CUDA
 	uint64_t short_gdrcopy_recv;
 	uint64_t short_gdrcopy_recv_bytes;
@@ -204,6 +208,8 @@ struct ptl_strategy_stats {
 	uint64_t eager_copy_cpu_isend_bytes;
 	uint64_t eager_dma_cpu_isend;
 	uint64_t eager_dma_cpu_isend_bytes;
+	uint64_t eager_sysbuf_recv;	/* to unexpected Q sysbuf */
+	uint64_t eager_sysbuf_recv_bytes;
 #ifdef PSM_CUDA
 	uint64_t eager_cuCopy_isend;
 	uint64_t eager_cuCopy_isend_bytes;
@@ -221,12 +227,12 @@ struct ptl_strategy_stats {
 	uint64_t eager_gdr_send_bytes;
 #endif
 
-	uint64_t eager_cpu_recv;	/* packet count */
+	uint64_t eager_cpu_recv;
 	uint64_t eager_cpu_recv_bytes;
 #ifdef PSM_CUDA
-	uint64_t eager_gdrcopy_recv;	/* packet count */
+	uint64_t eager_gdrcopy_recv;
 	uint64_t eager_gdrcopy_recv_bytes;
-	uint64_t eager_cuCopy_recv;	/* packet count */
+	uint64_t eager_cuCopy_recv;
 	uint64_t eager_cuCopy_recv_bytes;
 #endif
 
@@ -243,30 +249,45 @@ struct ptl_strategy_stats {
 	uint64_t rndv_gpu_send_bytes;
 #endif
 
+	/* Payload in RTS for small sync send */
+	uint64_t rndv_rts_cpu_recv;
+	uint64_t rndv_rts_cpu_recv_bytes;
+	uint64_t rndv_rts_sysbuf_recv;
+	uint64_t rndv_rts_sysbuf_recv_bytes;
+#ifdef PSM_CUDA
+	uint64_t rndv_rts_cuCopy_recv;
+	uint64_t rndv_rts_cuCopy_recv_bytes;
+#endif
+
+	/* Payload in RTS approach used by sender */
+	/* this approach uses a LONG DATA CTS, but sends no more data */
+	uint64_t rndv_rts_copy_cpu_send;	/* per CTS  (1 per RTS) */
+	uint64_t rndv_rts_copy_cpu_send_bytes;
+
 	/* LONG DATA approach selected by receiver */
 	uint64_t rndv_long_cpu_recv;	/* per RTS */
 	uint64_t rndv_long_cpu_recv_bytes;
 	uint64_t rndv_long_gpu_recv;	/* per RTS */
 	uint64_t rndv_long_gpu_recv_bytes;
 #ifdef PSM_CUDA
-	uint64_t rndv_long_cuCopy_recv;	/* packet count */
+	uint64_t rndv_long_cuCopy_recv;
 	uint64_t rndv_long_cuCopy_recv_bytes;
-	uint64_t rndv_long_gdr_recv;	/* packet count */
+	uint64_t rndv_long_gdr_recv;
 	uint64_t rndv_long_gdr_recv_bytes;
 #endif
 
 	/* LONG DATA approach used by sender after LONG selected by receiver */
 	/* LONG DATA only uses 1 CTS per RTS */
-	uint64_t rndv_long_copy_cpu_send;	/* per CTS */
+	uint64_t rndv_long_copy_cpu_send;	/* per CTS  (1 per RTS) */
 	uint64_t rndv_long_copy_cpu_send_bytes;
-	uint64_t rndv_long_dma_cpu_send;	/* per CTS */
+	uint64_t rndv_long_dma_cpu_send;	/* per CTS  (1 per RTS) */
 	uint64_t rndv_long_dma_cpu_send_bytes;
 #ifdef PSM_CUDA
-	uint64_t rndv_long_cuCopy_send;	/* per CTS */
+	uint64_t rndv_long_cuCopy_send;	/* per CTS  (1 per RTS) */
 	uint64_t rndv_long_cuCopy_send_bytes;
-	uint64_t rndv_long_gdrcopy_send;	/* per CTS */
+	uint64_t rndv_long_gdrcopy_send;	/* per CTS  (1 per RTS) */
 	uint64_t rndv_long_gdrcopy_send_bytes;
-	uint64_t rndv_long_gdr_send;	/* per CTS */	/* SDMA */
+	uint64_t rndv_long_gdr_send;	/* per CTS  (1 per RTS) */ /* SDMA */
 	uint64_t rndv_long_gdr_send_bytes;		/* SDMA */
 #endif
 
diff --git a/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c b/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c
index 81d8dcc..3990f9f 100644
--- a/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c
+++ b/prov/psm3/psm3/ptl_am/am_reqrep_shmem.c
@@ -2054,7 +2054,7 @@ amsh_mq_rndv(ptl_t *ptl, psm2_mq_t mq, psm2_mq_req_t req,
 	mq->stats.tx_num++;
 	mq->stats.tx_shm_num++;
 	mq->stats.tx_rndv_num++;
-	mq->stats.tx_rndv_bytes += len;
+	// tx_rndv_bytes tabulated when get CTS
 
 	return err;
 }
@@ -2112,6 +2112,7 @@ amsh_mq_send_inner_eager(psm2_mq_t mq, psm2_mq_req_t req, psm2_epaddr_t epaddr,
 
 	mq->stats.tx_num++;
 	mq->stats.tx_shm_num++;
+	mq->stats.tx_shm_bytes += len;
 	mq->stats.tx_eager_num++;
 	mq->stats.tx_eager_bytes += len;
 
diff --git a/prov/psm3/psm3/ptl_am/ptl.c b/prov/psm3/psm3/ptl_am/ptl.c
index dffd7f3..f3ee1c3 100644
--- a/prov/psm3/psm3/ptl_am/ptl.c
+++ b/prov/psm3/psm3/ptl_am/ptl.c
@@ -63,7 +63,7 @@
 #endif
 
 /* not reported yet, so just track in a global so can pass a pointer to
- * psmi_mq_handle_envelope
+ * psmi_mq_handle_envelope and psmi_mq_handle_rts
  */
 static struct ptl_strategy_stats strat_stats;
 
@@ -175,6 +175,11 @@ send_cts:
 		psmi_amsh_short_request((struct ptl *)ptl, epaddr, mq_handler_rtsmatch_hidx,
 					args, 5, NULL, 0, 0);
 
+	req->mq->stats.rx_user_num++;
+	req->mq->stats.rx_user_bytes += req->req_data.recv_msglen;
+	req->mq->stats.rx_shm_num++;
+	req->mq->stats.rx_shm_bytes += req->req_data.recv_msglen;
+
 	/* 0-byte completion or we used kassist */
 	if (pid || cma_succeed ||
 		req->req_data.recv_msglen == 0 || cuda_ipc_send_completion == 1) {
@@ -223,6 +228,10 @@ psmi_am_mq_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 		/* for eager matching */
 		req->ptl_req_ptr = (void *)tok->tok.epaddr_incoming;
 		req->msg_seqnum = 0;	/* using seqnum 0 */
+		req->mq->stats.rx_shm_num++;
+		// close enough, may not yet be matched,
+		//  don't know recv buf_len, so assume no truncation
+		req->mq->stats.rx_shm_bytes += msglen;
 		break;
 	default:{
 			void *sreq = (void *)(uintptr_t) args[3].u64w0;
@@ -230,7 +239,7 @@ psmi_am_mq_handler(void *toki, psm2_amarg_t *args, int narg, void *buf,
 			psmi_assert(narg == 5);
 			psmi_assert_always(opcode == MQ_MSG_LONGRTS);
 			rc = psmi_mq_handle_rts(tok->mq, tok->tok.epaddr_incoming,
-						&tag, msglen, NULL, 0, 1,
+						&tag, &strat_stats, msglen, NULL, 0, 1,
 						ptl_handle_rtsmatch, &req);
 
 			req->rts_peer = tok->tok.epaddr_incoming;
@@ -296,6 +305,8 @@ psmi_am_mq_handler_rtsmatch(void *toki, psm2_amarg_t *args, int narg, void *buf,
 	 */
 	if (sreq->cuda_ipc_handle_attached) {
 		sreq->cuda_ipc_handle_attached = 0;
+		sreq->mq->stats.tx_shm_bytes += sreq->req_data.send_msglen;
+		sreq->mq->stats.tx_rndv_bytes += sreq->req_data.send_msglen;
 		psmi_mq_handle_rts_complete(sreq);
 		return;
 	}
@@ -341,6 +352,8 @@ no_kassist:
 					     1, sreq->req_data.buf, msglen, dest, 0);
 		}
 	}
+	sreq->mq->stats.tx_shm_bytes += sreq->req_data.send_msglen;
+	sreq->mq->stats.tx_rndv_bytes += sreq->req_data.send_msglen;
 	psmi_mq_handle_rts_complete(sreq);
 }
 
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto.c b/prov/psm3/psm3/ptl_ips/ips_proto.c
index dad5ec9..f001979 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto.c
@@ -440,6 +440,10 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 					   &proto->strat_stats.tiny_cpu_recv),
 			PSMI_STATS_DECLU64("tiny_cpu_recv_bytes",
 					   &proto->strat_stats.tiny_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("tiny_sysbuf_recv",
+					   &proto->strat_stats.tiny_sysbuf_recv),
+			PSMI_STATS_DECLU64("tiny_sysbuf_recv_bytes",
+					   &proto->strat_stats.tiny_sysbuf_recv_bytes),
 #ifdef PSM_CUDA
 			PSMI_STATS_DECLU64("tiny_gdrcopy_recv",
 					   &proto->strat_stats.tiny_gdrcopy_recv),
@@ -500,6 +504,10 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 					   &proto->strat_stats.short_cpu_recv),
 			PSMI_STATS_DECLU64("short_cpu_recv_bytes",
 					   &proto->strat_stats.short_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("short_sysbuf_recv",
+					   &proto->strat_stats.short_sysbuf_recv),
+			PSMI_STATS_DECLU64("short_sysbuf_recv_bytes",
+					   &proto->strat_stats.short_sysbuf_recv_bytes),
 #ifdef PSM_CUDA
 			PSMI_STATS_DECLU64("short_gdrcopy_recv",
 					   &proto->strat_stats.short_gdrcopy_recv),
@@ -552,6 +560,10 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 					   &proto->strat_stats.eager_cpu_recv),
 			PSMI_STATS_DECLU64("eager_cpu_recv_bytes",
 					   &proto->strat_stats.eager_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("eager_sysbuf_recv",
+					   &proto->strat_stats.eager_sysbuf_recv),
+			PSMI_STATS_DECLU64("eager_sysbuf_recv_bytes",
+					   &proto->strat_stats.eager_sysbuf_recv_bytes),
 #ifdef PSM_CUDA
 			PSMI_STATS_DECLU64("eager_gdrcopy_recv",
 					   &proto->strat_stats.eager_gdrcopy_recv),
@@ -584,6 +596,25 @@ ips_proto_init(const psmi_context_t *context, const ptl_t *ptl,
 					   &proto->strat_stats.rndv_gpu_send_bytes),
 #endif
 
+			PSMI_STATS_DECLU64("rndv_rts_cpu_recv",
+					   &proto->strat_stats.rndv_rts_cpu_recv),
+			PSMI_STATS_DECLU64("rndv_rts_cpu_recv_bytes",
+					   &proto->strat_stats.rndv_rts_cpu_recv_bytes),
+			PSMI_STATS_DECLU64("rndv_rts_sysbuf_recv",
+					   &proto->strat_stats.rndv_rts_sysbuf_recv),
+			PSMI_STATS_DECLU64("rndv_rts_sysbuf_recv_bytes",
+					   &proto->strat_stats.rndv_rts_sysbuf_recv_bytes),
+#ifdef PSM_CUDA
+			PSMI_STATS_DECLU64("rndv_rts_cuCopy_recv",
+					   &proto->strat_stats.rndv_rts_cuCopy_recv),
+			PSMI_STATS_DECLU64("rndv_rts_cuCopy_recv_bytes",
+					   &proto->strat_stats.rndv_rts_cuCopy_recv_bytes),
+#endif
+			PSMI_STATS_DECLU64("rndv_rts_copy_cpu_send",
+					   &proto->strat_stats.rndv_rts_copy_cpu_send),
+			PSMI_STATS_DECLU64("rndv_rts_copy_cpu_send_bytes",
+					   &proto->strat_stats.rndv_rts_copy_cpu_send_bytes),
+
 			PSMI_STATS_DECLU64("rndv_long_cpu_recv",
 					   &proto->strat_stats.rndv_long_cpu_recv),
 			PSMI_STATS_DECLU64("rndv_long_cpu_recv_bytes",
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_expected.c b/prov/psm3/psm3/ptl_ips/ips_proto_expected.c
index abe8397..643a271 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_expected.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_expected.c
@@ -1882,6 +1882,10 @@ psm2_error_t ips_tid_send_exp(struct ips_tid_send_desc *tidsendc)
 			}
 			psmi_cuda_run_prefetcher(protoexp, tidsendc);
 		}
+		/* Clean Up tidsendc ref's to split cuda hostbufs when no longer needed */
+		tidsendc->cuda_num_buf = 0;
+		tidsendc->cuda_hostbuf[0] = NULL;
+		tidsendc->cuda_hostbuf[1] = NULL;
 	}
 #endif
 	err = ips_tid_issue_rdma_write(tidsendc);
diff --git a/prov/psm3/psm3/ptl_ips/ips_proto_mq.c b/prov/psm3/psm3/ptl_ips/ips_proto_mq.c
index 7192543..15256d4 100644
--- a/prov/psm3/psm3/ptl_ips/ips_proto_mq.c
+++ b/prov/psm3/psm3/ptl_ips/ips_proto_mq.c
@@ -450,6 +450,7 @@ ips_ptl_mq_rndv(struct ips_proto *proto, psm2_mq_req_t req,
 		ips_scb_buffer(scb) = (void *)buf;
 		ips_scb_length(scb) = len;
 		req->send_msgoff = len;
+		req->mq->stats.tx_rndv_bytes += len;
 	} else {
 		ips_scb_length(scb) = 0;
 		req->send_msgoff = 0;
@@ -990,6 +991,10 @@ do_rendezvous:
 		}
 #endif
 
+		mq->stats.tx_num++;
+		mq->stats.tx_rndv_num++;
+		// we count tx_rndv_bytes as we get CTS
+
 		err = ips_ptl_mq_rndv(proto, req, ipsaddr, ubuf, len);
 		*req_o = req;
 		return err;
@@ -1344,6 +1349,10 @@ do_rendezvous:
 		}
 #endif
 
+		mq->stats.tx_num++;
+		mq->stats.tx_rndv_num++;
+		// we count tx_rndv_bytes as we get CTS
+
 		err = ips_ptl_mq_rndv(proto, req, ipsaddr, ubuf, len);
 		if (err != PSM2_OK)
 			return err;
@@ -1383,6 +1392,8 @@ ips_proto_mq_rts_match_callback(psm2_mq_req_t req, int was_posted)
 	_HFI_MMDBG("rts_match_callback\n");
 	// while matching RTS we set both recv and send msglen to min of the two
 	psmi_assert(req->req_data.recv_msglen == req->req_data.send_msglen);
+	req->mq->stats.rx_user_num++;
+	req->mq->stats.rx_user_bytes += req->req_data.recv_msglen;
 #ifdef PSM_CUDA
 	/* Cases where we do not use TIDs:
 	 * 0) Received full message as payload to RTS, CTS is just an ack
@@ -1425,17 +1436,20 @@ ips_proto_mq_rts_match_callback(psm2_mq_req_t req, int was_posted)
 			ips_epaddr_connected((ips_epaddr_t *) epaddr),
 			epaddr, proto->protoexp != NULL);
 
+		if (req->recv_msgoff < req->req_data.recv_msglen) {
+			// RTS did not have the message as payload
 #ifdef PSM_CUDA
-		if (req->is_buf_gpu_mem) {
-			proto->strat_stats.rndv_long_gpu_recv++;
-			proto->strat_stats.rndv_long_gpu_recv_bytes += req->req_data.recv_msglen;
-		} else {
+			if (req->is_buf_gpu_mem) {
+				proto->strat_stats.rndv_long_gpu_recv++;
+				proto->strat_stats.rndv_long_gpu_recv_bytes += req->req_data.recv_msglen;
+			} else {
 #endif
-			proto->strat_stats.rndv_long_cpu_recv++;
-			proto->strat_stats.rndv_long_cpu_recv_bytes += req->req_data.recv_msglen;
+				proto->strat_stats.rndv_long_cpu_recv++;
+				proto->strat_stats.rndv_long_cpu_recv_bytes += req->req_data.recv_msglen;
 #ifdef PSM_CUDA
-		}
+			}
 #endif
+		}
 		if (ips_proto_mq_push_cts_req(proto, req) != PSM2_OK) {
 			struct ips_pend_sends *pends = &proto->pend_sends;
 			struct ips_pend_sreq *sreq =
@@ -1800,6 +1814,7 @@ ips_proto_mq_handle_cts(struct ips_recvhdrq_event *rcv_ev)
 		/* ptl_req_ptr will be set to each tidsendc */
 		if (req->ptl_req_ptr == NULL) {
 			req->req_data.send_msglen = p_hdr->data[1].u32w1;
+			req->mq->stats.tx_rndv_bytes += req->req_data.send_msglen;
 		}
 		psmi_assert(req->req_data.send_msglen == p_hdr->data[1].u32w1);
 
@@ -1846,10 +1861,11 @@ ips_proto_mq_handle_cts(struct ips_recvhdrq_event *rcv_ev)
 // TBD - should cleanup from pin as needed
 			/* already sent enough bytes, may truncate so using >= */
 			/* RTS payload is only used for CPU memory */
-			proto->strat_stats.rndv_long_copy_cpu_send++;
-			proto->strat_stats.rndv_long_copy_cpu_send_bytes += req->req_data.send_msglen;
+			proto->strat_stats.rndv_rts_copy_cpu_send++;
+			proto->strat_stats.rndv_rts_copy_cpu_send_bytes += req->req_data.send_msglen;
 			ips_proto_mq_rv_complete(req);
 		} else {
+			req->mq->stats.tx_rndv_bytes += (req->req_data.send_msglen - req->send_msgoff);
 #ifdef RNDV_MOD
 			// If we have an MR due to incorrect prediction of RDMA
 			// release it if can't be used for send DMA or don't
@@ -1956,6 +1972,7 @@ ips_proto_mq_handle_rts(struct ips_recvhdrq_event *rcv_ev)
 				    (psm2_epaddr_t) &ipsaddr->msgctl->
 				    master_epaddr,
 				    (psm2_mq_tag_t *) p_hdr->tag,
+				    &rcv_ev->proto->strat_stats,
 				    p_hdr->data[1].u32w1, payload, paylen,
 				    msgorder, ips_proto_mq_rts_match_callback,
 				    &req);
@@ -2244,26 +2261,37 @@ ips_proto_mq_handle_eager(struct ips_recvhdrq_event *rcv_ev)
 		 * error is caught below.
 		 */
 		if (req) {
+			//u32w0 is offset - only cnt recv msgs on 1st pkt in msg
 #ifdef PSM_CUDA
 			int use_gdrcopy = 0;
 			if (!req->is_buf_gpu_mem) {
-				rcv_ev->proto->strat_stats.eager_cpu_recv++;
-				rcv_ev->proto->strat_stats.eager_cpu_recv_bytes +=  req->req_data.send_msglen;
+				if (req->state == MQ_STATE_UNEXP) {
+					if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_sysbuf_recv++;
+					rcv_ev->proto->strat_stats.eager_sysbuf_recv_bytes += paylen;
+				} else {
+					if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_cpu_recv++;
+					rcv_ev->proto->strat_stats.eager_cpu_recv_bytes += paylen;
+				}
 			} else if (PSMI_USE_GDR_COPY_RECV(paylen)) {
 				use_gdrcopy = 1;
-				rcv_ev->proto->strat_stats.eager_gdrcopy_recv++;
-				rcv_ev->proto->strat_stats.eager_gdrcopy_recv_bytes +=  req->req_data.send_msglen;
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_gdrcopy_recv++;
+				rcv_ev->proto->strat_stats.eager_gdrcopy_recv_bytes += paylen;
 			} else {
-				rcv_ev->proto->strat_stats.eager_cuCopy_recv++;
-				rcv_ev->proto->strat_stats.eager_cuCopy_recv_bytes +=  req->req_data.send_msglen;
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_cuCopy_recv++;
+				rcv_ev->proto->strat_stats.eager_cuCopy_recv_bytes += paylen;
 			}
 			psmi_mq_handle_data(mq, req,
 				p_hdr->data[1].u32w0, payload, paylen,
 				use_gdrcopy,
 				rcv_ev->proto->ep);
 #else
-			rcv_ev->proto->strat_stats.eager_cpu_recv++;
-			rcv_ev->proto->strat_stats.eager_cpu_recv_bytes +=  req->req_data.send_msglen;
+			if (req->state == MQ_STATE_UNEXP) {
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_sysbuf_recv++;
+				rcv_ev->proto->strat_stats.eager_sysbuf_recv_bytes += paylen;
+			} else {
+				if (p_hdr->data[1].u32w0<4) rcv_ev->proto->strat_stats.eager_cpu_recv++;
+				rcv_ev->proto->strat_stats.eager_cpu_recv_bytes += paylen;
+			}
 			psmi_mq_handle_data(mq, req,
 				p_hdr->data[1].u32w0, payload, paylen);
 #endif // PSM_CUDA
@@ -2422,19 +2450,19 @@ ips_proto_mq_handle_data(struct ips_recvhdrq_event *rcv_ev)
 				req->req_data.buf = buf;
 				psmi_copy_tiny_fn = mq_copy_tiny_host_mem;
 				proto->strat_stats.rndv_long_gdr_recv++;
-				proto->strat_stats.rndv_long_gdr_recv_bytes += req->req_data.recv_msglen;
+				proto->strat_stats.rndv_long_gdr_recv_bytes += paylen;
 			} else {
 				proto->strat_stats.rndv_long_cuCopy_recv++;
-				proto->strat_stats.rndv_long_cuCopy_recv_bytes += req->req_data.recv_msglen;
+				proto->strat_stats.rndv_long_cuCopy_recv_bytes += paylen;
 			}
 		} else if (PSMI_USE_GDR_COPY_RECV(paylen)) {
 			// let mq_handle_data do the conversion
 			use_gdrcopy = 1;
-			proto->strat_stats.rndv_long_gdr_recv++;
-			proto->strat_stats.rndv_long_gdr_recv_bytes += req->req_data.recv_msglen;
+			//proto->strat_stats.rndv_long_gdr_recv++;
+			proto->strat_stats.rndv_long_gdr_recv_bytes += paylen;
 		} else {
-			proto->strat_stats.rndv_long_cuCopy_recv++;
-			proto->strat_stats.rndv_long_cuCopy_recv_bytes += req->req_data.recv_msglen;
+			if (p_hdr->data[1].u32w0 < 4) proto->strat_stats.rndv_long_cuCopy_recv++;
+			proto->strat_stats.rndv_long_cuCopy_recv_bytes += paylen;
 		}
 	}
 #endif
diff --git a/prov/psm3/psm3/ptl_self/ptl.c b/prov/psm3/psm3/ptl_self/ptl.c
index 4af6b0e..6d2fc2d 100644
--- a/prov/psm3/psm3/ptl_self/ptl.c
+++ b/prov/psm3/psm3/ptl_self/ptl.c
@@ -68,6 +68,11 @@ struct ptl_self {
 	ptl_ctl_t *ctl;
 } __attribute__((aligned(16)));
 
+/* not reported yet, so just track in a global so can pass a pointer to
+ * psmi_mq_handle_envelope and psmi_mq_handle_rts
+ */
+static struct ptl_strategy_stats strat_stats;
+
 static
 psm2_error_t
 ptl_handle_rtsmatch(psm2_mq_req_t recv_req, int was_posted)
@@ -79,12 +84,14 @@ ptl_handle_rtsmatch(psm2_mq_req_t recv_req, int was_posted)
 			       recv_req->req_data.recv_msglen);
 	}
 
+	recv_req->mq->stats.rx_user_num++;
+	recv_req->mq->stats.rx_user_bytes += recv_req->req_data.recv_msglen;
 	psmi_mq_handle_rts_complete(recv_req);
 
+	send_req->mq->stats.tx_rndv_bytes += send_req->req_data.send_msglen;
 	/* If the send is already marked complete, that's because it was internally
 	 * buffered. */
 	if (send_req->state == MQ_STATE_COMPLETE) {
-		psmi_mq_stats_rts_account(send_req);
 		if (send_req->req_data.buf != NULL && send_req->req_data.send_msglen > 0)
 			psmi_mq_sysbuf_free(send_req->mq, send_req->req_data.buf);
 		/* req was left "live" even though the sender was told that the
@@ -150,7 +157,10 @@ self_mq_isend(psm2_mq_t mq, psm2_epaddr_t epaddr, uint32_t flags_user,
 		send_req->is_buf_gpu_mem = 0;
 #endif
 
-	rc = psmi_mq_handle_rts(mq, epaddr, tag,
+	mq->stats.tx_num++;
+	mq->stats.tx_rndv_num++;
+
+	rc = psmi_mq_handle_rts(mq, epaddr, tag, &strat_stats,
 				len, NULL, 0, 1,
 				ptl_handle_rtsmatch, &recv_req);
 	send_req->req_data.tag = *tag;
diff --git a/prov/rxm/src/rxm.h b/prov/rxm/src/rxm.h
index 7bf589b..36e746d 100644
--- a/prov/rxm/src/rxm.h
+++ b/prov/rxm/src/rxm.h
@@ -64,6 +64,37 @@
 #define RXM_OP_VERSION		3
 #define RXM_CTRL_VERSION	4
 
+enum {
+	RXM_REJECT_UNSPEC,
+	RXM_REJECT_ECONNREFUSED,
+	RXM_REJECT_EALREADY,
+};
+
+union rxm_cm_data {
+	struct _connect {
+		uint8_t version;
+		uint8_t endianness;
+		uint8_t ctrl_version;
+		uint8_t op_version;
+		uint16_t port;
+		uint8_t padding[2];
+		uint32_t eager_limit;
+		uint32_t rx_size; /* used? */
+		uint64_t client_conn_id;
+	} connect;
+
+	struct _accept {
+		uint64_t server_conn_id;
+		uint32_t rx_size; /* used? */
+	} accept;
+
+	struct _reject {
+		uint8_t version;
+		uint8_t reason;
+	} reject;
+};
+
+
 extern size_t rxm_buffer_size;
 extern size_t rxm_packet_size;
 
@@ -139,141 +170,63 @@ extern int rxm_use_write_rndv;
 extern enum fi_wait_obj def_wait_obj, def_tcp_wait_obj;
 
 struct rxm_ep;
+struct rxm_av;
 
 
-/*
- * Connection Map
- */
-
-#define RXM_CMAP_IDX_BITS OFI_IDX_INDEX_BITS
-
-enum rxm_cmap_signal {
-	RXM_CMAP_UNSPEC,
-	RXM_CMAP_FREE,
-	RXM_CMAP_EXIT,
+enum rxm_cm_state {
+	RXM_CM_IDLE,
+	RXM_CM_CONNECTING,
+	RXM_CM_ACCEPTING,
+	RXM_CM_CONNECTED,
 };
 
-#define RXM_CM_STATES(FUNC)		\
-	FUNC(RXM_CMAP_IDLE),		\
-	FUNC(RXM_CMAP_CONNREQ_SENT),	\
-	FUNC(RXM_CMAP_CONNREQ_RECV),	\
-	FUNC(RXM_CMAP_CONNECTED),	\
-	FUNC(RXM_CMAP_SHUTDOWN),	\
-
-enum rxm_cmap_state {
-	RXM_CM_STATES(OFI_ENUM_VAL)
+enum {
+	RXM_CONN_INDEXED = BIT(0),
 };
 
-extern char *rxm_cm_state_str[];
-
-#define RXM_CM_UPDATE_STATE(handle, new_state)				\
-	do {								\
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "[CM] handle: "	\
-		       "%p %s -> %s\n",	handle,				\
-		       rxm_cm_state_str[handle->state],			\
-		       rxm_cm_state_str[new_state]);			\
-		handle->state = new_state;				\
-	} while (0)
-
-struct rxm_cmap_handle {
-	struct rxm_cmap *cmap;
-	enum rxm_cmap_state state;
-	/* Unique identifier for a connection. Can be exchanged with a peer
-	 * during connection setup and can later be used in a message header
-	 * to identify the source of the message (Used for FI_SOURCE, RNDV
-	 * protocol, etc.) */
-	uint64_t key;
-	uint64_t remote_key;
+/* There will be at most 1 peer address per AV entry.  There
+ * may be addresses that have not been inserted into the local
+ * AV, and have no matching entry.  This can occur if we are
+ * only receiving data from the remote rxm ep.
+ */
+struct rxm_peer_addr {
+	struct rxm_av *av;
 	fi_addr_t fi_addr;
-	struct rxm_cmap_peer *peer;
-};
-
-struct rxm_cmap_peer {
-	struct rxm_cmap_handle *handle;
-	struct dlist_entry entry;
-	uint8_t addr[];
-};
-
-struct rxm_cmap_attr {
-	void 				*name;
+	struct ofi_rbnode *node;
+	int index;
+	int refcnt;
+	union ofi_sock_ip addr;
 };
 
-struct rxm_cmap {
-	struct rxm_ep		*ep;
-	struct util_av		*av;
-
-	/* cmap handles that correspond to addresses in AV */
-	struct rxm_cmap_handle **handles_av;
-	size_t			num_allocated;
+struct rxm_peer_addr *rxm_get_peer(struct rxm_av *av, const void *addr);
+void rxm_put_peer(struct rxm_peer_addr *peer);
 
-	/* Store all cmap handles (inclusive of handles_av) in an indexer.
-	 * This allows reverse lookup of the handle using the index. */
-	struct indexer		handles_idx;
-
-	struct ofi_key_idx	key_idx;
-
-	struct dlist_entry	peer_list;
-	struct rxm_cmap_attr	attr;
-	pthread_t		cm_thread;
-	ofi_fastlock_acquire_t	acquire;
-	ofi_fastlock_release_t	release;
-	fastlock_t		lock;
-};
-
-enum rxm_cmap_reject_reason {
-	RXM_CMAP_REJECT_UNSPEC,
-	RXM_CMAP_REJECT_GENUINE,
-	RXM_CMAP_REJECT_SIMULT_CONN,
-};
-
-union rxm_cm_data {
-	struct _connect {
-		uint8_t version;
-		uint8_t endianness;
-		uint8_t ctrl_version;
-		uint8_t op_version;
-		uint16_t port;
-		uint8_t padding[2];
-		uint32_t eager_limit;
-		uint32_t rx_size;
-		uint64_t client_conn_id;
-	} connect;
+/* Each local rxm ep will have at most 1 connection to a single
+ * remote rxm ep.  A local rxm ep may not be connected to all
+ * remote rxm ep's.
+ */
+struct rxm_conn {
+	enum rxm_cm_state state;
+	struct rxm_peer_addr *peer;
+	struct fid_ep *msg_ep;
+	struct rxm_ep *ep;
 
-	struct _accept {
-		uint64_t server_conn_id;
-		uint32_t rx_size;
-	} accept;
+	/* Prior versions of libfabric did not guarantee that all connections
+	 * from the same peer would have the same conn_id.  For compatibility
+	 * we need to store the remote_index per connection, rather than with
+	 * the peer_addr.
+	 */
+	int remote_index;
+	uint8_t flags;
 
-	struct _reject {
-		uint8_t version;
-		uint8_t reason;
-	} reject;
+	struct dlist_entry deferred_entry;
+	struct dlist_entry deferred_tx_queue;
+	struct dlist_entry deferred_sar_msgs;
+	struct dlist_entry deferred_sar_segments;
+	struct dlist_entry loopback_entry;
 };
 
-int rxm_cmap_alloc_handle(struct rxm_cmap *cmap, fi_addr_t fi_addr,
-			  enum rxm_cmap_state state,
-			  struct rxm_cmap_handle **handle);
-struct rxm_cmap_handle *rxm_cmap_key2handle(struct rxm_cmap *cmap, uint64_t key);
-int rxm_cmap_update(struct rxm_cmap *cmap, const void *addr, fi_addr_t fi_addr);
-
-void rxm_cmap_process_reject(struct rxm_cmap *cmap,
-			     struct rxm_cmap_handle *handle,
-			     enum rxm_cmap_reject_reason cm_reject_reason);
-void rxm_cmap_process_shutdown(struct rxm_cmap *cmap,
-			       struct rxm_cmap_handle *handle);
-int rxm_cmap_connect(struct rxm_ep *rxm_ep, fi_addr_t fi_addr,
-		     struct rxm_cmap_handle *handle);
-void rxm_cmap_free(struct rxm_cmap *cmap);
-int rxm_cmap_alloc(struct rxm_ep *rxm_ep, struct rxm_cmap_attr *attr);
-int rxm_cmap_remove(struct rxm_cmap *cmap, int index);
-int rxm_msg_eq_progress(struct rxm_ep *rxm_ep);
-
-static inline struct rxm_cmap_handle *
-rxm_cmap_acquire_handle(struct rxm_cmap *cmap, fi_addr_t fi_addr)
-{
-	assert(fi_addr < cmap->num_allocated);
-	return cmap->handles_av[fi_addr];
-}
+void rxm_freeall_conns(struct rxm_ep *ep);
 
 struct rxm_fabric {
 	struct util_fabric util_fabric;
@@ -292,8 +245,29 @@ struct rxm_domain {
 	fastlock_t amo_bufpool_lock;
 };
 
+/* All peer addresses, whether they've been inserted into the AV
+ * or an endpoint has an active connection to it, are stored in
+ * the addr_map.  Peers are allocated from a buffer pool and
+ * assigned a local index using the pool.  All rxm endpoints
+ * maintain a connection array which is aligned with the peer_pool.
+ *
+ * We technically only need to store the index of each peer in
+ * the AV itself.  The 'util_av' could basically be replaced by
+ * an ofi_index_map.  However, too much of the existing code
+ * relies on the util_av existing and storing the AV addresses.
+ *
+ * A future cleanup would be to remove using the util_av and have the
+ * rxm_av implementation be independent.
+ */
+ struct rxm_av {
+	struct util_av util_av;
+	struct ofi_rbmap addr_map;
+	struct ofi_bufpool *peer_pool;
+	struct ofi_bufpool *conn_pool;
+};
+
 int rxm_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
-		struct fid_av **av, void *context);
+		struct fid_av **fid_av, void *context);
 
 struct rxm_mr {
 	struct fid_mr mr_fid;
@@ -622,21 +596,6 @@ struct rxm_recv_queue {
 	dlist_func_t		*match_unexp;
 };
 
-struct rxm_msg_eq_entry {
-	ssize_t			rd;
-	uint32_t		event;
-	/* Used for connection refusal */
-	void			*context;
-	struct fi_eq_err_entry	err_entry;
-	/* must stay at the bottom */
-	struct fi_eq_cm_entry	cm_entry;
-};
-
-#define RXM_MSG_EQ_ENTRY_SZ (sizeof(struct rxm_msg_eq_entry) + \
-			     sizeof(union rxm_cm_data))
-#define RXM_CM_ENTRY_SZ (sizeof(struct fi_eq_cm_entry) + \
-			 sizeof(union rxm_cm_data))
-
 ssize_t rxm_get_dyn_rbuf(struct ofi_cq_rbuf_entry *entry, struct iovec *iov,
 			 size_t *count);
 
@@ -663,12 +622,18 @@ struct rxm_ep {
 	struct util_ep 		util_ep;
 	struct fi_info 		*rxm_info;
 	struct fi_info 		*msg_info;
-	struct rxm_cmap		*cmap;
+
+	struct index_map	conn_idx_map;
+	struct dlist_entry	loopback_list;
+	union ofi_sock_ip	addr;
+
+	pthread_t		cm_thread;
 	struct fid_pep 		*msg_pep;
 	struct fid_eq 		*msg_eq;
+	struct fid_ep 		*srx_ctx;
+
 	struct fid_cq 		*msg_cq;
 	uint64_t		msg_cq_last_poll;
-	struct fid_ep 		*srx_ctx;
 	size_t 			comp_per_progress;
 	int			cq_eq_fairness;
 
@@ -690,7 +655,7 @@ struct rxm_ep {
 	struct ofi_bufpool	*tx_pool;
 	struct rxm_pkt		*inject_pkt;
 
-	struct dlist_entry	deferred_tx_conn_queue;
+	struct dlist_entry	deferred_queue;
 	struct dlist_entry	rndv_wait_list;
 
 	struct rxm_recv_queue	recv_queue;
@@ -701,17 +666,10 @@ struct rxm_ep {
 	struct rxm_rndv_ops	*rndv_ops;
 };
 
-struct rxm_conn {
-	/* This should stay at the top */
-	struct rxm_cmap_handle handle;
-
-	struct fid_ep *msg_ep;
+int rxm_start_listen(struct rxm_ep *ep);
+void rxm_stop_listen(struct rxm_ep *ep);
+void rxm_conn_progress(struct rxm_ep *ep);
 
-	struct dlist_entry deferred_conn_entry;
-	struct dlist_entry deferred_tx_queue;
-	struct dlist_entry sar_rx_msg_list;
-	struct dlist_entry sar_deferred_rx_msg_list;
-};
 
 extern struct fi_provider rxm_prov;
 extern struct fi_fabric_attr rxm_fabric_attr;
@@ -736,7 +694,6 @@ ssize_t rxm_handle_rx_buf(struct rxm_rx_buf *rx_buf);
 int rxm_endpoint(struct fid_domain *domain, struct fi_info *info,
 			  struct fid_ep **ep, void *context);
 
-int rxm_conn_cmap_alloc(struct rxm_ep *rxm_ep);
 void rxm_cq_write_error(struct util_cq *cq, struct util_cntr *cntr,
 			void *op_context, int err);
 void rxm_cq_write_error_all(struct rxm_ep *rxm_ep, int err);
@@ -789,17 +746,6 @@ rxm_atomic_send_respmsg(struct rxm_ep *rxm_ep, struct rxm_conn *conn,
 	return fi_sendmsg(conn->msg_ep, &msg, FI_COMPLETION);
 }
 
-static inline int rxm_needs_atomic_progress(const struct fi_info *info)
-{
-	return (info->caps & FI_ATOMIC) && info->domain_attr &&
-			info->domain_attr->data_progress == FI_PROGRESS_AUTO;
-}
-
-static inline struct rxm_conn *rxm_key2conn(struct rxm_ep *rxm_ep, uint64_t key)
-{
-	return (struct rxm_conn *)rxm_cmap_key2handle(rxm_ep->cmap, key);
-}
-
 void rxm_ep_progress_deferred_queue(struct rxm_ep *rxm_ep,
 				    struct rxm_conn *rxm_conn);
 
@@ -808,29 +754,32 @@ rxm_ep_alloc_deferred_tx_entry(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 			       enum rxm_deferred_tx_entry_type type);
 
 static inline void
-rxm_ep_enqueue_deferred_tx_queue(struct rxm_deferred_tx_entry *tx_entry)
+rxm_queue_deferred_tx(struct rxm_deferred_tx_entry *tx_entry,
+		      enum ofi_list_end list_end)
 {
-	if (dlist_empty(&tx_entry->rxm_conn->deferred_tx_queue))
-		dlist_insert_tail(&tx_entry->rxm_conn->deferred_conn_entry,
-				  &tx_entry->rxm_ep->deferred_tx_conn_queue);
-	dlist_insert_tail(&tx_entry->entry, &tx_entry->rxm_conn->deferred_tx_queue);
+	struct rxm_conn *conn = tx_entry->rxm_conn;
+
+	if (dlist_empty(&conn->deferred_tx_queue))
+		dlist_insert_tail(&conn->deferred_entry,
+				  &conn->ep->deferred_queue);
+	if (list_end == OFI_LIST_HEAD) {
+		dlist_insert_head(&tx_entry->entry,
+				  &conn->deferred_tx_queue);
+	} else  {
+		dlist_insert_tail(&tx_entry->entry,
+				  &conn->deferred_tx_queue);
+	}
 }
 
 static inline void
-rxm_ep_enqueue_deferred_tx_queue_priority(struct rxm_deferred_tx_entry *tx_entry)
+rxm_dequeue_deferred_tx(struct rxm_deferred_tx_entry *tx_entry)
 {
-	if (dlist_empty(&tx_entry->rxm_conn->deferred_tx_queue))
-		dlist_insert_head(&tx_entry->rxm_conn->deferred_conn_entry,
-				  &tx_entry->rxm_ep->deferred_tx_conn_queue);
-	dlist_insert_head(&tx_entry->entry, &tx_entry->rxm_conn->deferred_tx_queue);
-}
+	struct rxm_conn *conn = tx_entry->rxm_conn;
 
-static inline void
-rxm_ep_dequeue_deferred_tx_queue(struct rxm_deferred_tx_entry *tx_entry)
-{
-	dlist_remove_init(&tx_entry->entry);
-	if (dlist_empty(&tx_entry->rxm_conn->deferred_tx_queue))
-		dlist_remove(&tx_entry->rxm_conn->deferred_conn_entry);
+	assert(!dlist_empty(&conn->deferred_tx_queue));
+	dlist_remove(&tx_entry->entry);
+	if (dlist_empty(&conn->deferred_tx_queue))
+		dlist_remove_init(&conn->deferred_entry);
 }
 
 int rxm_conn_process_eq_events(struct rxm_ep *rxm_ep);
@@ -891,7 +840,7 @@ rxm_ep_format_tx_buf_pkt(struct rxm_conn *rxm_conn, size_t len, uint8_t op,
 			 uint64_t data, uint64_t tag, uint64_t flags,
 			 struct rxm_pkt *pkt)
 {
-	pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
+	pkt->ctrl_hdr.conn_id = rxm_conn->remote_index;
 	pkt->hdr.size = len;
 	pkt->hdr.op = op;
 	pkt->hdr.tag = tag;
@@ -934,7 +883,7 @@ rxm_cq_write_recv_comp(struct rxm_rx_buf *rx_buf, void *context, uint64_t flags,
 		rxm_cq_write_src(rx_buf->ep->util_ep.rx_cq, context,
 				 flags, len, buf, rx_buf->pkt.hdr.data,
 				 rx_buf->pkt.hdr.tag,
-				 rx_buf->conn->handle.fi_addr);
+				 rx_buf->conn->peer->fi_addr);
 	else
 		rxm_cq_write(rx_buf->ep->util_ep.rx_cq, context,
 			     flags, len, buf, rx_buf->pkt.hdr.data,
diff --git a/prov/rxm/src/rxm_av.c b/prov/rxm/src/rxm_av.c
index b278dcb..33d33fe 100644
--- a/prov/rxm/src/rxm_av.c
+++ b/prov/rxm/src/rxm_av.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018 Intel Corporation. All rights reserved.
+ * Copyright (c) 2018-2021 Intel Corporation. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -34,142 +34,216 @@
 
 #include "rxm.h"
 
-static int rxm_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
-			 size_t count, uint64_t flags)
+
+static int rxm_addr_compare(struct ofi_rbmap *map, void *key, void *data)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
-	struct rxm_ep *rxm_ep;
-	int i, ret = 0;
-
-	fastlock_acquire(&av->ep_list_lock);
-	/* This should be before ofi_ip_av_remove as we need to know
-	 * fi_addr -> addr mapping when moving handle to peer list. */
-	dlist_foreach_container(&av->ep_list, struct rxm_ep,
-				rxm_ep, util_ep.av_entry) {
-		ofi_ep_lock_acquire(&rxm_ep->util_ep);
-		for (i = 0; i < count; i++) {
-			ret = rxm_cmap_remove(rxm_ep->cmap, *fi_addr + i);
-			if (ret)
-				FI_WARN(&rxm_prov, FI_LOG_AV,
-					"cmap remove failed for fi_addr: %"
-					PRIu64 "\n", *fi_addr + i);
-		}
-		ofi_ep_lock_release(&rxm_ep->util_ep);
+	return memcmp(&((struct rxm_peer_addr *) data)->addr, key,
+		container_of(map, struct rxm_av, addr_map)->util_av.addrlen);
+}
+
+static struct rxm_peer_addr *
+rxm_alloc_peer(struct rxm_av *av, const void *addr)
+{
+	struct rxm_peer_addr *peer;
+
+	peer = ofi_ibuf_alloc(av->peer_pool);
+	if (!peer)
+		return NULL;
+
+	peer->av = av;
+	peer->index = (int) ofi_buf_index(peer);
+	peer->fi_addr = FI_ADDR_NOTAVAIL;
+	peer->refcnt = 1;
+	memcpy(&peer->addr, addr, av->util_av.addrlen);
+
+	if (ofi_rbmap_insert(&av->addr_map, &peer->addr, peer, &peer->node)) {
+		ofi_ibuf_free(peer);
+		peer = NULL;
 	}
-	fastlock_release(&av->ep_list_lock);
 
-	return ofi_ip_av_remove(av_fid, fi_addr, count, flags);
+	return peer;
+}
+
+static void rxm_free_peer(struct rxm_peer_addr *peer)
+{
+	assert(!peer->refcnt);
+	ofi_rbmap_delete(&peer->av->addr_map, peer->node);
+	ofi_ibuf_free(peer);
+}
+
+struct rxm_peer_addr *
+rxm_get_peer(struct rxm_av *av, const void *addr)
+{
+	struct rxm_peer_addr *peer;
+	struct ofi_rbnode *node;
+
+	fastlock_acquire(&av->util_av.lock);
+	node = ofi_rbmap_find(&av->addr_map, (void *) addr);
+	if (node) {
+		peer = node->data;
+		peer->refcnt++;
+	} else {
+		peer = rxm_alloc_peer(av, addr);
+	}
+
+	fastlock_release(&av->util_av.lock);
+	return peer;
+}
+
+void rxm_put_peer(struct rxm_peer_addr *peer)
+{
+	struct rxm_av *av;
+
+	av = peer->av;
+	fastlock_acquire(&av->util_av.lock);
+	if (--peer->refcnt == 0)
+		rxm_free_peer(peer);
+	fastlock_release(&av->util_av.lock);
+}
+
+static void
+rxm_set_av_context(struct rxm_av *av, fi_addr_t fi_addr,
+		   struct rxm_peer_addr *peer)
+{
+	struct rxm_peer_addr **peer_ctx;
+
+	peer_ctx = ofi_av_addr_context(&av->util_av, fi_addr);
+	*peer_ctx = peer;
+}
+
+static void
+rxm_put_peer_addr(struct rxm_av *av, fi_addr_t fi_addr)
+{
+	struct rxm_peer_addr **peer;
+
+	fastlock_acquire(&av->util_av.lock);
+	peer = ofi_av_addr_context(&av->util_av, fi_addr);
+	if (--(*peer)->refcnt == 0)
+		rxm_free_peer(*peer);
+
+	rxm_set_av_context(av, fi_addr, NULL);
+	fastlock_release(&av->util_av.lock);
 }
 
-/* TODO: Determine if it's cleaner to insert an address into the cmap only
- * when we need to send to that address, rather than inserting the address
- * into the cmap when adding it to the AV.
- */
 static int
-rxm_av_insert_cmap(struct fid_av *av_fid, const void *addr, size_t count,
-		   fi_addr_t *fi_addr, uint64_t flags)
+rxm_av_add_peers(struct rxm_av *av, const void *addr, size_t count,
+		 fi_addr_t *fi_addr)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
-	struct rxm_ep *rxm_ep;
-	fi_addr_t fi_addr_tmp;
-	size_t i;
-	int ret = 0;
+	struct rxm_peer_addr *peer;
 	const void *cur_addr;
+	fi_addr_t cur_fi_addr;
+	size_t i;
+
+	for (i = 0; i < count; i++) {
+		cur_addr = ((char *) addr + i * av->util_av.addrlen);
+		peer = rxm_get_peer(av, cur_addr);
+		if (!peer)
+			goto err;
 
-	fastlock_acquire(&av->ep_list_lock);
-	dlist_foreach_container(&av->ep_list, struct rxm_ep,
-				rxm_ep, util_ep.av_entry) {
-		ofi_ep_lock_acquire(&rxm_ep->util_ep);
-		for (i = 0; i < count; i++) {
-			if (!rxm_ep->cmap)
-				break;
-
-			cur_addr = (const void *) ((char *) addr + i * av->addrlen);
-			fi_addr_tmp = (fi_addr ? fi_addr[i] :
-				       ofi_av_lookup_fi_addr_unsafe(av, cur_addr));
-			if (fi_addr_tmp == FI_ADDR_NOTAVAIL)
-				continue;
-
-			ret = rxm_cmap_update(rxm_ep->cmap, cur_addr, fi_addr_tmp);
-			if (OFI_UNLIKELY(ret)) {
-				FI_WARN(&rxm_prov, FI_LOG_AV,
-					"cmap update failed for fi_addr: %"
-					PRIu64 "\n", fi_addr_tmp);
-				break;
-			}
+		peer->fi_addr = fi_addr ? fi_addr[i] :
+				ofi_av_lookup_fi_addr(&av->util_av, cur_addr);
+
+		/* lookup can fail if prior AV insertion failed */
+		if (peer->fi_addr != FI_ADDR_NOTAVAIL)
+			rxm_set_av_context(av, peer->fi_addr, peer);
+	}
+	return 0;
+
+err:
+	while (i--) {
+		if (fi_addr) {
+			cur_fi_addr = fi_addr[i];
+		} else {
+			cur_addr = ((char *) addr + i * av->util_av.addrlen);
+			cur_fi_addr = ofi_av_lookup_fi_addr(&av->util_av,
+							    cur_addr);
 		}
-		ofi_ep_lock_release(&rxm_ep->util_ep);
+		if (cur_fi_addr != FI_ADDR_NOTAVAIL)
+			rxm_put_peer_addr(av, cur_fi_addr);
 	}
-	fastlock_release(&av->ep_list_lock);
-	return ret;
+	return -FI_ENOMEM;
+}
+
+static int rxm_av_remove(struct fid_av *av_fid, fi_addr_t *fi_addr,
+			 size_t count, uint64_t flags)
+{
+	struct rxm_av *av;
+	size_t i;
+
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid);
+	for (i = 0; i < count; i++)
+		rxm_put_peer_addr(av, fi_addr[i]);
+
+	return ofi_ip_av_remove(av_fid, fi_addr, count, flags);
 }
 
 static int rxm_av_insert(struct fid_av *av_fid, const void *addr, size_t count,
 			 fi_addr_t *fi_addr, uint64_t flags, void *context)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
-	int ret, retv;
+	struct rxm_av *av;
+	int ret;
 
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid.fid);
 	ret = ofi_ip_av_insert(av_fid, addr, count, fi_addr, flags, context);
 	if (ret < 0)
 		return ret;
 
-	if (!av->eq && !ret)
-		return ret;
+	if (!av->util_av.eq)
+		count = ret;
 
-	retv = rxm_av_insert_cmap(av_fid, addr, count, fi_addr, flags);
-	if (retv) {
-		ret = rxm_av_remove(av_fid, fi_addr, count, flags);
-		if (ret)
-			FI_WARN(&rxm_prov, FI_LOG_AV, "Failed to remove addr "
-				"from AV during error handling\n");
-		return retv;
+	ret = rxm_av_add_peers(av, addr, count, fi_addr);
+	if (ret) {
+		/* If insert was async, ofi_ip_av_insert() will have written
+		 * an event to the EQ with the number of insertions.  For
+		 * correctness we need to delay writing the event to the EQ
+		 * until all processing has completed.  This should be done
+		 * when separating the rxm av from the util av.  For now,
+		 * assume synchronous operation (most common case) and fail
+		 * the insert.  This could leave a bogus entry on the EQ.
+		 * But the app should detect that insert failed and is likely
+		 * to abort.
+		 */
+		rxm_av_remove(av_fid, fi_addr, count, flags);
+		return ret;
 	}
-	return ret;
+
+	return av->util_av.eq ? 0 : count;
 }
 
 static int rxm_av_insertsym(struct fid_av *av_fid, const char *node,
 			    size_t nodecnt, const char *service, size_t svccnt,
 			    fi_addr_t *fi_addr, uint64_t flags, void *context)
 {
-	struct util_av *av = container_of(av_fid, struct util_av, av_fid);
+	struct rxm_av *av;
 	void *addr;
-	size_t addrlen, count = nodecnt * svccnt;
-	int ret, retv;
+	size_t addrlen, count;
+	int ret;
 
-	ret = ofi_verify_av_insert(av, flags, context);
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid.fid);
+	ret = ofi_verify_av_insert(&av->util_av, flags, context);
 	if (ret)
 		return ret;
 
-	ret = ofi_ip_av_sym_getaddr(av, node, nodecnt, service,
+	ret = ofi_ip_av_sym_getaddr(&av->util_av, node, nodecnt, service,
 				    svccnt, &addr, &addrlen);
 	if (ret <= 0)
 		return ret;
 
-	assert(ret == count);
-
-	ret = ofi_ip_av_insertv(av, addr, addrlen, count, fi_addr, flags,
+	count = ret;
+	ret = ofi_ip_av_insertv(&av->util_av, addr, addrlen, count, fi_addr, flags,
 				context);
-	if (!av->eq && ret < count) {
+	if (ret > 0 && ret < count)
 		count = ret;
-	}
 
-	/* If the AV is bound to an EQ, we can't determine which entries were
-	 * added successfully to the AV until we process the insertion events
-	 * later when reading the EQ.  Add all addresses to the cmap
-	 * optimistically.
-	 */
-	retv = rxm_av_insert_cmap(av_fid, addr, count, fi_addr, flags);
-	if (retv) {
-		ret = rxm_av_remove(av_fid, fi_addr, count, flags);
-		if (ret)
-			FI_WARN(&rxm_prov, FI_LOG_AV, "Failed to remove addr "
-				"from AV during error handling\n");
-		ret = retv;
+	ret = rxm_av_add_peers(av, addr, count, fi_addr);
+	if (ret) {
+		/* See comment in rxm_av_insert. */
+		rxm_av_remove(av_fid, fi_addr, count, flags);
+		return ret;
 	}
 
 	free(addr);
-	return ret;
+	return av->util_av.eq ? 0 : count;
 }
 
 int rxm_av_insertsvc(struct fid_av *av, const char *node, const char *service,
@@ -190,6 +264,30 @@ int rxm_av_lookup(struct fid_av *av_fid, fi_addr_t fi_addr,
 	return ofi_ip_av_lookup(av_fid, fi_addr, addr, addrlen);
 }
 
+static int rxm_av_close(struct fid *av_fid)
+{
+	struct rxm_av *av;
+	int ret;
+
+	av = container_of(av_fid, struct rxm_av, util_av.av_fid.fid);
+	ret = ofi_av_close(&av->util_av);
+	if (ret)
+		return ret;
+
+	ofi_rbmap_cleanup(&av->addr_map);
+	ofi_bufpool_destroy(av->conn_pool);
+	ofi_bufpool_destroy(av->peer_pool);
+	free(av);
+	return 0;
+}
+
+static struct fi_ops rxm_av_fi_ops = {
+	.size = sizeof(struct fi_ops),
+	.close = rxm_av_close,
+	.bind = ofi_av_bind,
+	.control = fi_no_control,
+	.ops_open = fi_no_ops_open,
+};
 
 static struct fi_ops_av rxm_av_ops = {
 	.size = sizeof(struct fi_ops_av),
@@ -203,15 +301,54 @@ static struct fi_ops_av rxm_av_ops = {
 };
 
 int rxm_av_open(struct fid_domain *domain_fid, struct fi_av_attr *attr,
-		struct fid_av **av, void *context)
+		struct fid_av **fid_av, void *context)
 {
+	struct rxm_domain *domain;
+	struct util_av_attr util_attr;
+	struct rxm_av *av;
 	int ret;
 
-	ret = ofi_ip_av_create(domain_fid, attr, av, context);
+	av = calloc(1, sizeof(*av));
+	if (!av)
+		return -FI_ENOMEM;
+
+	ret = ofi_bufpool_create(&av->peer_pool, sizeof(struct rxm_peer_addr),
+				 0, 0, 0, OFI_BUFPOOL_INDEXED |
+				 OFI_BUFPOOL_NO_TRACK);
 	if (ret)
-		return ret;
+		goto free;
+
+	ret = ofi_bufpool_create(&av->conn_pool, sizeof(struct rxm_conn),
+				 0, 0, 0, 0);
+	if (ret)
+		goto destroy1;
+
+	ofi_rbmap_init(&av->addr_map, rxm_addr_compare);
+	domain = container_of(domain_fid, struct rxm_domain,
+			      util_domain.domain_fid);
+
+	util_attr.context_len = sizeof(struct rxm_peer_addr *);
+	util_attr.flags = 0;
+	util_attr.addrlen = ofi_sizeof_addr_format(domain->util_domain.
+						   addr_format);
+	if (attr->type == FI_AV_UNSPEC)
+		attr->type = FI_AV_TABLE;
 
-	(*av)->ops = &rxm_av_ops;
+	ret = ofi_av_init(&domain->util_domain, attr, &util_attr,
+			  &av->util_av, context);
+	if (ret)
+		goto destroy2;
+
+	av->util_av.av_fid.fid.ops = &rxm_av_fi_ops;
+	av->util_av.av_fid.ops = &rxm_av_ops;
+	*fid_av = &av->util_av.av_fid;
 	return 0;
-}
 
+destroy2:
+	ofi_bufpool_destroy(av->conn_pool);
+destroy1:
+	ofi_bufpool_destroy(av->peer_pool);
+free:
+	free(av);
+	return ret;
+}
diff --git a/prov/rxm/src/rxm_conn.c b/prov/rxm/src/rxm_conn.c
index 14f0bf1..2fb4b52 100644
--- a/prov/rxm/src/rxm_conn.c
+++ b/prov/rxm/src/rxm_conn.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2016 Intel Corporation, Inc.  All rights reserved.
+ * Copyright (c) 2016-2021 Intel Corporation, Inc.  All rights reserved.
  * Copyright (c) 2019 Amazon.com, Inc. or its affiliates. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -39,1244 +39,717 @@
 #include <ofi_util.h>
 #include "rxm.h"
 
-static struct rxm_cmap_handle *rxm_conn_alloc(struct rxm_cmap *cmap);
-static int rxm_conn_connect(struct rxm_ep *ep,
-			    struct rxm_cmap_handle *handle, const void *addr);
-static int rxm_conn_signal(struct rxm_ep *ep, void *context,
-			   enum rxm_cmap_signal signal);
-static void rxm_conn_av_updated_handler(struct rxm_cmap_handle *handle);
-static void *rxm_conn_progress(void *arg);
-static void *rxm_conn_atomic_progress(void *arg);
-static int rxm_conn_handle_event(struct rxm_ep *rxm_ep,
-				 struct rxm_msg_eq_entry *entry);
 
+static void *rxm_cm_progress(void *arg);
+static void *rxm_cm_atomic_progress(void *arg);
+static void rxm_flush_msg_cq(struct rxm_ep *rxm_ep);
 
-/*
- * Connection map
- */
 
-char *rxm_cm_state_str[] = {
-	RXM_CM_STATES(OFI_STR)
+/* castable to fi_eq_cm_entry - we can't use fi_eq_cm_entry directly
+ * here because of a compiler error with a 0-sized array
+ */
+struct rxm_eq_cm_entry {
+	fid_t fid;
+	struct fi_info *info;
+	union rxm_cm_data data;
 };
 
-static inline ssize_t rxm_eq_readerr(struct rxm_ep *rxm_ep,
-				     struct rxm_msg_eq_entry *entry)
+
+static void rxm_close_conn(struct rxm_conn *conn)
 {
-	ssize_t ret;
+	struct rxm_deferred_tx_entry *tx_entry;
+	struct rxm_recv_entry *rx_entry;
+	struct rxm_rx_buf *buf;
 
-	/* reset previous err data info */
-	entry->err_entry.err_data_size = 0;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "closing conn %p\n", conn);
 
-	ret = fi_eq_readerr(rxm_ep->msg_eq, &entry->err_entry, 0);
-	if (ret != sizeof(entry->err_entry)) {
-		if (ret != -FI_EAGAIN)
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"unable to fi_eq_readerr: %zd\n", ret);
-		return ret < 0 ? ret : -FI_EINVAL;
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+	/* All deferred transfers are internally generated */
+	while (!dlist_empty(&conn->deferred_tx_queue)) {
+		tx_entry = container_of(conn->deferred_tx_queue.next,
+				     struct rxm_deferred_tx_entry, entry);
+		rxm_dequeue_deferred_tx(tx_entry);
+		free(tx_entry);
 	}
 
-	if (entry->err_entry.err == ECONNREFUSED) {
-		entry->context = entry->err_entry.fid->context;
-		return -FI_ECONNREFUSED;
+	while (!dlist_empty(&conn->deferred_sar_segments)) {
+		buf = container_of(conn->deferred_sar_segments.next,
+				   struct rxm_rx_buf, unexp_msg.entry);
+		dlist_remove(&buf->unexp_msg.entry);
+		rxm_rx_buf_free(buf);
 	}
 
-	OFI_EQ_STRERROR(&rxm_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
-			rxm_ep->msg_eq, &entry->err_entry);
-	return -entry->err_entry.err;
-}
-
-static ssize_t rxm_eq_read(struct rxm_ep *ep, size_t len,
-			   struct rxm_msg_eq_entry *entry)
-{
-	ssize_t ret;
-
-	ret = fi_eq_read(ep->msg_eq, &entry->event, &entry->cm_entry, len, 0);
-	if (ret == -FI_EAVAIL)
-		ret = rxm_eq_readerr(ep, entry);
-
-	return ret;
-}
-
-static void rxm_cmap_set_key(struct rxm_cmap_handle *handle)
-{
-	handle->key = ofi_idx2key(&handle->cmap->key_idx,
-		ofi_idx_insert(&handle->cmap->handles_idx, handle));
-}
-
-static void rxm_cmap_clear_key(struct rxm_cmap_handle *handle)
-{
-	int index = ofi_key2idx(&handle->cmap->key_idx, handle->key);
-
-	if (!ofi_idx_is_valid(&handle->cmap->handles_idx, index))
-		FI_WARN(handle->cmap->av->prov, FI_LOG_AV, "Invalid key!\n");
-	else
-		ofi_idx_remove(&handle->cmap->handles_idx, index);
-}
-
-struct rxm_cmap_handle *rxm_cmap_key2handle(struct rxm_cmap *cmap, uint64_t key)
-{
-	struct rxm_cmap_handle *handle;
-
-	if (!(handle = ofi_idx_lookup(&cmap->handles_idx,
-				      ofi_key2idx(&cmap->key_idx, key)))) {
-		FI_WARN(cmap->av->prov, FI_LOG_AV, "Invalid key!\n");
-	} else {
-		if (handle->key != key) {
-			FI_WARN(cmap->av->prov, FI_LOG_AV,
-				"handle->key not matching given key\n");
-			handle = NULL;
-		}
+	while (!dlist_empty(&conn->deferred_sar_msgs)) {
+		rx_entry = container_of(conn->deferred_sar_msgs.next,
+					struct rxm_recv_entry, sar.entry);
+		dlist_remove(&rx_entry->entry);
+		rxm_recv_entry_release(rx_entry);
 	}
-	return handle;
+	fi_close(&conn->msg_ep->fid);
+	rxm_flush_msg_cq(conn->ep);
+	dlist_remove_init(&conn->loopback_entry);
+	conn->msg_ep = NULL;
+	conn->state = RXM_CM_IDLE;
 }
 
-static void rxm_cmap_init_handle(struct rxm_cmap_handle *handle,
-				  struct rxm_cmap *cmap,
-				  enum rxm_cmap_state state,
-				  fi_addr_t fi_addr,
-				  struct rxm_cmap_peer *peer)
+static int rxm_open_conn(struct rxm_conn *conn, struct fi_info *msg_info)
 {
-	handle->cmap = cmap;
-	RXM_CM_UPDATE_STATE(handle, state);
-	rxm_cmap_set_key(handle);
-	handle->fi_addr = fi_addr;
-	handle->peer = peer;
-}
-
-static int rxm_cmap_match_peer(struct dlist_entry *entry, const void *addr)
-{
-	struct rxm_cmap_peer *peer;
-
-	peer = container_of(entry, struct rxm_cmap_peer, entry);
-	return !memcmp(peer->addr, addr, peer->handle->cmap->av->addrlen);
-}
-
-static int rxm_cmap_del_handle(struct rxm_cmap_handle *handle)
-{
-	struct rxm_cmap *cmap = handle->cmap;
+	struct rxm_domain *domain;
+	struct rxm_ep *ep;
+	struct fid_ep *msg_ep;
 	int ret;
 
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-	       "marking connection handle: %p for deletion\n", handle);
-	rxm_cmap_clear_key(handle);
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "open msg ep %p\n", conn);
 
-	RXM_CM_UPDATE_STATE(handle, RXM_CMAP_SHUTDOWN);
-
-	/* Signal CM thread to delete the handle. This is required
-	 * so that the CM thread handles any pending events for this
-	 * ep correctly. Handle would be freed finally after processing the
-	 * events */
-	ret = rxm_conn_signal(cmap->ep, handle, RXM_CMAP_FREE);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+	ep = conn->ep;
+	domain = container_of(ep->util_ep.domain, struct rxm_domain,
+			      util_domain);
+	ret = fi_endpoint(domain->msg_domain, msg_info, &msg_ep, conn);
 	if (ret) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Unable to signal CM thread\n");
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to create msg_ep: %d\n", ret);
 		return ret;
 	}
-	return 0;
-}
-
-ssize_t rxm_get_conn(struct rxm_ep *rxm_ep, fi_addr_t addr,
-		     struct rxm_conn **rxm_conn)
-{
-	struct rxm_cmap_handle *handle;
-	ssize_t ret;
-
-	assert(rxm_ep->util_ep.tx_cq);
-	handle = rxm_cmap_acquire_handle(rxm_ep->cmap, addr);
-	if (!handle) {
-		ret = rxm_cmap_alloc_handle(rxm_ep->cmap, addr,
-					    RXM_CMAP_IDLE, &handle);
-		if (ret)
-			return ret;
-	}
 
-	*rxm_conn = container_of(handle, struct rxm_conn, handle);
-
-	if (handle->state != RXM_CMAP_CONNECTED) {
-		ret = rxm_cmap_connect(rxm_ep, addr, handle);
-		if (ret)
-			return ret;
-	}
-
-	if (!dlist_empty(&(*rxm_conn)->deferred_tx_queue)) {
-		rxm_ep_do_progress(&rxm_ep->util_ep);
-		if (!dlist_empty(&(*rxm_conn)->deferred_tx_queue))
-			return -FI_EAGAIN;
+	ret = fi_ep_bind(msg_ep, &ep->msg_eq->fid, 0);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to bind msg EP to EQ: %d\n", ret);
+		goto err;
 	}
-	return 0;
-}
-
-static inline int
-rxm_cmap_check_and_realloc_handles_table(struct rxm_cmap *cmap,
-					 fi_addr_t fi_addr)
-{
-	void *new_handles;
-	size_t grow_size;
-
-	if (OFI_LIKELY(fi_addr < cmap->num_allocated))
-		return 0;
-
-	grow_size = MAX(ofi_av_size(cmap->av), fi_addr - cmap->num_allocated + 1);
 
-	new_handles = realloc(cmap->handles_av,
-			      (grow_size + cmap->num_allocated) *
-			      sizeof(*cmap->handles_av));
-	if (OFI_LIKELY(!new_handles))
-		return -FI_ENOMEM;
-
-	cmap->handles_av = new_handles;
-	memset(&cmap->handles_av[cmap->num_allocated], 0,
-	       sizeof(*cmap->handles_av) * grow_size);
-	cmap->num_allocated += grow_size;
-	return 0;
-}
-
-static void rxm_conn_close(struct rxm_cmap_handle *handle)
-{
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-	struct rxm_conn *rxm_conn_tmp;
-	struct rxm_deferred_tx_entry *def_tx_entry;
-	struct dlist_entry *conn_entry_tmp;
-
-	dlist_foreach_container_safe(&handle->cmap->ep->deferred_tx_conn_queue,
-				     struct rxm_conn, rxm_conn_tmp,
-				     deferred_conn_entry, conn_entry_tmp)
-	{
-		if (rxm_conn_tmp->handle.key != handle->key)
-			continue;
-
-		while (!dlist_empty(&rxm_conn_tmp->deferred_tx_queue)) {
-			def_tx_entry =
-				container_of(rxm_conn_tmp->deferred_tx_queue.next,
-					     struct rxm_deferred_tx_entry, entry);
-			FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-			       "cancelled deferred message\n");
-			rxm_ep_dequeue_deferred_tx_queue(def_tx_entry);
-			free(def_tx_entry);
+	if (ep->srx_ctx) {
+		ret = fi_ep_bind(msg_ep, &ep->srx_ctx->fid, 0);
+		if (ret) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to bind msg "
+				"EP to shared RX ctx: %d\n", ret);
+			goto err;
 		}
 	}
 
-	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "closing msg ep\n");
-	if (!rxm_conn->msg_ep)
-		return;
-
-	if (fi_close(&rxm_conn->msg_ep->fid))
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to close msg_ep\n");
-
-	rxm_conn->msg_ep = NULL;
-}
-
-static void rxm_conn_free(struct rxm_cmap_handle *handle)
-{
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-
-	rxm_conn_close(handle);
-	free(rxm_conn);
-}
-
-int rxm_cmap_alloc_handle(struct rxm_cmap *cmap, fi_addr_t fi_addr,
-			  enum rxm_cmap_state state,
-			  struct rxm_cmap_handle **handle)
-{
-	int ret;
-
-	*handle = rxm_conn_alloc(cmap);
-	if (!*handle)
-		return -FI_ENOMEM;
-
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-	       "Allocated handle: %p for fi_addr: %" PRIu64 "\n",
-	       *handle, fi_addr);
-
-	ret = rxm_cmap_check_and_realloc_handles_table(cmap, fi_addr);
+	ret = fi_ep_bind(msg_ep, &ep->msg_cq->fid, FI_TRANSMIT | FI_RECV);
 	if (ret) {
-		rxm_conn_free(*handle);
-		return ret;
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"unable to bind msg_ep to msg_cq: %d\n", ret);
+		goto err;
 	}
 
-	rxm_cmap_init_handle(*handle, cmap, state, fi_addr, NULL);
-	cmap->handles_av[fi_addr] = *handle;
-	return 0;
-}
-
-static int rxm_cmap_alloc_handle_peer(struct rxm_cmap *cmap, void *addr,
-				       enum rxm_cmap_state state,
-				       struct rxm_cmap_handle **handle)
-{
-	struct rxm_cmap_peer *peer;
-
-	peer = calloc(1, sizeof(*peer) + cmap->av->addrlen);
-	if (!peer)
-		return -FI_ENOMEM;
+	ret = fi_enable(msg_ep);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to enable msg_ep: %d\n", ret);
+		goto err;
+	}
 
-	*handle = rxm_conn_alloc(cmap);
-	if (!*handle) {
-		free(peer);
-		return -FI_ENOMEM;
+	ret = domain->flow_ctrl_ops->enable(msg_ep);
+	if (!ret) {
+		domain->flow_ctrl_ops->set_threshold(msg_ep,
+					ep->msg_info->rx_attr->size / 2);
 	}
 
-	ofi_straddr_dbg(cmap->av->prov, FI_LOG_AV,
-			"Allocated handle for addr", addr);
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "handle: %p\n", *handle);
+	if (!ep->srx_ctx) {
+		ret = rxm_prepost_recv(ep, msg_ep);
+		if (ret)
+			goto err;
+	}
 
-	rxm_cmap_init_handle(*handle, cmap, state, FI_ADDR_NOTAVAIL, peer);
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Adding handle to peer list\n");
-	peer->handle = *handle;
-	memcpy(peer->addr, addr, cmap->av->addrlen);
-	dlist_insert_tail(&peer->entry, &cmap->peer_list);
+	conn->msg_ep = msg_ep;
 	return 0;
+err:
+	fi_close(&msg_ep->fid);
+	return ret;
 }
 
-static struct rxm_cmap_handle *
-rxm_cmap_get_handle_peer(struct rxm_cmap *cmap, const void *addr)
+/* We send passive endpoint's port to the server as connection request
+ * would be from a different one.
+ */
+static int rxm_init_connect_data(struct rxm_conn *conn,
+				 union rxm_cm_data *cm_data)
 {
-	struct rxm_cmap_peer *peer;
-	struct dlist_entry *entry;
-
-	entry = dlist_find_first_match(&cmap->peer_list, rxm_cmap_match_peer,
-				       addr);
-	if (!entry)
-		return NULL;
+	size_t cm_data_size = 0;
+	size_t opt_size = sizeof(cm_data_size);
+	int ret;
 
-	ofi_straddr_dbg(cmap->av->prov, FI_LOG_AV,
-			"handle found in peer list for addr", addr);
-	peer = container_of(entry, struct rxm_cmap_peer, entry);
-	return peer->handle;
-}
+	memset(cm_data, 0, sizeof(*cm_data));
+	cm_data->connect.version = RXM_CM_DATA_VERSION;
+	cm_data->connect.ctrl_version = RXM_CTRL_VERSION;
+	cm_data->connect.op_version = RXM_OP_VERSION;
+	cm_data->connect.endianness = ofi_detect_endianness();
+	cm_data->connect.eager_limit = conn->ep->eager_limit;
+	cm_data->connect.rx_size = conn->ep->msg_info->rx_attr->size;
 
-int rxm_cmap_remove(struct rxm_cmap *cmap, int index)
-{
-	struct rxm_cmap_handle *handle;
-	int ret = -FI_ENOENT;
-
-	handle = cmap->handles_av[index];
-	if (!handle) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cmap entry not found\n");
+	ret = fi_getopt(&conn->ep->msg_pep->fid, FI_OPT_ENDPOINT,
+			FI_OPT_CM_DATA_SIZE, &cm_data_size, &opt_size);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "fi_getopt failed\n");
 		return ret;
 	}
 
-	handle->peer = calloc(1, sizeof(*handle->peer) + cmap->av->addrlen);
-	if (!handle->peer) {
-		ret = -FI_ENOMEM;
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to allocate memory "
-			"for moving handle to peer list, deleting it instead\n");
-		rxm_cmap_del_handle(handle);
-		return ret;
+	if (cm_data_size < sizeof(*cm_data)) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data too small\n");
+		return -FI_EOTHER;
 	}
-	handle->fi_addr = FI_ADDR_NOTAVAIL;
-	cmap->handles_av[index] = NULL;
-	handle->peer->handle = handle;
-	memcpy(handle->peer->addr, ofi_av_get_addr(cmap->av, index),
-	       cmap->av->addrlen);
-	dlist_insert_tail(&handle->peer->entry, &cmap->peer_list);
-	return 0;
-}
 
-static int rxm_cmap_move_handle(struct rxm_cmap_handle *handle,
-				fi_addr_t fi_addr)
-{
-	int ret;
-
-	dlist_remove(&handle->peer->entry);
-	free(handle->peer);
-	handle->peer = NULL;
-	handle->fi_addr = fi_addr;
-	ret = rxm_cmap_check_and_realloc_handles_table(handle->cmap, fi_addr);
-	if (OFI_UNLIKELY(ret))
-		return ret;
-	handle->cmap->handles_av[fi_addr] = handle;
+	cm_data->connect.port = ofi_addr_get_port(&conn->ep->addr.sa);
+	cm_data->connect.client_conn_id = conn->peer->index;
 	return 0;
 }
 
-int rxm_cmap_update(struct rxm_cmap *cmap, const void *addr, fi_addr_t fi_addr)
+static int rxm_send_connect(struct rxm_conn *conn)
 {
-	struct rxm_cmap_handle *handle;
+	union rxm_cm_data cm_data;
+	struct fi_info *info;
 	int ret;
 
-	/* Check whether we have already allocated a handle for this `fi_addr`. */
-	/* We rely on the fact that `ofi_ip_av_insert`/`ofi_av_insert_addr` returns
-	 * the same `fi_addr` for the equal addresses */
-	if (fi_addr < cmap->num_allocated) {
-		handle = rxm_cmap_acquire_handle(cmap, fi_addr);
-		if (handle)
-			return 0;
-	}
-
-	handle = rxm_cmap_get_handle_peer(cmap, addr);
-	if (!handle) {
-		ret = rxm_cmap_alloc_handle(cmap, fi_addr,
-					    RXM_CMAP_IDLE, &handle);
-		return ret;
-	}
-	ret = rxm_cmap_move_handle(handle, fi_addr);
-	if (ret)
-		return ret;
-
-	rxm_conn_av_updated_handler(handle);
-	return 0;
-}
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connecting %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
 
-void rxm_cmap_process_shutdown(struct rxm_cmap *cmap,
-			       struct rxm_cmap_handle *handle)
-{
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-		"Processing shutdown for handle: %p\n", handle);
-	if (handle->state > RXM_CMAP_SHUTDOWN) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Invalid handle on shutdown event\n");
-	} else if (handle->state != RXM_CMAP_SHUTDOWN) {
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Got remote shutdown\n");
-		rxm_cmap_del_handle(handle);
-	} else {
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL, "Got local shutdown\n");
-	}
-}
-
-void rxm_cmap_process_connect(struct rxm_cmap *cmap,
-			      struct rxm_cmap_handle *handle,
-			      union rxm_cm_data *cm_data)
-{
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-	       "processing FI_CONNECTED event for handle: %p\n", handle);
-	if (cm_data) {
-		assert(handle->state == RXM_CMAP_CONNREQ_SENT);
-		handle->remote_key = cm_data->accept.server_conn_id;
-	} else {
-		assert(handle->state == RXM_CMAP_CONNREQ_RECV);
-	}
-	RXM_CM_UPDATE_STATE(handle, RXM_CMAP_CONNECTED);
-}
+	info = conn->ep->msg_info;
+	info->dest_addrlen = conn->ep->msg_info->src_addrlen;
 
-void rxm_cmap_process_reject(struct rxm_cmap *cmap,
-			     struct rxm_cmap_handle *handle,
-			     enum rxm_cmap_reject_reason reject_reason)
-{
-	FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-		"Processing reject for handle: %p\n", handle);
-	switch (handle->state) {
-	case RXM_CMAP_CONNREQ_RECV:
-	case RXM_CMAP_CONNECTED:
-		/* Handle is being re-used for incoming connection request */
-		break;
-	case RXM_CMAP_CONNREQ_SENT:
-		if (reject_reason == RXM_CMAP_REJECT_GENUINE) {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			       "Deleting connection handle\n");
-			rxm_cmap_del_handle(handle);
-		} else {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			       "Connection handle is being re-used. Close the connection\n");
-			rxm_conn_close(handle);
-		}
-		break;
-	case RXM_CMAP_SHUTDOWN:
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Connection handle already being deleted\n");
-		break;
-	default:
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL, "Invalid cmap state: "
-			"%d when receiving connection reject\n", handle->state);
-		assert(0);
-	}
-}
+	free(info->dest_addr);
+	info->dest_addr = mem_dup(&conn->peer->addr, info->dest_addrlen);
+	if (!info->dest_addr)
+		return -FI_ENOMEM;
 
-int rxm_cmap_process_connreq(struct rxm_cmap *cmap, void *addr,
-			     struct rxm_cmap_handle **handle_ret,
-			     uint8_t *reject_reason)
-{
-	struct rxm_cmap_handle *handle;
-	int ret = 0, cmp;
-	fi_addr_t fi_addr = ofi_ip_av_get_fi_addr(cmap->av, addr);
-
-	ofi_straddr_dbg(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Processing connreq from remote pep", addr);
-
-	if (fi_addr == FI_ADDR_NOTAVAIL) {
-		handle = rxm_cmap_get_handle_peer(cmap, addr);
-		if (!handle)
-			ret = rxm_cmap_alloc_handle_peer(cmap, addr,
-							 RXM_CMAP_CONNREQ_RECV,
-							 &handle);
-	} else {
-		handle = rxm_cmap_acquire_handle(cmap, fi_addr);
-		if (!handle)
-			ret = rxm_cmap_alloc_handle(cmap, fi_addr,
-						    RXM_CMAP_CONNREQ_RECV,
-						    &handle);
-	}
+	ret = rxm_open_conn(conn, info);
 	if (ret)
 		return ret;
 
-	switch (handle->state) {
-	case RXM_CMAP_CONNECTED:
-		FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Connection already present.\n");
-		ret = -FI_EALREADY;
-		break;
-	case RXM_CMAP_CONNREQ_SENT:
-		ofi_straddr_dbg(cmap->av->prov, FI_LOG_EP_CTRL, "local_name",
-				cmap->attr.name);
-		ofi_straddr_dbg(cmap->av->prov, FI_LOG_EP_CTRL, "remote_name",
-				addr);
-
-		cmp = ofi_addr_cmp(cmap->av->prov, addr, cmap->attr.name);
+	ret = rxm_init_connect_data(conn, &cm_data);
+	if (ret)
+		goto err;
 
-		if (cmp < 0) {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-				"Remote name lower than local name.\n");
-			*reject_reason = RXM_CMAP_REJECT_SIMULT_CONN;
-			ret = -FI_EALREADY;
-			break;
-		} else if (cmp > 0) {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-				"Re-using handle: %p to accept remote "
-				"connection\n", handle);
-			*reject_reason = RXM_CMAP_REJECT_GENUINE;
-			rxm_conn_close(handle);
-		} else {
-			FI_DBG(cmap->av->prov, FI_LOG_EP_CTRL,
-				"Endpoint connects to itself\n");
-			ret = rxm_cmap_alloc_handle_peer(cmap, addr,
-							  RXM_CMAP_CONNREQ_RECV,
-							  &handle);
-			if (ret)
-				return ret;
-
-			assert(fi_addr != FI_ADDR_NOTAVAIL);
-			handle->fi_addr = fi_addr;
-		}
-		/* Fall through */
-	case RXM_CMAP_IDLE:
-		RXM_CM_UPDATE_STATE(handle, RXM_CMAP_CONNREQ_RECV);
-		/* Fall through */
-	case RXM_CMAP_CONNREQ_RECV:
-		*handle_ret = handle;
-		break;
-	case RXM_CMAP_SHUTDOWN:
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL, "handle :%p marked for "
-			"deletion / shutdown, reject connection\n", handle);
-		*reject_reason = RXM_CMAP_REJECT_GENUINE;
-		ret = -FI_EOPBADSTATE;
-		break;
-	default:
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-		       "invalid handle state: %d\n", handle->state);
-		assert(0);
-		ret = -FI_EOPBADSTATE;
+	ret = fi_connect(conn->msg_ep, info->dest_addr, &cm_data,
+			 sizeof(cm_data));
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to connect msg_ep\n");
+		goto err;
 	}
+	conn->state = RXM_CM_CONNECTING;
+	return 0;
 
+err:
+	fi_close(&conn->msg_ep->fid);
+	conn->msg_ep = NULL;
 	return ret;
 }
 
-int rxm_msg_eq_progress(struct rxm_ep *rxm_ep)
+static int rxm_connect(struct rxm_conn *conn)
 {
-	struct rxm_msg_eq_entry *entry;
 	int ret;
 
-	entry = alloca(RXM_MSG_EQ_ENTRY_SZ);
-	if (!entry) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to allocate memory!\n");
-		return -FI_ENOMEM;
-	}
-
-	while (1) {
-		entry->rd = rxm_eq_read(rxm_ep, RXM_MSG_EQ_ENTRY_SZ, entry);
-		if (entry->rd < 0 && entry->rd != -FI_ECONNREFUSED) {
-			ret = (int) entry->rd;
-			break;
-		}
-		ret = rxm_conn_handle_event(rxm_ep, entry);
-		if (ret) {
-			FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-			       "invalid connection handle event: %d\n", ret);
-			break;
-		}
-	}
-	return ret;
-}
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
 
-int rxm_cmap_connect(struct rxm_ep *rxm_ep, fi_addr_t fi_addr,
-		     struct rxm_cmap_handle *handle)
-{
-	int ret = FI_SUCCESS;
-
-	switch (handle->state) {
-	case RXM_CMAP_IDLE:
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "initiating MSG_EP connect "
-		       "for fi_addr: %" PRIu64 "\n", fi_addr);
-		ret = rxm_conn_connect(rxm_ep, handle,
-				       ofi_av_get_addr(rxm_ep->cmap->av, fi_addr));
-		if (ret) {
-			if (ret == -FI_ECONNREFUSED)
-				return -FI_EAGAIN;
-
-			rxm_cmap_del_handle(handle);
-		} else {
-			RXM_CM_UPDATE_STATE(handle, RXM_CMAP_CONNREQ_SENT);
-			ret = -FI_EAGAIN;
-		}
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		ret = rxm_send_connect(conn);
+		if (ret)
+			return ret;
 		break;
-	case RXM_CMAP_CONNREQ_SENT:
-	case RXM_CMAP_CONNREQ_RECV:
-	case RXM_CMAP_SHUTDOWN:
-		ret = -FI_EAGAIN;
+	case RXM_CM_CONNECTING:
+	case RXM_CM_ACCEPTING:
 		break;
+	case RXM_CM_CONNECTED:
+		return 0;
 	default:
-		FI_WARN(rxm_ep->cmap->av->prov, FI_LOG_EP_CTRL,
-			"Invalid cmap handle state\n");
 		assert(0);
-		ret = -FI_EOPBADSTATE;
+		conn->state = RXM_CM_IDLE;
+		break;
 	}
-	if (ret == -FI_EAGAIN)
-		rxm_msg_eq_progress(rxm_ep);
 
-	return ret;
+	return -FI_EAGAIN;
 }
 
-static int rxm_cmap_cm_thread_close(struct rxm_cmap *cmap)
+static void rxm_free_conn(struct rxm_conn *conn)
 {
-	int ret;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "free conn %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
 
-	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "stopping CM thread\n");
-	if (!cmap->cm_thread)
-		return 0;
+	if (conn->flags & RXM_CONN_INDEXED)
+		ofi_idm_clear(&conn->ep->conn_idx_map, conn->peer->index);
 
-	ofi_ep_lock_acquire(&cmap->ep->util_ep);
-	cmap->ep->do_progress = false;
-	ofi_ep_lock_release(&cmap->ep->util_ep);
-	ret = rxm_conn_signal(cmap->ep, NULL, RXM_CMAP_EXIT);
-	if (ret) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Unable to signal CM thread\n");
-		return ret;
-	}
-	ret = pthread_join(cmap->cm_thread, NULL);
-	if (ret) {
-		FI_WARN(cmap->av->prov, FI_LOG_EP_CTRL,
-			"Unable to join CM thread\n");
-		return ret;
-	}
-	return 0;
+	conn->peer->refcnt--;
+	ofi_buf_free(conn);
 }
 
-void rxm_cmap_free(struct rxm_cmap *cmap)
+void rxm_freeall_conns(struct rxm_ep *ep)
 {
-	struct rxm_cmap_peer *peer;
-	struct dlist_entry *entry;
-	size_t i;
-
-	FI_INFO(cmap->av->prov, FI_LOG_EP_CTRL, "Closing cmap\n");
-	rxm_cmap_cm_thread_close(cmap);
-
-	for (i = 0; i < cmap->num_allocated; i++) {
-		if (cmap->handles_av[i]) {
-			rxm_cmap_clear_key(cmap->handles_av[i]);
-			rxm_conn_free(cmap->handles_av[i]);
-		}
-	}
-
-	while (!dlist_empty(&cmap->peer_list)) {
-		entry = cmap->peer_list.next;
-		peer = container_of(entry, struct rxm_cmap_peer, entry);
-		dlist_remove(&peer->entry);
-		rxm_cmap_clear_key(peer->handle);
-		rxm_conn_free(peer->handle);
-		free(peer);
-	}
+	struct rxm_conn *conn;
+	struct dlist_entry *tmp;
+	struct rxm_av *av;
+	int i;
 
-	free(cmap->handles_av);
-	free(cmap->attr.name);
-	ofi_idx_reset(&cmap->handles_idx);
-	free(cmap);
-}
-
-static int
-rxm_cmap_update_addr(struct util_av *av, void *addr,
-		     fi_addr_t fi_addr, void *arg)
-{
-	return rxm_cmap_update((struct rxm_cmap *)arg, addr, fi_addr);
-}
-
-int rxm_cmap_bind_to_av(struct rxm_cmap *cmap, struct util_av *av)
-{
-	cmap->av = av;
-	return ofi_av_elements_iter(av, rxm_cmap_update_addr, (void *)cmap);
-}
-
-int rxm_cmap_alloc(struct rxm_ep *rxm_ep, struct rxm_cmap_attr *attr)
-{
-	struct rxm_cmap *cmap;
-	struct util_ep *ep = &rxm_ep->util_ep;
-	int ret;
-
-	cmap = calloc(1, sizeof *cmap);
-	if (!cmap)
-		return -FI_ENOMEM;
-
-	cmap->ep = rxm_ep;
-	cmap->av = ep->av;
+	av = container_of(ep->util_ep.av, struct rxm_av, util_av);
+	ofi_ep_lock_acquire(&ep->util_ep);
 
-	cmap->handles_av = calloc(ofi_av_size(ep->av), sizeof(*cmap->handles_av));
-	if (!cmap->handles_av) {
-		ret = -FI_ENOMEM;
-		goto err1;
-	}
-	cmap->num_allocated = ofi_av_size(ep->av);
+	/* We can't have more connections than known peers */
+	for (i = 0; i < av->peer_pool->entry_cnt; i++) {
+		conn = ofi_idm_lookup(&ep->conn_idx_map, i);
+		if (!conn)
+			continue;
 
-	cmap->attr = *attr;
-	cmap->attr.name = mem_dup(attr->name, ep->av->addrlen);
-	if (!cmap->attr.name) {
-		ret = -FI_ENOMEM;
-		goto err2;
+		if (conn->state != RXM_CM_IDLE)
+			rxm_close_conn(conn);
+		rxm_free_conn(conn);
 	}
 
-	memset(&cmap->handles_idx, 0, sizeof(cmap->handles_idx));
-	ofi_key_idx_init(&cmap->key_idx, RXM_CMAP_IDX_BITS);
-
-	dlist_init(&cmap->peer_list);
-
-	rxm_ep->cmap = cmap;
-
-	if (ep->domain->data_progress == FI_PROGRESS_AUTO || force_auto_progress) {
-
-		assert(ep->domain->threading == FI_THREAD_SAFE);
-		rxm_ep->do_progress = true;
-		if (pthread_create(&cmap->cm_thread, 0,
-				   rxm_ep->rxm_info->caps & FI_ATOMIC ?
-				   rxm_conn_atomic_progress :
-				   rxm_conn_progress, ep)) {
-			FI_WARN(ep->av->prov, FI_LOG_EP_CTRL,
-				"unable to create cmap thread\n");
-			ret = -ofi_syserr();
-			goto err3;
-		}
+	dlist_foreach_container_safe(&ep->loopback_list, struct rxm_conn,
+				     conn, loopback_entry, tmp) {
+		rxm_close_conn(conn);
+		rxm_free_conn(conn);
 	}
 
-	assert(ep->av);
-	ret = rxm_cmap_bind_to_av(cmap, ep->av);
-	if (ret)
-		goto err4;
-
-	return FI_SUCCESS;
-err4:
-	rxm_cmap_cm_thread_close(cmap);
-err3:
-	rxm_ep->cmap = NULL;
-	free(cmap->attr.name);
-err2:
-	free(cmap->handles_av);
-err1:
-	free(cmap);
-	return ret;
+	ofi_ep_lock_release(&ep->util_ep);
 }
 
-static int rxm_msg_ep_open(struct rxm_ep *rxm_ep, struct fi_info *msg_info,
-			   struct rxm_conn *rxm_conn, void *context)
+static struct rxm_conn *
+rxm_alloc_conn(struct rxm_ep *ep, struct rxm_peer_addr *peer)
 {
-	struct rxm_domain *rxm_domain;
-	struct fid_ep *msg_ep;
-	int ret;
+	struct rxm_conn *conn;
+	struct rxm_av *av;
 
-	rxm_domain = container_of(rxm_ep->util_ep.domain, struct rxm_domain,
-			util_domain);
-
-	ret = fi_endpoint(rxm_domain->msg_domain, msg_info, &msg_ep, context);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to create msg_ep: %d\n", ret);
-		return ret;
-	}
-
-	ret = fi_ep_bind(msg_ep, &rxm_ep->msg_eq->fid, 0);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to bind msg EP to EQ: %d\n", ret);
-		goto err;
-	}
-
-	if (rxm_ep->srx_ctx) {
-		ret = fi_ep_bind(msg_ep, &rxm_ep->srx_ctx->fid, 0);
-		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to bind msg "
-				"EP to shared RX ctx: %d\n", ret);
-			goto err;
-		}
-	}
-
-	// TODO add other completion flags
-	ret = fi_ep_bind(msg_ep, &rxm_ep->msg_cq->fid, FI_TRANSMIT | FI_RECV);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"unable to bind msg_ep to msg_cq: %d\n", ret);
-		goto err;
-	}
-
-	ret = fi_enable(msg_ep);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to enable msg_ep: %d\n", ret);
-		goto err;
-	}
-
-	ret = rxm_domain->flow_ctrl_ops->enable(msg_ep);
-	if (!ret) {
-		rxm_domain->flow_ctrl_ops->set_threshold(
-			msg_ep, rxm_ep->msg_info->rx_attr->size / 2);
-	}
-
-	if (!rxm_ep->srx_ctx) {
-		ret = rxm_prepost_recv(rxm_ep, msg_ep);
-		if (ret)
-			goto err;
-	}
-
-	rxm_conn->msg_ep = msg_ep;
-	return 0;
-err:
-	fi_close(&msg_ep->fid);
-	return ret;
-}
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	av = container_of(ep->util_ep.av, struct rxm_av, util_av);
+	conn = ofi_buf_alloc(av->conn_pool);
+	if (!conn)
+		return NULL;
 
-static int rxm_conn_reprocess_directed_recvs(struct rxm_recv_queue *recv_queue)
-{
-	struct rxm_rx_buf *rx_buf;
-	struct dlist_entry *entry, *tmp_entry;
-	struct rxm_recv_match_attr match_attr;
-	struct fi_cq_err_entry err_entry = {0};
-	int ret, count = 0;
-
-	dlist_foreach_container_safe(&recv_queue->unexp_msg_list,
-				     struct rxm_rx_buf, rx_buf,
-				     unexp_msg.entry, tmp_entry) {
-		if (rx_buf->unexp_msg.addr == rx_buf->conn->handle.fi_addr)
-			continue;
+	conn->ep = ep;
+	conn->state = RXM_CM_IDLE;
+	conn->remote_index = -1;
+	conn->flags = 0;
+	dlist_init(&conn->deferred_entry);
+	dlist_init(&conn->deferred_tx_queue);
+	dlist_init(&conn->deferred_sar_msgs);
+	dlist_init(&conn->deferred_sar_segments);
+	dlist_init(&conn->loopback_entry);
 
-		assert(rx_buf->unexp_msg.addr == FI_ADDR_NOTAVAIL);
+	conn->peer = peer;
+	peer->refcnt++;
 
-		rx_buf->unexp_msg.addr = rx_buf->conn->handle.fi_addr;
-		match_attr.addr = rx_buf->unexp_msg.addr;
-		match_attr.tag = rx_buf->unexp_msg.tag;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "allocated conn %p\n", conn);
+	return conn;
+}
 
-		entry = dlist_remove_first_match(&recv_queue->recv_list,
-						 recv_queue->match_recv,
-						 &match_attr);
-		if (!entry)
-			continue;
+static struct rxm_conn *
+rxm_add_conn(struct rxm_ep *ep, struct rxm_peer_addr *peer)
+{
+	struct rxm_conn *conn;
 
-		dlist_remove(&rx_buf->unexp_msg.entry);
-		rx_buf->recv_entry = container_of(entry, struct rxm_recv_entry,
-						  entry);
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	conn = ofi_idm_lookup(&ep->conn_idx_map, peer->index);
+	if (conn)
+		return conn;
 
-		ret = rxm_handle_rx_buf(rx_buf);
-		if (ret) {
-			err_entry.op_context = rx_buf;
-			err_entry.flags = rx_buf->recv_entry->comp_flags;
-			err_entry.len = rx_buf->pkt.hdr.size;
-			err_entry.data = rx_buf->pkt.hdr.data;
-			err_entry.tag = rx_buf->pkt.hdr.tag;
-			err_entry.err = ret;
-			err_entry.prov_errno = ret;
-			ofi_cq_write_error(recv_queue->rxm_ep->util_ep.rx_cq,
-					   &err_entry);
-			if (rx_buf->ep->util_ep.flags & OFI_CNTR_ENABLED)
-				rxm_cntr_incerr(rx_buf->ep->util_ep.rx_cntr);
-
-			rxm_rx_buf_free(rx_buf);
-
-			if (!(rx_buf->recv_entry->flags & FI_MULTI_RECV))
-				rxm_recv_entry_release(rx_buf->recv_entry);
-		}
-		count++;
+	conn = rxm_alloc_conn(ep, peer);
+	if (!conn)
+		return NULL;
+
+	if (ofi_idm_set(&ep->conn_idx_map, peer->index, conn) < 0) {
+		rxm_free_conn(conn);
+		return NULL;
 	}
-	return count;
+
+	conn->flags |= RXM_CONN_INDEXED;
+	return conn;
 }
 
-static void
-rxm_conn_av_updated_handler(struct rxm_cmap_handle *handle)
+/* The returned conn is only valid if the function returns success. */
+ssize_t rxm_get_conn(struct rxm_ep *ep, fi_addr_t addr, struct rxm_conn **conn)
 {
-	struct rxm_ep *ep = handle->cmap->ep;
-	int count = 0;
+	struct rxm_peer_addr **peer;
+	ssize_t ret;
 
-	if (ep->rxm_info->caps & FI_DIRECTED_RECV) {
-		count += rxm_conn_reprocess_directed_recvs(&ep->recv_queue);
-		count += rxm_conn_reprocess_directed_recvs(&ep->trecv_queue);
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	peer = ofi_av_addr_context(ep->util_ep.av, addr);
+	*conn = rxm_add_conn(ep, *peer);
+	if (!*conn)
+		return -FI_ENOMEM;
 
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-		       "Reprocessed directed recvs - %d\n", count);
+	if ((*conn)->state == RXM_CM_CONNECTED) {
+		if (!dlist_empty(&(*conn)->deferred_tx_queue)) {
+			rxm_ep_do_progress(&ep->util_ep);
+			if (!dlist_empty(&(*conn)->deferred_tx_queue))
+				return -FI_EAGAIN;
+		}
+		return 0;
 	}
+
+	ret = rxm_connect(*conn);
+
+	/* If the progress function encounters an error trying to establish
+	 * the connection, it may free the connection object.  This resets
+	 * the connection process to restart from the beginning.
+	 */
+	if (ret == -FI_EAGAIN)
+		rxm_conn_progress(ep);
+	return ret;
 }
 
-static struct rxm_cmap_handle *rxm_conn_alloc(struct rxm_cmap *cmap)
+void rxm_process_connect(struct rxm_eq_cm_entry *cm_entry)
 {
-	struct rxm_conn *rxm_conn;
+	struct rxm_conn *conn;
 
-	rxm_conn = calloc(1, sizeof(*rxm_conn));
-	if (!rxm_conn)
-		return NULL;
+	conn = cm_entry->fid->context;
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
+	       "processing connected for handle: %p\n", conn);
 
-	dlist_init(&rxm_conn->deferred_conn_entry);
-	dlist_init(&rxm_conn->deferred_tx_queue);
-	dlist_init(&rxm_conn->sar_rx_msg_list);
-	dlist_init(&rxm_conn->sar_deferred_rx_msg_list);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+	if (conn->state == RXM_CM_CONNECTING)
+		conn->remote_index = cm_entry->data.accept.server_conn_id;
 
-	return &rxm_conn->handle;
+	conn->state = RXM_CM_CONNECTED;
 }
 
-static inline int
-rxm_conn_verify_cm_data(union rxm_cm_data *remote_cm_data,
-			union rxm_cm_data *local_cm_data)
+/* For simultaneous connection requests, if the peer won the coin
+ * flip (reject EALREADY), our connection request is discarded.
+ */
+static void
+rxm_process_reject(struct rxm_conn *conn, struct fi_eq_err_entry *entry)
 {
-	/* This should stay at top as it helps to avoid endian conversion
-	 * for other fields in rxm_cm_data */
-	if (remote_cm_data->connect.version != local_cm_data->connect.version) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data version mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.version,
-			remote_cm_data->connect.version);
-		goto err;
-	}
-	if (remote_cm_data->connect.endianness != local_cm_data->connect.endianness) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data endianness mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.endianness,
-			remote_cm_data->connect.endianness);
-		goto err;
-	}
-	if (remote_cm_data->connect.ctrl_version != local_cm_data->connect.ctrl_version) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data ctrl_version mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.ctrl_version,
-			remote_cm_data->connect.ctrl_version);
-		goto err;
-	}
-	if (remote_cm_data->connect.op_version != local_cm_data->connect.op_version) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data op_version mismatch "
-			"(local: %" PRIu8 ", remote:  %" PRIu8 ")\n",
-			local_cm_data->connect.op_version,
-			remote_cm_data->connect.op_version);
-		goto err;
-	}
-	if (remote_cm_data->connect.eager_limit !=
-	    local_cm_data->connect.eager_limit) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm data eager_limit mismatch "
-			"(local: %" PRIu32 ", remote:  %" PRIu32 ")\n",
-			local_cm_data->connect.eager_limit,
-			remote_cm_data->connect.eager_limit);
-		goto err;
+	union rxm_cm_data *cm_data;
+	uint8_t reason;
+
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
+	       "Processing reject for handle: %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+
+	if (entry->err_data_size >= sizeof(cm_data->reject)) {
+		cm_data = entry->err_data;
+		if (cm_data->reject.version != RXM_CM_DATA_VERSION) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "invalid reject version\n");
+			reason = RXM_REJECT_ECONNREFUSED;
+		} else {
+			reason = cm_data->reject.reason;
+		}
+	} else {
+		reason = RXM_REJECT_ECONNREFUSED;
 	}
-	return FI_SUCCESS;
-err:
-	return -FI_EINVAL;
-}
 
-static size_t rxm_conn_get_rx_size(struct rxm_ep *rxm_ep,
-				   struct fi_info *msg_info)
-{
-	if (msg_info->ep_attr->rx_ctx_cnt == FI_SHARED_CONTEXT)
-		return MAX(MIN(16, msg_info->rx_attr->size),
-			   (msg_info->rx_attr->size /
-			    ofi_av_size(rxm_ep->util_ep.av)));
-	else
-		return msg_info->rx_attr->size;
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		/* Unlikely, but can occur if our request was rejected, and
+		 * there was a failure trying to accept the peer's.
+		 */
+		break;
+	case RXM_CM_CONNECTING:
+		rxm_close_conn(conn);
+		if (reason != RXM_REJECT_EALREADY)
+			rxm_free_conn(conn);
+		break;
+	case RXM_CM_ACCEPTING:
+	case RXM_CM_CONNECTED:
+		/* Our request was rejected, but we accepted the peer's. */
+		break;
+	default:
+		assert(0);
+		break;
+	}
 }
 
 static int
-rxm_msg_process_connreq(struct rxm_ep *rxm_ep, struct fi_info *msg_info,
-			union rxm_cm_data *remote_cm_data)
+rxm_verify_connreq(struct rxm_ep *ep, union rxm_cm_data *cm_data)
 {
-	struct rxm_conn *rxm_conn;
-	union rxm_cm_data cm_data = {
-		.connect = {
-			.version = RXM_CM_DATA_VERSION,
-			.endianness = ofi_detect_endianness(),
-			.ctrl_version = RXM_CTRL_VERSION,
-			.op_version = RXM_OP_VERSION,
-			.eager_limit = rxm_ep->eager_limit,
-		},
-	};
-	union rxm_cm_data reject_cm_data = {
-		.reject = {
-			.version = RXM_CM_DATA_VERSION,
-			.reason = RXM_CMAP_REJECT_GENUINE,
-		}
-	};
-	struct rxm_cmap_handle *handle;
-	struct sockaddr_storage remote_pep_addr;
-	int ret;
-
-	assert(sizeof(uint32_t) == sizeof(cm_data.accept.rx_size));
-	assert(msg_info->rx_attr->size <= (uint32_t)-1);
-
-	if (rxm_conn_verify_cm_data(remote_cm_data, &cm_data)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"CM data mismatch was detected\n");
-		ret = -FI_EINVAL;
-		goto err1;
+	if (cm_data->connect.version != RXM_CM_DATA_VERSION) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm version mismatch");
+		return -FI_EINVAL;
 	}
 
-	memcpy(&remote_pep_addr, msg_info->dest_addr, msg_info->dest_addrlen);
-	ofi_addr_set_port((struct sockaddr *)&remote_pep_addr,
-			  remote_cm_data->connect.port);
+	if (cm_data->connect.endianness != ofi_detect_endianness()) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "endianness mismatch");
+		return -FI_EINVAL;
+	}
 
-	ret = rxm_cmap_process_connreq(rxm_ep->cmap, &remote_pep_addr,
-				       &handle, &reject_cm_data.reject.reason);
-	if (ret)
-		goto err1;
+	if (cm_data->connect.ctrl_version != RXM_CTRL_VERSION) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm ctrl_version mismatch");
+		return -FI_EINVAL;
+	}
 
-	rxm_conn = container_of(handle, struct rxm_conn, handle);
+	if (cm_data->connect.op_version != RXM_OP_VERSION) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "cm op_version mismatch");
+		return -FI_EINVAL;
+	}
 
-	rxm_conn->handle.remote_key = remote_cm_data->connect.client_conn_id;
+	if (cm_data->connect.eager_limit != ep->eager_limit) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "eager_limit mismatch");
+		return -FI_EINVAL;
+	}
 
-	ret = rxm_msg_ep_open(rxm_ep, msg_info, rxm_conn, handle);
-	if (ret)
-		goto err2;
+	return FI_SUCCESS;
+}
 
-	cm_data.accept.server_conn_id = rxm_conn->handle.key;
-	cm_data.accept.rx_size = rxm_conn_get_rx_size(rxm_ep, msg_info);
+static void
+rxm_reject_connreq(struct rxm_ep *ep, struct rxm_eq_cm_entry *cm_entry,
+		   uint8_t reason)
+{
+	union rxm_cm_data cm_data;
 
-	ret = fi_accept(rxm_conn->msg_ep, &cm_data.accept.server_conn_id,
-			sizeof(cm_data.accept));
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to accept incoming connection\n");
-		goto err2;
-	}
+	cm_data.reject.version = RXM_CM_DATA_VERSION;
+	cm_data.reject.reason = reason;
 
-	return ret;
-err2:
-	rxm_cmap_del_handle(&rxm_conn->handle);
-err1:
-	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-	       "rejecting incoming connection request (reject reason: %d)\n",
-	       (enum rxm_cmap_reject_reason)reject_cm_data.reject.reason);
-	fi_reject(rxm_ep->msg_pep, msg_info->handle,
-		  &reject_cm_data.reject, sizeof(reject_cm_data.reject));
-	return ret;
+	fi_reject(ep->msg_pep, cm_entry->info->handle,
+		  &cm_data.reject, sizeof(cm_data.reject));
 }
 
-static void rxm_flush_msg_cq(struct rxm_ep *rxm_ep)
+static int
+rxm_accept_connreq(struct rxm_conn *conn, struct rxm_eq_cm_entry *cm_entry)
 {
-	struct fi_cq_data_entry comp;
-	int ret;
-	do {
-		ret = fi_cq_read(rxm_ep->msg_cq, &comp, 1);
-		if (ret > 0) {
-			ret = rxm_handle_comp(rxm_ep, &comp);
-			if (OFI_UNLIKELY(ret)) {
-				rxm_cq_write_error_all(rxm_ep, ret);
-			} else {
-				ret = 1;
-			}
-		} else if (ret == -FI_EAVAIL) {
-			rxm_handle_comp_error(rxm_ep);
-			ret = 1;
-		} else if (ret < 0 && ret != -FI_EAGAIN) {
-			rxm_cq_write_error_all(rxm_ep, ret);
-		}
-	} while (ret > 0);
+	union rxm_cm_data cm_data;
+
+	cm_data.accept.server_conn_id = conn->peer->index;
+	cm_data.accept.rx_size = cm_entry->info->rx_attr->size;
+
+	return fi_accept(conn->msg_ep, &cm_data.accept, sizeof(cm_data.accept));
 }
 
-static int rxm_conn_handle_notify(struct fi_eq_entry *eq_entry)
+static void
+rxm_process_connreq(struct rxm_ep *ep, struct rxm_eq_cm_entry *cm_entry)
 {
-	struct rxm_cmap *cmap;
-	struct rxm_cmap_handle *handle;
+	union ofi_sock_ip peer_addr;
+	struct rxm_peer_addr *peer;
+	struct rxm_conn *conn;
+	struct rxm_av *av;
+	ssize_t ret;
+	int cmp;
 
-	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "notify event %" PRIu64 "\n",
-		eq_entry->data);
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	if (rxm_verify_connreq(ep, &cm_entry->data))
+		goto reject;
 
-	if ((enum rxm_cmap_signal) eq_entry->data != RXM_CMAP_FREE)
-		return -FI_EOTHER;
+	memcpy(&peer_addr, cm_entry->info->dest_addr,
+	       cm_entry->info->dest_addrlen);
+	ofi_addr_set_port(&peer_addr.sa, cm_entry->data.connect.port);
 
-	handle = eq_entry->context;
-	assert(handle->state == RXM_CMAP_SHUTDOWN);
-	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "freeing handle: %p\n", handle);
-	cmap = handle->cmap;
+	av = container_of(ep->util_ep.av, struct rxm_av, util_av);
+	peer = rxm_get_peer(av, &peer_addr);
+	if (!peer)
+		goto reject;
 
-	rxm_conn_close(handle);
+	conn = rxm_add_conn(ep, peer);
+	if (!conn)
+		goto remove;
 
-	// after closing the connection, we need to flush any dangling references to the
-	// handle from msg_cq entries that have not been cleaned up yet, otherwise we
-	// could run into problems during CQ cleanup.  these entries will be errored so
-	// keep reading through EAVAIL.
-	rxm_flush_msg_cq(cmap->ep);
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connreq for %p\n", conn);
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		break;
+	case RXM_CM_CONNECTING:
+		/* simultaneous connections */
+		cmp = ofi_addr_cmp(&rxm_prov, &peer_addr.sa, &ep->addr.sa);
+		if (cmp < 0) {
+			/* let our request finish */
+			rxm_reject_connreq(ep, cm_entry,
+					   RXM_REJECT_ECONNREFUSED);
+			goto put;
+		} else if (cmp > 0) {
+			/* accept peer's request */
+			rxm_close_conn(conn);
+		} else {
+			/* connecting to ourself, create loopback conn */
+			conn = rxm_alloc_conn(ep, peer);
+			if (!conn)
+				goto remove;
 
-	if (handle->peer) {
-		dlist_remove(&handle->peer->entry);
-		free(handle->peer);
-		handle->peer = NULL;
+			dlist_insert_tail(&conn->loopback_entry, &ep->loopback_list);
+			break;
+		}
+		break;
+	case RXM_CM_ACCEPTING:
+	case RXM_CM_CONNECTED:
+		goto put;
+	default:
+		assert(0);
+		break;
 	}
 
-	if (handle->fi_addr != FI_ADDR_NOTAVAIL) {
-		cmap->handles_av[handle->fi_addr] = NULL;
-		handle->fi_addr = FI_ADDR_NOTAVAIL;
-	}
+	conn->remote_index = cm_entry->data.connect.client_conn_id;
+	ret = rxm_open_conn(conn, cm_entry->info);
+	if (ret)
+		goto free;
 
-	rxm_conn_free(handle);
-	return 0;
+	ret = rxm_accept_connreq(conn, cm_entry);
+	if (ret)
+		goto close;
+
+	conn->state = RXM_CM_ACCEPTING;
+put:
+	rxm_put_peer(peer);
+	fi_freeinfo(cm_entry->info);
+	return;
+
+close:
+	rxm_close_conn(conn);
+free:
+	rxm_free_conn(conn);
+remove:
+	rxm_put_peer(peer);
+reject:
+	rxm_reject_connreq(ep, cm_entry, RXM_REJECT_ECONNREFUSED);
+	fi_freeinfo(cm_entry->info);
 }
 
-static void rxm_conn_wake_up_wait_obj(struct rxm_ep *rxm_ep)
+void rxm_process_shutdown(struct rxm_conn *conn)
 {
-	if (rxm_ep->util_ep.tx_cq && rxm_ep->util_ep.tx_cq->wait)
-		util_cq_signal(rxm_ep->util_ep.tx_cq);
-	if (rxm_ep->util_ep.tx_cntr && rxm_ep->util_ep.tx_cntr->wait)
-		util_cntr_signal(rxm_ep->util_ep.tx_cntr);
+	FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "shutdown conn %p\n", conn);
+	assert(ofi_ep_lock_held(&conn->ep->util_ep));
+
+	switch (conn->state) {
+	case RXM_CM_IDLE:
+		break;
+	case RXM_CM_CONNECTING:
+	case RXM_CM_ACCEPTING:
+	case RXM_CM_CONNECTED:
+		rxm_close_conn(conn);
+		rxm_free_conn(conn);
+		break;
+	default:
+		break;
+	}
 }
 
-static int
-rxm_conn_handle_reject(struct rxm_ep *rxm_ep, struct rxm_msg_eq_entry *entry)
+static void rxm_handle_error(struct rxm_ep *ep)
 {
-	union rxm_cm_data *cm_data = entry->err_entry.err_data;
-
-	if (!cm_data || entry->err_entry.err_data_size != sizeof(cm_data->reject)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-			"no reject error data (cm_data) was found "
-			"(data length expected: %zu found: %zu)\n",
-			sizeof(cm_data->reject),
-			entry->err_entry.err_data_size);
-		return -FI_EOTHER;
-	}
+	struct fi_eq_err_entry entry = {0};
+	ssize_t ret;
 
-	if (cm_data->reject.version != RXM_CM_DATA_VERSION) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-			"cm data version mismatch (local: %" PRIu8
-			", remote:  %" PRIu8 ")\n",
-			(uint8_t) RXM_CM_DATA_VERSION,
-			cm_data->reject.version);
-		return -FI_EOTHER;
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	ret = fi_eq_readerr(ep->msg_eq, &entry, 0);
+	if (ret != sizeof(entry)) {
+		if (ret != -FI_EAGAIN)
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"unable to fi_eq_readerr: %zd\n", ret);
+		return;
 	}
 
-	if (cm_data->reject.reason == RXM_CMAP_REJECT_GENUINE) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		       "remote peer didn't accept the connection\n");
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		       "(reason: RXM_CMAP_REJECT_GENUINE)\n");
-		OFI_EQ_STRERROR(&rxm_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
-				rxm_ep->msg_eq, &entry->err_entry);
-	} else if (cm_data->reject.reason == RXM_CMAP_REJECT_SIMULT_CONN) {
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		       "(reason: RXM_CMAP_REJECT_SIMULT_CONN)\n");
+	if (entry.err == ECONNREFUSED) {
+		rxm_process_reject(entry.fid->context, &entry);
 	} else {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "connection reject: "
-		        "received unknown reject reason: %d\n",
-			cm_data->reject.reason);
+		OFI_EQ_STRERROR(&rxm_prov, FI_LOG_WARN, FI_LOG_EP_CTRL,
+				ep->msg_eq, &entry);
 	}
-	rxm_cmap_process_reject(rxm_ep->cmap, entry->context,
-				cm_data->reject.reason);
-	return 0;
 }
 
-static int
-rxm_conn_handle_event(struct rxm_ep *rxm_ep, struct rxm_msg_eq_entry *entry)
+static void
+rxm_handle_event(struct rxm_ep *ep, uint32_t event,
+		 struct rxm_eq_cm_entry *cm_entry, size_t len)
 {
-	if (entry->rd == -FI_ECONNREFUSED)
-		return rxm_conn_handle_reject(rxm_ep, entry);
-
-	switch (entry->event) {
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	switch (event) {
 	case FI_NOTIFY:
-		return rxm_conn_handle_notify((struct fi_eq_entry *)
-					      &entry->cm_entry);
+		break;
 	case FI_CONNREQ:
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL, "Got new connection\n");
-		if ((size_t)entry->rd != RXM_CM_ENTRY_SZ) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"Received a connection request with no CM data. "
-				"Is sender running FI_PROTO_RXM?\n");
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Received CM entry "
-				"size (%zd) not matching expected (%zu)\n",
-				entry->rd, RXM_CM_ENTRY_SZ);
-			return -FI_EOTHER;
-		}
-		rxm_msg_process_connreq(rxm_ep, entry->cm_entry.info,
-					(union rxm_cm_data *) entry->cm_entry.data);
-		fi_freeinfo(entry->cm_entry.info);
+		rxm_process_connreq(ep, cm_entry);
 		break;
 	case FI_CONNECTED:
-		assert(entry->cm_entry.fid->context);
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-		       "connection successful\n");
-		rxm_cmap_process_connect(rxm_ep->cmap,
-			entry->cm_entry.fid->context,
-			entry->rd - sizeof(entry->cm_entry) > 0 ?
-			(union rxm_cm_data *) entry->cm_entry.data : NULL);
-		rxm_conn_wake_up_wait_obj(rxm_ep);
+		rxm_process_connect(cm_entry);
 		break;
 	case FI_SHUTDOWN:
-		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
-		       "Received connection shutdown\n");
-		rxm_cmap_process_shutdown(rxm_ep->cmap,
-					  entry->cm_entry.fid->context);
+		rxm_process_shutdown(cm_entry->fid->context);
 		break;
 	default:
 		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unknown event: %u\n", entry->event);
-		return -FI_EOTHER;
+			"Unknown event: %u\n", event);
+		break;
 	}
-	return 0;
 }
 
-static ssize_t rxm_eq_sread(struct rxm_ep *rxm_ep, size_t len,
-			    struct rxm_msg_eq_entry *entry)
+void rxm_conn_progress(struct rxm_ep *ep)
 {
-	ssize_t rd;
+	struct rxm_eq_cm_entry cm_entry;
+	uint32_t event;
+	int ret;
 
-	/* TODO convert this to poll + fi_eq_read so that we can grab
-	 * rxm_ep lock before reading the EQ. This is needed to avoid
-	 * processing events / error entries from closed MSG EPs. This
-	 * can be done only for non-Windows OSes as Windows doesn't
-	 * have poll for a generic file descriptor.
-	 */
-	rd = fi_eq_sread(rxm_ep->msg_eq, &entry->event, &entry->cm_entry,
-			 len, -1, 0);
-	if (rd >= 0)
-		return rd;
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	do {
+		ret = fi_eq_read(ep->msg_eq, &event, &cm_entry,
+				 sizeof(cm_entry), 0);
+		if (ret > 0) {
+			rxm_handle_event(ep, event, &cm_entry, ret);
+		} else if (ret == -FI_EAVAIL) {
+			rxm_handle_error(ep);
+			ret = 1;
+		}
+	} while (ret > 0);
+}
 
-	if (rd != -FI_EAVAIL) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"unable to fi_eq_sread: %s (%zd)\n",
-			fi_strerror(-rd), -rd);
-		return rd;
+void rxm_stop_listen(struct rxm_ep *ep)
+{
+	struct fi_eq_entry entry = {0};
+	int ret;
+
+	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "stopping CM thread\n");
+	if (!ep->cm_thread)
+		return;
+
+	ofi_ep_lock_acquire(&ep->util_ep);
+	ep->do_progress = false;
+	ofi_ep_lock_release(&ep->util_ep);
+
+	ret = fi_eq_write(ep->msg_eq, FI_NOTIFY, &entry, sizeof(entry), 0);
+	if (ret != sizeof(entry)) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to signal\n");
+		return;
 	}
 
-	ofi_ep_lock_acquire(&rxm_ep->util_ep);
-	rd = rxm_eq_readerr(rxm_ep, entry);
-	ofi_ep_lock_release(&rxm_ep->util_ep);
-	return rd;
+	ret = pthread_join(ep->cm_thread, NULL);
+	if (ret) {
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"Unable to join CM thread\n");
+	}
 }
 
-static inline int rxm_conn_eq_event(struct rxm_ep *rxm_ep,
-				    struct rxm_msg_eq_entry *entry)
+static void rxm_flush_msg_cq(struct rxm_ep *ep)
 {
+	struct fi_cq_data_entry comp;
 	int ret;
 
-	ofi_ep_lock_acquire(&rxm_ep->util_ep);
-	ret = rxm_conn_handle_event(rxm_ep, entry) ? -1 : 0;
-	ofi_ep_lock_release(&rxm_ep->util_ep);
-
-	return ret;
+	assert(ofi_ep_lock_held(&ep->util_ep));
+	do {
+		ret = fi_cq_read(ep->msg_cq, &comp, 1);
+		if (ret > 0) {
+			ret = rxm_handle_comp(ep, &comp);
+			if (ret) {
+				rxm_cq_write_error_all(ep, ret);
+			} else {
+				ret = 1;
+			}
+		} else if (ret == -FI_EAVAIL) {
+			rxm_handle_comp_error(ep);
+			ret = 1;
+		} else if (ret < 0 && ret != -FI_EAGAIN) {
+			rxm_cq_write_error_all(ep, ret);
+		}
+	} while (ret > 0);
 }
 
-static void *rxm_conn_progress(void *arg)
+static void *rxm_cm_progress(void *arg)
 {
 	struct rxm_ep *ep = container_of(arg, struct rxm_ep, util_ep);
-	struct rxm_msg_eq_entry *entry;
-
-	entry = alloca(RXM_MSG_EQ_ENTRY_SZ);
-	if (!entry)
-		return NULL;
+	struct rxm_eq_cm_entry cm_entry;
+	uint32_t event;
+	ssize_t ret;
 
 	FI_INFO(&rxm_prov, FI_LOG_EP_CTRL, "Starting auto-progress thread\n");
 
 	ofi_ep_lock_acquire(&ep->util_ep);
 	while (ep->do_progress) {
 		ofi_ep_lock_release(&ep->util_ep);
-		memset(entry, 0, RXM_MSG_EQ_ENTRY_SZ);
-		entry->rd = rxm_eq_sread(ep, RXM_CM_ENTRY_SZ, entry);
-		if (entry->rd >= 0 || entry->rd == -FI_ECONNREFUSED)
-			rxm_conn_eq_event(ep, entry);
+
+		ret = fi_eq_sread(ep->msg_eq, &event, &cm_entry,
+				  sizeof(cm_entry), -1, 0);
 
 		ofi_ep_lock_acquire(&ep->util_ep);
+		if (ret > 0) {
+			rxm_handle_event(ep, event, &cm_entry, ret);
+		} else if (ret == -FI_EAVAIL) {
+			rxm_handle_error(ep);
+		} else {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"Fatal error reading from msg eq");
+			break;
+		}
 	}
 	ofi_ep_lock_release(&ep->util_ep);
 
@@ -1284,27 +757,9 @@ static void *rxm_conn_progress(void *arg)
 	return NULL;
 }
 
-static inline int
-rxm_conn_auto_progress_eq(struct rxm_ep *rxm_ep, struct rxm_msg_eq_entry *entry)
-{
-	memset(entry, 0, RXM_MSG_EQ_ENTRY_SZ);
-
-	ofi_ep_lock_acquire(&rxm_ep->util_ep);
-	entry->rd = rxm_eq_read(rxm_ep, RXM_CM_ENTRY_SZ, entry);
-	ofi_ep_lock_release(&rxm_ep->util_ep);
-
-	if (!entry->rd || entry->rd == -FI_EAGAIN)
-		return FI_SUCCESS;
-	if (entry->rd < 0 && entry->rd != -FI_ECONNREFUSED)
-		return entry->rd;
-
-	return rxm_conn_eq_event(rxm_ep, entry);
-}
-
-static void *rxm_conn_atomic_progress(void *arg)
+static void *rxm_cm_atomic_progress(void *arg)
 {
 	struct rxm_ep *ep = container_of(arg, struct rxm_ep, util_ep);
-	struct rxm_msg_eq_entry *entry;
 	struct rxm_fabric *fabric;
 	struct fid *fids[2] = {
 		&ep->msg_eq->fid,
@@ -1316,13 +771,8 @@ static void *rxm_conn_atomic_progress(void *arg)
 	};
 	int ret;
 
-	entry = alloca(RXM_MSG_EQ_ENTRY_SZ);
-	if (!entry)
-		return NULL;
-
 	fabric = container_of(ep->util_ep.domain->fabric,
 			      struct rxm_fabric, util_fabric);
-
 	ret = fi_control(&ep->msg_eq->fid, FI_GETWAIT, &fds[0].fd);
 	if (ret) {
 		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
@@ -1344,18 +794,15 @@ static void *rxm_conn_atomic_progress(void *arg)
 		ret = fi_trywait(fabric->msg_fabric, fids, 2);
 
 		if (!ret) {
-			fds[0].revents = 0;
-			fds[1].revents = 0;
-
 			ret = poll(fds, 2, -1);
 			if (ret == -1) {
 				FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
 					"Select error %s\n", strerror(errno));
 			}
 		}
-		rxm_conn_auto_progress_eq(ep, entry);
 		ep->util_ep.progress(&ep->util_ep);
 		ofi_ep_lock_acquire(&ep->util_ep);
+		rxm_conn_progress(ep);
 	}
 	ofi_ep_lock_release(&ep->util_ep);
 
@@ -1363,141 +810,39 @@ static void *rxm_conn_atomic_progress(void *arg)
 	return NULL;
 }
 
-static int rxm_prepare_cm_data(struct fid_pep *pep, struct rxm_cmap_handle *handle,
-		union rxm_cm_data *cm_data)
+int rxm_start_listen(struct rxm_ep *ep)
 {
-	struct sockaddr_storage name;
-	size_t cm_data_size = 0;
-	size_t name_size = sizeof(name);
-	size_t opt_size = sizeof(cm_data_size);
+	size_t addr_len;
 	int ret;
 
-	ret = fi_getopt(&pep->fid, FI_OPT_ENDPOINT, FI_OPT_CM_DATA_SIZE,
-			&cm_data_size, &opt_size);
+	ret = fi_listen(ep->msg_pep);
 	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "fi_getopt failed\n");
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"unable to set msg PEP to listen state\n");
 		return ret;
 	}
 
-	if (cm_data_size < sizeof(*cm_data)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "MSG EP CM data size too small\n");
-		return -FI_EOTHER;
-	}
-
-	ret = fi_getname(&pep->fid, &name, &name_size);
+	addr_len = sizeof(ep->addr);
+	ret = fi_getname(&ep->msg_pep->fid, &ep->addr, &addr_len);
 	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to get msg pep name\n");
-		return ret;
-	}
-
-	cm_data->connect.port = ofi_addr_get_port((struct sockaddr *)&name);
-	cm_data->connect.client_conn_id = handle->key;
-	return 0;
-}
-
-static int
-rxm_conn_connect(struct rxm_ep *ep, struct rxm_cmap_handle *handle,
-		 const void *addr)
-{
-	int ret;
-	struct rxm_conn *rxm_conn = container_of(handle, struct rxm_conn, handle);
-	union rxm_cm_data cm_data = {
-		.connect = {
-			.version = RXM_CM_DATA_VERSION,
-			.ctrl_version = RXM_CTRL_VERSION,
-			.op_version = RXM_OP_VERSION,
-			.endianness = ofi_detect_endianness(),
-			.eager_limit = ep->eager_limit,
-		},
-	};
-
-	assert(sizeof(uint32_t) == sizeof(cm_data.connect.eager_limit));
-	assert(sizeof(uint32_t) == sizeof(cm_data.connect.rx_size));
-	assert(ep->msg_info->rx_attr->size <= (uint32_t) -1);
-
-	free(ep->msg_info->dest_addr);
-	ep->msg_info->dest_addrlen = ep->msg_info->src_addrlen;
-
-	ep->msg_info->dest_addr = mem_dup(addr, ep->msg_info->dest_addrlen);
-	if (!ep->msg_info->dest_addr) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "mem_dup failed, len %zu\n",
-			ep->msg_info->dest_addrlen);
-		return -FI_ENOMEM;
-	}
-
-	ret = rxm_msg_ep_open(ep, ep->msg_info, rxm_conn, &rxm_conn->handle);
-	if (ret)
+		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+			"Unable to get msg pep name\n");
 		return ret;
-
-	/* We have to send passive endpoint's address to the server since the
-	 * address from which connection request would be sent would have a
-	 * different port. */
-	ret = rxm_prepare_cm_data(ep->msg_pep, &rxm_conn->handle, &cm_data);
-	if (ret)
-		goto err;
-
-	cm_data.connect.rx_size = rxm_conn_get_rx_size(ep, ep->msg_info);
-
-	ret = fi_connect(rxm_conn->msg_ep, ep->msg_info->dest_addr,
-			 &cm_data, sizeof(cm_data));
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "unable to connect msg_ep\n");
-		goto err;
 	}
-	return 0;
-
-err:
-	fi_close(&rxm_conn->msg_ep->fid);
-	rxm_conn->msg_ep = NULL;
-	return ret;
-}
-
-static int rxm_conn_signal(struct rxm_ep *ep, void *context,
-			   enum rxm_cmap_signal signal)
-{
-	struct fi_eq_entry entry = {0};
-	ssize_t rd;
 
-	entry.context = context;
-	entry.data = (uint64_t) signal;
+	if (ep->util_ep.domain->data_progress == FI_PROGRESS_AUTO ||
+	    force_auto_progress) {
 
-	rd = fi_eq_write(ep->msg_eq, FI_NOTIFY, &entry, sizeof(entry), 0);
-	if (rd != sizeof(entry)) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to signal\n");
-		return (int)rd;
+		assert(ep->util_ep.domain->threading == FI_THREAD_SAFE);
+		ep->do_progress = true;
+		if (pthread_create(&ep->cm_thread, 0,
+				   ep->rxm_info->caps & FI_ATOMIC ?
+				   rxm_cm_atomic_progress :
+				   rxm_cm_progress, ep)) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"unable to create cm thread\n");
+			return -ofi_syserr();
+		}
 	}
 	return 0;
 }
-
-int rxm_conn_cmap_alloc(struct rxm_ep *rxm_ep)
-{
-	struct rxm_cmap_attr attr;
-	int ret;
-	size_t len = rxm_ep->util_ep.av->addrlen;
-	void *name = calloc(1, len);
-	if (!name) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to allocate memory for EP name\n");
-		return -FI_ENOMEM;
-	}
-
-	/* Passive endpoint should already have fi_setname or fi_listen
-	 * called on it for this to work */
-	ret = fi_getname(&rxm_ep->msg_pep->fid, name, &len);
-	if (ret) {
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to fi_getname on msg_ep\n");
-		goto fn;
-	}
-	ofi_straddr_dbg(&rxm_prov, FI_LOG_EP_CTRL, "local_name", name);
-
-	attr.name		= name;
-
-	ret = rxm_cmap_alloc(rxm_ep, &attr);
-	if (ret)
-		FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-			"Unable to allocate CMAP\n");
-fn:
-	free(name);
-	return ret;
-}
diff --git a/prov/rxm/src/rxm_cq.c b/prov/rxm/src/rxm_cq.c
index d2e43c7..579c955 100644
--- a/prov/rxm/src/rxm_cq.c
+++ b/prov/rxm/src/rxm_cq.c
@@ -77,10 +77,8 @@ rxm_rx_buf_alloc(struct rxm_ep *rxm_ep, struct fid_ep *rx_ep)
 	rx_buf->rx_ep = rx_ep;
 	rx_buf->repost = true;
 
-	if (!rxm_ep->srx_ctx) {
-		rx_buf->conn = container_of(rx_ep->fid.context,
-					    struct rxm_conn, handle);
-	}
+	if (!rxm_ep->srx_ctx)
+		rx_buf->conn = rx_ep->fid.context;
 
 	return rx_buf;
 }
@@ -112,7 +110,7 @@ static void rxm_finish_buf_recv(struct rxm_rx_buf *rx_buf)
 	if ((rx_buf->pkt.ctrl_hdr.type == rxm_ctrl_seg) &&
 	    rxm_sar_get_seg_type(&rx_buf->pkt.ctrl_hdr) != RXM_SAR_SEG_FIRST) {
 		dlist_insert_tail(&rx_buf->unexp_msg.entry,
-				  &rx_buf->conn->sar_deferred_rx_msg_list);
+				  &rx_buf->conn->deferred_sar_segments);
 		rxm_replace_rx_buf(rx_buf);
 	}
 
@@ -411,15 +409,15 @@ static void rxm_process_seg_data(struct rxm_rx_buf *rx_buf, int *done)
 	} else {
 		if (rx_buf->recv_entry->sar.msg_id == RXM_SAR_RX_INIT) {
 			if (!rx_buf->conn) {
-				rx_buf->conn = rxm_key2conn(rx_buf->ep,
-							    rx_buf->pkt.ctrl_hdr.conn_id);
+				rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+						(int) rx_buf->pkt.ctrl_hdr.conn_id);
 			}
 
 			rx_buf->recv_entry->sar.conn = rx_buf->conn;
 			rx_buf->recv_entry->sar.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 
 			dlist_insert_tail(&rx_buf->recv_entry->sar.entry,
-					  &rx_buf->conn->sar_rx_msg_list);
+					  &rx_buf->conn->deferred_sar_msgs);
 		}
 
 		/* The RX buffer can be reposted for further re-use */
@@ -446,7 +444,7 @@ static void rxm_handle_seg_data(struct rxm_rx_buf *rx_buf)
 	conn = rx_buf->conn;
 	msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 
-	dlist_foreach_container_safe(&conn->sar_deferred_rx_msg_list,
+	dlist_foreach_container_safe(&conn->deferred_sar_segments,
 				     struct rxm_rx_buf, rx_buf,
 				     unexp_msg.entry, entry) {
 		if (!rxm_rx_buf_match_msg_id(&rx_buf->unexp_msg.entry, &msg_id))
@@ -495,7 +493,7 @@ static ssize_t rxm_rndv_xfer(struct rxm_ep *rxm_ep, struct fid_ep *msg_ep,
 
 				if (ret)
 					break;
-				rxm_ep_enqueue_deferred_tx_queue(def_tx_entry);
+				rxm_queue_deferred_tx(def_tx_entry, OFI_LIST_TAIL);
 				continue;
 			}
 			break;
@@ -579,8 +577,8 @@ static ssize_t rxm_handle_rndv(struct rxm_rx_buf *rx_buf)
 
 	if (!rx_buf->conn) {
 		assert(rx_buf->ep->srx_ctx);
-		rx_buf->conn = rxm_key2conn(rx_buf->ep,
-					    rx_buf->pkt.ctrl_hdr.conn_id);
+		rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					  (int) rx_buf->pkt.ctrl_hdr.conn_id);
 		if (!rx_buf->conn)
 			return -FI_EOTHER;
 	}
@@ -770,11 +768,11 @@ static ssize_t rxm_handle_recv_comp(struct rxm_rx_buf *rx_buf)
 
 	if (rx_buf->ep->rxm_info->caps & (FI_SOURCE | FI_DIRECTED_RECV)) {
 		if (rx_buf->ep->srx_ctx)
-			rx_buf->conn = rxm_key2conn(rx_buf->ep, rx_buf->
-						    pkt.ctrl_hdr.conn_id);
+			rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					(int) rx_buf->pkt.ctrl_hdr.conn_id);
 		if (!rx_buf->conn)
 			return -FI_EOTHER;
-		match_attr.addr = rx_buf->conn->handle.fi_addr;
+		match_attr.addr = rx_buf->conn->peer->fi_addr;
 	}
 
 	if (rx_buf->ep->rxm_info->mode & FI_BUFFERED_RECV) {
@@ -812,15 +810,15 @@ static ssize_t rxm_sar_handle_segment(struct rxm_rx_buf *rx_buf)
 {
 	struct dlist_entry *sar_entry;
 
-	rx_buf->conn = rxm_key2conn(rx_buf->ep,
-				    rx_buf->pkt.ctrl_hdr.conn_id);
+	rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+				  (int) rx_buf->pkt.ctrl_hdr.conn_id);
 	if (!rx_buf->conn)
 		return -FI_EOTHER;
 
 	FI_DBG(&rxm_prov, FI_LOG_CQ,
 	       "Got incoming recv with msg_id: 0x%" PRIx64 " for conn - %p\n",
 	       rx_buf->pkt.ctrl_hdr.msg_id, rx_buf->conn);
-	sar_entry = dlist_find_first_match(&rx_buf->conn->sar_rx_msg_list,
+	sar_entry = dlist_find_first_match(&rx_buf->conn->deferred_sar_msgs,
 					   rxm_sar_match_msg_id,
 					   &rx_buf->pkt.ctrl_hdr.msg_id);
 	if (!sar_entry)
@@ -849,7 +847,7 @@ static void rxm_rndv_send_rd_done(struct rxm_rx_buf *rx_buf)
 	rx_buf->recv_entry->rndv.tx_buf = buf;
 
 	buf->pkt.ctrl_hdr.type = rxm_ctrl_rndv_rd_done;
-	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->handle.remote_key;
+	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->remote_index;
 	buf->pkt.ctrl_hdr.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 
 	ret = fi_send(rx_buf->conn->msg_ep, &buf->pkt, sizeof(buf->pkt),
@@ -862,7 +860,7 @@ static void rxm_rndv_send_rd_done(struct rxm_rx_buf *rx_buf)
 			if (def_entry) {
 				def_entry->rndv_ack.rx_buf = rx_buf;
 				def_entry->rndv_ack.pkt_size = sizeof(rx_buf->pkt);
-				rxm_ep_enqueue_deferred_tx_queue(def_entry);
+				rxm_queue_deferred_tx(def_entry, OFI_LIST_TAIL);
 				return;
 			}
 		}
@@ -915,7 +913,7 @@ rxm_rndv_send_wr_done(struct rxm_ep *rxm_ep, struct rxm_tx_buf *tx_buf)
 						RXM_DEFERRED_TX_RNDV_DONE);
 			if (def_entry) {
 				def_entry->rndv_done.tx_buf = tx_buf;
-				rxm_ep_enqueue_deferred_tx_queue(def_entry);
+				rxm_queue_deferred_tx(def_entry, OFI_LIST_TAIL);
 				return;
 			}
 		}
@@ -956,7 +954,7 @@ ssize_t rxm_rndv_send_wr_data(struct rxm_rx_buf *rx_buf)
 	rx_buf->recv_entry->rndv.tx_buf = buf;
 
 	buf->pkt.ctrl_hdr.type = rxm_ctrl_rndv_wr_data;
-	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->handle.remote_key;
+	buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->remote_index;
 	buf->pkt.ctrl_hdr.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 	rxm_rndv_hdr_init(rx_buf->ep, buf->pkt.data,
 			  rx_buf->recv_entry->rxm_iov.iov,
@@ -974,7 +972,7 @@ ssize_t rxm_rndv_send_wr_data(struct rxm_rx_buf *rx_buf)
 				def_entry->rndv_ack.pkt_size =
 						sizeof(buf->pkt) +
 						sizeof(struct rxm_rndv_hdr);
-				rxm_ep_enqueue_deferred_tx_queue(def_entry);
+				rxm_queue_deferred_tx(def_entry, OFI_LIST_TAIL);
 				return 0;
 			}
 		}
@@ -1040,7 +1038,7 @@ static ssize_t rxm_atomic_send_resp(struct rxm_ep *rxm_ep,
 				       rx_buf->pkt.hdr.op,
 				       rx_buf->pkt.hdr.atomic.datatype,
 				       rx_buf->pkt.hdr.atomic.op);
-	resp_buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->handle.remote_key;
+	resp_buf->pkt.ctrl_hdr.conn_id = rx_buf->conn->remote_index;
 	resp_buf->pkt.ctrl_hdr.msg_id = rx_buf->pkt.ctrl_hdr.msg_id;
 	atomic_hdr = (struct rxm_atomic_resp_hdr *) resp_buf->pkt.data;
 	atomic_hdr->status = htonl(status);
@@ -1071,7 +1069,7 @@ static ssize_t rxm_atomic_send_resp(struct rxm_ep *rxm_ep,
 
 			def_tx_entry->atomic_resp.tx_buf = resp_buf;
 			def_tx_entry->atomic_resp.len = tot_len;
-			rxm_ep_enqueue_deferred_tx_queue(def_tx_entry);
+			rxm_queue_deferred_tx(def_tx_entry, OFI_LIST_TAIL);
 			ret = 0;
 		}
 	}
@@ -1172,8 +1170,8 @@ static ssize_t rxm_handle_atomic_req(struct rxm_ep *rxm_ep,
 	       op == ofi_op_atomic_compare);
 
 	if (rx_buf->ep->srx_ctx)
-		rx_buf->conn = rxm_key2conn(rx_buf->ep,
-					    rx_buf->pkt.ctrl_hdr.conn_id);
+		rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					  (int) rx_buf->pkt.ctrl_hdr.conn_id);
 	if (!rx_buf->conn)
 		return -FI_EOTHER;
 
@@ -1491,14 +1489,14 @@ static void rxm_get_recv_entry(struct rxm_rx_buf *rx_buf,
 			       struct ofi_cq_rbuf_entry *cq_entry)
 {
 	struct rxm_recv_match_attr match_attr;
-	struct rxm_cmap_handle *cm_handle;
+	struct rxm_conn *conn;
 	struct rxm_recv_queue *recv_queue;
 	struct dlist_entry *entry;
 
 	assert(!rx_buf->recv_entry);
 	if (rx_buf->ep->rxm_info->caps & (FI_SOURCE | FI_DIRECTED_RECV)) {
-		cm_handle = cq_entry->ep_context;
-		match_attr.addr = cm_handle->fi_addr;
+		conn = cq_entry->ep_context;
+		match_attr.addr = conn->peer->fi_addr;
 	} else {
 		match_attr.addr = FI_ADDR_UNSPEC;
 	}
@@ -1533,14 +1531,14 @@ static void rxm_get_recv_entry(struct rxm_rx_buf *rx_buf,
 static void rxm_fake_rx_hdr(struct rxm_rx_buf *rx_buf,
 			    struct ofi_cq_rbuf_entry *entry)
 {
-	struct rxm_cmap_handle *cm_handle;
+	struct rxm_conn *conn;
 
-	cm_handle = entry->ep_context;
+	conn = entry->ep_context;
 
 	OFI_DBG_SET(rx_buf->pkt.hdr.version, OFI_OP_VERSION);
 	OFI_DBG_SET(rx_buf->pkt.ctrl_hdr.version, RXM_CTRL_VERSION);
 	rx_buf->pkt.ctrl_hdr.type = rxm_ctrl_eager;
-	rx_buf->pkt.ctrl_hdr.conn_id = cm_handle->remote_key;
+	rx_buf->pkt.ctrl_hdr.conn_id = conn->remote_index;
 	rx_buf->pkt.hdr.op = ofi_op_tagged;
 	rx_buf->pkt.hdr.tag = entry->tag;
 	rx_buf->pkt.hdr.size = entry->len;
@@ -1821,7 +1819,7 @@ void rxm_handle_comp_error(struct rxm_ep *rxm_ep)
 int rxm_post_recv(struct rxm_rx_buf *rx_buf)
 {
 	struct rxm_domain *domain;
-	int ret, level;
+	int ret;
 
 	if (rx_buf->ep->srx_ctx)
 		rx_buf->conn = NULL;
@@ -1837,9 +1835,7 @@ int rxm_post_recv(struct rxm_rx_buf *rx_buf)
 		return 0;
 
 	if (ret != -FI_EAGAIN) {
-		level = (rx_buf->conn->handle.state == RXM_CMAP_SHUTDOWN) ?
-			FI_LOG_DEBUG : FI_LOG_WARN;
-		FI_LOG(&rxm_prov, level, FI_LOG_EP_CTRL,
+		FI_DBG(&rxm_prov, FI_LOG_EP_CTRL,
 		       "unable to post recv buf: %d\n", ret);
 	}
 	return ret;
@@ -1899,15 +1895,15 @@ void rxm_ep_do_progress(struct util_ep *util_ep)
 			if (timestamp - rxm_ep->msg_cq_last_poll >
 				rxm_cm_progress_interval) {
 				rxm_ep->msg_cq_last_poll = timestamp;
-				rxm_msg_eq_progress(rxm_ep);
+				rxm_conn_progress(rxm_ep);
 			}
 		}
 	} while ((ret > 0) && (++comp_read < rxm_ep->comp_per_progress));
 
-	if (!dlist_empty(&rxm_ep->deferred_tx_conn_queue)) {
-		dlist_foreach_container_safe(&rxm_ep->deferred_tx_conn_queue,
+	if (!dlist_empty(&rxm_ep->deferred_queue)) {
+		dlist_foreach_container_safe(&rxm_ep->deferred_queue,
 					     struct rxm_conn, rxm_conn,
-					     deferred_conn_entry, conn_entry_tmp) {
+					     deferred_entry, conn_entry_tmp) {
 			rxm_ep_progress_deferred_queue(rxm_ep, rxm_conn);
 		}
 	}
diff --git a/prov/rxm/src/rxm_domain.c b/prov/rxm/src/rxm_domain.c
index bab0b4c..cc1b86d 100644
--- a/prov/rxm/src/rxm_domain.c
+++ b/prov/rxm/src/rxm_domain.c
@@ -380,9 +380,8 @@ static struct fi_ops_mr rxm_domain_mr_ops = {
 
 static ssize_t rxm_send_credits(struct fid_ep *ep, size_t credits)
 {
-	struct rxm_conn *rxm_conn =
-		container_of(ep->fid.context, struct rxm_conn, handle);
-	struct rxm_ep *rxm_ep = rxm_conn->handle.cmap->ep;
+	struct rxm_conn *rxm_conn = ep->fid.context;
+	struct rxm_ep *rxm_ep = rxm_conn->ep;
 	struct rxm_deferred_tx_entry *def_tx_entry;
 	struct rxm_tx_buf *tx_buf;
 	struct iovec iov;
@@ -403,7 +402,7 @@ static ssize_t rxm_send_credits(struct fid_ep *ep, size_t credits)
 	tx_buf->pkt.ctrl_hdr.msg_id = ofi_buf_index(tx_buf);
 	tx_buf->pkt.ctrl_hdr.ctrl_data = credits;
 
-	if (rxm_conn->handle.state != RXM_CMAP_CONNECTED)
+	if (rxm_conn->state != RXM_CM_CONNECTED)
 		goto defer;
 
 	iov.iov_base = &tx_buf->pkt;
@@ -428,7 +427,7 @@ defer:
 	}
 
 	def_tx_entry->credit_msg.tx_buf = tx_buf;
-	rxm_ep_enqueue_deferred_tx_queue_priority(def_tx_entry);
+	rxm_queue_deferred_tx(def_tx_entry, OFI_LIST_HEAD);
 	return FI_SUCCESS;
 }
 
diff --git a/prov/rxm/src/rxm_ep.c b/prov/rxm/src/rxm_ep.c
index 3d616c9..62dfdc3 100644
--- a/prov/rxm/src/rxm_ep.c
+++ b/prov/rxm/src/rxm_ep.c
@@ -236,6 +236,7 @@ static void rxm_recv_queue_close(struct rxm_recv_queue *recv_queue)
 	/* It indicates that the recv_queue were allocated */
 	if (recv_queue->fs) {
 		rxm_recv_fs_free(recv_queue->fs);
+		recv_queue->fs = NULL;
 	}
 	// TODO cleanup recv_list and unexp msg list
 }
@@ -325,18 +326,23 @@ err_recv_tag:
 
 /* It is safe to call this function, even if `rxm_ep_txrx_res_open`
  * has not yet been called */
-static void rxm_ep_txrx_res_close(struct rxm_ep *rxm_ep)
+static void rxm_ep_txrx_res_close(struct rxm_ep *ep)
 {
-	rxm_recv_queue_close(&rxm_ep->trecv_queue);
-	rxm_recv_queue_close(&rxm_ep->recv_queue);
+	rxm_recv_queue_close(&ep->trecv_queue);
+	rxm_recv_queue_close(&ep->recv_queue);
 
-	if (rxm_ep->multi_recv_pool)
-		ofi_bufpool_destroy(rxm_ep->multi_recv_pool);
-
-	if (rxm_ep->rx_pool)
-		ofi_bufpool_destroy(rxm_ep->rx_pool);
-	if (rxm_ep->tx_pool)
-		ofi_bufpool_destroy(rxm_ep->tx_pool);
+	if (ep->multi_recv_pool) {
+		ofi_bufpool_destroy(ep->multi_recv_pool);
+		ep->multi_recv_pool = NULL;
+	}
+	if (ep->rx_pool) {
+		ofi_bufpool_destroy(ep->rx_pool);
+		ep->rx_pool = NULL;
+	}
+	if (ep->tx_pool) {
+		ofi_bufpool_destroy(ep->tx_pool);
+		ep->tx_pool = NULL;
+	}
 }
 
 static int rxm_setname(fid_t fid, void *addr, size_t addrlen)
@@ -594,8 +600,8 @@ static int rxm_handle_unexp_sar(struct rxm_recv_queue *recv_queue,
 			continue;
 
 		if (!rx_buf->conn) {
-			rx_buf->conn = rxm_key2conn(rx_buf->ep,
-							rx_buf->pkt.ctrl_hdr.conn_id);
+			rx_buf->conn = ofi_idm_at(&rx_buf->ep->conn_idx_map,
+					(int) rx_buf->pkt.ctrl_hdr.conn_id);
 		}
 		if (recv_entry->sar.conn != rx_buf->conn)
 			continue;
@@ -1253,7 +1259,7 @@ defer:
 	def_tx->sar_seg.msg_id = msg_id;
 	def_tx->sar_seg.iface = iface;
 	def_tx->sar_seg.device = device;
-	rxm_ep_enqueue_deferred_tx_queue(def_tx);
+	rxm_queue_deferred_tx(def_tx, OFI_LIST_TAIL);
 	return 0;
 }
 
@@ -1317,7 +1323,7 @@ rxm_ep_inject_send(struct rxm_ep *rxm_ep, struct rxm_conn *rxm_conn,
 
 	assert(len <= rxm_ep->rxm_info->tx_attr->inject_size);
 
-	inject_pkt->ctrl_hdr.conn_id = rxm_conn->handle.remote_key;
+	inject_pkt->ctrl_hdr.conn_id = rxm_conn->remote_index;
 	if (pkt_size <= rxm_ep->inject_limit && !rxm_ep->util_ep.tx_cntr) {
 		if (rxm_use_msg_tinject(rxm_ep, inject_pkt->hdr.op)) {
 			return rxm_msg_tinject(rxm_conn->msg_ep, buf, len,
@@ -1636,7 +1642,7 @@ void rxm_ep_progress_deferred_queue(struct rxm_ep *rxm_ep,
 	struct fi_msg msg;
 	ssize_t ret = 0;
 
-	if (rxm_conn->handle.state != RXM_CMAP_CONNECTED)
+	if (rxm_conn->state != RXM_CM_CONNECTED)
 		return;
 
 	while (!dlist_empty(&rxm_conn->deferred_tx_queue) && !ret) {
@@ -1761,7 +1767,7 @@ void rxm_ep_progress_deferred_queue(struct rxm_ep *rxm_ep,
 			break;
 		}
 
-		rxm_ep_dequeue_deferred_tx_queue(def_tx_entry);
+		rxm_dequeue_deferred_tx(def_tx_entry);
 		free(def_tx_entry);
 	}
 }
@@ -2260,76 +2266,76 @@ static struct fi_ops_collective rxm_ops_collective_none = {
 	.msg = fi_coll_no_msg,
 };
 
-static int rxm_ep_msg_res_close(struct rxm_ep *rxm_ep)
-{
-	int ret = 0;
-
-	if (rxm_ep->srx_ctx) {
-		ret = fi_close(&rxm_ep->srx_ctx->fid);
-		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, \
-				"Unable to close msg shared ctx\n");
-		}
-	}
-
-	fi_freeinfo(rxm_ep->msg_info);
-	return ret;
-}
 
-static int rxm_listener_close(struct rxm_ep *rxm_ep)
+static int rxm_listener_close(struct rxm_ep *ep)
 {
-	int ret, retv = 0;
+	int ret;
 
-	if (rxm_ep->msg_pep) {
-		ret = fi_close(&rxm_ep->msg_pep->fid);
+	if (ep->msg_pep) {
+		ret = fi_close(&ep->msg_pep->fid);
 		if (ret) {
 			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
 				"Unable to close msg pep\n");
-			retv = ret;
+			return ret;
 		}
+		ep->msg_pep = NULL;
 	}
-	if (rxm_ep->msg_eq) {
-		ret = fi_close(&rxm_ep->msg_eq->fid);
+
+	if (ep->msg_eq) {
+		ret = fi_close(&ep->msg_eq->fid);
 		if (ret) {
 			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
 				"Unable to close msg EQ\n");
-			retv = ret;
+			return ret;
 		}
+		ep->msg_eq = NULL;
 	}
-	return retv;
+	return 0;
 }
 
 static int rxm_ep_close(struct fid *fid)
 {
-	int ret, retv = 0;
-	struct rxm_ep *rxm_ep;
+	struct rxm_ep *ep;
+	int ret;
 
-	rxm_ep = container_of(fid, struct rxm_ep, util_ep.ep_fid.fid);
-	if (rxm_ep->cmap)
-		rxm_cmap_free(rxm_ep->cmap);
+	ep = container_of(fid, struct rxm_ep, util_ep.ep_fid.fid);
 
-	ret = rxm_listener_close(rxm_ep);
+	/* Stop listener thread to halt event processing before closing all
+	 * connections.
+	 */
+	rxm_stop_listen(ep);
+	rxm_freeall_conns(ep);
+	ret = rxm_listener_close(ep);
 	if (ret)
-		retv = ret;
+		return ret;
 
-	rxm_ep_txrx_res_close(rxm_ep);
-	ret = rxm_ep_msg_res_close(rxm_ep);
-	if (ret)
-		retv = ret;
+	rxm_ep_txrx_res_close(ep);
+	if (ep->srx_ctx) {
+		ret = fi_close(&ep->srx_ctx->fid);
+		if (ret) {
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, \
+				"Unable to close msg shared ctx\n");
+			return ret;
+		}
+		ep->srx_ctx = NULL;
+	}
 
-	if (rxm_ep->msg_cq) {
-		ret = fi_close(&rxm_ep->msg_cq->fid);
+	if (ep->msg_cq) {
+		ret = fi_close(&ep->msg_cq->fid);
 		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL, "Unable to close msg CQ\n");
-			retv = ret;
+			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
+				"Unable to close msg CQ\n");
+			return ret;
 		}
+		ep->msg_cq = NULL;
 	}
 
-	free(rxm_ep->inject_pkt);
-	ofi_endpoint_close(&rxm_ep->util_ep);
-	fi_freeinfo(rxm_ep->rxm_info);
-	free(rxm_ep);
-	return retv;
+	free(ep->inject_pkt);
+	ofi_endpoint_close(&ep->util_ep);
+	fi_freeinfo(ep->msg_info);
+	fi_freeinfo(ep->rxm_info);
+	free(ep);
+	return 0;
 }
 
 static int rxm_ep_trywait_cq(void *arg)
@@ -2375,6 +2381,12 @@ static int rxm_ep_wait_fd_add(struct rxm_ep *rxm_ep, struct util_wait *wait)
 				rxm_ep_trywait_eq);
 }
 
+static bool rxm_needs_atomic_progress(const struct fi_info *info)
+{
+	return (info->caps & FI_ATOMIC) && info->domain_attr &&
+		info->domain_attr->data_progress == FI_PROGRESS_AUTO;
+}
+
 static int rxm_msg_cq_fd_needed(struct rxm_ep *rxm_ep)
 {
 	return (rxm_needs_atomic_progress(rxm_ep->rxm_info) ||
@@ -2568,7 +2580,7 @@ static int rxm_ep_txrx_res_open(struct rxm_ep *rxm_ep)
 	if (ret)
 		return ret;
 
-	dlist_init(&rxm_ep->deferred_tx_conn_queue);
+	dlist_init(&rxm_ep->deferred_queue);
 
 	ret = rxm_ep_rx_queue_init(rxm_ep);
 	if (ret)
@@ -2628,23 +2640,13 @@ static int rxm_ep_ctrl(struct fid *fid, int command, void *arg)
 		 * and then progressing both MSG EQ and MSG CQ once the latter
 		 * is opened) */
 		assert(!(rxm_ep->rxm_info->caps & FI_ATOMIC) ||
-		       !rxm_ep->cmap || !rxm_ep->cmap->cm_thread);
+		       !rxm_ep->cm_thread);
 
 		ret = rxm_ep_msg_cq_open(rxm_ep);
 		if (ret)
 			return ret;
 
-		/* fi_listen should be called before cmap alloc as cmap alloc
-		 * calls fi_getname on pep which would succeed only if fi_listen
-		 * was called first */
-		ret = fi_listen(rxm_ep->msg_pep);
-		if (ret) {
-			FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-				"unable to set msg PEP to listen state\n");
-			return ret;
-		}
-
-		ret = rxm_conn_cmap_alloc(rxm_ep);
+		ret = rxm_start_listen(rxm_ep);
 		if (ret)
 			return ret;
 
@@ -2658,19 +2660,17 @@ static int rxm_ep_ctrl(struct fid *fid, int command, void *arg)
 
 		if (rxm_ep->srx_ctx) {
 			ret = rxm_prepost_recv(rxm_ep, rxm_ep->srx_ctx);
-			if (ret) {
-				rxm_cmap_free(rxm_ep->cmap);
-				FI_WARN(&rxm_prov, FI_LOG_EP_CTRL,
-					"unable to prepost recv bufs\n");
+			if (ret)
 				goto err;
-			}
 		}
 		break;
 	default:
 		return -FI_ENOSYS;
 	}
 	return 0;
+
 err:
+	/* TODO: cleanup all allocated resources on error */
 	rxm_ep_txrx_res_close(rxm_ep);
 	return ret;
 }
@@ -2802,7 +2802,7 @@ rxm_prepare_deferred_rndv_write(struct rxm_deferred_tx_entry **def_tx_entry,
 {
 	uint8_t i;
 	struct rxm_tx_buf *tx_buf = buf;
-	struct rxm_ep *rxm_ep = tx_buf->write_rndv.conn->handle.cmap->ep;
+	struct rxm_ep *rxm_ep = tx_buf->write_rndv.conn->ep;
 
 	*def_tx_entry = rxm_ep_alloc_deferred_tx_entry(rxm_ep, tx_buf->write_rndv.conn,
 						       RXM_DEFERRED_TX_RNDV_WRITE);
@@ -2914,6 +2914,8 @@ int rxm_endpoint(struct fid_domain *domain, struct fi_info *info,
 	if (rxm_ep->rxm_info->caps & FI_ATOMIC)
 		(*ep_fid)->atomic = &rxm_ops_atomic;
 
+	dlist_init(&rxm_ep->loopback_list);
+
 	return 0;
 err2:
 	ofi_endpoint_close(&rxm_ep->util_ep);
diff --git a/prov/shm/src/smr_progress.c b/prov/shm/src/smr_progress.c
index 51811d4..16f57eb 100644
--- a/prov/shm/src/smr_progress.c
+++ b/prov/shm/src/smr_progress.c
@@ -72,6 +72,7 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 	struct smr_inject_buf *tx_buf = NULL;
 	struct smr_sar_msg *sar_msg = NULL;
 	uint8_t *src;
+	ssize_t hmem_copy_ret;
 
 	peer_smr = smr_peer_region(ep->region, pending->peer_id);
 
@@ -110,14 +111,24 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 			break;
 		if (pending->cmd.msg.hdr.op == ofi_op_read_req) {
 			if (!*err) {
-				pending->bytes_done = ofi_copy_to_iov(pending->iov,
-						pending->iov_count, 0,
-						pending->map_ptr,
-						pending->cmd.msg.hdr.size);
-				if (pending->bytes_done != pending->cmd.msg.hdr.size) {
+				hmem_copy_ret =
+					ofi_copy_to_hmem_iov(pending->iface,
+							     pending->device,
+							     pending->iov,
+							     pending->iov_count,
+							     0, pending->map_ptr,
+							     pending->cmd.msg.hdr.size);
+				if (hmem_copy_ret < 0) {
+					FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+						"Copy from mmapped file failed with code %d\n",
+						(int)(-hmem_copy_ret));
+					*err = hmem_copy_ret;
+				} else if (hmem_copy_ret != pending->cmd.msg.hdr.size) {
 					FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 						"Incomplete copy from mmapped file\n");
-					*err = -FI_EIO;
+					*err = -FI_ETRUNC;
+				} else {
+					pending->bytes_done = (size_t) hmem_copy_ret;
 				}
 			}
 			munmap(pending->map_ptr, pending->cmd.msg.hdr.size);
@@ -136,14 +147,21 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 
 		src = pending->cmd.msg.hdr.op == ofi_op_atomic_compare ?
 		      tx_buf->buf : tx_buf->data;
-		pending->bytes_done = ofi_copy_to_hmem_iov(pending->iface, pending->device,
-							   pending->iov, pending->iov_count,
-							   0, src, pending->cmd.msg.hdr.size);
+		hmem_copy_ret  = ofi_copy_to_hmem_iov(pending->iface, pending->device,
+						      pending->iov, pending->iov_count,
+						      0, src, pending->cmd.msg.hdr.size);
 
-		if (pending->bytes_done != pending->cmd.msg.hdr.size) {
+		if (hmem_copy_ret < 0) {
+			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+				"RMA read/fetch failed with code %d\n",
+				(int)(-hmem_copy_ret));
+			*err = hmem_copy_ret;
+		} else if (hmem_copy_ret != pending->cmd.msg.hdr.size) {
 			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 				"Incomplete rma read/fetch buffer copied\n");
-			*err = FI_EIO;
+			*err = -FI_ETRUNC;
+		} else {
+			pending->bytes_done = (size_t) hmem_copy_ret;
 		}
 		break;
 	default:
@@ -172,7 +190,7 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 	if (peer_smr != ep->region)
 		fastlock_release(&peer_smr->lock);
 
-	return 0;
+	return FI_SUCCESS;
 }
 
 static void smr_progress_resp(struct smr_ep *ep)
@@ -212,14 +230,24 @@ static int smr_progress_inline(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 			       uint64_t device, struct iovec *iov,
 			       size_t iov_count, size_t *total_len)
 {
-	*total_len = ofi_copy_to_hmem_iov(iface, device, iov, iov_count, 0,
-					  cmd->msg.data.msg, cmd->msg.hdr.size);
-	if (*total_len != cmd->msg.hdr.size) {
+	ssize_t hmem_copy_ret;
+
+	hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov, iov_count, 0,
+					     cmd->msg.data.msg, cmd->msg.hdr.size);
+	if (hmem_copy_ret < 0) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-			"recv truncated");
-		return -FI_EIO;
+			"inline recv failed with code %d\n",
+			(int)(-hmem_copy_ret));
+		return hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"inline recv truncated\n");
+		return -FI_ETRUNC;
 	}
-	return 0;
+
+	*total_len = hmem_copy_ret;
+
+	return FI_SUCCESS;
 }
 
 static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
@@ -229,6 +257,7 @@ static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 {
 	struct smr_inject_buf *tx_buf;
 	size_t inj_offset;
+	ssize_t hmem_copy_ret;
 
 	inj_offset = (size_t) cmd->msg.hdr.src_data;
 	tx_buf = smr_get_ptr(ep->region, inj_offset);
@@ -239,20 +268,30 @@ static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 	}
 
 	if (cmd->msg.hdr.op == ofi_op_read_req) {
-		*total_len = ofi_copy_from_hmem_iov(tx_buf->data, cmd->msg.hdr.size,
-						    iface, device, iov, iov_count, 0);
+		hmem_copy_ret = ofi_copy_from_hmem_iov(tx_buf->data,
+						       cmd->msg.hdr.size,
+						       iface, device, iov,
+						       iov_count, 0);
 	} else {
-		*total_len = ofi_copy_to_hmem_iov(iface, device, iov, iov_count, 0,
-						  tx_buf->data, cmd->msg.hdr.size);
+		hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov,
+						     iov_count, 0, tx_buf->data,
+						     cmd->msg.hdr.size);
 		smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
 	}
 
-	if (*total_len != cmd->msg.hdr.size) {
+	if (hmem_copy_ret < 0) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-			"recv truncated");
-		return -FI_EIO;
+			"inject recv failed with code %d\n",
+			(int)(-hmem_copy_ret));
+		return hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"inject recv truncated\n");
+		return -FI_ETRUNC;
 	}
 
+	*total_len = hmem_copy_ret;
+
 	return FI_SUCCESS;
 }
 
@@ -287,13 +326,15 @@ out:
 }
 
 static int smr_mmap_peer_copy(struct smr_ep *ep, struct smr_cmd *cmd,
-				 struct iovec *iov, size_t iov_count,
-				 size_t *total_len)
+			      enum fi_hmem_iface iface, uint64_t device,
+			      struct iovec *iov, size_t iov_count,
+			      size_t *total_len)
 {
 	char shm_name[SMR_NAME_MAX];
 	void *mapped_ptr;
 	int fd, num;
 	int ret = 0;
+	ssize_t hmem_copy_ret;
 
 	num = smr_mmap_name(shm_name,
 			ep->region->map->peers[cmd->msg.hdr.id].peer.name,
@@ -318,26 +359,28 @@ static int smr_mmap_peer_copy(struct smr_ep *ep, struct smr_cmd *cmd,
 	}
 
 	if (cmd->msg.hdr.op == ofi_op_read_req) {
-		*total_len = ofi_total_iov_len(iov, iov_count);
-		if (ofi_copy_from_iov(mapped_ptr, *total_len, iov, iov_count, 0)
-		    != *total_len) {
-			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-				"mmap iov copy in error\n");
-			ret = -FI_EIO;
-			goto munmap;
-		}
+		hmem_copy_ret = ofi_copy_from_hmem_iov(mapped_ptr,
+						    cmd->msg.hdr.size, iface,
+						    device, iov, iov_count, 0);
 	} else {
-		*total_len = ofi_copy_to_iov(iov, iov_count, 0, mapped_ptr,
-				      cmd->msg.hdr.size);
-		if (*total_len != cmd->msg.hdr.size) {
-			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
-				"mmap iov copy out error\n");
-			ret = -FI_EIO;
-			goto munmap;
-		}
+		hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov,
+						  iov_count, 0, mapped_ptr,
+						  cmd->msg.hdr.size);
 	}
 
-munmap:
+	if (hmem_copy_ret < 0) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"mmap copy iov failed with code %d\n",
+			(int)(-hmem_copy_ret));
+		ret = hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
+			"mmap copy iov truncated\n");
+		ret = -FI_ETRUNC;
+	}
+
+	*total_len = hmem_copy_ret;
+
 	munmap(mapped_ptr, cmd->msg.hdr.size);
 unlink_close:
 	shm_unlink(shm_name);
@@ -345,7 +388,8 @@ unlink_close:
 	return ret;
 }
 
-static int smr_progress_mmap(struct smr_cmd *cmd, struct iovec *iov,
+static int smr_progress_mmap(struct smr_cmd *cmd, enum fi_hmem_iface iface,
+			     uint64_t device, struct iovec *iov,
 			     size_t iov_count, size_t *total_len,
 			     struct smr_ep *ep)
 {
@@ -356,7 +400,8 @@ static int smr_progress_mmap(struct smr_cmd *cmd, struct iovec *iov,
 	peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.id);
 	resp = smr_get_ptr(peer_smr, cmd->msg.hdr.src_data);
 
-	ret = smr_mmap_peer_copy(ep, cmd, iov, iov_count, total_len);
+	ret = smr_mmap_peer_copy(ep, cmd, iface, device,
+				 iov, iov_count, total_len);
 
 	//Status must be set last (signals peer: op done, valid resp entry)
 	resp->status = ret;
@@ -428,6 +473,7 @@ static int smr_progress_ipc(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 	uint64_t ipc_device;
 	int64_t id;
 	int ret, fd, ipc_fd;
+	ssize_t hmem_copy_ret;
 
 	peer_smr = smr_peer_region(ep->region, cmd->msg.hdr.id);
 	resp = smr_get_ptr(peer_smr, cmd->msg.hdr.src_data);
@@ -455,20 +501,29 @@ static int smr_progress_ipc(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 		ptr = (char *) ptr + (uintptr_t) cmd->msg.data.ipc_info.offset;
 
 	if (cmd->msg.hdr.op == ofi_op_read_req) {
-		*total_len = ofi_copy_from_hmem_iov(ptr, cmd->msg.hdr.size,
-						    cmd->msg.data.ipc_info.iface,
-						    device, iov, iov_count, 0);
+		hmem_copy_ret = ofi_copy_from_hmem_iov(ptr, cmd->msg.hdr.size,
+						       cmd->msg.data.ipc_info.iface,
+						       device, iov, iov_count, 0);
 	} else {
-		*total_len = ofi_copy_to_hmem_iov(cmd->msg.data.ipc_info.iface,
-						  device, iov, iov_count, 0, ptr,
-						  cmd->msg.hdr.size);
+		hmem_copy_ret = ofi_copy_to_hmem_iov(cmd->msg.data.ipc_info.iface,
+						     device, iov, iov_count, 0,
+						     ptr, cmd->msg.hdr.size);
 	}
-	if (!ret)
-		*total_len = cmd->msg.hdr.size;
 
 	if (cmd->msg.data.ipc_info.iface == FI_HMEM_ZE)
 		close(ipc_fd);
+
+	/* Truncation error takes precedence over close_handle error */
 	ret = ofi_hmem_close_handle(cmd->msg.data.ipc_info.iface, base);
+
+	if (hmem_copy_ret < 0) {
+		ret = hmem_copy_ret;
+	} else if (hmem_copy_ret != cmd->msg.hdr.size) {
+		ret = -FI_ETRUNC;
+	}
+
+	*total_len = hmem_copy_ret;
+
 out:
 	//Status must be set last (signals peer: op done, valid resp entry)
 	resp->status = ret;
@@ -544,9 +599,9 @@ static int smr_progress_inline_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
 	if (*len != cmd->msg.hdr.size) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 			"recv truncated");
-		return -FI_EIO;
+		return -FI_ETRUNC;
 	}
-	return 0;
+	return FI_SUCCESS;
 }
 
 static int smr_progress_inject_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
@@ -584,7 +639,7 @@ static int smr_progress_inject_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
 	if (*len != cmd->msg.hdr.size) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 			"recv truncated");
-		err = -FI_EIO;
+		err = -FI_ETRUNC;
 	}
 
 out:
@@ -622,7 +677,8 @@ static int smr_progress_msg_common(struct smr_ep *ep, struct smr_cmd *cmd,
 					      &total_len, ep, 0);
 		break;
 	case smr_src_mmap:
-		entry->err = smr_progress_mmap(cmd, entry->iov, entry->iov_count,
+		entry->err = smr_progress_mmap(cmd, entry->iface, entry->device,
+					       entry->iov, entry->iov_count,
 					       &total_len, ep);
 		break;
 	case smr_src_sar:
@@ -825,7 +881,8 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 		err = smr_progress_iov(cmd, iov, iov_count, &total_len, ep, ret);
 		break;
 	case smr_src_mmap:
-		err = smr_progress_mmap(cmd, iov, iov_count, &total_len, ep);
+		err = smr_progress_mmap(cmd, iface, device, iov,
+					iov_count, &total_len, ep);
 		break;
 	case smr_src_sar:
 		if (smr_progress_sar(cmd, NULL, iface, device, iov, iov_count,
diff --git a/prov/tcp/src/tcpx_conn_mgr.c b/prov/tcp/src/tcpx_conn_mgr.c
index 14a3924..c3807a2 100644
--- a/prov/tcp/src/tcpx_conn_mgr.c
+++ b/prov/tcp/src/tcpx_conn_mgr.c
@@ -337,6 +337,7 @@ static void tcpx_cm_recv_req(struct util_wait *wait,
 		goto err2;
 
 	len = cm_entry->info->dest_addrlen = handle->pep->info->src_addrlen;
+	free(cm_entry->info->dest_addr);
 	cm_entry->info->dest_addr = malloc(len);
 	if (!cm_entry->info->dest_addr)
 		goto err3;
diff --git a/prov/util/src/util_av.c b/prov/util/src/util_av.c
index 4777ae8..9401189 100644
--- a/prov/util/src/util_av.c
+++ b/prov/util/src/util_av.c
@@ -247,6 +247,14 @@ void *ofi_av_get_addr(struct util_av *av, fi_addr_t fi_addr)
 	return entry->data;
 }
 
+void *ofi_av_addr_context(struct util_av *av, fi_addr_t fi_addr)
+{
+	void *addr;
+
+	addr = ofi_av_get_addr(av, fi_addr);
+	return (char *) addr + av->context_offset;
+}
+
 int ofi_verify_av_insert(struct util_av *av, uint64_t flags, void *context)
 {
 	if (av->flags & FI_EVENT) {
diff --git a/prov/util/src/util_mem_monitor.c b/prov/util/src/util_mem_monitor.c
index 4dabfa9..0ccf709 100644
--- a/prov/util/src/util_mem_monitor.c
+++ b/prov/util/src/util_mem_monitor.c
@@ -60,6 +60,7 @@ struct ofi_mem_monitor *uffd_monitor = &uffd.monitor;
 struct ofi_mem_monitor *default_monitor;
 struct ofi_mem_monitor *default_cuda_monitor;
 struct ofi_mem_monitor *default_rocr_monitor;
+struct ofi_mem_monitor *default_ze_monitor;
 
 static size_t ofi_default_cache_size(void)
 {
@@ -97,6 +98,7 @@ void ofi_monitors_init(void)
 	memhooks_monitor->init(memhooks_monitor);
 	cuda_monitor->init(cuda_monitor);
 	rocr_monitor->init(rocr_monitor);
+	ze_monitor->init(ze_monitor);
 
 #if HAVE_MEMHOOKS_MONITOR
         default_monitor = memhooks_monitor;
@@ -133,6 +135,9 @@ void ofi_monitors_init(void)
 	fi_param_define(NULL, "mr_rocr_cache_monitor_enabled", FI_PARAM_BOOL,
 			"Enable or disable the ROCR cache memory monitor. "
 			"Monitor is enabled by default.");
+	fi_param_define(NULL, "mr_ze_cache_monitor_enabled", FI_PARAM_BOOL,
+			"Enable or disable the ZE cache memory monitor. "
+			"Monitor is enabled by default.");
 
 	fi_param_get_size_t(NULL, "mr_cache_max_size", &cache_params.max_size);
 	fi_param_get_size_t(NULL, "mr_cache_max_count", &cache_params.max_cnt);
@@ -141,6 +146,8 @@ void ofi_monitors_init(void)
 			  &cache_params.cuda_monitor_enabled);
 	fi_param_get_bool(NULL, "mr_rocr_cache_monitor_enabled",
 			  &cache_params.rocr_monitor_enabled);
+	fi_param_get_bool(NULL, "mr_ze_cache_monitor_enabled",
+			  &cache_params.ze_monitor_enabled);
 
 	if (!cache_params.max_size)
 		cache_params.max_size = ofi_default_cache_size();
@@ -174,6 +181,11 @@ void ofi_monitors_init(void)
 		default_rocr_monitor = rocr_monitor;
 	else
 		default_rocr_monitor = NULL;
+
+	if (cache_params.ze_monitor_enabled)
+		default_ze_monitor = ze_monitor;
+	else
+		default_ze_monitor = NULL;
 }
 
 void ofi_monitors_cleanup(void)
@@ -182,6 +194,7 @@ void ofi_monitors_cleanup(void)
 	memhooks_monitor->cleanup(memhooks_monitor);
 	cuda_monitor->cleanup(cuda_monitor);
 	rocr_monitor->cleanup(rocr_monitor);
+	ze_monitor->cleanup(ze_monitor);
 }
 
 /* Monitors array must be of size OFI_HMEM_MAX. */
@@ -706,4 +719,4 @@ int ofi_monitor_import(struct fid *fid)
 		"setting imported memory monitor as default\n");
 	default_monitor = &impmon.monitor;
 	return 0;
-}
\ No newline at end of file
+}
diff --git a/prov/util/src/util_mr_cache.c b/prov/util/src/util_mr_cache.c
index d4d3386..0334753 100644
--- a/prov/util/src/util_mr_cache.c
+++ b/prov/util/src/util_mr_cache.c
@@ -48,6 +48,7 @@ struct ofi_mr_cache_params cache_params = {
 	.max_cnt = 1024,
 	.cuda_monitor_enabled = true,
 	.rocr_monitor_enabled = true,
+	.ze_monitor_enabled = true,
 };
 
 static int util_mr_find_within(struct ofi_rbmap *map, void *key, void *data)
diff --git a/prov/util/src/ze_mem_monitor.c b/prov/util/src/ze_mem_monitor.c
new file mode 100644
index 0000000..866885b
--- /dev/null
+++ b/prov/util/src/ze_mem_monitor.c
@@ -0,0 +1,117 @@
+/*
+ * (C) Copyright 2021 Intel Corporation
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "ofi_mr.h"
+
+#if HAVE_LIBZE
+
+#include "ofi_hmem.h"
+
+static int ze_mm_subscribe(struct ofi_mem_monitor *monitor, const void *addr,
+			   size_t len, union ofi_mr_hmem_info *hmem_info)
+{
+	return ze_hmem_get_id(addr, &hmem_info->ze_id);
+}
+
+static void ze_mm_unsubscribe(struct ofi_mem_monitor *monitor,
+			      const void *addr, size_t len,
+			      union ofi_mr_hmem_info *hmem_info)
+{
+	/* no-op */
+}
+
+static bool ze_mm_valid(struct ofi_mem_monitor *monitor,
+			const void *addr, size_t len,
+			union ofi_mr_hmem_info *hmem_info)
+{
+	uint64_t id;
+	int ret;
+
+	ret = ze_hmem_get_id(addr, &id);
+	if (ret)
+		return false;
+
+
+	return id == hmem_info->ze_id;
+}
+
+static int ze_monitor_start(struct ofi_mem_monitor *monitor)
+{
+	/* no-op */
+	return FI_SUCCESS;
+}
+
+#else
+
+static int ze_mm_subscribe(struct ofi_mem_monitor *monitor, const void *addr,
+			   size_t len, union ofi_mr_hmem_info *hmem_info)
+{
+	return -FI_ENOSYS;
+}
+
+static void ze_mm_unsubscribe(struct ofi_mem_monitor *monitor,
+			      const void *addr, size_t len,
+			      union ofi_mr_hmem_info *hmem_info)
+{
+}
+
+static bool ze_mm_valid(struct ofi_mem_monitor *monitor,
+			const void *addr, size_t len,
+			union ofi_mr_hmem_info *hmem_info)
+{
+	return false;
+}
+
+static int ze_monitor_start(struct ofi_mem_monitor *monitor)
+{
+	return -FI_ENOSYS;
+}
+
+#endif /* HAVE_LIBZE */
+
+void ze_monitor_stop(struct ofi_mem_monitor *monitor)
+{
+	/* no-op */
+}
+
+static struct ofi_mem_monitor ze_mm = {
+	.iface = FI_HMEM_ZE,
+	.init = ofi_monitor_init,
+	.cleanup = ofi_monitor_cleanup,
+	.start = ze_monitor_start,
+	.stop = ze_monitor_stop,
+	.subscribe = ze_mm_subscribe,
+	.unsubscribe = ze_mm_unsubscribe,
+	.valid = ze_mm_valid,
+};
+
+struct ofi_mem_monitor *ze_monitor = &ze_mm;
diff --git a/prov/verbs/src/verbs_domain.c b/prov/verbs/src/verbs_domain.c
index 59f66f6..d2125a4 100644
--- a/prov/verbs/src/verbs_domain.c
+++ b/prov/verbs/src/verbs_domain.c
@@ -283,6 +283,7 @@ vrb_domain(struct fid_fabric *fabric, struct fi_info *info,
 		[FI_HMEM_SYSTEM] = default_monitor,
 		[FI_HMEM_CUDA] = default_cuda_monitor,
 		[FI_HMEM_ROCR] = default_rocr_monitor,
+		[FI_HMEM_ZE] = default_ze_monitor,
 	};
 	enum fi_hmem_iface iface;
 	struct vrb_domain *_domain;
diff --git a/src/hmem_ze.c b/src/hmem_ze.c
index 5896c7d..b77be85 100644
--- a/src/hmem_ze.c
+++ b/src/hmem_ze.c
@@ -66,6 +66,224 @@ static const ze_command_list_desc_t cl_desc = {
 	.flags				= 0,
 };
 
+struct libze_ops {
+	ze_result_t (*zeInit)(ze_init_flags_t flags);
+	ze_result_t (*zeDriverGet)(uint32_t *pCount,
+				   ze_driver_handle_t *phDrivers);
+	ze_result_t (*zeDeviceGet)(ze_driver_handle_t hDriver,
+				   uint32_t *pCount,
+				   ze_device_handle_t *phDevices);
+	ze_result_t (*zeDeviceCanAccessPeer)(ze_device_handle_t hDevice,
+					     ze_device_handle_t hPeerDevice,
+					     ze_bool_t *value);
+	ze_result_t (*zeContextCreate)(ze_driver_handle_t hDriver,
+				       const ze_context_desc_t *desc,
+				       ze_context_handle_t *phContext);
+	ze_result_t (*zeContextDestroy)(ze_context_handle_t hContext);
+	ze_result_t (*zeCommandQueueCreate)(ze_context_handle_t hContext,
+					    ze_device_handle_t hDevice,
+					    const ze_command_queue_desc_t *desc,
+					    ze_command_queue_handle_t *phCommandQueue);
+	ze_result_t (*zeCommandQueueDestroy)(ze_command_queue_handle_t hCommandQueue);
+	ze_result_t (*zeCommandQueueExecuteCommandLists)(
+					ze_command_queue_handle_t hCommandQueue,
+					uint32_t numCommandLists,
+					ze_command_list_handle_t *phCommandLists,
+					ze_fence_handle_t hFence);
+	ze_result_t (*zeCommandListCreate)(ze_context_handle_t hContext,
+					   ze_device_handle_t hDevice,
+					   const ze_command_list_desc_t *desc,
+					   ze_command_list_handle_t *phCommandList);
+	ze_result_t (*zeCommandListDestroy)(ze_command_list_handle_t hCommandList);
+	ze_result_t (*zeCommandListClose)(ze_command_list_handle_t hCommandList);
+	ze_result_t (*zeCommandListAppendMemoryCopy)(
+				ze_command_list_handle_t hCommandList,
+				void *dstptr, const void *srcptr, size_t size,
+				ze_event_handle_t hSignalEvent,
+				uint32_t numWaitEvents,
+				ze_event_handle_t *phWaitEvents);
+	ze_result_t (*zeMemGetAllocProperties)(
+				ze_context_handle_t hContext, const void *ptr,
+				ze_memory_allocation_properties_t *pMemAllocProperties,
+				ze_device_handle_t *phDevice);
+	ze_result_t (*zeMemGetAddressRange)(
+				ze_context_handle_t hContext, const void *ptr,
+				void **pBase, size_t *pSize);
+	ze_result_t (*zeMemGetIpcHandle)(ze_context_handle_t hContext,
+					 const void *ptr,
+					 ze_ipc_mem_handle_t *pIpcHandle);
+	ze_result_t (*zeMemOpenIpcHandle)(ze_context_handle_t hContext,
+					  ze_device_handle_t hDevice,
+					  ze_ipc_mem_handle_t handle,
+					  ze_ipc_memory_flags_t flags,
+					  void **pptr);
+	ze_result_t (*zeMemCloseIpcHandle)(ze_context_handle_t hContext,
+					   const void *ptr);
+};
+
+#ifdef ENABLE_ZE_DLOPEN
+
+#include <dlfcn.h>
+
+static void *libze_handle;
+static struct libze_ops libze_ops;
+
+#else
+
+static struct libze_ops libze_ops = {
+	.zeInit = zeInit,
+	.zeDriverGet = zeDriverGet,
+	.zeDeviceGet = zeDeviceGet,
+	.zeDeviceCanAccessPeer = zeDeviceCanAccessPeer,
+	.zeContextCreate = zeContextCreate,
+	.zeContextDestroy = zeContextDestroy,
+	.zeCommandQueueCreate = zeCommandQueueCreate,
+	.zeCommandQueueDestroy = zeCommandQueueDestroy,
+	.zeCommandQueueExecuteCommandLists = zeCommandQueueExecuteCommandLists,
+	.zeCommandListCreate = zeCommandListCreate,
+	.zeCommandListDestroy = zeCommandListDestroy,
+	.zeCommandListClose = zeCommandListClose,
+	.zeCommandListAppendMemoryCopy = zeCommandListAppendMemoryCopy,
+	.zeMemGetAllocProperties = zeMemGetAllocProperties,
+	.zeMemGetAddressRange = zeMemGetAddressRange,
+	.zeMemGetIpcHandle = zeMemGetIpcHandle,
+	.zeMemOpenIpcHandle = zeMemOpenIpcHandle,
+	.zeMemCloseIpcHandle = zeMemCloseIpcHandle,
+};
+
+#endif /* ENABLE_ZE_DLOPEN */
+
+ze_result_t ofi_zeInit(ze_init_flags_t flags)
+{
+	return (*libze_ops.zeInit)(flags);
+}
+
+ze_result_t ofi_zeDriverGet(uint32_t *pCount, ze_driver_handle_t *phDrivers)
+{
+	return (*libze_ops.zeDriverGet)(pCount, phDrivers);
+}
+
+ze_result_t ofi_zeDeviceGet(ze_driver_handle_t hDriver, uint32_t *pCount,
+			    ze_device_handle_t *phDevices)
+{
+	return (*libze_ops.zeDeviceGet)(hDriver, pCount, phDevices);
+}
+
+ze_result_t ofi_zeDeviceCanAccessPeer(ze_device_handle_t hDevice,
+				      ze_device_handle_t hPeerDevice,
+				      ze_bool_t *value)
+{
+	return (*libze_ops.zeDeviceCanAccessPeer)(hDevice, hPeerDevice, value);
+}
+
+ze_result_t ofi_zeContextCreate(ze_driver_handle_t hDriver,
+				const ze_context_desc_t *desc,
+				ze_context_handle_t *phContext)
+{
+	return (*libze_ops.zeContextCreate)(hDriver, desc, phContext);
+}
+
+ze_result_t ofi_zeContextDestroy(ze_context_handle_t hContext)
+{
+	return (*libze_ops.zeContextDestroy)(hContext);
+}
+
+ze_result_t ofi_zeCommandQueueCreate(ze_context_handle_t hContext,
+				     ze_device_handle_t hDevice,
+				     const ze_command_queue_desc_t *desc,
+				     ze_command_queue_handle_t *phCommandQueue)
+{
+	return (*libze_ops.zeCommandQueueCreate)(hContext, hDevice, desc,
+						 phCommandQueue);
+}
+
+ze_result_t ofi_zeCommandQueueDestroy(ze_command_queue_handle_t hCommandQueue)
+{
+	return (*libze_ops.zeCommandQueueDestroy)(hCommandQueue);
+}
+
+ze_result_t ofi_zeCommandQueueExecuteCommandLists(
+				ze_command_queue_handle_t hCommandQueue,
+				uint32_t numCommandLists,
+				ze_command_list_handle_t *phCommandLists,
+				ze_fence_handle_t hFence)
+{
+	return (*libze_ops.zeCommandQueueExecuteCommandLists)(
+				hCommandQueue, numCommandLists, phCommandLists,
+				hFence);
+}
+
+ze_result_t ofi_zeCommandListCreate(ze_context_handle_t hContext,
+				    ze_device_handle_t hDevice,
+				    const ze_command_list_desc_t *desc,
+				    ze_command_list_handle_t *phCommandList)
+{
+	return (*libze_ops.zeCommandListCreate)(hContext, hDevice, desc,
+						phCommandList);
+}
+
+ze_result_t ofi_zeCommandListDestroy(ze_command_list_handle_t hCommandList)
+{
+	return (*libze_ops.zeCommandListDestroy)(hCommandList);
+}
+
+ze_result_t ofi_zeCommandListClose(ze_command_list_handle_t hCommandList)
+{
+	return (*libze_ops.zeCommandListClose)(hCommandList);
+}
+
+ze_result_t ofi_zeCommandListAppendMemoryCopy(
+				ze_command_list_handle_t hCommandList,
+				void *dstptr, const void *srcptr, size_t size,
+				ze_event_handle_t hSignalEvent,
+				uint32_t numWaitEvents,
+				ze_event_handle_t *phWaitEvents)
+{
+	return (*libze_ops.zeCommandListAppendMemoryCopy)(
+				hCommandList, dstptr, srcptr, size, hSignalEvent,
+				numWaitEvents, phWaitEvents);
+}
+
+ze_result_t ofi_zeMemGetAllocProperties(ze_context_handle_t hContext,
+					const void *ptr,
+					ze_memory_allocation_properties_t
+						*pMemAllocProperties,
+					ze_device_handle_t *phDevice)
+{
+	return (*libze_ops.zeMemGetAllocProperties)(
+					hContext, ptr, pMemAllocProperties,
+					phDevice);
+}
+
+ze_result_t ofi_zeMemGetAddressRange(ze_context_handle_t hContext,
+				     const void *ptr, void **pBase,
+				     size_t *pSize)
+{
+	return (*libze_ops.zeMemGetAddressRange)(hContext, ptr, pBase, pSize);
+}
+
+ze_result_t ofi_zeMemGetIpcHandle(ze_context_handle_t hContext, const void *ptr,
+				  ze_ipc_mem_handle_t *pIpcHandle)
+{
+	return (*libze_ops.zeMemGetIpcHandle)(hContext, ptr, pIpcHandle);
+}
+
+ze_result_t ofi_zeMemOpenIpcHandle(ze_context_handle_t hContext,
+				   ze_device_handle_t hDevice,
+				   ze_ipc_mem_handle_t handle,
+				   ze_ipc_memory_flags_t flags,
+				   void **pptr)
+{
+	return (*libze_ops.zeMemOpenIpcHandle)(hContext, hDevice, handle, flags,
+					       pptr);
+}
+
+ze_result_t ofi_zeMemCloseIpcHandle(ze_context_handle_t hContext,
+				    const void *ptr)
+{
+	return (*libze_ops.zeMemCloseIpcHandle)(hContext, ptr);
+}
+
 #if HAVE_DRM
 #include <drm/i915_drm.h>
 #include <sys/ioctl.h>
@@ -182,6 +400,150 @@ bool ze_hmem_p2p_enabled(void)
 
 #endif //HAVE_DRM
 
+static int ze_hmem_dl_init(void)
+{
+#ifdef ENABLE_ZE_DLOPEN
+	libze_handle = dlopen("libze_loader.so", RTLD_NOW);
+	if (!libze_handle) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Failed to dlopen libze_loader.so\n");
+		goto err_out;
+	}
+
+	libze_ops.zeInit = dlsym(libze_handle, "zeInit");
+	if (!libze_ops.zeInit) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeInit\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDriverGet = dlsym(libze_handle, "zeDriverGet");
+	if (!libze_ops.zeDriverGet) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeDriverGet\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceGet = dlsym(libze_handle, "zeDeviceGet");
+	if (!libze_ops.zeDeviceGet) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeDeviceGet\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeDeviceCanAccessPeer = dlsym(libze_handle, "zeDeviceCanAccessPeer");
+	if (!libze_ops.zeDeviceCanAccessPeer) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeDeviceCanAccessPeer\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextCreate = dlsym(libze_handle, "zeContextCreate");
+	if (!libze_ops.zeContextCreate) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeContextCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextDestroy = dlsym(libze_handle, "zeContextDestroy");
+	if (!libze_ops.zeContextDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeContextDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeContextDestroy = dlsym(libze_handle, "zeContextDestroy");
+	if (!libze_ops.zeContextDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeContextDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueCreate = dlsym(libze_handle, "zeCommandQueueCreate");
+	if (!libze_ops.zeCommandQueueCreate) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandQueueCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueDestroy = dlsym(libze_handle, "zeCommandQueueDestroy");
+	if (!libze_ops.zeCommandQueueDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandQueueDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandQueueExecuteCommandLists = dlsym(libze_handle, "zeCommandQueueExecuteCommandLists");
+	if (!libze_ops.zeCommandQueueExecuteCommandLists) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandQueueExecuteCommandLists\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListCreate = dlsym(libze_handle, "zeCommandListCreate");
+	if (!libze_ops.zeCommandListCreate) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListCreate\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListDestroy = dlsym(libze_handle, "zeCommandListDestroy");
+	if (!libze_ops.zeCommandListDestroy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListDestroy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListClose = dlsym(libze_handle, "zeCommandListClose");
+	if (!libze_ops.zeCommandListClose) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListClose\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeCommandListAppendMemoryCopy = dlsym(libze_handle, "zeCommandListAppendMemoryCopy");
+	if (!libze_ops.zeCommandListAppendMemoryCopy) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeCommandListAppendMemoryCopy\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemGetAllocProperties = dlsym(libze_handle, "zeMemGetAllocProperties");
+	if (!libze_ops.zeMemGetAllocProperties) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemGetAllocProperties\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemGetAddressRange = dlsym(libze_handle, "zeMemGetAddressRange");
+	if (!libze_ops.zeMemGetAddressRange) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemGetAddressRange\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemGetIpcHandle = dlsym(libze_handle, "zeMemGetIpcHandle");
+	if (!libze_ops.zeMemGetIpcHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemGetIpcHandle\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemOpenIpcHandle = dlsym(libze_handle, "zeMemOpenIpcHandle");
+	if (!libze_ops.zeMemOpenIpcHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemOpenIpcHandle\n");
+		goto err_dlclose;
+	}
+
+	libze_ops.zeMemCloseIpcHandle = dlsym(libze_handle, "zeMemCloseIpcHandle");
+	if (!libze_ops.zeMemCloseIpcHandle) {
+		FI_WARN(&core_prov, FI_LOG_CORE, "Failed to find zeMemCloseIpcHandle\n");
+		goto err_dlclose;
+	}
+
+	return FI_SUCCESS;
+
+err_dlclose:
+	dlclose(libze_handle);
+
+err_out:
+	return -FI_ENODATA;
+
+#else
+	return FI_SUCCESS;
+#endif /* ENABLE_ZE_DLOPEN */
+}
+
+static void ze_hmem_dl_cleanup(void)
+{
+#ifdef ENABLE_ZE_DLOPEN
+	dlclose(libze_handle);
+#endif
+}
+
 int ze_hmem_init(void)
 {
 	ze_driver_handle_t driver;
@@ -192,16 +554,20 @@ int ze_hmem_init(void)
 	bool p2p = true;
 	int ret;
 
-	ze_ret = zeInit(ZE_INIT_FLAG_GPU_ONLY);
+	ret = ze_hmem_dl_init();
+	if (ret)
+		return ret;
+
+	ze_ret = ofi_zeInit(ZE_INIT_FLAG_GPU_ONLY);
 	if (ze_ret)
 		return -FI_EIO;
 
 	count = 1;
-	ze_ret = zeDriverGet(&count, &driver);
+	ze_ret = ofi_zeDriverGet(&count, &driver);
 	if (ze_ret)
 		return -FI_EIO;
 
-	ze_ret = zeContextCreate(driver, &context_desc, &context);
+	ze_ret = ofi_zeContextCreate(driver, &context_desc, &context);
 	if (ze_ret)
 		return -FI_EIO;
 
@@ -209,11 +575,11 @@ int ze_hmem_init(void)
 		;
 
 	count = 0;
-	ze_ret = zeDeviceGet(driver, &count, NULL);
+	ze_ret = ofi_zeDeviceGet(driver, &count, NULL);
 	if (ze_ret || count > ZE_MAX_DEVICES)
 		goto err;
 
-	ze_ret = zeDeviceGet(driver, &count, devices);
+	ze_ret = ofi_zeDeviceGet(driver, &count, devices);
 	if (ze_ret)
 		goto err;
 
@@ -222,13 +588,15 @@ int ze_hmem_init(void)
 		goto err;
 
 	for (num_devices = 0; num_devices < count; num_devices++) {
-		ze_ret = zeCommandQueueCreate(context, devices[num_devices], &cq_desc,
-					      &cmd_queue[num_devices]);
+		ze_ret = ofi_zeCommandQueueCreate(context,
+						  devices[num_devices],
+						  &cq_desc,
+						  &cmd_queue[num_devices]);
 		if (ze_ret)
 			goto err;
 
 		for (i = 0; i < count; i++) {
-			if (zeDeviceCanAccessPeer(devices[num_devices],
+			if (ofi_zeDeviceCanAccessPeer(devices[num_devices],
 					devices[i], &access) || !access)
 				p2p = false;
 		}
@@ -250,7 +618,7 @@ int ze_hmem_cleanup(void)
 	int i, ret = FI_SUCCESS;
 
 	for (i = 0; i < num_devices; i++) {
-		if (cmd_queue[i] && zeCommandQueueDestroy(cmd_queue[i])) {
+		if (cmd_queue[i] && ofi_zeCommandQueueDestroy(cmd_queue[i])) {
 			FI_WARN(&core_prov, FI_LOG_CORE,
 				"Failed to destroy ZE cmd_queue\n");
 			ret = -FI_EINVAL;
@@ -261,9 +629,10 @@ int ze_hmem_cleanup(void)
 		}
 	}
 
-	if (zeContextDestroy(context))
-		return -FI_EINVAL;
+	if (ofi_zeContextDestroy(context))
+		ret = -FI_EINVAL;
 
+	ze_hmem_dl_cleanup();
 	return ret;
 }
 
@@ -273,23 +642,25 @@ int ze_hmem_copy(uint64_t device, void *dst, const void *src, size_t size)
 	ze_result_t ze_ret;
 	int dev_id = (int) device;
 
-	ze_ret = zeCommandListCreate(context, devices[dev_id], &cl_desc, &cmd_list);
+	ze_ret = ofi_zeCommandListCreate(context, devices[dev_id], &cl_desc,
+					 &cmd_list);
 	if (ze_ret)
 		goto err;
 
-	ze_ret = zeCommandListAppendMemoryCopy(cmd_list, dst, src, size, NULL, 0, NULL);
+	ze_ret = ofi_zeCommandListAppendMemoryCopy(cmd_list, dst, src, size,
+						   NULL, 0, NULL);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandListClose(cmd_list);
+	ze_ret = ofi_zeCommandListClose(cmd_list);
 	if (ze_ret)
 		goto free;
 
-	ze_ret = zeCommandQueueExecuteCommandLists(cmd_queue[dev_id], 1,
-						   &cmd_list, NULL);
+	ze_ret = ofi_zeCommandQueueExecuteCommandLists(cmd_queue[dev_id], 1,
+						       &cmd_list, NULL);
 
 free:
-	if (!zeCommandListDestroy(cmd_list) && !ze_ret)
+	if (!ofi_zeCommandListDestroy(cmd_list) && !ze_ret)
 		return FI_SUCCESS;
 err:
 	FI_WARN(&core_prov, FI_LOG_CORE,
@@ -301,25 +672,23 @@ err:
 bool ze_is_addr_valid(const void *addr)
 {
 	ze_result_t ze_ret;
-	ze_memory_allocation_properties_t mem_prop;
+	ze_memory_allocation_properties_t mem_props;
 	ze_device_handle_t device;
-	int i;
 
-	for (i = 0; i < num_devices; i++) {
-		ze_ret = zeMemGetAllocProperties(context, addr, &mem_prop,
-						 &device);
-		if (!ze_ret && mem_prop.type == ZE_MEMORY_TYPE_DEVICE)
-			return true;
-	}
-	return false;
+	mem_props.stype = ZE_STRUCTURE_TYPE_MEMORY_ALLOCATION_PROPERTIES;
+	mem_props.pNext = NULL;
+	ze_ret = ofi_zeMemGetAllocProperties(context, addr, &mem_props,
+					     &device);
+
+	return (!ze_ret && mem_props.type == ZE_MEMORY_TYPE_DEVICE);
 }
 
 int ze_hmem_get_handle(void *dev_buf, void **handle)
 {
 	ze_result_t ze_ret;
 
-	ze_ret = zeMemGetIpcHandle(context, dev_buf,
-				   (ze_ipc_mem_handle_t *) handle);
+	ze_ret = ofi_zeMemGetIpcHandle(context, dev_buf,
+				       (ze_ipc_mem_handle_t *) handle);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE, "Unable to get handle\n");
 		return -FI_EINVAL;
@@ -332,9 +701,9 @@ int ze_hmem_open_handle(void **handle, uint64_t device, void **ipc_ptr)
 {
 	ze_result_t ze_ret;
 
-	ze_ret = zeMemOpenIpcHandle(context, devices[device],
-				    *((ze_ipc_mem_handle_t *) handle),
-				    0, ipc_ptr);
+	ze_ret = ofi_zeMemOpenIpcHandle(context, devices[device],
+					*((ze_ipc_mem_handle_t *) handle),
+					0, ipc_ptr);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
 			"Unable to open memory handle\n");
@@ -348,7 +717,7 @@ int ze_hmem_close_handle(void *ipc_ptr)
 {
 	ze_result_t ze_ret;
 
-	ze_ret = zeMemCloseIpcHandle(context, ipc_ptr);
+	ze_ret = ofi_zeMemCloseIpcHandle(context, ipc_ptr);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
 			"Unable to close memory handle\n");
@@ -363,7 +732,7 @@ int ze_hmem_get_base_addr(const void *ptr, void **base)
 	ze_result_t ze_ret;
 	size_t size;
 
-	ze_ret = zeMemGetAddressRange(context, ptr, base, &size);
+	ze_ret = ofi_zeMemGetAddressRange(context, ptr, base, &size);
 	if (ze_ret) {
 		FI_WARN(&core_prov, FI_LOG_CORE,
 			"Could not get base addr\n");
@@ -372,6 +741,25 @@ int ze_hmem_get_base_addr(const void *ptr, void **base)
 	return FI_SUCCESS;
 }
 
+int ze_hmem_get_id(const void *ptr, uint64_t *id)
+{
+	ze_result_t ze_ret;
+	ze_memory_allocation_properties_t mem_props;
+	ze_device_handle_t device;
+
+	mem_props.stype = ZE_STRUCTURE_TYPE_MEMORY_ALLOCATION_PROPERTIES;
+	mem_props.pNext = NULL;
+	ze_ret = ofi_zeMemGetAllocProperties(context, ptr, &mem_props, &device);
+	if (ze_ret || mem_props.type == ZE_MEMORY_TYPE_UNKNOWN) {
+		FI_WARN(&core_prov, FI_LOG_CORE,
+			"Could not get memory id\n");
+		return -FI_EINVAL;
+	}
+
+	*id = mem_props.id;
+	return FI_SUCCESS;
+}
+
 int *ze_hmem_get_dev_fds(int *nfds)
 {
 	*nfds = num_devices;
@@ -437,6 +825,11 @@ int ze_hmem_get_base_addr(const void *ptr, void **base)
 	return -FI_ENOSYS;
 }
 
+int ze_hmem_get_id(const void *ptr, uint64_t *id)
+{
+	return -FI_ENOSYS;
+}
+
 int *ze_hmem_get_dev_fds(int *nfds)
 {
 	*nfds = 0;
diff --git a/util/pingpong.c b/util/pingpong.c
index f554189..173b71b 100644
--- a/util/pingpong.c
+++ b/util/pingpong.c
@@ -120,6 +120,7 @@ struct pp_opts {
 		__LINE__, ##__VA_ARGS__)
 
 int pp_debug;
+int pp_ipv6;
 
 #define PP_DEBUG(fmt, ...)                                                     \
 	do {                                                                   \
@@ -299,7 +300,7 @@ static int pp_getaddrinfo(char *name, uint16_t port, struct addrinfo **results)
 	char port_s[6];
 
 	struct addrinfo hints = {
-	    .ai_family = AF_INET,       /* IPv4 */
+	    .ai_family = pp_ipv6 ? AF_INET6 : AF_INET,
 	    .ai_socktype = SOCK_STREAM, /* TCP socket */
 	    .ai_protocol = IPPROTO_TCP, /* Any protocol */
 	    .ai_flags = AI_NUMERICSERV /* numeric port is used */
@@ -320,9 +321,22 @@ out:
 	return ret;
 }
 
+static void pp_print_addrinfo(struct addrinfo *ai, char *msg)
+{
+	char s[80] = {0};
+	void *addr;
+
+	if (ai->ai_family == AF_INET6)
+		addr = &((struct sockaddr_in6 *)ai->ai_addr)->sin6_addr;
+	else
+		addr = &((struct sockaddr_in *)ai->ai_addr)->sin_addr;
+
+	inet_ntop(ai->ai_family, addr, s, 80);
+	PP_DEBUG("%s %s\n", msg, s);
+}
+
 static int pp_ctrl_init_client(struct ct_pingpong *ct)
 {
-	struct sockaddr_in in_addr = {0};
 	struct addrinfo *results;
 	struct addrinfo *rp;
 	int errno_save = 0;
@@ -346,13 +360,27 @@ static int pp_ctrl_init_client(struct ct_pingpong *ct)
 		}
 
 		if (ct->opts.src_port != 0) {
-			in_addr.sin_family = AF_INET;
-			in_addr.sin_port = htons(ct->opts.src_port);
-			in_addr.sin_addr.s_addr = htonl(INADDR_ANY);
+			if (pp_ipv6) {
+				struct sockaddr_in6 in6_addr = {0};
+
+				in6_addr.sin6_family = AF_INET6;
+				in6_addr.sin6_port = htons(ct->opts.src_port);
+				in6_addr.sin6_addr = in6addr_any;
+
+				ret =
+				    bind(ct->ctrl_connfd, (struct sockaddr *)&in6_addr,
+					 sizeof(in6_addr));
+			} else {
+				struct sockaddr_in in_addr = {0};
+
+				in_addr.sin_family = AF_INET;
+				in_addr.sin_port = htons(ct->opts.src_port);
+				in_addr.sin_addr.s_addr = htonl(INADDR_ANY);
 
-			ret =
-			    bind(ct->ctrl_connfd, (struct sockaddr *)&in_addr,
-				 sizeof(in_addr));
+				ret =
+				    bind(ct->ctrl_connfd, (struct sockaddr *)&in_addr,
+					 sizeof(in_addr));
+			}
 			if (ret == -1) {
 				errno_save = ofi_sockerr();
 				ofi_close_socket(ct->ctrl_connfd);
@@ -360,6 +388,8 @@ static int pp_ctrl_init_client(struct ct_pingpong *ct)
 			}
 		}
 
+		pp_print_addrinfo(rp, "CLIENT: connecting to");
+
 		ret = connect(ct->ctrl_connfd, rp->ai_addr, rp->ai_addrlen);
 		if (ret != -1)
 			break;
@@ -383,12 +413,11 @@ static int pp_ctrl_init_client(struct ct_pingpong *ct)
 
 static int pp_ctrl_init_server(struct ct_pingpong *ct)
 {
-	struct sockaddr_in ctrl_addr = {0};
 	int optval = 1;
 	SOCKET listenfd;
 	int ret;
 
-	listenfd = ofi_socket(AF_INET, SOCK_STREAM, 0);
+	listenfd = ofi_socket(pp_ipv6 ? AF_INET6 : AF_INET, SOCK_STREAM, 0);
 	if (listenfd == INVALID_SOCKET) {
 		ret = -ofi_sockerr();
 		PP_PRINTERR("socket", ret);
@@ -403,12 +432,25 @@ static int pp_ctrl_init_server(struct ct_pingpong *ct)
 		goto fail_close_socket;
 	}
 
-	ctrl_addr.sin_family = AF_INET;
-	ctrl_addr.sin_port = htons(ct->opts.src_port);
-	ctrl_addr.sin_addr.s_addr = htonl(INADDR_ANY);
+	if (pp_ipv6) {
+		struct sockaddr_in6 ctrl6_addr = {0};
+
+		ctrl6_addr.sin6_family = AF_INET6;
+		ctrl6_addr.sin6_port = htons(ct->opts.src_port);
+		ctrl6_addr.sin6_addr = in6addr_any;
+
+		ret = bind(listenfd, (struct sockaddr *)&ctrl6_addr,
+			   sizeof(ctrl6_addr));
+	} else {
+		struct sockaddr_in ctrl_addr = {0};
+
+		ctrl_addr.sin_family = AF_INET;
+		ctrl_addr.sin_port = htons(ct->opts.src_port);
+		ctrl_addr.sin_addr.s_addr = htonl(INADDR_ANY);
 
-	ret = bind(listenfd, (struct sockaddr *)&ctrl_addr,
-		   sizeof(ctrl_addr));
+		ret = bind(listenfd, (struct sockaddr *)&ctrl_addr,
+			   sizeof(ctrl_addr));
+	}
 	if (ret == -1) {
 		ret = -ofi_sockerr();
 		PP_PRINTERR("bind", ret);
@@ -1970,6 +2012,7 @@ static void pp_pingpong_usage(struct ct_pingpong *ct, char *name, char *desc)
 
 	fprintf(stderr, " %-20s %s\n", "-h", "display this help output");
 	fprintf(stderr, " %-20s %s\n", "-v", "enable debugging output");
+	fprintf(stderr, " %-20s %s\n", "-6", "use IPv6 address");
 }
 
 static void pp_parse_opts(struct ct_pingpong *ct, int op, char *optarg)
@@ -2050,6 +2093,12 @@ static void pp_parse_opts(struct ct_pingpong *ct, int op, char *optarg)
 	case 'v':
 		pp_debug = 1;
 		break;
+
+	/* IPV6 */
+	case '6':
+		pp_ipv6 = 1;
+		break;
+
 	default:
 		/* let getopt handle unknown opts*/
 		break;
@@ -2153,7 +2202,9 @@ static int run_pingpong_dgram(struct ct_pingpong *ct)
 	/* Post an extra receive to avoid lacking a posted receive in the
 	 * finalize.
 	 */
-	ret = fi_recv(ct->ep, ct->rx_buf, ct->rx_size, fi_mr_desc(ct->mr), 0,
+	ret = fi_recv(ct->ep, ct->rx_buf,
+		      MAX(ct->rx_size, PP_MAX_CTRL_MSG) +  ct->rx_prefix_size,
+		      fi_mr_desc(ct->mr), 0,
 		      ct->rx_ctx_ptr);
 	if (ret)
 		return ret;
@@ -2243,7 +2294,7 @@ int main(int argc, char **argv)
 
 	ofi_osd_init();
 
-	while ((op = getopt(argc, argv, "hvd:p:e:I:S:B:P:cm:")) != -1) {
+	while ((op = getopt(argc, argv, "hvd:p:e:I:S:B:P:cm:6")) != -1) {
 		switch (op) {
 		default:
 			pp_parse_opts(&ct, op, optarg);
